import os
import json
import argparse
from datetime import datetime, timedelta
from dotenv import load_dotenv
import sys
from fix_md_headings import fix_markdown_headings
import logging

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from collectors.tavily_collector import TavilyCollector
from collectors.google_search_collector import GoogleSearchCollector
from collectors.brave_search_collector import BraveSearchCollector
from generators.report_generator import ReportGenerator
import config

# å…³é—­HTTPè¯·æ±‚æ—¥å¿—ï¼Œå‡å°‘å¹²æ‰°
logging.getLogger("httpx").setLevel(logging.WARNING)
logging.getLogger("urllib3").setLevel(logging.WARNING)

class IntelligentReportAgent:
    """æ™ºèƒ½æŠ¥å‘Šç”Ÿæˆä»£ç†ï¼Œå…·å¤‡æ€è€ƒå’Œåæ€èƒ½åŠ›"""
    
    def __init__(self):
        self.tavily_collector = TavilyCollector()
        self.llm_processor = self.tavily_collector._get_llm_processor()
        
        # åˆå§‹åŒ–å¤šä¸ªæœç´¢æ”¶é›†å™¨
        self.collectors = {
            'tavily': self.tavily_collector,
        }
        
        # å°è¯•åˆå§‹åŒ–Googleæœç´¢æ”¶é›†å™¨
        try:
            self.google_collector = GoogleSearchCollector()
            if self.google_collector.has_api_key:
                self.collectors['google'] = self.google_collector
                print("âœ… Googleæœç´¢æ”¶é›†å™¨å·²å¯ç”¨")
            else:
                print("âš ï¸ Googleæœç´¢æ”¶é›†å™¨æœªé…ç½®APIå¯†é’¥ï¼Œå·²è·³è¿‡")
                self.google_collector = None
        except Exception as e:
            print(f"âš ï¸ Googleæœç´¢æ”¶é›†å™¨ä¸å¯ç”¨: {str(e)}")
            self.google_collector = None
            
        # å°è¯•åˆå§‹åŒ–Braveæœç´¢æ”¶é›†å™¨
        try:
            self.brave_collector = BraveSearchCollector()
            if self.brave_collector.has_api_key:
                self.collectors['brave'] = self.brave_collector
                print("âœ… Braveæœç´¢æ”¶é›†å™¨å·²å¯ç”¨")
            else:
                print("âš ï¸ Braveæœç´¢æ”¶é›†å™¨æœªé…ç½®APIå¯†é’¥ï¼Œå·²è·³è¿‡")
                self.brave_collector = None
        except Exception as e:
            print(f"âš ï¸ Braveæœç´¢æ”¶é›†å™¨ä¸å¯ç”¨: {str(e)}")
            self.brave_collector = None
        
        self.max_iterations = 5  # æœ€å¤§è¿­ä»£æ¬¡æ•°
        self.knowledge_gaps = []  # çŸ¥è¯†ç¼ºå£è®°å½•
        self.search_history = []  # æœç´¢å†å²
        self.detailed_analysis_mode = True  # è¯¦ç»†åˆ†ææ¨¡å¼ï¼Œç”Ÿæˆæ›´é•¿æ›´æ·±å…¥çš„å†…å®¹
        
        print(f"ğŸ” å·²å¯ç”¨ {len(self.collectors)} ä¸ªæœç´¢æ¸ é“: {', '.join(self.collectors.keys())}")
    
    def multi_channel_search(self, query, max_results=5):
        """
        å¤šæ¸ é“æœç´¢æ–¹æ³•ï¼Œæ•´åˆå¤šä¸ªæœç´¢å¼•æ“çš„ç»“æœ
        """
        all_results = []
        used_urls = set()  # ç”¨äºå»é‡
        
        for name, collector in self.collectors.items():
            try:
                print(f"  ğŸ” {name}æœç´¢: {query[:50]}...")
                
                if name == 'tavily':
                    results = collector.search(query, max_results=max_results)
                elif name == 'google':
                    results = collector.search(query, max_results=max_results)
                elif name == 'brave':
                    results = collector.search(query, count=max_results)
                else:
                    continue
                    
                if results:
                    # å»é‡å¹¶æ·»åŠ æœç´¢æ¥æºæ ‡è¯†
                    for result in results:
                        url = result.get('url', '')
                        if url and url not in used_urls:
                            result['search_source'] = name
                            all_results.append(result)
                            used_urls.add(url)
                    
                    print(f"    âœ… {name}: è·å¾— {len(results)} æ¡ç»“æœ")
                else:
                    print(f"    âš ï¸ {name}: æ— ç»“æœ")
                    
            except Exception as e:
                print(f"    âŒ {name}æœç´¢å‡ºé”™: {str(e)}")
                continue
        
        print(f"  ğŸ“Š å¤šæ¸ é“æœç´¢å®Œæˆï¼Œå…±è·å¾— {len(all_results)} æ¡å»é‡ç»“æœ")
        return all_results
        
    def generate_initial_queries(self, topic, days=7, companies=None):
        """
        ç¬¬ä¸€æ­¥ï¼šæ™ºèƒ½æŸ¥è¯¢ç”Ÿæˆ
        åˆ†æç”¨æˆ·éœ€æ±‚ï¼Œç”Ÿæˆåˆå§‹æœç´¢ç­–ç•¥
        """
        print(f"\nğŸ§  [æ€è€ƒé˜¶æ®µ] æ­£åœ¨åˆ†æ'{topic}'è¡Œä¸šæŠ¥å‘Šéœ€æ±‚...")
        
        # è®¡ç®—æ—¶é—´èŒƒå›´
        from datetime import datetime, timedelta
        today = datetime.now()
        
        # æ„å»ºåˆå§‹æŸ¥è¯¢ç”Ÿæˆæç¤º
        query_prompt = f"""
        ä½œä¸ºä¸€ä¸ªä¸“ä¸šçš„è¡Œä¸šåˆ†æå¸ˆï¼Œæˆ‘éœ€è¦ä¸º'{topic}'è¡Œä¸šç”Ÿæˆä¸€ä»½å…¨é¢çš„æœ€æ–°åŠ¨æ€æŠ¥å‘Šã€‚
        è¯·å¸®æˆ‘åˆ†æè¿™ä¸ªä¸»é¢˜çš„æ·±åº¦å’Œå¹¿åº¦ï¼Œç„¶åç”Ÿæˆä¸€ç³»åˆ—åˆå§‹æœç´¢æŸ¥è¯¢ç­–ç•¥ã€‚
        
        âš ï¸ **é‡è¦æ—¶é—´è¦æ±‚**ï¼šæ‰€æœ‰æŸ¥è¯¢å¿…é¡»èšç„¦äº{today.year}å¹´æœ€æ–°ä¿¡æ¯ï¼Œç‰¹åˆ«æ˜¯æœ€è¿‘{days}å¤©çš„åŠ¨æ€ï¼
        
        åˆ†æè¦æ±‚ï¼š
        1. è¿™ä¸ªè¡Œä¸šçš„æ ¸å¿ƒå…³æ³¨ç‚¹æ˜¯ä»€ä¹ˆï¼Ÿ
        2. å½“å‰å¯èƒ½çš„çƒ­ç‚¹è¯é¢˜æœ‰å“ªäº›ï¼Ÿ
        3. éœ€è¦ä»å“ªäº›è§’åº¦æ¥å…¨é¢äº†è§£è¿™ä¸ªè¡Œä¸šï¼Ÿ
        4. ä»€ä¹ˆç±»å‹çš„ä¿¡æ¯å¯¹è¯»è€…æœ€æœ‰ä»·å€¼ï¼Ÿ
        
        è¯·ç”Ÿæˆä»¥ä¸‹ç±»å‹çš„æœç´¢æŸ¥è¯¢ï¼ˆæ¯ä¸ªæŸ¥è¯¢éƒ½è¦åŒ…å«æ—¶é—´é™åˆ¶ï¼‰ï¼š
        - é‡å¤§äº‹ä»¶ç±»ï¼ˆ4-6ä¸ªæŸ¥è¯¢ï¼‰- å¿…é¡»åŒ…å«"{today.year}å¹´æœ€æ–°"ã€"recent"ç­‰æ—¶é—´è¯
        - æŠ€æœ¯åˆ›æ–°ç±»ï¼ˆ3-4ä¸ªæŸ¥è¯¢ï¼‰- å¿…é¡»åŒ…å«"{today.year}å¹´æ–°æŠ€æœ¯"ã€"latest innovation"ç­‰
        - æŠ•èµ„åŠ¨æ€ç±»ï¼ˆ3-4ä¸ªæŸ¥è¯¢ï¼‰- å¿…é¡»åŒ…å«"{today.year}å¹´æŠ•èµ„"ã€"recent funding"ç­‰
        - æ”¿ç­–ç›‘ç®¡ç±»ï¼ˆ2-3ä¸ªæŸ¥è¯¢ï¼‰- å¿…é¡»åŒ…å«"{today.year}å¹´æ”¿ç­–"ã€"latest policy"ç­‰
        - è¡Œä¸šè¶‹åŠ¿ç±»ï¼ˆ3-4ä¸ªæŸ¥è¯¢ï¼‰- å¿…é¡»åŒ…å«"{today.year}å¹´è¶‹åŠ¿"ã€"current trends"ç­‰
        
        æ—¶é—´èŒƒå›´ï¼šæœ€è¿‘{days}å¤©ï¼Œé‡ç‚¹å…³æ³¨{today.strftime('%Yå¹´%mæœˆ')}
        {'é‡ç‚¹å…³æ³¨å…¬å¸ï¼š' + ', '.join(companies) if companies else ''}
        
        è¯·ç¡®ä¿æ¯ä¸ªæŸ¥è¯¢éƒ½åŒ…å«æ˜ç¡®çš„æ—¶é—´é™åˆ¶è¯æ±‡ï¼Œé¿å…æœç´¢åˆ°è¿‡æ—¶ä¿¡æ¯ã€‚
        """
        
        system_msg = f"ä½ æ˜¯ä¸€ä½ç»éªŒä¸°å¯Œçš„{topic}è¡Œä¸šç ”ç©¶ä¸“å®¶ï¼Œæ“…é•¿åˆ¶å®šå…¨é¢çš„ä¿¡æ¯æ”¶é›†ç­–ç•¥ã€‚"
        
        try:
            if not self.llm_processor:
                print("âš ï¸ [é™çº§æ¨¡å¼] LLMå¤„ç†å™¨ä¸å¯ç”¨ï¼Œä½¿ç”¨é»˜è®¤æœç´¢ç­–ç•¥")
                return self._get_fallback_queries(topic, days, companies)
                
            response = self.llm_processor.call_llm_api(query_prompt, system_msg, max_tokens=4000)
            # è§£ææŸ¥è¯¢ç­–ç•¥ï¼ˆè¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…å¯ä»¥è§£æJSONï¼‰
            print(f"âœ… [æŸ¥è¯¢ç­–ç•¥] å·²ç”Ÿæˆ{topic}è¡Œä¸šçš„å¤šç»´åº¦æœç´¢ç­–ç•¥")
            return self._parse_query_strategy(response, topic, days, companies)
        except Exception as e:
            print(f"âŒ [é”™è¯¯] ç”ŸæˆæŸ¥è¯¢ç­–ç•¥æ—¶å‡ºé”™: {str(e)}")
            print("ğŸ”„ [é™çº§æ¨¡å¼] åˆ‡æ¢åˆ°é»˜è®¤æœç´¢ç­–ç•¥")
            return self._get_fallback_queries(topic, days, companies)
    
    def _parse_query_strategy(self, response, topic, days, companies):
        """è§£ææŸ¥è¯¢ç­–ç•¥å“åº” - çœŸæ­£çš„å¤šæ¸ é“æ•´åˆæœç´¢"""
        print(f"ğŸ”„ [å¤šæ¸ é“æ•´åˆ] æ­£åœ¨æ•´åˆå¤šä¸ªæœç´¢å¼•æ“çš„ç»“æœ...")
        
        # åˆå§‹åŒ–åˆå¹¶åçš„æ•°æ®ç»“æ„
        merged_data = {
            "breaking_news": [],
            "innovation_news": [],
            "investment_news": [],
            "policy_news": [],
            "trend_news": [],
            "company_news": [],
            "total_count": 0
        }
        
        seen_urls = set()  # ç”¨äºå»é‡
        
        # 1. ä½¿ç”¨Braveæœç´¢
        if self.brave_collector:
            try:
                print("  ğŸ” æ‰§è¡ŒBraveç»¼åˆæœç´¢...")
                brave_data = self.brave_collector.get_comprehensive_research(topic, days)
                
                # å¯¹Braveæ•°æ®è¿›è¡Œæ—¶é—´è¿‡æ»¤
                for category in ["breaking_news", "innovation_news", "investment_news", "policy_news", "trend_news"]:
                    category_items = brave_data.get(category, [])
                    if category_items:
                        # åº”ç”¨æ—¶é—´è¿‡æ»¤
                        filtered_items = self.brave_collector._filter_by_date(category_items, days)
                        
                        # åˆå¹¶è¿‡æ»¤åçš„æ•°æ®
                        for item in filtered_items:
                            url = item.get('url', '')
                            if url and url not in seen_urls:
                                seen_urls.add(url)
                                merged_data[category].append(item)
                
                print(f"    âœ… Braveæœç´¢å®Œæˆï¼Œè·å¾— {brave_data.get('total_count', 0)} æ¡ç»“æœ")
            except Exception as e:
                print(f"    âŒ Braveæœç´¢å‡ºé”™: {str(e)}")
        
        # 2. ä½¿ç”¨Googleæœç´¢è¡¥å……
        if self.google_collector:
            try:
                print("  ğŸ” æ‰§è¡ŒGoogleè¡¥å……æœç´¢...")
                
                # ä¸ºæ¯ä¸ªç±»åˆ«æ‰§è¡ŒGoogleæœç´¢
                google_queries = {
                    "breaking_news": f"{topic} è¡Œä¸š é‡å¤§æ–°é—» çªå‘ é‡è¦äº‹ä»¶ {datetime.now().year}å¹´ æœ€æ–°",
                    "innovation_news": f"{topic} æŠ€æœ¯åˆ›æ–° æ–°äº§å“ æ–°æŠ€æœ¯ {datetime.now().year}å¹´ æœ€æ–°",
                    "investment_news": f"{topic} æŠ•èµ„ èèµ„ å¹¶è´­ {datetime.now().year}å¹´ æœ€æ–°",
                    "policy_news": f"{topic} æ”¿ç­– ç›‘ç®¡ æ³•è§„ {datetime.now().year}å¹´ æœ€æ–°",
                    "trend_news": f"{topic} è¶‹åŠ¿ å‘å±• å‰æ™¯ {datetime.now().year}å¹´ æœ€æ–°"
                }
                
                google_added_count = 0
                for category, query in google_queries.items():
                    try:
                        google_results = self.google_collector.search(query, days_back=days, max_results=5)
                        if google_results:
                            # åº”ç”¨æ—¶é—´è¿‡æ»¤
                            filtered_results = self.google_collector._filter_by_date(google_results, days)
                            
                            # åˆå¹¶è¿‡æ»¤åçš„æ•°æ®
                            for item in filtered_results:
                                url = item.get('url', '')
                                if url and url not in seen_urls:
                                    seen_urls.add(url)
                                    merged_data[category].append(item)
                                    google_added_count += 1
                    except Exception as e:
                        print(f"    âš ï¸ Googleæœç´¢ {category} å‡ºé”™: {str(e)}")
                        continue
                
                print(f"    âœ… Googleæœç´¢å®Œæˆï¼Œæ–°å¢ {google_added_count} æ¡å»é‡ç»“æœ")
            except Exception as e:
                print(f"    âŒ Googleæœç´¢å‡ºé”™: {str(e)}")
        
        # 3. ä½¿ç”¨Tavilyæœç´¢è¡¥å……
        try:
            print("  ğŸ” æ‰§è¡ŒTavilyè¡¥å……æœç´¢...")
            tavily_data = self.tavily_collector.get_industry_news_direct(topic, days)
            
            # åˆå¹¶Tavilyæ•°æ®ï¼Œé¿å…é‡å¤ï¼ˆTavilyå·²ç»åœ¨å†…éƒ¨è¿›è¡Œäº†æ—¶é—´è¿‡æ»¤ï¼‰
            added_count = 0
            for category in ["breaking_news", "innovation_news", "investment_news", "policy_news", "trend_news"]:
                for item in tavily_data.get(category, []):
                    url = item.get('url', '')
                    if url and url not in seen_urls:
                        seen_urls.add(url)
                        merged_data[category].append(item)
                        added_count += 1
            
            print(f"    âœ… Tavilyæœç´¢å®Œæˆï¼Œæ–°å¢ {added_count} æ¡å»é‡ç»“æœ")
        except Exception as e:
            print(f"    âŒ Tavilyæœç´¢å‡ºé”™: {str(e)}")
        
        # 4. è®¡ç®—æ€»æ•°
        merged_data["total_count"] = sum(
            len(merged_data[key]) for key in merged_data.keys() 
            if key != "total_count"
        )
        
        print(f"ğŸ“Š [æ•´åˆå®Œæˆ] å¤šæ¸ é“æœç´¢æ€»è®¡è·å¾— {merged_data['total_count']} æ¡å»é‡ä¸”æ—¶é—´è¿‡æ»¤åçš„ç»“æœ")
        return merged_data
    
    def _get_fallback_queries(self, topic, days, companies):
        """å¤‡ç”¨æŸ¥è¯¢ç­–ç•¥ï¼Œå½“LLMä¸å¯ç”¨æ—¶ä½¿ç”¨"""
        print(f"ğŸ›¡ï¸ [å¤‡ç”¨ç­–ç•¥] ä½¿ç”¨é¢„è®¾çš„{topic}è¡Œä¸šæœç´¢ç­–ç•¥")
        
        # ä½¿ç”¨ç°æœ‰çš„æœç´¢æ–¹æ³•ï¼Œä½†ç¡®ä¿æ—¶é—´èŒƒå›´æ­£ç¡®
        try:
            return self.tavily_collector.get_industry_news_direct(topic, days)
        except Exception as e:
            print(f"âŒ [é”™è¯¯] å¤‡ç”¨æŸ¥è¯¢ç­–ç•¥ä¹Ÿå¤±è´¥: {str(e)}")
            # è¿”å›ç©ºæ•°æ®ç»“æ„ï¼Œé¿å…ç¨‹åºå´©æºƒ
            return {
                "breaking_news": [],
                "innovation_news": [],
                "investment_news": [],
                "policy_news": [],
                "trend_news": [],
                "company_news": [],
                "total_count": 0
            }
    
    def reflect_on_information_gaps(self, collected_data, topic, days=7):
        """
        ç¬¬ä¸‰æ­¥ï¼šåæ€ä¸çŸ¥è¯†ç¼ºå£åˆ†æ  
        åˆ†æå·²æ”¶é›†ä¿¡æ¯çš„å®Œæ•´æ€§å’Œè´¨é‡
        """
        print(f"\nğŸ¤” [åæ€é˜¶æ®µ] æ­£åœ¨åˆ†æ{topic}è¡Œä¸šä¿¡æ¯çš„å®Œæ•´æ€§...")
        
        # ç»Ÿè®¡å·²æ”¶é›†çš„ä¿¡æ¯
        info_stats = {
            'breaking_news': len(collected_data.get('breaking_news', [])),
            'innovation_news': len(collected_data.get('innovation_news', [])),
            'investment_news': len(collected_data.get('investment_news', [])),
            'policy_news': len(collected_data.get('policy_news', [])),
            'trend_news': len(collected_data.get('trend_news', [])),
            'company_news': len(collected_data.get('company_news', []))
        }
        
        # è®¡ç®—æ€»ä¿¡æ¯æ¡æ•°
        total_items = sum(info_stats.values())
        
        # åˆ†æä¿¡æ¯ç¼ºå£ - æ›´ä¸¥æ ¼çš„æ ‡å‡†
        reflection_prompt = f"""
        ä½œä¸º{topic}è¡Œä¸šçš„èµ„æ·±åˆ†æå¸ˆï¼Œè¯·å¯¹ä»¥ä¸‹ä¿¡æ¯æ”¶é›†æƒ…å†µè¿›è¡Œä¸¥æ ¼è¯„ä¼°ï¼š
        
        ğŸ“Š **ä¿¡æ¯ç»Ÿè®¡**ï¼š
        - é‡å¤§äº‹ä»¶: {info_stats['breaking_news']}æ¡
        - æŠ€æœ¯åˆ›æ–°: {info_stats['innovation_news']}æ¡  
        - æŠ•èµ„åŠ¨æ€: {info_stats['investment_news']}æ¡
        - æ”¿ç­–ç›‘ç®¡: {info_stats['policy_news']}æ¡
        - è¡Œä¸šè¶‹åŠ¿: {info_stats['trend_news']}æ¡
        - å…¬å¸åŠ¨æ€: {info_stats['company_news']}æ¡
        - **æ€»è®¡**: {total_items}æ¡
        
        ğŸ¯ **ä¸¥æ ¼è¯„ä¼°æ ‡å‡†**ï¼š
        è¯·æŒ‰ä»¥ä¸‹ä¸¥æ ¼æ ‡å‡†è¿›è¡Œè¯„ä¼°ï¼Œåªæœ‰**åŒæ—¶æ»¡è¶³**æ‰€æœ‰æ¡ä»¶æ‰èƒ½è®¤ä¸ºä¿¡æ¯å……åˆ†ï¼š
        
        âœ… **æ•°é‡è¦æ±‚**ï¼š
        - æ¯ä¸ªç±»åˆ«è‡³å°‘5æ¡é«˜è´¨é‡ä¿¡æ¯
        - æ€»ä¿¡æ¯é‡ä¸å°‘äº40æ¡
        - å„ç±»åˆ«ä¿¡æ¯åˆ†å¸ƒç›¸å¯¹å‡åŒ€
        
        âœ… **è´¨é‡è¦æ±‚**ï¼š
        - ä¿¡æ¯æ¥æºæƒå¨å¯é 
        - æ—¶æ•ˆæ€§å¼ºï¼ˆæœ€è¿‘7å¤©å†…çš„ä¿¡æ¯æ¯”ä¾‹è¶³å¤Ÿï¼‰
        - æ¶µç›–è¡Œä¸šæ ¸å¿ƒå‘å±•åŠ¨æ€
        
        âœ… **å®Œæ•´æ€§è¦æ±‚**ï¼š
        - æŠ€æœ¯ã€å¸‚åœºã€æ”¿ç­–ã€æŠ•èµ„å››å¤§ç»´åº¦ä¿¡æ¯é½å…¨
        - åŒ…å«ä¸åŒè§„æ¨¡ä¼ä¸šçš„åŠ¨æ€
        - æ¶µç›–äº§ä¸šé“¾ä¸Šä¸‹æ¸¸ä¿¡æ¯
        
        âœ… **å¹³è¡¡æ€§è¦æ±‚**ï¼š
        - åŒ…å«æ­£é¢å’Œè´Ÿé¢è§‚ç‚¹
        - æ¶µç›–ä¸åŒç«‹åœºçš„å£°éŸ³ï¼ˆä¼ä¸šã€ç›‘ç®¡ã€å­¦æœ¯ã€æŠ•èµ„è€…ï¼‰
        - åŒ…å«å›½é™…å’Œå›½å†…è§†è§’
        - å­˜åœ¨äº‰è®®æ€§è¯é¢˜çš„å¤šå…ƒåŒ–è§‚ç‚¹
        
        **è¯·ä¸¥æ ¼æŒ‰ç…§ä¸Šè¿°æ ‡å‡†è¯„ä¼°**ï¼š
        
        å¦‚æœ**ä»»ä½•ä¸€ä¸ª**æ–¹é¢ä¸æ»¡è¶³è¦æ±‚ï¼Œè¯·æ˜ç¡®æŒ‡å‡ºï¼š
        1. å…·ä½“ç¼ºå£æ˜¯ä»€ä¹ˆï¼Ÿ
        2. ä¸ºä»€ä¹ˆè¿™ä¸ªç¼ºå£å¾ˆé‡è¦ï¼Ÿ
        3. éœ€è¦æœç´¢ä»€ä¹ˆç±»å‹çš„ä¿¡æ¯ï¼Ÿ
        4. å»ºè®®çš„å…·ä½“æœç´¢ç­–ç•¥ï¼Ÿ
        
        âš ï¸ **é‡è¦**ï¼šåªæœ‰åœ¨**å®Œå…¨æ»¡è¶³**æ‰€æœ‰è¯„ä¼°æ ‡å‡†æ—¶ï¼Œæ‰èƒ½è¯´"ä¿¡æ¯æ”¶é›†å……åˆ†ï¼Œå¯ä»¥å¼€å§‹æŠ¥å‘Šç”Ÿæˆ"ã€‚
        å¦åˆ™ï¼Œè¯·è¯¦ç»†è¯´æ˜éœ€è¦è¡¥å……çš„å…·ä½“ä¿¡æ¯ç¼ºå£ã€‚
        """
        
        system_msg = f"ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„{topic}è¡Œä¸šåˆ†æå¸ˆï¼Œå…·æœ‰æ•é”çš„ä¿¡æ¯å®Œæ•´æ€§åˆ¤æ–­èƒ½åŠ›ã€‚"
        
        try:
            if not self.llm_processor:
                print("âš ï¸ [é™çº§æ¨¡å¼] LLMä¸å¯ç”¨ï¼Œè·³è¿‡åæ€åˆ†æ")
                return [], True  # å‡è®¾ä¿¡æ¯å……åˆ†ï¼Œç›´æ¥ç”ŸæˆæŠ¥å‘Š
                
            reflection_result = self.llm_processor.call_llm_api(reflection_prompt, system_msg, max_tokens=6000)
            
            # æ›´åˆç†çš„è§£æåæ€ç»“æœ - è°ƒæ•´æ ‡å‡†ä½¿å…¶æ›´å®ç”¨
            has_sufficient_text = "ä¿¡æ¯æ”¶é›†å……åˆ†" in reflection_result or "å¯ä»¥å¼€å§‹" in reflection_result
            
            # åŠ¨æ€è°ƒæ•´æ•°é‡æ ‡å‡†ï¼ŒåŸºäºå®é™…æœç´¢éš¾åº¦
            min_total = max(15, days * 2)  # åŸºäºå¤©æ•°åŠ¨æ€è°ƒæ•´ï¼Œæœ€å°‘15æ¡
            min_per_category = max(2, days // 4)  # æ¯ç±»åˆ«æœ€å°‘æ¡æ•°ä¹ŸåŠ¨æ€è°ƒæ•´
            
            has_sufficient_quantity = (
                total_items >= min_total and  # åŠ¨æ€æ€»é‡è¦æ±‚
                sum(1 for count in info_stats.values() if count >= min_per_category) >= 3  # è‡³å°‘3ä¸ªç±»åˆ«æœ‰è¶³å¤Ÿæ•°æ®
            )
            
            # ç‰¹æ®Šæƒ…å†µï¼šå¦‚æœé‡å¤§äº‹ä»¶ç±»åˆ«æœ‰è¶³å¤Ÿæ•°æ®ï¼Œå¯ä»¥é€‚å½“æ”¾å®½å…¶ä»–è¦æ±‚
            has_major_events = info_stats.get('breaking_news', 0) >= 3
            
            print(f"ğŸ“Š [é‡åŒ–æ£€æŸ¥] æ€»é‡:{total_items}/{min_total}, å„ç±»åˆ«æœ€å°‘:{min(info_stats.values())}/{min_per_category}")
            print(f"ğŸ“Š [ç±»åˆ«åˆ†å¸ƒ] é‡å¤§äº‹ä»¶:{info_stats.get('breaking_news', 0)}, åˆ›æ–°:{info_stats.get('innovation_news', 0)}, æŠ•èµ„:{info_stats.get('investment_news', 0)}")
            
            if has_sufficient_text and (has_sufficient_quantity or has_major_events):
                print("âœ… [åæ€ç»“æœ] ä¿¡æ¯æ”¶é›†å……åˆ†ï¼Œå‡†å¤‡ç”ŸæˆæŠ¥å‘Š")
                return [], True
            else:
                if not has_sufficient_quantity and not has_major_events:
                    print(f"âš ï¸ [æ•°é‡ä¸è¶³] éœ€è¦è¡¥å……æœç´¢ - æ€»é‡:{total_items}/{min_total}")
                else:
                    print("âš ï¸ [è´¨é‡ä¸è¶³] AIåˆ†æè®¤ä¸ºéœ€è¦è¡¥å……æœç´¢")
                print("ğŸ”„ [ç»§ç»­è¿­ä»£] å‘ç°ä¿¡æ¯ç¼ºå£ï¼Œè¿›å…¥ä¸‹è½®è¡¥å……æœç´¢")
                return [reflection_result], False
                
        except Exception as e:
            print(f"âŒ [é”™è¯¯] åæ€åˆ†ææ—¶å‡ºé”™: {str(e)}")
            print("ğŸ”„ [é™çº§æ¨¡å¼] å‡è®¾ä¿¡æ¯å……åˆ†ï¼Œç»§ç»­ç”ŸæˆæŠ¥å‘Š")
            return [], True  # å‡ºé”™æ—¶å‡è®¾ä¿¡æ¯å……åˆ†
    
    def generate_targeted_queries(self, gaps, topic, days=7):
        """
        ç¬¬å››æ­¥ï¼šè¿­ä»£ä¼˜åŒ–æœç´¢
        æ ¹æ®çŸ¥è¯†ç¼ºå£ç”Ÿæˆé’ˆå¯¹æ€§æœç´¢
        """
        print(f"\nğŸ¯ [ä¼˜åŒ–æœç´¢] æ­£åœ¨ä¸º{topic}è¡Œä¸šç”Ÿæˆé’ˆå¯¹æ€§æŸ¥è¯¢...")
        
        if not gaps:
            return {}
            
        # è®¡ç®—å½“å‰æ—¶é—´å’Œæœç´¢èŒƒå›´
        from datetime import datetime, timedelta
        today = datetime.now()
        start_date = today - timedelta(days=days)  # ä½¿ç”¨ä¼ å…¥çš„å¤©æ•°å‚æ•°
        
        targeted_prompt = f"""
        åŸºäºä»¥ä¸‹{topic}è¡Œä¸šæŠ¥å‘Šçš„çŸ¥è¯†ç¼ºå£åˆ†æï¼š
        
        {gaps[0]}
        
        è¯·åˆ†æå…·ä½“çš„ä¿¡æ¯ç¼ºå£ï¼Œå¹¶ç”Ÿæˆ3-5ä¸ªé’ˆå¯¹æ€§çš„æœç´¢æŸ¥è¯¢æ¥è¡¥å……è¿™äº›ç¼ºå£ã€‚
        
        âš ï¸ **é‡è¦æ—¶é—´è¦æ±‚**ï¼šæŸ¥è¯¢å¿…é¡»åŒ…å«æœ€æ–°æ—¶é—´é™åˆ¶ï¼Œè·å–{today.strftime('%Yå¹´%mæœˆ')}çš„æœ€æ–°ä¿¡æ¯ï¼
        
        ğŸ¯ **è§‚ç‚¹å¯¹æ¯”æœç´¢ç­–ç•¥**ï¼š
        - å¦‚æœç¼ºå£åˆ†ææåˆ°éœ€è¦"è§‚ç‚¹å¯¹æ¯”åˆ†æ"ï¼Œè¯·ä¸“é—¨è®¾è®¡1-2ä¸ªæŸ¥è¯¢æ¥è·å–ä¸åŒè§‚ç‚¹
        - æœç´¢å…³é”®è¯åŒ…æ‹¬ï¼šäº‰è®®ã€è´¨ç–‘ã€æ‰¹è¯„ã€åå¯¹ã€é£é™©ã€æŒ‘æˆ˜ã€ä¸åŒè§‚ç‚¹ã€alternative view
        - å¹³è¡¡æ­£é¢å’Œè´Ÿé¢ä¿¡æ¯ï¼Œç¡®ä¿å®¢è§‚æ€§
        
        è¾“å‡ºæ ¼å¼ï¼š
        ç¼ºå£1: [ç¼ºå£æè¿°]
        æŸ¥è¯¢: "[å…·ä½“æœç´¢è¯] {today.strftime('%Yå¹´%mæœˆ')} æœ€æ–°"
        
        ç¼ºå£2: [ç¼ºå£æè¿°] 
        æŸ¥è¯¢: "[å…·ä½“æœç´¢è¯] {today.year} latest news"
        
        è§‚ç‚¹å¯¹æ¯”: [å¦‚éœ€è¦ï¼Œæè¿°è§‚ç‚¹ç¼ºå£]
        å¯¹æ¯”æŸ¥è¯¢: "[è¡Œä¸šå…³é”®è¯] äº‰è®® è´¨ç–‘ é£é™© {today.year}å¹´ ä¸åŒè§‚ç‚¹"
        
        æŸ¥è¯¢è¦æ±‚ï¼š
        1. å…·ä½“ä¸”æœ‰é’ˆå¯¹æ€§
        2. å¿…é¡»åŒ…å«æ—¶é—´é™åˆ¶è¯ï¼š{today.year}å¹´ã€latestã€recentã€æœ€æ–°ã€æœ€è¿‘
        3. è¦†ç›–è¯†åˆ«å‡ºçš„ä¸»è¦ç¼ºå£
        4. é€‚åˆæœç´¢å¼•æ“æŸ¥è¯¢
        5. åŒ…å«è¡Œä¸šå…³é”®è¯ï¼š{topic}
        6. æ¯ä¸ªæŸ¥è¯¢éƒ½è¦åŒ…å«æ—¶é—´ç›¸å…³è¯æ±‡ç¡®ä¿è·å–æœ€æ–°ä¿¡æ¯
        7. å¦‚éœ€è§‚ç‚¹å¯¹æ¯”ï¼Œä¸“é—¨æœç´¢è´¨ç–‘ã€æ‰¹è¯„ã€ä¸åŒç«‹åœºçš„å£°éŸ³
        """
        
        try:
            if not self.llm_processor:
                print("âš ï¸ [é™çº§æ¨¡å¼] LLMä¸å¯ç”¨ï¼Œä½¿ç”¨é¢„è®¾æŸ¥è¯¢è¡¥å……æœç´¢")
                return self._fallback_targeted_search(topic, days)
                
            response = self.llm_processor.call_llm_api(targeted_prompt, f"ä½ æ˜¯{topic}è¡Œä¸šçš„æœç´¢ä¸“å®¶")
            
            # è§£ææŸ¥è¯¢
            queries = self._parse_targeted_queries(response)
            
            if queries:
                print(f"ğŸ” [æ‰§è¡Œæœç´¢] å¼€å§‹æ‰§è¡Œ{len(queries)}ä¸ªé’ˆå¯¹æ€§æŸ¥è¯¢...")
                
                # å®é™…æ‰§è¡Œæœç´¢
                additional_data = {}
                for i, query in enumerate(queries, 1):
                    print(f"  ğŸ“Š æŸ¥è¯¢ {i}/{len(queries)}: {query[:50]}...")
                    
                    # ä½¿ç”¨å¤šæ¸ é“æœç´¢
                    try:
                        search_results = self.multi_channel_search(query, max_results=5)
                        if search_results:
                            # å°†ç»“æœæŒ‰ç±»å‹åˆ†ç±»
                            category = self._categorize_search_result(query, topic)
                            if category not in additional_data:
                                additional_data[category] = []
                            additional_data[category].extend(search_results)
                            print(f"    âœ… æ‰¾åˆ° {len(search_results)} æ¡ç›¸å…³ä¿¡æ¯")
                        else:
                            print(f"    âš ï¸ æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯")
                    except Exception as e:
                        print(f"    âŒ æœç´¢å‡ºé”™: {str(e)}")
                
                print(f"âœ… [æœç´¢å®Œæˆ] è·å¾—é¢å¤–æ•°æ®: {sum(len(v) for v in additional_data.values())} æ¡")
                return additional_data
            else:
                print("âš ï¸ [è­¦å‘Š] æœªèƒ½è§£æå‡ºæœ‰æ•ˆçš„æœç´¢æŸ¥è¯¢")
                return self._fallback_targeted_search(topic, days)
            
        except Exception as e:
            print(f"âŒ [é”™è¯¯] ç”Ÿæˆé’ˆå¯¹æ€§æŸ¥è¯¢æ—¶å‡ºé”™: {str(e)}")
            print("ğŸ”„ [é™çº§æ¨¡å¼] ä½¿ç”¨é¢„è®¾æŸ¥è¯¢")
            return self._fallback_targeted_search(topic, days)
    
    def _parse_targeted_queries(self, response):
        """è§£æAIç”Ÿæˆçš„æŸ¥è¯¢å“åº”"""
        queries = []
        lines = response.split('\n')
        
        for line in lines:
            line = line.strip()
            if 'æŸ¥è¯¢:' in line or 'Query:' in line:
                # æå–å¼•å·å†…çš„å†…å®¹
                if '"' in line:
                    start = line.find('"') + 1
                    end = line.rfind('"')
                    if start > 0 and end > start:
                        query = line[start:end].strip()
                        if query:
                            queries.append(query)
                # å¦‚æœæ²¡æœ‰å¼•å·ï¼Œæå–å†’å·åçš„å†…å®¹
                elif ':' in line:
                    query = line.split(':', 1)[1].strip()
                    if query:
                        queries.append(query)
        
        # å¦‚æœè§£æå¤±è´¥ï¼Œå°è¯•ç®€å•åˆ†å‰²
        if not queries:
            for line in lines:
                line = line.strip()
                if line and not line.startswith('ç¼ºå£') and not line.startswith('Gap'):
                    if len(line) > 5 and len(line) < 100:  # åˆç†çš„æŸ¥è¯¢é•¿åº¦
                        queries.append(line)
        
        return queries[:5]  # æœ€å¤šè¿”å›5ä¸ªæŸ¥è¯¢
    
    def _fallback_targeted_search(self, topic, days=7):
        """å½“LLMä¸å¯ç”¨æ—¶çš„å¤‡ç”¨æœç´¢ç­–ç•¥ - å¢å¼ºç‰ˆï¼Œä¸“æ³¨é‡å¤§äº‹ä»¶"""
        from datetime import datetime, timedelta
        today = datetime.now()
        
        # é¢„è®¾çš„è¡¥å……æœç´¢æŸ¥è¯¢ï¼ˆåŒ…å«æ—¶é—´é™åˆ¶ï¼‰- ä¸“é—¨é’ˆå¯¹é‡å¤§äº‹ä»¶ä¼˜åŒ–
        end_date = today
        start_date = today - timedelta(days=days)
        
        # é‡å¤§äº‹ä»¶ä¸“ç”¨æŸ¥è¯¢
        major_event_queries = [
            f"{topic} é‡å¤§äº‹ä»¶ çªå‘ {today.year}å¹´ æœ€æ–° breaking news major event",
            f"{topic} è¡Œä¸šéœ‡åŠ¨ é‡ç£…æ¶ˆæ¯ {today.year} æœ€è¿‘{days}å¤© industry shock major news",
            f"{topic} å¹¶è´­ æ”¶è´­ åˆå¹¶ {today.year}å¹´ æœ€æ–° merger acquisition latest",
            f"{topic} é‡å¤§å‘å¸ƒ äº§å“å‘å¸ƒ {today.year} æœ€æ–° major launch product release",
            f"{topic} ç›‘ç®¡ æ”¿ç­–å˜åŒ– {today.year}å¹´ æœ€æ–° regulation policy change latest"
        ]
        
        # é€šç”¨è¡¥å……æŸ¥è¯¢
        general_queries = [
            f"{topic} {today.year}å¹´æœ€æ–°å‘å±• recent developments latest {days} days",
            f"{topic} industry news {today.strftime('%B %Y')} latest recent {days} days",
            f"{topic} è¡Œä¸šåŠ¨æ€ {today.year} æ–°é—» trends æœ€è¿‘{days}å¤©",
            f"{topic} äº‰è®® è´¨ç–‘ é£é™© æŒ‘æˆ˜ {today.year}å¹´ ä¸åŒè§‚ç‚¹ criticism"
        ]
        
        # åˆå¹¶æ‰€æœ‰æŸ¥è¯¢ï¼Œé‡å¤§äº‹ä»¶æŸ¥è¯¢ä¼˜å…ˆ
        all_queries = major_event_queries + general_queries
        
        print(f"ğŸ” [å¤‡ç”¨æœç´¢] æ‰§è¡Œ{len(all_queries)}ä¸ªé¢„è®¾æŸ¥è¯¢ï¼ˆé‡å¤§äº‹ä»¶ä¼˜å…ˆï¼‰...")
        additional_data = {}
        
        for i, query in enumerate(all_queries, 1):
            print(f"  ğŸ“Š é¢„è®¾æŸ¥è¯¢ {i}/{len(all_queries)}: {query[:50]}...")
            try:
                # å¯¹é‡å¤§äº‹ä»¶æŸ¥è¯¢ä½¿ç”¨æ›´å¤šç»“æœ
                max_results = 5 if i <= len(major_event_queries) else 3
                search_results = self.multi_channel_search(query, max_results=max_results)
                if search_results:
                    # é‡å¤§äº‹ä»¶æŸ¥è¯¢çš„ç»“æœä¼˜å…ˆåˆ†ç±»ä¸ºbreaking_news
                    if i <= len(major_event_queries):
                        category = 'breaking_news'
                    else:
                        category = self._categorize_search_result(query, topic)
                    
                    if category not in additional_data:
                        additional_data[category] = []
                    additional_data[category].extend(search_results)
                    print(f"    âœ… æ‰¾åˆ° {len(search_results)} æ¡ç›¸å…³ä¿¡æ¯ -> {category}")
                else:
                    print(f"    âš ï¸ æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯")
            except Exception as e:
                print(f"    âŒ æœç´¢å‡ºé”™: {str(e)}")
        
        # ç»Ÿè®¡ç»“æœ
        total_found = sum(len(v) for v in additional_data.values())
        breaking_found = len(additional_data.get('breaking_news', []))
        print(f"âœ… [å¤‡ç”¨æœç´¢å®Œæˆ] æ€»è®¡æ‰¾åˆ° {total_found} æ¡ä¿¡æ¯ï¼Œå…¶ä¸­é‡å¤§äº‹ä»¶ {breaking_found} æ¡")
        
        return additional_data
    
    def _calculate_text_similarity(self, text1, text2):
        """è®¡ç®—ä¸¤ä¸ªæ–‡æœ¬çš„ç›¸ä¼¼åº¦ï¼ˆç®€å•ç‰ˆæœ¬ï¼‰"""
        if not text1 or not text2:
            return 0.0
        
        # è½¬æ¢ä¸ºå°å†™å¹¶åˆ†è¯
        words1 = set(text1.lower().split())
        words2 = set(text2.lower().split())
        
        # è®¡ç®—äº¤é›†å’Œå¹¶é›†
        intersection = words1.intersection(words2)
        union = words1.union(words2)
        
        # è®¡ç®—Jaccardç›¸ä¼¼åº¦
        if len(union) == 0:
            return 0.0
        
        return len(intersection) / len(union)
    
    def _is_content_similar(self, item1, item2, similarity_threshold=0.6):
        """åˆ¤æ–­ä¸¤ä¸ªæ–°é—»é¡¹ç›®æ˜¯å¦å†…å®¹ç›¸ä¼¼"""
        # æ¯”è¾ƒæ ‡é¢˜ç›¸ä¼¼åº¦
        title1 = item1.get('title', '')
        title2 = item2.get('title', '')
        title_similarity = self._calculate_text_similarity(title1, title2)
        
        # æ¯”è¾ƒå†…å®¹ç›¸ä¼¼åº¦
        content1 = item1.get('content', '')[:200]  # åªæ¯”è¾ƒå‰200å­—ç¬¦
        content2 = item2.get('content', '')[:200]
        content_similarity = self._calculate_text_similarity(content1, content2)
        
        # å¦‚æœæ ‡é¢˜ç›¸ä¼¼åº¦å¾ˆé«˜ï¼Œè®¤ä¸ºæ˜¯é‡å¤å†…å®¹
        if title_similarity > 0.7:
            return True
        
        # å¦‚æœæ ‡é¢˜å’Œå†…å®¹éƒ½æœ‰ä¸€å®šç›¸ä¼¼åº¦ï¼Œè®¤ä¸ºæ˜¯é‡å¤å†…å®¹
        if title_similarity > 0.4 and content_similarity > similarity_threshold:
            return True
        
        return False
    
    def _deduplicate_by_content(self, items, category_name=""):
        """åŸºäºå†…å®¹ç›¸ä¼¼åº¦å»é‡"""
        if not items:
            return items
        
        print(f"ğŸ” [æ™ºèƒ½å»é‡] æ­£åœ¨å¯¹{category_name}ç±»åˆ«çš„{len(items)}æ¡æ•°æ®è¿›è¡Œå†…å®¹å»é‡...")
        
        deduplicated = []
        removed_count = 0
        
        for item in items:
            is_duplicate = False
            
            # æ£€æŸ¥æ˜¯å¦ä¸å·²æœ‰é¡¹ç›®ç›¸ä¼¼
            for existing_item in deduplicated:
                if self._is_content_similar(item, existing_item):
                    is_duplicate = True
                    removed_count += 1
                    print(f"  âš ï¸ å‘ç°é‡å¤å†…å®¹: {item.get('title', 'æ— æ ‡é¢˜')[:50]}...")
                    break
            
            if not is_duplicate:
                deduplicated.append(item)
        
        if removed_count > 0:
            print(f"  âœ… {category_name}: å»é™¤{removed_count}æ¡é‡å¤å†…å®¹ï¼Œä¿ç•™{len(deduplicated)}æ¡")
        else:
            print(f"  âœ… {category_name}: æ— é‡å¤å†…å®¹")
        
        return deduplicated
    
    def _merge_data(self, existing_data, new_data):
        """åˆå¹¶æ–°æ•°æ®åˆ°ç°æœ‰æ•°æ®ä¸­ï¼Œåªè¿›è¡ŒURLå»é‡"""
        print(f"ğŸ”„ [æ•°æ®åˆå¹¶] æ­£åœ¨åˆå¹¶æ–°æ•°æ®...")
        
        # åˆ›å»ºåˆå¹¶åçš„æ•°æ®å‰¯æœ¬
        merged_data = existing_data.copy()
        
        for category, new_items in new_data.items():
            if category not in merged_data:
                merged_data[category] = []
            
            # è·å–ç°æœ‰æ•°æ®çš„URLé›†åˆï¼Œç”¨äºåŸºç¡€å»é‡
            existing_urls = set()
            for item in merged_data[category]:
                if item.get('url'):
                    existing_urls.add(item['url'])
            
            # åŸºäºURLçš„åŸºç¡€å»é‡
            url_filtered_items = []
            for new_item in new_items:
                if new_item.get('url') and new_item['url'] not in existing_urls:
                    url_filtered_items.append(new_item)
                    existing_urls.add(new_item['url'])
                elif not new_item.get('url'):  # å¦‚æœæ²¡æœ‰URLï¼ŒåŸºäºæ ‡é¢˜å»é‡
                    title = new_item.get('title', '')
                    existing_titles = [item.get('title', '') for item in merged_data[category]]
                    if title and title not in existing_titles:
                        url_filtered_items.append(new_item)
            
            # ç›´æ¥æ·»åŠ å»é‡åçš„æ–°æ•°æ®
            if url_filtered_items:
                merged_data[category].extend(url_filtered_items)
                print(f"  âœ… {category}: æ–°å¢ {len(url_filtered_items)} æ¡æ•°æ®")
            else:
                print(f"  â– {category}: æ— æ–°å¢æ•°æ®ï¼ˆURLé‡å¤ï¼‰")
        
        # ç»Ÿè®¡åˆå¹¶åçš„æ•°æ®ï¼ˆåªç»Ÿè®¡åˆ—è¡¨ç±»å‹çš„æ•°æ®ï¼‰
        total_before = sum(len(v) for v in existing_data.values() if isinstance(v, list))
        total_after = sum(len(v) for v in merged_data.values() if isinstance(v, list))
        print(f"ğŸ“Š [åˆå¹¶ç»“æœ] æ•°æ®æ€»é‡: {total_before} â†’ {total_after} (+{total_after - total_before})")
        
        return merged_data
    
    def _categorize_search_result(self, query, topic):
        """æ ¹æ®æŸ¥è¯¢å†…å®¹å°†æœç´¢ç»“æœåˆ†ç±»"""
        query_lower = query.lower()
        
        # ä¼˜å…ˆæ£€æŸ¥æ˜¯å¦ä¸ºè§‚ç‚¹å¯¹æ¯”ç±»
        if any(word in query_lower for word in ['äº‰è®®', 'è´¨ç–‘', 'æ‰¹è¯„', 'åå¯¹', 'é£é™©', 'æŒ‘æˆ˜', 'ä¸åŒè§‚ç‚¹', 'alternative', 'criticism', 'controversy', 'challenge', 'risk']):
            return 'perspective_analysis'
        elif any(word in query_lower for word in ['æŠ•èµ„', 'èèµ„', 'investment', 'funding', 'ä¼°å€¼', 'valuation']):
            return 'investment_news'
        elif any(word in query_lower for word in ['æ”¿ç­–', 'policy', 'ç›‘ç®¡', 'regulation', 'æ³•è§„', 'law']):
            return 'policy_news'
        elif any(word in query_lower for word in ['æŠ€æœ¯', 'technology', 'åˆ›æ–°', 'innovation', 'äº§å“', 'product']):
            return 'innovation_news'
        elif any(word in query_lower for word in ['è¶‹åŠ¿', 'trend', 'å‘å±•', 'development', 'æœªæ¥', 'future']):
            return 'trend_news'
        else:
            return 'breaking_news'  # é»˜è®¤åˆ†ç±»
    
    def generate_comprehensive_report_with_thinking(self, topic, days=7, companies=None):
        """
        æ™ºèƒ½æŠ¥å‘Šç”Ÿæˆä¸»æµç¨‹ï¼ŒåŒ…å«å®Œæ•´çš„æ€è€ƒè¿‡ç¨‹
        """
        print(f"\nğŸš€ å¼€å§‹ä¸º'{topic}'è¡Œä¸šç”Ÿæˆæ™ºèƒ½åˆ†ææŠ¥å‘Š...")
        print("=" * 60)
        
        # ç¬¬ä¸€æ­¥ï¼šæ™ºèƒ½æŸ¥è¯¢ç”Ÿæˆ
        initial_data = self.generate_initial_queries(topic, days, companies)
        
        # ç¬¬äºŒæ­¥ï¼šä¿¡æ¯æ”¶é›†ï¼ˆä½¿ç”¨ç°æœ‰æ–¹æ³•ï¼‰
        print(f"\nğŸ“Š [ä¿¡æ¯æ”¶é›†] æ­£åœ¨æ”¶é›†{topic}è¡Œä¸šå¤šç»´åº¦ä¿¡æ¯...")
        all_news_data = initial_data
        
        # å¦‚æœæœ‰å…¬å¸åˆ—è¡¨ï¼Œè¡¥å……å…¬å¸ç‰¹å®šä¿¡æ¯
        if companies and isinstance(companies, list):
            print(f"ğŸ¢ [å…¬å¸åˆ†æ] æ­£åœ¨æ”¶é›†{len(companies)}å®¶é‡ç‚¹å…¬å¸ä¿¡æ¯...")
            company_specifics = []
            for company in companies:
                # å°è¯•ä½¿ç”¨å¤šä¸ªæ”¶é›†å™¨è·å–å…¬å¸æ–°é—»
                company_news = []
                
                # æš‚æ—¶æ³¨é‡Šæ‰Googleæœç´¢
                if self.google_collector:
                    try:
                        google_news = self.google_collector.search(f"{company} {topic} æ–°é—» news {days}å¤© latest")
                        if google_news:
                            company_news.extend(google_news[:3])
                    except Exception as e:
                        print(f"  âš ï¸ Googleæœç´¢{company}å¤±è´¥: {str(e)}")
                
                # å°è¯•ä½¿ç”¨Braveæœç´¢
                if self.brave_collector and len(company_news) < 3:
                    try:
                        brave_news = self.brave_collector.search(f"{company} {topic} åŠ¨æ€ latest news", count=3-len(company_news))
                        if brave_news:
                            company_news.extend(brave_news)
                    except Exception as e:
                        print(f"  âš ï¸ Braveæœç´¢{company}å¤±è´¥: {str(e)}")
                
                # æœ€åä½¿ç”¨Tavilyæœç´¢
                if len(company_news) < 3:
                    try:
                        tavily_news = self.tavily_collector.get_company_news(company, topic, days, max_results=3-len(company_news))
                        if tavily_news:
                            company_news.extend(tavily_news)
                    except Exception as e:
                        print(f"  âš ï¸ Tavilyæœç´¢{company}å¤±è´¥: {str(e)}")
                        
                if company_news:
                    company_specifics.append({"company": company, "news": company_news})
            
            if company_specifics:
                all_news_data["company_news"] = [item for company_data in company_specifics for item in company_data["news"]]
        
        iteration_count = 0
        
        # ç¬¬ä¸‰æ­¥ï¼šåæ€ä¸è¿­ä»£
        while iteration_count < self.max_iterations:
            iteration_count += 1
            print(f"\nğŸ”„ [è¿­ä»£è½®æ¬¡ {iteration_count}/{self.max_iterations}] - å½“å‰æ•°æ®æ€»é‡: {all_news_data.get('total_count', 0)}æ¡")
            
            # åæ€åˆ†æ
            gaps, is_sufficient = self.reflect_on_information_gaps(all_news_data, topic, days)
            
            if is_sufficient:
                print("âœ… [å†³ç­–] ä¿¡æ¯æ”¶é›†å®Œæˆï¼Œå¼€å§‹ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š")
                break
            
            if iteration_count < self.max_iterations:
                print(f"ğŸ¯ [ç¬¬{iteration_count}è½®æœç´¢] å¼€å§‹è¡¥å……ä¿¡æ¯ç¼ºå£...")
                # ç”Ÿæˆé’ˆå¯¹æ€§æŸ¥è¯¢å¹¶æ‰§è¡Œæœç´¢
                additional_data = self.generate_targeted_queries(gaps, topic, days)
                
                # åˆå¹¶æ–°æ•°æ®
                if additional_data:
                    old_total = all_news_data.get('total_count', 0)
                    all_news_data = self._merge_data(all_news_data, additional_data)
                    new_total = all_news_data.get('total_count', 0)
                    print(f"ğŸ“ˆ [æ•°æ®æ›´æ–°] ç¬¬{iteration_count}è½®æ–°å¢ {new_total - old_total} æ¡æ•°æ®ï¼Œæ€»é‡: {new_total}")
                else:
                    print(f"ğŸ“Š [ç¬¬{iteration_count}è½®ç»“æœ] æœ¬è½®æœªè·å¾—æ–°æ•°æ®ï¼Œå¯èƒ½éœ€è¦è°ƒæ•´æœç´¢ç­–ç•¥")
            else:
                print(f"âš ï¸ [è¾¾åˆ°ä¸Šé™] å·²å®Œæˆ{self.max_iterations}è½®è¿­ä»£ï¼Œå¼€å§‹ç”ŸæˆæŠ¥å‘Š")
                
        # ç¬¬äº”æ­¥ï¼šç»¼åˆæŠ¥å‘Šç”Ÿæˆ
        print(f"\nğŸ“ [æŠ¥å‘Šç”Ÿæˆ] æ­£åœ¨ç»¼åˆåˆ†æç”Ÿæˆ{topic}è¡Œä¸šæŠ¥å‘Š...")
        return self._generate_final_report(topic, all_news_data, companies, days)
    
    def _generate_final_report(self, topic, all_news_data, companies, days=7):
        """ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Šï¼Œä½¿ç”¨åŸæœ‰çš„æŠ¥å‘Šç”Ÿæˆé€»è¾‘"""
        
        # åˆå§‹åŒ–æŠ¥å‘Šå†…å®¹
        content = f"# {topic}è¡Œä¸šæ™ºèƒ½åˆ†ææŠ¥å‘Š\n\n"
        # content += f"*æœ¬æŠ¥å‘Šç”±AIæ™ºèƒ½ä»£ç†ç”Ÿæˆï¼Œå…·å¤‡æ·±åº¦æ€è€ƒå’Œåæ€èƒ½åŠ›*\n\n"
        date_str = datetime.now().strftime('%Y-%m-%d')
        content += f"æŠ¥å‘Šæ—¥æœŸ: {date_str}\n\n"
        
        # æ·»åŠ æŠ¥å‘Šæ¦‚è¿°
        content += f"""## ğŸ“‹ æŠ¥å‘Šæ¦‚è¿°

æœ¬æŠ¥å‘Šé‡‡ç”¨AIæ™ºèƒ½ä»£ç†çš„äº”æ­¥åˆ†ææ³•ï¼Œå¯¹{topic}è¡Œä¸šè¿›è¡Œå…¨æ–¹ä½æ·±åº¦è§£æã€‚é€šè¿‡æ™ºèƒ½æŸ¥è¯¢ç”Ÿæˆã€
å¤šç»´ä¿¡æ¯æœé›†ã€åæ€å¼ç¼ºå£åˆ†æã€è¿­ä»£ä¼˜åŒ–æœç´¢å’Œç»¼åˆæŠ¥å‘Šç”Ÿæˆï¼Œç¡®ä¿ä¿¡æ¯çš„å…¨é¢æ€§å’Œåˆ†æçš„æ·±åº¦ã€‚

**æŠ¥å‘Šç‰¹è‰²ï¼š**
- ğŸ§  æ·±åº¦æ€è€ƒï¼šæ¨¡æ‹Ÿä¸“å®¶çº§åˆ†æå¸ˆçš„æ€ç»´è¿‡ç¨‹
- ğŸ”„ å¤šè½®è¿­ä»£ï¼šé€šè¿‡åæ€æœºåˆ¶ç¡®ä¿ä¿¡æ¯å……åˆ†æ€§
- ğŸ¯ é’ˆå¯¹æ€§å¼ºï¼šæ ¹æ®è¯†åˆ«çš„çŸ¥è¯†ç¼ºå£è¿›è¡Œè¡¥å……æœç´¢
- ğŸ“Š æ•°æ®ä¸°å¯Œï¼šæ•´åˆå¤šæºä¿¡æ¯ï¼Œæä¾›å…¨é¢è§†è§’
- ğŸ”® å‰ç»æ€§å¼ºï¼šä¸ä»…åˆ†æç°çŠ¶ï¼Œæ›´é¢„æµ‹æœªæ¥è¶‹åŠ¿

---

"""
        
        # ä½¿ç”¨åŸæœ‰çš„å¤„ç†å‡½æ•°ç”Ÿæˆå„éƒ¨åˆ†å†…å®¹
        content += self._process_breaking_news_enhanced(topic, all_news_data.get("breaking_news", []), days)
        content += self._process_innovation_news_enhanced(topic, all_news_data.get("innovation_news", []))  
        content += self._process_investment_news_enhanced(topic, all_news_data.get("investment_news", []))
        content += self._process_policy_news_enhanced(topic, all_news_data.get("policy_news", []))
        content += self._process_industry_trends_enhanced(topic, all_news_data.get("trend_news", []), days)
        
        # æ–°å¢ï¼šè§‚ç‚¹å¯¹æ¯”åˆ†æéƒ¨åˆ†
        if all_news_data.get("perspective_analysis"):
            content += self._process_perspective_analysis_enhanced(topic, all_news_data.get("perspective_analysis", []))
        
        # å…¬å¸åŠ¨æ€éƒ¨åˆ†
        if companies and all_news_data.get("company_news"):
            content += "## é‡ç‚¹å…¬å¸åŠ¨æ€åˆ†æ\n\n"
            # è¿™é‡Œå¯ä»¥æ·»åŠ å…¬å¸åˆ†æé€»è¾‘
        
        # æ™ºèƒ½æ€»ç»“
        content += self._generate_intelligent_summary(topic, all_news_data, days)
        
        # å‚è€ƒèµ„æ–™
        content += self._generate_references(all_news_data)
        
        return {
            "content": content,
            "data": all_news_data,
            "date": date_str
        }
    
    def _process_breaking_news_enhanced(self, topic, breaking_news, days=7):
        """å¢å¼ºç‰ˆé‡å¤§æ–°é—»å¤„ç†ï¼ŒåŒ…å«æ€è€ƒè¿‡ç¨‹"""
        if not breaking_news:
            return f"## è¡Œä¸šé‡å¤§äº‹ä»¶\n\nğŸ“Š **åˆ†æè¯´æ˜**: åœ¨å½“å‰æ—¶é—´çª—å£å†…ï¼Œæš‚æœªå‘ç°{topic}è¡Œä¸šçš„é‡å¤§çªå‘äº‹ä»¶ã€‚\n\n"
        
        print(f"ğŸ” [æ·±åº¦åˆ†æ] æ­£åœ¨åˆ†æ{len(breaking_news)}æ¡é‡å¤§äº‹ä»¶...")
        
        # ä½¿ç”¨å¢å¼ºçš„åˆ†ææç¤º
        all_news_text = "\n\n".join([
            f"æ ‡é¢˜: {item.get('title', 'æ— æ ‡é¢˜')}\nå†…å®¹: {item.get('content', 'æ— å†…å®¹')[:500]}...\næ¥æº: {item.get('source', 'æœªçŸ¥')}"
            for item in breaking_news
        ])
        
        # é¦–å…ˆç”Ÿæˆé‡å¤§äº‹ä»¶æ‘˜è¦ï¼ˆä¸¥æ ¼æŒ‰ç…§dayså‚æ•°è¿‡æ»¤æ—¶é—´ï¼‰
        summary_section = self._generate_major_events_summary(topic, breaking_news, days)
        
        enhanced_prompt = f"""
        ä½œä¸º{topic}è¡Œä¸šçš„é¦–å¸­åˆ†æå¸ˆï¼Œè¯·å¯¹ä»¥ä¸‹é‡å¤§äº‹ä»¶è¿›è¡Œæ·±åº¦åˆ†æï¼š
        
        {all_news_text}
        
        åˆ†ææ¡†æ¶ï¼š
        1. **äº‹ä»¶é‡è¦æ€§è¯„ä¼°**: æŒ‰å½±å“ç¨‹åº¦å¯¹äº‹ä»¶è¿›è¡Œæ’åºå’Œåˆ†ç±»
        2. **å¤šç»´åº¦å½±å“åˆ†æ**: åˆ†æå¯¹æŠ€æœ¯ã€å¸‚åœºã€æ”¿ç­–ã€ç«äº‰æ ¼å±€çš„å½±å“
        3. **å…³è”æ€§åˆ†æ**: è¯†åˆ«äº‹ä»¶ä¹‹é—´çš„å†…åœ¨è”ç³»å’Œå› æœå…³ç³»
        4. **è¶‹åŠ¿æŒ‡å‘æ€§**: è¿™äº›äº‹ä»¶åæ˜ äº†ä»€ä¹ˆè¶‹åŠ¿ä¿¡å·ï¼Ÿ
        5. **é£é™©ä¸æœºé‡**: ä¸ºè¡Œä¸šå‚ä¸è€…å¸¦æ¥çš„æœºé‡å’ŒæŒ‘æˆ˜
        
        ğŸ¤” **åˆ†æå¸ˆæ€è€ƒè¿‡ç¨‹**:
        - é¦–å…ˆæ¢³ç†äº‹ä»¶çš„æ—¶é—´çº¿å’Œé€»è¾‘å…³ç³»
        - ç„¶åè¯„ä¼°æ¯ä¸ªäº‹ä»¶çš„çŸ­æœŸå’Œé•¿æœŸå½±å“
        - æœ€åç»¼åˆåˆ¤æ–­å¯¹è¡Œä¸šå‘å±•çš„æŒ‡å‘æ„ä¹‰
        
                 è¯·ä¿æŒåˆ†æçš„å®¢è§‚æ€§å’Œå‰ç»æ€§ï¼Œè¦æ±‚æ·±åº¦åˆ†æï¼Œå­—æ•°æ§åˆ¶åœ¨2000-2500å­—ã€‚
         
         ğŸ“ **æ·±åº¦åˆ†æè¦æ±‚**:
         - æ¯ä¸ªé‡å¤§äº‹ä»¶éƒ½è¦ä»å¤šä¸ªè§’åº¦æ·±å…¥å‰–æ
         - æä¾›è¯¦ç»†çš„èƒŒæ™¯ä¿¡æ¯å’Œå‘å±•è„‰ç»œ
         - åˆ†æäº‹ä»¶å¯¹äº§ä¸šé“¾å„ç¯èŠ‚çš„å…·ä½“å½±å“
         - è¯„ä¼°çŸ­æœŸã€ä¸­æœŸã€é•¿æœŸçš„å½±å“ç¨‹åº¦
         - è¯†åˆ«äº‹ä»¶èƒŒåçš„æ·±å±‚æ¬¡åŸå› å’Œè§„å¾‹
         - æä¾›å…·ä½“çš„åº”å¯¹ç­–ç•¥å’Œå‘å±•å»ºè®®
         - é¢„æµ‹åç»­å¯èƒ½çš„è¿é”ååº”å’Œå‘å±•è¶‹åŠ¿
        """
        
        system_msg = f"""ä½ æ˜¯{topic}è¡Œä¸šçš„èµ„æ·±é¦–å¸­åˆ†æå¸ˆï¼Œå…·å¤‡ï¼š
        1. æ•é”çš„è¡Œä¸šæ´å¯ŸåŠ›
        2. ç³»ç»Ÿæ€§çš„åˆ†ææ€ç»´
        3. å‰ç»æ€§çš„åˆ¤æ–­èƒ½åŠ›
        4. å®¢è§‚ç†æ€§çš„åˆ†ææ€åº¦
        è¯·å±•ç°å‡ºä¸“ä¸šåˆ†æå¸ˆçš„æ€è€ƒæ·±åº¦ã€‚"""
        
        try:
            analysis = self.llm_processor.call_llm_api(enhanced_prompt, system_msg, max_tokens=8000)
            
            # æ·»åŠ åˆ†ææ¥æº
            sources = []
            for item in breaking_news:
                if item.get('url', '#') != '#':
                    sources.append(f"- [{item.get('title', 'æœªçŸ¥æ ‡é¢˜')}]({item.get('url')}) - {item.get('source', 'æœªçŸ¥æ¥æº')}")
            
            if sources:
                analysis += "\n\n**ä¿¡æ¯æ¥æº:**\n" + "\n".join(sources)
            
            return f"## ğŸš¨ è¡Œä¸šé‡å¤§äº‹ä»¶æ·±åº¦åˆ†æ\n\n{summary_section}\n\n### ğŸ“Š ç»¼åˆåˆ†æä¸å½±å“è¯„ä¼°\n\n{analysis}\n\n"
            
        except Exception as e:
            print(f"âŒ [é”™è¯¯] ç”Ÿæˆé‡å¤§äº‹ä»¶åˆ†ææ—¶å‡ºé”™: {str(e)}")
            return f"## è¡Œä¸šé‡å¤§äº‹ä»¶\n\n{summary_section}\n\n"
    
    def _generate_major_events_summary(self, topic, breaking_news, days=7):
        """ç”Ÿæˆè¡Œä¸šé‡å¤§äº‹ä»¶æ‘˜è¦ä¸å…³é”®ç»†èŠ‚éƒ¨åˆ†ï¼ˆ5-7ä¸ªé‡ç‚¹äº‹ä»¶ï¼‰"""
        if not breaking_news:
            return f"### ä¸€ã€é‡å¤§æ–°é—»æ‘˜è¦ä¸å…³é”®ç»†èŠ‚\n\nğŸ“Š **äº‹ä»¶æ¦‚è§ˆ**: åœ¨æœ€è¿‘{days}å¤©å†…æš‚æ— é‡å¤§äº‹ä»¶ã€‚"
        
        print(f"ğŸ“‹ [äº‹ä»¶æ‘˜è¦] æ­£åœ¨ç­›é€‰å’Œæ€»ç»“æœ€é‡è¦çš„{min(7, len(breaking_news))}ä¸ªé‡å¤§äº‹ä»¶...")
        
        # å‡†å¤‡æ‰€æœ‰æ–°é—»æ•°æ®ï¼Œå¹¶è¿›è¡Œæ—¶é—´è¿‡æ»¤éªŒè¯
        from datetime import datetime, timedelta
        today = datetime.now()
        
        # ä¸¥æ ¼çš„æ—¶é—´è¿‡æ»¤é€»è¾‘
        time_filtered_news = []
        current_year = today.year
        cutoff_date = today - timedelta(days=days)
        
        for item in breaking_news:
            should_include = False
            title = item.get('title', '')
            content = item.get('content', '')
            source = item.get('source', '')
            news_date = item.get('date', '') or item.get('published_date', '')
            
            # æ£€æŸ¥æ˜¯å¦åŒ…å«æ˜æ˜¾çš„æ—§å¹´ä»½æ ‡è¯†
            text_content = f"{title} {content} {source}".lower()
            old_year_patterns = ['2024å¹´', '2023å¹´', '2022å¹´', '2021å¹´', '2020å¹´']
            has_old_year = any(pattern in text_content for pattern in old_year_patterns)
            
            if has_old_year:
                print(f"âš ï¸ [æ—¶é—´è¿‡æ»¤] è·³è¿‡åŒ…å«æ—§å¹´ä»½çš„æ–°é—»: {title[:50]}")
                continue
            
            # æ£€æŸ¥æ˜¯å¦åŒ…å«å½“å‰å¹´ä»½æˆ–æœ€æ–°æ—¶é—´è¯æ±‡
            current_time_patterns = [
                f'{current_year}å¹´', f'{current_year}', 'latest', 'recent', 
                'æœ€æ–°', 'æœ€è¿‘', 'breaking', 'åˆšåˆš', 'ä»Šæ—¥', 'ä»Šå¤©',
                today.strftime('%Yå¹´%mæœˆ'), today.strftime('%mæœˆ')
            ]
            
            has_recent_indicators = any(pattern in text_content for pattern in current_time_patterns)
            
            # å¦‚æœæœ‰æ˜ç¡®çš„å‘å¸ƒæ—¥æœŸï¼Œæ£€æŸ¥æ˜¯å¦åœ¨æ—¶é—´èŒƒå›´å†…
            if news_date and news_date != "æœªçŸ¥æ—¥æœŸ":
                try:
                    # å°è¯•è§£ææ—¥æœŸ
                    parsed_date = None
                    try:
                        from dateutil import parser
                        parsed_date = parser.parse(str(news_date))
                    except ImportError:
                        # å¦‚æœæ²¡æœ‰dateutilï¼Œä½¿ç”¨åŸºç¡€è§£æ
                        if isinstance(news_date, str):
                            # å°è¯•åŸºç¡€çš„ISOæ—¥æœŸæ ¼å¼
                            for fmt in ['%Y-%m-%d', '%Y-%m-%d %H:%M:%S', '%Y/%m/%d']:
                                try:
                                    parsed_date = datetime.strptime(news_date, fmt)
                                    break
                                except ValueError:
                                    continue
                    
                    if parsed_date and parsed_date >= cutoff_date:
                        should_include = True
                        print(f"âœ… [æ—¶é—´éªŒè¯] æ—¥æœŸç¬¦åˆè¦æ±‚: {title[:30]} ({parsed_date.strftime('%Y-%m-%d')})")
                    elif parsed_date:
                        print(f"âš ï¸ [æ—¶é—´è¿‡æ»¤] æ—¥æœŸè¿‡æ—©: {title[:30]} ({parsed_date.strftime('%Y-%m-%d')})")
                        continue
                    else:
                        raise ValueError("æ— æ³•è§£ææ—¥æœŸ")
                except:
                    # æ—¥æœŸè§£æå¤±è´¥ï¼Œä½¿ç”¨å…³é”®è¯åˆ¤æ–­
                    if has_recent_indicators:
                        should_include = True
                        print(f"âœ… [å…³é”®è¯éªŒè¯] åŒ…å«æœ€æ–°æ—¶é—´è¯æ±‡: {title[:30]}")
                    else:
                        print(f"âš ï¸ [æ—¶é—´è¿‡æ»¤] æ— æ³•ç¡®å®šæ—¶é—´ä¸”ç¼ºä¹æœ€æ–°æ ‡è¯†: {title[:30]}")
                        continue
            else:
                # æ²¡æœ‰å‘å¸ƒæ—¥æœŸï¼Œå®Œå…¨ä¾é å†…å®¹å…³é”®è¯
                if has_recent_indicators:
                    should_include = True
                    print(f"âœ… [å…³é”®è¯éªŒè¯] åŒ…å«æœ€æ–°æ—¶é—´è¯æ±‡: {title[:30]}")
                else:
                    print(f"âš ï¸ [æ—¶é—´è¿‡æ»¤] ç¼ºä¹æ—¶é—´ä¿¡æ¯å’Œæœ€æ–°æ ‡è¯†: {title[:30]}")
                    continue
            
            if should_include:
                time_filtered_news.append(item)
        
        if not time_filtered_news:
            return f"### ä¸€ã€é‡å¤§æ–°é—»æ‘˜è¦ä¸å…³é”®ç»†èŠ‚\n\nâš ï¸ **æ—¶é—´è¿‡æ»¤ç»“æœ**: åœ¨æœ€è¿‘{days}å¤©å†…æš‚æ— ç¬¦åˆè¦æ±‚çš„é‡å¤§äº‹ä»¶ï¼Œå¯èƒ½éœ€è¦æ”¾å®½æ—¶é—´èŒƒå›´æˆ–æ£€æŸ¥æ•°æ®æºã€‚"
        
        # é€‰æ‹©æœ€é‡è¦çš„5-7ä¸ªäº‹ä»¶
        selected_news = time_filtered_news[:min(7, len(time_filtered_news))]
        
        all_news_text = "\n\n".join([
            f"äº‹ä»¶{i+1}:\næ ‡é¢˜: {item.get('title', 'æ— æ ‡é¢˜')}\næ—¶é—´: {item.get('date', 'æœ€è¿‘')}\nå†…å®¹: {item.get('content', 'æ— å†…å®¹')[:400]}...\næ¥æº: {item.get('source', 'æœªçŸ¥æ¥æº')}\nç½‘å€: {item.get('url', '#')}"
            for i, item in enumerate(selected_news)
        ])
        
        summary_prompt = f"""
        ä½œä¸º{topic}è¡Œä¸šçš„èµ„æ·±åˆ†æå¸ˆï¼Œè¯·å¯¹ä»¥ä¸‹æœ€æ–°é‡å¤§äº‹ä»¶è¿›è¡Œæ™ºèƒ½ç­›é€‰ã€å»é‡å’Œæ•´ç†ã€‚

        æœ€æ–°äº‹ä»¶ä¿¡æ¯ï¼š
        {all_news_text}

        ğŸ” **æ™ºèƒ½ç­›é€‰ä»»åŠ¡**ï¼š
        1. **å»é‡è¯†åˆ«**ï¼šä»”ç»†åˆ†ææ‰€æœ‰äº‹ä»¶ï¼Œè¯†åˆ«å†…å®¹ç›¸ä¼¼æˆ–é‡å¤çš„äº‹ä»¶ï¼ˆå¦‚åŒä¸€äº‹ä»¶çš„ä¸åŒæŠ¥é“ï¼‰
        2. **é‡è¦æ€§è¯„ä¼°**ï¼šè¯„ä¼°æ¯ä¸ªäº‹ä»¶å¯¹{topic}è¡Œä¸šçš„å½±å“ç¨‹åº¦å’Œé‡è¦æ€§
        3. **æ—¶æ•ˆæ€§åˆ¤æ–­**ï¼šä¼˜å…ˆé€‰æ‹©æœ€æ–°ã€æœ€å…·æ—¶æ•ˆæ€§çš„äº‹ä»¶
        4. **å¤šæ ·æ€§ä¿è¯**ï¼šç¡®ä¿é€‰å‡ºçš„äº‹ä»¶æ¶µç›–ä¸åŒæ–¹é¢ï¼ˆæŠ€æœ¯ã€æ”¿ç­–ã€å¸‚åœºã€æŠ•èµ„ç­‰ï¼‰

        ğŸ“‹ **è¾“å‡ºè¦æ±‚** - è¯·ä»æ‰€æœ‰äº‹ä»¶ä¸­ç­›é€‰å‡ºæœ€é‡è¦çš„3-5ä¸ªä¸é‡å¤äº‹ä»¶ï¼ŒæŒ‰ä»¥ä¸‹æ ¼å¼è¾“å‡ºï¼š

        1. [äº‹ä»¶æ ‡é¢˜] (æ¥æºç½‘ç«™åŸŸå)
           â—‹ äº‹ä»¶ï¼š[ç”¨1-2å¥è¯ç®€æ´æè¿°äº‹ä»¶æ ¸å¿ƒå†…å®¹]
           â—‹ å…³é”®ç‚¹ï¼š[åˆ—å‡º2-3ä¸ªæœ€é‡è¦çš„å…³é”®ä¿¡æ¯ç‚¹]

        2. [äº‹ä»¶æ ‡é¢˜] (æ¥æºç½‘ç«™åŸŸå)
           â—‹ äº‹ä»¶ï¼š[ç”¨1-2å¥è¯ç®€æ´æè¿°äº‹ä»¶æ ¸å¿ƒå†…å®¹]  
           â—‹ å…³é”®ç‚¹ï¼š[åˆ—å‡º2-3ä¸ªæœ€é‡è¦çš„å…³é”®ä¿¡æ¯ç‚¹]

        [ç»§ç»­ç›¸åŒæ ¼å¼...]

        ğŸ¯ **ç­›é€‰æ ‡å‡†**ï¼š
        - **å»é‡ä¼˜å…ˆ**ï¼šå¦‚æœå¤šä¸ªäº‹ä»¶è®²è¿°åŒä¸€ä»¶äº‹ï¼Œåªé€‰æ‹©ä¿¡æ¯æœ€å…¨é¢ã€æ¥æºæœ€æƒå¨çš„ä¸€ä¸ª
        - **å½±å“åŠ›ä¼˜å…ˆ**ï¼šä¼˜å…ˆé€‰æ‹©å¯¹{topic}è¡Œä¸šå½±å“æœ€å¤§çš„äº‹ä»¶
        - **æ—¶æ•ˆæ€§ä¼˜å…ˆ**ï¼šä¼˜å…ˆé€‰æ‹©æœ€æ–°å‘ç”Ÿçš„äº‹ä»¶
        - **å¤šæ ·æ€§ä¿è¯**ï¼šé¿å…æ‰€æœ‰äº‹ä»¶éƒ½é›†ä¸­åœ¨åŒä¸€ä¸ªå­é¢†åŸŸ
        - **ä¿¡æ¯å®Œæ•´æ€§**ï¼šä¼˜å…ˆé€‰æ‹©ä¿¡æ¯è¯¦ç»†ã€å…·ä½“çš„äº‹ä»¶

        âš ï¸ **é‡è¦æé†’**ï¼š
        - å¦‚æœå‘ç°å¤šä¸ªäº‹ä»¶æ˜¯å…³äºåŒä¸€ä»¶äº‹çš„ä¸åŒæŠ¥é“ï¼Œè¯·åˆå¹¶ä¿¡æ¯å¹¶åªè¾“å‡ºä¸€ä¸ªäº‹ä»¶
        - ä¸¥æ ¼æŒ‰ç…§é‡è¦æ€§æ’åºï¼Œæœ€é‡è¦çš„äº‹ä»¶æ’åœ¨å‰é¢
        - æ¯ä¸ªäº‹ä»¶çš„æè¿°æ§åˆ¶åœ¨100-150å­—ä»¥å†…
        - æ¥æºç½‘ç«™åªå†™åŸŸåï¼Œä¸è¦å®Œæ•´URL
        - ä¸è¦æ·»åŠ é¢å¤–çš„æ ‡é¢˜æˆ–è¯´æ˜æ–‡å­—
        """
        
        system_msg = f"""ä½ æ˜¯{topic}è¡Œä¸šçš„ä¸“ä¸šä¿¡æ¯æ•´ç†ä¸“å®¶ï¼Œæ“…é•¿å°†å¤æ‚ä¿¡æ¯æç‚¼æˆç®€æ´å®ç”¨çš„æ‘˜è¦æ ¼å¼ã€‚è¯·ç¡®ä¿è¾“å‡ºæ ¼å¼å‡†ç¡®ï¼Œå†…å®¹ç®€æ´æœ‰ç”¨ã€‚"""
        
        try:
            if not self.llm_processor:
                return self._generate_fallback_events_summary_simple(topic, selected_news)
            
            summary_analysis = self.llm_processor.call_llm_api(summary_prompt, system_msg, max_tokens=6000)
            
            return f"### ä¸€ã€é‡å¤§æ–°é—»æ‘˜è¦ä¸å…³é”®ç»†èŠ‚\n\n{summary_analysis}"
            
        except Exception as e:
            print(f"âŒ [é”™è¯¯] ç”Ÿæˆé‡å¤§äº‹ä»¶æ‘˜è¦æ—¶å‡ºé”™: {str(e)}")
            # æä¾›å¤‡ç”¨çš„ç®€å•æ‘˜è¦
            fallback_summary = self._generate_fallback_events_summary_simple(topic, selected_news)
            return f"### ä¸€ã€é‡å¤§æ–°é—»æ‘˜è¦ä¸å…³é”®ç»†èŠ‚\n\n{fallback_summary}"
    
    def _generate_fallback_events_summary(self, topic, breaking_news):
        """å¤‡ç”¨çš„äº‹ä»¶æ‘˜è¦ç”Ÿæˆæ–¹æ³•ï¼ˆä¿ç•™åŸæ ¼å¼ï¼Œå…¼å®¹æ€§è€ƒè™‘ï¼‰"""
        if not breaking_news:
            return "ğŸ“Š **å½“å‰çŠ¶å†µ**: æš‚æ— é‡å¤§äº‹ä»¶ã€‚"
        
        # é€‰æ‹©å‰5-7ä¸ªäº‹ä»¶
        selected_events = breaking_news[:min(7, len(breaking_news))]
        
        summary_text = f"ğŸ“Š **äº‹ä»¶æ¦‚è§ˆ**: å½“å‰ç›‘æµ‹åˆ°{len(selected_events)}ä¸ª{topic}è¡Œä¸šé‡å¤§äº‹ä»¶\n\n"
        
        for i, event in enumerate(selected_events, 1):
            title = event.get('title', 'æœªçŸ¥äº‹ä»¶')
            source = event.get('source', 'æœªçŸ¥æ¥æº')
            content = event.get('content', 'æ— è¯¦ç»†å†…å®¹')[:200]
            
            summary_text += f"#### ğŸ”¥ äº‹ä»¶{i}ï¼š{title}\n"
            summary_text += f"**ğŸ“° æ¥æº**: {source}\n"
            summary_text += f"**æ¦‚è¿°**: {content}...\n\n"
        
        return summary_text
    
    def _generate_fallback_events_summary_simple(self, topic, selected_news):
        """å¤‡ç”¨çš„ç®€æ´æ ¼å¼äº‹ä»¶æ‘˜è¦ç”Ÿæˆæ–¹æ³•"""
        if not selected_news:
            return "ğŸ“Š **å½“å‰çŠ¶å†µ**: åœ¨æŒ‡å®šæ—¶é—´èŒƒå›´å†…æš‚æ— é‡å¤§äº‹ä»¶ã€‚"
        
        summary_text = ""
        
        for i, event in enumerate(selected_news, 1):
            title = event.get('title', 'æœªçŸ¥äº‹ä»¶')
            source = event.get('source', 'æœªçŸ¥æ¥æº')
            content = event.get('content', 'æ— è¯¦ç»†å†…å®¹')[:150]
            
            # æå–åŸŸå
            source_domain = source
            if 'http' in source.lower():
                try:
                    from urllib.parse import urlparse
                    parsed = urlparse(source)
                    source_domain = parsed.netloc or source
                except:
                    source_domain = source
            
            summary_text += f"{i}. {title} ({source_domain})\n"
            summary_text += f"   â—‹ äº‹ä»¶ï¼š{content}...\n"
            summary_text += f"   â—‹ å…³é”®ç‚¹ï¼š{topic}è¡Œä¸šç›¸å…³é‡è¦åŠ¨æ€ï¼Œéœ€è¦æŒç»­å…³æ³¨ã€‚\n\n"
        
        return summary_text
    
    def _process_innovation_news_enhanced(self, topic, innovation_news):
        """å¢å¼ºç‰ˆåˆ›æ–°æ–°é—»å¤„ç†"""
        if not innovation_news:
            return f"## ğŸ”¬ æŠ€æœ¯åˆ›æ–°ä¸æ–°äº§å“\n\nğŸ“Š **è§‚å¯Ÿ**: å½“å‰æ—¶é—´çª—å£å†…{topic}è¡Œä¸šæŠ€æœ¯åˆ›æ–°æ´»åŠ¨ç›¸å¯¹å¹³é™ã€‚\n\n"
        
        print(f"ğŸ§ª [æŠ€æœ¯åˆ†æ] æ­£åœ¨æ·±åº¦åˆ†æ{len(innovation_news)}é¡¹æŠ€æœ¯åˆ›æ–°...")
        
        all_news_text = "\n\n".join([
            f"æ ‡é¢˜: {item.get('title', 'æ— æ ‡é¢˜')}\nå†…å®¹: {item.get('content', 'æ— å†…å®¹')[:500]}...\næ¥æº: {item.get('source', 'æœªçŸ¥')}"
            for item in innovation_news
        ])
        
        enhanced_prompt = f"""
        ä½œä¸º{topic}è¡Œä¸šçš„æŠ€æœ¯ä¸“å®¶ï¼Œè¯·å¯¹ä»¥ä¸‹æŠ€æœ¯åˆ›æ–°è¿›è¡Œæ™ºèƒ½ç­›é€‰ã€å»é‡å’Œæ·±åº¦åˆ†æï¼š
        
        {all_news_text}
        
        ğŸ” **æ™ºèƒ½ç­›é€‰ä»»åŠ¡**ï¼š
        1. **å»é‡è¯†åˆ«**ï¼šè¯†åˆ«ç›¸ä¼¼æˆ–é‡å¤çš„æŠ€æœ¯åˆ›æ–°æŠ¥é“ï¼Œåˆå¹¶ç›¸åŒæŠ€æœ¯çš„ä¸åŒæŠ¥é“
        2. **é‡è¦æ€§è¯„ä¼°**ï¼šè¯„ä¼°æ¯ä¸ªæŠ€æœ¯åˆ›æ–°çš„çªç ´æ€§å’Œå½±å“åŠ›
        3. **å¤šæ ·æ€§ä¿è¯**ï¼šç¡®ä¿æ¶µç›–ä¸åŒæŠ€æœ¯é¢†åŸŸå’Œåº”ç”¨æ–¹å‘
        
        ğŸ”¬ **æŠ€æœ¯åˆ†ææ¡†æ¶**:
        1. **åˆ›æ–°çªç ´æ€§è¯„ä¼°**: ç­›é€‰å‡ºçš„æŠ€æœ¯çš„é¢ è¦†æ€§ç¨‹åº¦å¦‚ä½•ï¼Ÿ
        2. **æŠ€æœ¯æˆç†Ÿåº¦åˆ†æ**: å½“å‰å¤„äºä»€ä¹ˆå‘å±•é˜¶æ®µï¼Ÿ
        3. **å•†ä¸šåŒ–å¯è¡Œæ€§**: è·ç¦»è§„æ¨¡åŒ–åº”ç”¨è¿˜æœ‰å¤šè¿œï¼Ÿ
        4. **ç«äº‰æ ¼å±€å½±å“**: å¯¹ç°æœ‰æŠ€æœ¯è·¯çº¿çš„å†²å‡»ç¨‹åº¦ï¼Ÿ
        5. **æœªæ¥å‘å±•è¶‹åŠ¿**: æŠ€æœ¯æ¼”è¿›çš„å¯èƒ½æ–¹å‘ï¼Ÿ
        
        ğŸ¤” **åˆ†æå¸ˆæ€è€ƒè¿‡ç¨‹**:
        - é¦–å…ˆå»é™¤é‡å¤æŠ€æœ¯æŠ¥é“ï¼Œä¿ç•™ä¿¡æ¯æœ€å…¨é¢çš„ç‰ˆæœ¬
        - ç„¶åè¯„ä¼°æŠ€æœ¯çš„åŸåˆ›æ€§å’Œçªç ´æ€§
        - æ¥ç€åˆ†ææŠ€æœ¯çš„å®ç”¨æ€§å’Œå•†ä¸šä»·å€¼
        - æœ€åé¢„æµ‹å¯¹è¡Œä¸šç”Ÿæ€çš„é•¿è¿œå½±å“
        
        è¯·æä¾›å®¢è§‚ã€ä¸“ä¸šçš„æŠ€æœ¯è§£è¯»ï¼Œè¦æ±‚è¯¦ç»†æ·±å…¥ï¼Œå­—æ•°1500-2000å­—ã€‚
         
         ğŸ“ **å†…å®¹è¦æ±‚**:
         - å¦‚æœå‘ç°ç›¸åŒæŠ€æœ¯çš„å¤šä¸ªæŠ¥é“ï¼Œè¯·åˆå¹¶ä¿¡æ¯åªåˆ†æä¸€æ¬¡
         - æ¯ä¸ªç‹¬ç‰¹æŠ€æœ¯åˆ›æ–°éƒ½è¦è¯¦ç»†å±•å¼€åˆ†æ
         - æä¾›å…·ä½“çš„æŠ€æœ¯ç»†èŠ‚å’Œåº”ç”¨åœºæ™¯
         - åŒ…å«å¸‚åœºå‰æ™¯å’Œå•†ä¸šåŒ–æ—¶é—´è¡¨
         - åˆ†ææŠ€æœ¯çš„ä¼˜åŠ¿ã€å±€é™æ€§å’Œå‘å±•ç“¶é¢ˆ
         - å¯¹æ¯”åŒç±»æŠ€æœ¯æ–¹æ¡ˆçš„å·®å¼‚
        """
        
        try:
            analysis = self.llm_processor.call_llm_api(enhanced_prompt, 
                f"ä½ æ˜¯{topic}è¡Œä¸šçš„èµ„æ·±æŠ€æœ¯ä¸“å®¶ï¼Œå…·å¤‡æ·±åšçš„æŠ€æœ¯æ´å¯ŸåŠ›", max_tokens=8000)
            
            sources = []
            for item in innovation_news:
                if item.get('url', '#') != '#':
                    sources.append(f"- [{item.get('title', 'æœªçŸ¥æ ‡é¢˜')}]({item.get('url')}) - {item.get('source', 'æœªçŸ¥æ¥æº')}")
            
            if sources:
                analysis += "\n\n**æŠ€æœ¯èµ„æ–™æ¥æº:**\n" + "\n".join(sources)
            
            return f"## ğŸ”¬ æŠ€æœ¯åˆ›æ–°ä¸æ–°äº§å“æ·±åº¦è§£æ\n\n{analysis}\n\n"
            
        except Exception as e:
            print(f"âŒ [é”™è¯¯] ç”ŸæˆæŠ€æœ¯åˆ›æ–°åˆ†ææ—¶å‡ºé”™: {str(e)}")
            return f"## ğŸ”¬ æŠ€æœ¯åˆ›æ–°ä¸æ–°äº§å“\n\næŠ€æœ¯åˆ†ææš‚æ—¶ä¸å¯ç”¨ã€‚\n\n"
    
    def _process_investment_news_enhanced(self, topic, investment_news):
        """å¢å¼ºç‰ˆæŠ•èµ„æ–°é—»å¤„ç†"""  
        if not investment_news:
            return f"## ğŸ’° æŠ•èµ„ä¸å¸‚åœºåŠ¨å‘\n\nğŸ“Š **å¸‚åœºè§‚å¯Ÿ**: {topic}è¡Œä¸šæŠ•èµ„æ´»åŠ¨åœ¨å½“å‰æ—¶æ®µç›¸å¯¹å¹³é™ã€‚\n\n"
        
        print(f"ğŸ’° [æŠ•èµ„åˆ†æ] æ­£åœ¨åˆ†æ{len(investment_news)}ä¸ªæŠ•èµ„äº‹ä»¶...")
        
        all_news_text = "\n\n".join([
            f"æ ‡é¢˜: {item.get('title', 'æ— æ ‡é¢˜')}\næ—¶é—´: {item.get('date', 'æœªçŸ¥æ—¥æœŸ')}\nå†…å®¹: {item.get('content', 'æ— å†…å®¹')[:500]}...\næ¥æº: {item.get('source', 'æœªçŸ¥')}"
            for item in investment_news
        ])
        
        enhanced_prompt = f"""
        ä½œä¸º{topic}è¡Œä¸šçš„æŠ•èµ„åˆ†æä¸“å®¶ï¼Œè¯·å¯¹ä»¥ä¸‹æŠ•èµ„åŠ¨æ€è¿›è¡Œæ™ºèƒ½ç­›é€‰ã€å»é‡å’Œæ·±åº¦è§£è¯»ï¼š
        
        {all_news_text}
        
        ğŸ” **æ™ºèƒ½ç­›é€‰ä»»åŠ¡**ï¼š
        1. **å»é‡è¯†åˆ«**ï¼šè¯†åˆ«ç›¸åŒæŠ•èµ„äº‹ä»¶çš„ä¸åŒæŠ¥é“ï¼Œåˆå¹¶ä¿¡æ¯ä¿ç•™æœ€å®Œæ•´ç‰ˆæœ¬
        2. **é‡è¦æ€§è¯„ä¼°**ï¼šè¯„ä¼°æ¯ä¸ªæŠ•èµ„äº‹ä»¶çš„è§„æ¨¡å’Œå½±å“åŠ›
        3. **å¤šæ ·æ€§ä¿è¯**ï¼šç¡®ä¿æ¶µç›–ä¸åŒæŠ•èµ„ç±»å‹å’Œç»†åˆ†é¢†åŸŸ
        
        ğŸ’° **æŠ•èµ„åˆ†ææ¡†æ¶**:
        1. **èµ„æœ¬æµå‘åˆ†æ**: èµ„é‡‘ä¸»è¦æŠ•å‘å“ªäº›ç»†åˆ†é¢†åŸŸï¼Ÿ
        2. **æŠ•èµ„é€»è¾‘è§£è¯»**: æŠ•èµ„æ–¹çš„æˆ˜ç•¥è€ƒé‡æ˜¯ä»€ä¹ˆï¼Ÿ
        3. **ä¼°å€¼æ°´å¹³è¯„ä¼°**: å½“å‰ä¼°å€¼æ˜¯å¦åˆç†ï¼Ÿ
        4. **å¸‚åœºä¿¡å·è§£è¯»**: è¿™äº›æŠ•èµ„åæ˜ äº†ä»€ä¹ˆå¸‚åœºè¶‹åŠ¿ï¼Ÿ
        5. **é£é™©æœºé‡å¹¶å­˜**: æŠ•èµ„è€…åº”è¯¥å…³æ³¨ä»€ä¹ˆï¼Ÿ
        
        ğŸ¤” **æŠ•èµ„åˆ†æå¸ˆæ€è€ƒè¿‡ç¨‹**:
        - é¦–å…ˆå»é™¤é‡å¤æŠ•èµ„äº‹ä»¶æŠ¥é“ï¼Œåˆå¹¶ç›¸åŒäº‹ä»¶çš„ä¿¡æ¯
        - ç„¶åæ¢³ç†ç‹¬ç‰¹æŠ•èµ„äº‹ä»¶çš„è§„æ¨¡å’Œæ€§è´¨
        - æ¥ç€åˆ†ææŠ•èµ„èƒŒåçš„å•†ä¸šé€»è¾‘
        - è¯„ä¼°å¯¹è¡Œä¸šæ ¼å±€çš„å½±å“
        - æœ€åæå‡ºæŠ•èµ„ç­–ç•¥å»ºè®®
        
        è¯·æä¾›ä¸“ä¸šçš„æŠ•èµ„åˆ†æï¼Œæ³¨é‡æ•°æ®æ”¯æ’‘ï¼Œå­—æ•°2000-2500å­—ã€‚
         
         ğŸ“ **è¯¦ç»†è¦æ±‚**:
         - å¦‚æœå‘ç°ç›¸åŒæŠ•èµ„äº‹ä»¶çš„å¤šä¸ªæŠ¥é“ï¼Œè¯·åˆå¹¶ä¿¡æ¯åªåˆ†æä¸€æ¬¡
         - æ¯ä¸ªç‹¬ç‰¹æŠ•èµ„äº‹ä»¶è¦å•ç‹¬åˆ†æï¼ŒåŒ…å«èƒŒæ™¯ã€åŠ¨æœºã€å½±å“
         - æä¾›å…·ä½“çš„æŠ•èµ„é‡‘é¢ã€ä¼°å€¼å˜åŒ–ã€æŠ•èµ„æ–¹èƒŒæ™¯
         - åˆ†ææŠ•èµ„èƒŒåçš„æˆ˜ç•¥å¸ƒå±€å’Œå¸‚åœºåˆ¤æ–­
         - åŒ…å«é£é™©è¯„ä¼°å’Œæ”¶ç›Šé¢„æœŸåˆ†æ
         - å¯¹æ¯”å†å²æŠ•èµ„æ¡ˆä¾‹ï¼Œè¯†åˆ«è¶‹åŠ¿å˜åŒ–
         - æä¾›å…·ä½“çš„æŠ•èµ„å»ºè®®å’Œæ—¶æœºåˆ¤æ–­
        """
        
        try:
            analysis = self.llm_processor.call_llm_api(enhanced_prompt, 
                f"ä½ æ˜¯{topic}è¡Œä¸šçš„èµ„æ·±æŠ•èµ„åˆ†æå¸ˆï¼Œå…·å¤‡æ•é”çš„å¸‚åœºæ´å¯ŸåŠ›", max_tokens=8000)
            
            sources = []
            for item in investment_news:
                if item.get('url', '#') != '#':
                    sources.append(f"- [{item.get('title', 'æœªçŸ¥æ ‡é¢˜')}]({item.get('url')}) - {item.get('source', 'æœªçŸ¥æ¥æº')}")
            
            if sources:
                analysis += "\n\n**æŠ•èµ„èµ„è®¯æ¥æº:**\n" + "\n".join(sources)
            
            return f"## ğŸ’° æŠ•èµ„ä¸å¸‚åœºåŠ¨å‘æ·±åº¦åˆ†æ\n\n{analysis}\n\n"
            
        except Exception as e:
            print(f"âŒ [é”™è¯¯] ç”ŸæˆæŠ•èµ„åˆ†ææ—¶å‡ºé”™: {str(e)}")
            return f"## ğŸ’° æŠ•èµ„ä¸å¸‚åœºåŠ¨å‘\n\næŠ•èµ„åˆ†ææš‚æ—¶ä¸å¯ç”¨ã€‚\n\n"
    
    def _process_policy_news_enhanced(self, topic, policy_news):
        """å¢å¼ºç‰ˆæ”¿ç­–æ–°é—»å¤„ç†"""
        if not policy_news:
            return f"## ğŸ“œ æ”¿ç­–ä¸ç›‘ç®¡åŠ¨æ€\n\nğŸ“Š **æ”¿ç­–ç›‘æµ‹**: {topic}è¡Œä¸šæ”¿ç­–ç¯å¢ƒåœ¨å½“å‰æ—¶æ®µä¿æŒç¨³å®šã€‚\n\n"
        
        print(f"ğŸ“œ [æ”¿ç­–åˆ†æ] æ­£åœ¨åˆ†æ{len(policy_news)}é¡¹æ”¿ç­–åŠ¨æ€...")
        
        all_news_text = "\n\n".join([
            f"æ ‡é¢˜: {item.get('title', 'æ— æ ‡é¢˜')}\nå†…å®¹: {item.get('content', 'æ— å†…å®¹')[:500]}...\næ¥æº: {item.get('source', 'æœªçŸ¥')}"
            for item in policy_news
        ])
        
        enhanced_prompt = f"""
        ä½œä¸º{topic}è¡Œä¸šçš„æ”¿ç­–åˆ†æä¸“å®¶ï¼Œè¯·å¯¹ä»¥ä¸‹æ”¿ç­–åŠ¨æ€è¿›è¡Œæ™ºèƒ½ç­›é€‰ã€å»é‡å’Œæ·±åº¦è§£è¯»ï¼š
        
        {all_news_text}
        
        ğŸ” **æ™ºèƒ½ç­›é€‰ä»»åŠ¡**ï¼š
        1. **å»é‡è¯†åˆ«**ï¼šè¯†åˆ«ç›¸åŒæ”¿ç­–çš„ä¸åŒæŠ¥é“ï¼Œåˆå¹¶ä¿¡æ¯ä¿ç•™æœ€æƒå¨ç‰ˆæœ¬
        2. **é‡è¦æ€§è¯„ä¼°**ï¼šè¯„ä¼°æ¯é¡¹æ”¿ç­–å¯¹{topic}è¡Œä¸šçš„å½±å“ç¨‹åº¦
        3. **å¤šæ ·æ€§ä¿è¯**ï¼šç¡®ä¿æ¶µç›–ä¸åŒå±‚çº§å’Œç±»å‹çš„æ”¿ç­–åŠ¨æ€
        
        ğŸ“œ **æ”¿ç­–åˆ†ææ¡†æ¶**:
        1. **æ”¿ç­–å†…å®¹è§£è¯»**: æ ¸å¿ƒæ”¿ç­–æªæ–½å’Œè§„å®šæ˜¯ä»€ä¹ˆï¼Ÿ
        2. **æ”¿ç­–æ„å›¾åˆ†æ**: æ”¿åºœå¸Œæœ›è¾¾åˆ°ä»€ä¹ˆç›®æ ‡ï¼Ÿ
        3. **è¡Œä¸šå½±å“è¯„ä¼°**: å¯¹{topic}è¡Œä¸šå„ç¯èŠ‚çš„å…·ä½“å½±å“ï¼Ÿ
        4. **ä¼ä¸šåº”å¯¹ç­–ç•¥**: ä¼ä¸šåº”è¯¥å¦‚ä½•è°ƒæ•´æˆ˜ç•¥ï¼Ÿ
        5. **æ”¿ç­–è¶‹åŠ¿é¢„åˆ¤**: æœªæ¥æ”¿ç­–èµ°å‘å¦‚ä½•ï¼Ÿ
        
        ğŸ¤” **æ”¿ç­–åˆ†æå¸ˆæ€è€ƒè¿‡ç¨‹**:
        - é¦–å…ˆå»é™¤é‡å¤æ”¿ç­–æŠ¥é“ï¼Œåˆå¹¶ç›¸åŒæ”¿ç­–çš„ä¿¡æ¯
        - ç„¶åç†è§£ç‹¬ç‰¹æ”¿ç­–çš„èƒŒæ™¯å’Œç›®æ ‡
        - æ¥ç€åˆ†ææ”¿ç­–çš„å®æ–½è·¯å¾„å’Œæ—¶é—´èŠ‚ç‚¹
        - è¯„ä¼°å¯¹ä¸åŒä¼ä¸šçš„å·®å¼‚åŒ–å½±å“
        - æœ€åæå‡ºåˆè§„å’Œå‘å±•å»ºè®®
        
        è¯·æä¾›æƒå¨ã€å®¢è§‚çš„æ”¿ç­–è§£è¯»ï¼Œå­—æ•°1800-2200å­—ã€‚
         
         ğŸ“ **æ·±åº¦è¦æ±‚**:
         - å¦‚æœå‘ç°ç›¸åŒæ”¿ç­–çš„å¤šä¸ªæŠ¥é“ï¼Œè¯·åˆå¹¶ä¿¡æ¯åªåˆ†æä¸€æ¬¡
         - æ¯é¡¹ç‹¬ç‰¹æ”¿ç­–éƒ½è¦è¯¦ç»†è§£è¯»æ¡æ–‡å†…å®¹å’Œå®æ–½ç»†åˆ™
         - åˆ†ææ”¿ç­–å‡ºå°çš„èƒŒæ™¯ã€ç›®æ ‡å’Œé¢„æœŸæ•ˆæœ
         - è¯„ä¼°å¯¹ä¸åŒç±»å‹ä¼ä¸šçš„å·®å¼‚åŒ–å½±å“
         - æä¾›å…·ä½“çš„åˆè§„å»ºè®®å’Œæ“ä½œæŒ‡å—
         - é¢„æµ‹æ”¿ç­–æ‰§è¡Œè¿‡ç¨‹ä¸­å¯èƒ½çš„æŒ‘æˆ˜å’Œæœºé‡
         - å¯¹æ¯”å›½é™…åŒç±»æ”¿ç­–çš„ç»éªŒå’Œå¯ç¤º
        """
        
        try:
            analysis = self.llm_processor.call_llm_api(enhanced_prompt, 
                f"ä½ æ˜¯{topic}è¡Œä¸šçš„æ”¿ç­–åˆ†æä¸“å®¶ï¼Œå…·å¤‡æ·±åšçš„æ”¿ç­–ç†è§£èƒ½åŠ›", max_tokens=8000)
            
            sources = []
            for item in policy_news:
                if item.get('url', '#') != '#':
                    sources.append(f"- [{item.get('title', 'æœªçŸ¥æ ‡é¢˜')}]({item.get('url')}) - {item.get('source', 'æœªçŸ¥æ¥æº')}")
            
            if sources:
                analysis += "\n\n**æ”¿ç­–ä¿¡æ¯æ¥æº:**\n" + "\n".join(sources)
            
            return f"## ğŸ“œ æ”¿ç­–ä¸ç›‘ç®¡åŠ¨æ€æ·±åº¦è§£è¯»\n\n{analysis}\n\n"
            
        except Exception as e:
            print(f"âŒ [é”™è¯¯] ç”Ÿæˆæ”¿ç­–åˆ†ææ—¶å‡ºé”™: {str(e)}")
            return f"## ğŸ“œ æ”¿ç­–ä¸ç›‘ç®¡åŠ¨æ€\n\næ”¿ç­–åˆ†ææš‚æ—¶ä¸å¯ç”¨ã€‚\n\n"
    
    def _process_industry_trends_enhanced(self, topic, trend_news, days=7):
        """å¢å¼ºç‰ˆè¡Œä¸šè¶‹åŠ¿å¤„ç†"""
        if not trend_news:
            return f"## ğŸ“ˆ è¡Œä¸šè¶‹åŠ¿æ·±åº¦åˆ†æ\n\nğŸ“Š **è¶‹åŠ¿è§‚å¯Ÿ**: åŸºäºæœ€è¿‘{days}å¤©çš„æ•°æ®ï¼Œ{topic}è¡Œä¸šè¶‹åŠ¿åˆ†ææœ‰é™ã€‚\n\n"
        
        print(f"ğŸ“ˆ [è¶‹åŠ¿åˆ†æ] æ­£åœ¨ç»¼åˆåˆ†æ{len(trend_news)}ä¸ªè¡Œä¸šè¶‹åŠ¿...")
        
        # è®¡ç®—æ—¶é—´èŒƒå›´
        from datetime import datetime, timedelta
        today = datetime.now()
        start_date = today - timedelta(days=days)
        time_range = f"{start_date.strftime('%Yå¹´%mæœˆ%dæ—¥')} è‡³ {today.strftime('%Yå¹´%mæœˆ%dæ—¥')}"
        
        all_news_text = "\n\n".join([
            f"æ ‡é¢˜: {item.get('title', 'æ— æ ‡é¢˜')}\nå†…å®¹: {item.get('content', 'æ— å†…å®¹')[:500]}...\næ¥æº: {item.get('source', 'æœªçŸ¥')}"
            for item in trend_news
        ])
        
        enhanced_prompt = f"""
        ä½œä¸º{topic}è¡Œä¸šçš„é¦–å¸­è¶‹åŠ¿åˆ†æå¸ˆï¼Œè¯·å¯¹ä»¥ä¸‹**{time_range}**ï¼ˆæœ€è¿‘{days}å¤©ï¼‰æœŸé—´çš„è¡Œä¸šè¶‹åŠ¿è¿›è¡Œæ·±åº¦åˆ†æï¼š
        
        {all_news_text}
        
        âš ï¸ **æ—¶é—´èŒƒå›´é™åˆ¶**: æœ¬åˆ†æä¸¥æ ¼èšç„¦äº{time_range}è¿™ä¸ªæ—¶é—´çª—å£å†…çš„è¶‹åŠ¿ä¿¡å·ï¼Œä¸æ¶‰åŠæ›´æ—©æœŸçš„å†å²è¶‹åŠ¿ã€‚
        
        ğŸ“ˆ **è¶‹åŠ¿åˆ†ææ¡†æ¶**:
        1. **è¿‘æœŸè¶‹åŠ¿è¯†åˆ«**: æœ€è¿‘{days}å¤©å†…{topic}è¡Œä¸šå‡ºç°äº†å“ªäº›æ–°çš„å‘å±•è¶‹åŠ¿ï¼Ÿ
        2. **é©±åŠ¨å› ç´ åˆ†æ**: ä»€ä¹ˆåŠ›é‡åœ¨æ¨åŠ¨è¿™äº›æœ€æ–°è¶‹åŠ¿ï¼Ÿ
        3. **å½±å“ç¨‹åº¦è¯„ä¼°**: è¿™äº›æ–°è¶‹åŠ¿å¯¹è¡Œä¸šæ ¼å±€çš„å³æ—¶å’ŒçŸ­æœŸå½±å“ï¼Ÿ
        4. **å‘å±•è½¨è¿¹é¢„æµ‹**: åŸºäºè¿‘æœŸä¿¡å·ï¼Œè¿™äº›è¶‹åŠ¿çš„ä¸‹ä¸€æ­¥å‘å±•æ–¹å‘ï¼Ÿ
        5. **æœºé‡æŒ‘æˆ˜å¹¶å­˜**: æ–°è¶‹åŠ¿å¸¦æ¥çš„å³æ—¶æœºé‡å’Œé£é™©ï¼Ÿ
        
        ğŸ¤” **è¶‹åŠ¿åˆ†æå¸ˆæ€è€ƒè¿‡ç¨‹**:
        - é¦–å…ˆä»æœ€æ–°æ•°æ®ä¸­è¯†åˆ«è¶‹åŠ¿ä¿¡å·
        - ç„¶ååˆ†æè¿‘æœŸè¶‹åŠ¿ä¹‹é—´çš„ç›¸äº’å…³ç³»
        - æ¥ç€è¯„ä¼°è¶‹åŠ¿çš„ç´§è¿«æ€§å’Œå½±å“åŠ›
        - æœ€ååŸºäºæœ€æ–°å˜åŒ–é¢„æµ‹çŸ­æœŸå‘å±•è½¨è¿¹
        
        è¯·æä¾›åŸºäºè¿‘æœŸæ•°æ®çš„å‰ç»æ€§è¶‹åŠ¿åˆ†æï¼Œè¦æœ‰å…·ä½“çš„æ•°æ®æ”¯æ’‘å’Œæ¡ˆä¾‹è¯´æ˜ï¼Œå­—æ•°1500-2000å­—ã€‚
        
        ğŸ“ **åˆ†æè¦æ±‚**:
        - **ä¸¥æ ¼èšç„¦æ—¶é—´èŒƒå›´**: åªåˆ†æ{time_range}å†…çš„è¶‹åŠ¿ä¿¡å·
        - æ„å»ºåŸºäºæœ€æ–°æ•°æ®çš„è¶‹åŠ¿åˆ†ææ¡†æ¶
        - æ¯ä¸ªè¶‹åŠ¿éƒ½è¦è¯¦ç»†å±•å¼€ï¼ŒåŒ…å«æœ€æ–°é©±åŠ¨å› ç´ å’Œå‘å±•é˜¶æ®µ
        - æä¾›åŸºäºè¿‘æœŸå˜åŒ–çš„å…·ä½“é¢„æµ‹å’Œé‡åŒ–æŒ‡æ ‡
        - åˆ†ææœ€æ–°è¶‹åŠ¿ä¹‹é—´çš„ç›¸äº’å…³ç³»å’ŒååŒæ•ˆåº”
        - è¯†åˆ«æ½œåœ¨çš„æ–°å…´å˜åŒ–å’Œçªå‘å› ç´ 
        - æ„å»ºåŸºäºå½“å‰çŠ¶å†µçš„åº”å¯¹ç­–ç•¥
        - æä¾›çŸ­æœŸå†…çš„å…³é”®æ—¶é—´èŠ‚ç‚¹å’Œå‘å±•è·¯å¾„
        
        ğŸš« **é¿å…å†…å®¹**: 
        - ä¸è¦å¼•ç”¨{time_range}ä¹‹å¤–çš„å†å²è¶‹åŠ¿æ•°æ®
        - ä¸è¦è¿›è¡Œé•¿æœŸå†å²è¶‹åŠ¿çš„å›é¡¾åˆ†æ
        - ä¸“æ³¨äºå½“å‰æ—¶é—´çª—å£å†…çš„å…·ä½“è¶‹åŠ¿å˜åŒ–
        """
        
        try:
            analysis = self.llm_processor.call_llm_api(enhanced_prompt, 
                f"ä½ æ˜¯{topic}è¡Œä¸šçš„èµ„æ·±è¶‹åŠ¿åˆ†æä¸“å®¶ï¼Œå…·å¤‡å“è¶Šçš„å‰ç»æ€§åˆ¤æ–­èƒ½åŠ›", max_tokens=8000)
            
            sources = []
            for item in trend_news:
                if item.get('url', '#') != '#':
                    sources.append(f"- [{item.get('title', 'æœªçŸ¥æ ‡é¢˜')}]({item.get('url')}) - {item.get('source', 'æœªçŸ¥æ¥æº')}")
            
            if sources:
                analysis += "\n\n**è¶‹åŠ¿æ•°æ®æ¥æº:**\n" + "\n".join(sources)
            
            return f"## ğŸ“ˆ è¡Œä¸šè¶‹åŠ¿æ·±åº¦åˆ†æ\n\n{analysis}\n\n"
            
        except Exception as e:
            print(f"âŒ [é”™è¯¯] ç”Ÿæˆè¶‹åŠ¿åˆ†ææ—¶å‡ºé”™: {str(e)}")
            return f"## ğŸ“ˆ è¡Œä¸šè¶‹åŠ¿æ·±åº¦åˆ†æ\n\nè¶‹åŠ¿åˆ†ææš‚æ—¶ä¸å¯ç”¨ã€‚\n\n"
    
    def _generate_intelligent_summary(self, topic, all_news_data, days=7):
        """ç”Ÿæˆæ™ºèƒ½æ€»ç»“ï¼Œä½“ç°AIæ€è€ƒè¿‡ç¨‹"""
        from datetime import datetime, timedelta
        
        # è®¡ç®—æ—¶é—´èŒƒå›´
        today = datetime.now()
        start_date = today - timedelta(days=days)
        time_range = f"{start_date.strftime('%Yå¹´%mæœˆ%dæ—¥')} è‡³ {today.strftime('%Yå¹´%mæœˆ%dæ—¥')}"
        
        summary_prompt = f"""
        ä½œä¸º{topic}è¡Œä¸šçš„AIæ™ºèƒ½åˆ†æå¸ˆï¼Œæˆ‘å·²ç»å®Œæˆäº†é’ˆå¯¹**{time_range}**ï¼ˆæœ€è¿‘{days}å¤©ï¼‰çš„å…¨é¢ä¿¡æ¯æ”¶é›†å’Œåˆ†æã€‚
        ç°åœ¨éœ€è¦æä¾›ä¸€ä¸ªä½“ç°æ·±åº¦æ€è€ƒçš„è¡Œä¸šæ€»ç»“ã€‚
        
        âš ï¸ **é‡è¦æ—¶é—´é™åˆ¶**: æœ¬åˆ†æä¸¥æ ¼èšç„¦äº{time_range}è¿™ä¸ªæ—¶é—´çª—å£å†…çš„ä¿¡æ¯ï¼Œä¸æ¶‰åŠæ›´æ—©æœŸçš„å†å²æ•°æ®ã€‚
        
        ğŸ¤” **æˆ‘çš„åˆ†ææ€è·¯**:
        1. é¦–å…ˆè¯†åˆ«äº†æœ€è¿‘{days}å¤©å†…è¡Œä¸šçš„æ ¸å¿ƒåŠ¨æ€å’Œå˜åŒ–
        2. ç„¶ååˆ†æäº†è¿™äº›è¿‘æœŸäº‹ä»¶å’Œè¶‹åŠ¿ä¹‹é—´çš„å…³è”æ€§  
        3. æ¥ç€è¯„ä¼°äº†è¿™äº›æœ€æ–°å˜åŒ–å¯¹è¡Œä¸šæœªæ¥çš„æŒ‡å‘æ„ä¹‰
        4. æœ€åå½¢æˆäº†åŸºäºè¿‘æœŸæ•°æ®çš„ç»¼åˆæ€§åˆ¤æ–­å’Œå»ºè®®
        
        ğŸ“Š **æ•°æ®åŸºç¡€**ï¼ˆæœ€è¿‘{days}å¤©ï¼‰:
        - é‡å¤§äº‹ä»¶: {len(all_news_data.get('breaking_news', []))}æ¡
        - æŠ€æœ¯åˆ›æ–°: {len(all_news_data.get('innovation_news', []))}æ¡
        - æŠ•èµ„åŠ¨æ€: {len(all_news_data.get('investment_news', []))}æ¡
        - æ”¿ç­–ç›‘ç®¡: {len(all_news_data.get('policy_news', []))}æ¡
        - è¡Œä¸šè¶‹åŠ¿: {len(all_news_data.get('trend_news', []))}æ¡
        - è§‚ç‚¹å¯¹æ¯”: {len(all_news_data.get('perspective_analysis', []))}æ¡
        
        è¯·åŸºäºä»¥ä¸Šåˆ†ææ¡†æ¶ï¼Œæä¾›ä¸€ä¸ª800-1200å­—çš„æ™ºèƒ½æ€»ç»“ï¼Œéœ€è¦ï¼š
        1. **ä¸¥æ ¼èšç„¦æ—¶é—´èŒƒå›´**: åªåˆ†æ{time_range}å†…çš„ä¿¡æ¯å’Œè¶‹åŠ¿
        2. ä½“ç°AIçš„å®Œæ•´åˆ†ææ€è€ƒè¿‡ç¨‹å’Œé€»è¾‘é“¾æ¡
        3. çªå‡ºå…³é”®æ´å¯Ÿå’Œåˆ¤æ–­ï¼Œæä¾›å…·ä½“çš„æ•°æ®æ”¯æ’‘
        4. æä¾›åŸºäºè¿‘æœŸå˜åŒ–çš„å‰ç»æ€§å»ºè®®å’Œå…·ä½“è¡ŒåŠ¨å»ºè®®
        5. ä¿æŒå®¢è§‚å’Œä¸“ä¸šï¼ŒåŒæ—¶ä½“ç°æ·±åº¦æ€è€ƒ
        6. æ„å»ºå®Œæ•´çš„æˆ˜ç•¥å»ºè®®æ¡†æ¶
        7. è¯†åˆ«å…³é”®é£é™©ç‚¹å’Œæœºé‡çª—å£
        8. æä¾›ä¸åŒæƒ…æ™¯ä¸‹çš„åº”å¯¹ç­–ç•¥
        
        ğŸš« **é¿å…å†…å®¹**: 
        - ä¸è¦å¼•ç”¨{time_range}ä¹‹å¤–çš„å†å²æ•°æ®æˆ–äº‹ä»¶
        - ä¸è¦è¿›è¡Œè·¨å¹´åº¦çš„é•¿æœŸè¶‹åŠ¿åˆ†æ
        - ä¸“æ³¨äºå½“å‰æ—¶é—´çª—å£å†…çš„å…·ä½“å˜åŒ–å’Œå½±å“
        """
        
        try:
            summary = self.llm_processor.call_llm_api(summary_prompt, 
                f"ä½ æ˜¯å…·å¤‡æ·±åº¦æ€è€ƒèƒ½åŠ›çš„{topic}è¡Œä¸šAIåˆ†æå¸ˆ", max_tokens=8000)
            return f"## ğŸ§  AIæ™ºèƒ½åˆ†ææ€»ç»“\n\n{summary}\n\n"
        except Exception as e:
            print(f"âŒ [é”™è¯¯] ç”Ÿæˆæ™ºèƒ½æ€»ç»“æ—¶å‡ºé”™: {str(e)}")
            return f"## ğŸ§  AIæ™ºèƒ½åˆ†ææ€»ç»“\n\n{topic}è¡Œä¸šæ­£å¤„äºåŠ¨æ€å‘å±•é˜¶æ®µï¼ŒAIåˆ†ææ˜¾ç¤ºå¤šä¸ªç»´åº¦éƒ½æœ‰é‡è¦å˜åŒ–å€¼å¾—å…³æ³¨ã€‚\n\n"
    
    def _process_perspective_analysis_enhanced(self, topic, perspective_data):
        """å¤„ç†è§‚ç‚¹å¯¹æ¯”åˆ†æ"""
        if not perspective_data:
            return ""
        
        print(f"ğŸ” [è§‚ç‚¹åˆ†æ] æ­£åœ¨åˆ†æ{len(perspective_data)}æ¡ä¸åŒè§‚ç‚¹ä¿¡æ¯...")
        
        # æ„å»ºè§‚ç‚¹åˆ†ææç¤º
        all_perspectives_text = "\n\n".join([
            f"æ ‡é¢˜: {item.get('title', 'æ— æ ‡é¢˜')}\nå†…å®¹: {item.get('content', 'æ— å†…å®¹')[:500]}...\næ¥æº: {item.get('source', 'æœªçŸ¥')}"
            for item in perspective_data
        ])
        
        enhanced_prompt = f"""
        ä½œä¸º{topic}è¡Œä¸šçš„å®¢è§‚åˆ†æå¸ˆï¼Œè¯·å¯¹ä»¥ä¸‹ä¸åŒè§‚ç‚¹å’Œäº‰è®®æ€§ä¿¡æ¯è¿›è¡Œå¹³è¡¡åˆ†æï¼š
        
        {all_perspectives_text}
        
        ğŸ¯ **è§‚ç‚¹å¯¹æ¯”åˆ†ææ¡†æ¶**:
        1. **æ­£é¢è§‚ç‚¹æ€»ç»“**: æ”¯æŒæ€§ã€ä¹è§‚æ€§çš„è§‚ç‚¹æœ‰å“ªäº›ï¼Ÿ
        2. **è´¨ç–‘å£°éŸ³æ±‡æ€»**: æ‰¹è¯„ã€è´¨ç–‘ã€æ‹…å¿§çš„è§‚ç‚¹æœ‰å“ªäº›ï¼Ÿ
        3. **äº‰è®®ç„¦ç‚¹è¯†åˆ«**: ä¸»è¦çš„åˆ†æ­§ç‚¹åœ¨å“ªé‡Œï¼Ÿ
        4. **ä¸åŒç«‹åœºåˆ†æ**: 
           - ä¼ä¸švsç›‘ç®¡æ–¹
           - æŠ•èµ„è€…vsæ¶ˆè´¹è€…
           - å›½å†…vså›½é™…è§†è§’
           - å­¦æœ¯ç•Œvsäº§ä¸šç•Œ
        5. **å®¢è§‚è¯„ä¼°**: åŸºäºç°æœ‰è¯æ®ï¼Œå“ªäº›è§‚ç‚¹æ›´æœ‰è¯´æœåŠ›ï¼Ÿ
        6. **å¹³è¡¡å»ºè®®**: å¦‚ä½•åœ¨ä¸åŒè§‚ç‚¹é—´æ‰¾åˆ°å¹³è¡¡ï¼Ÿ
        
        ğŸ¤” **åˆ†æå¸ˆæ€è€ƒè¿‡ç¨‹**:
        - é¿å…åå‘ä»»ä½•ä¸€æ–¹ï¼Œä¿æŒä¸­ç«‹å®¢è§‚
        - åˆ†ææ¯ç§è§‚ç‚¹èƒŒåçš„åˆ©ç›Šè€ƒé‡å’Œé€»è¾‘åŸºç¡€
        - è¯†åˆ«å¯èƒ½çš„ä¿¡æ¯åå·®å’Œå±€é™æ€§
        - æä¾›å»ºè®¾æ€§çš„ç»¼åˆåˆ¤æ–­
        
        è¯·æä¾›å®¢è§‚ã€å¹³è¡¡çš„è§‚ç‚¹å¯¹æ¯”åˆ†æï¼Œå­—æ•°æ§åˆ¶åœ¨1500-2000å­—ã€‚
        
        ğŸ“ **å¯¹æ¯”åˆ†æè¦æ±‚**:
        - æ¯ä¸ªé‡è¦è§‚ç‚¹éƒ½è¦å®¢è§‚å‘ˆç°ï¼Œä¸åä¸å€š
        - åˆ†æè§‚ç‚¹èƒŒåçš„æ·±å±‚åŸå› å’ŒåŠ¨æœº
        - è¯†åˆ«ä¸åŒè§‚ç‚¹çš„åˆç†æ€§å’Œå±€é™æ€§
        - æä¾›åŸºäºäº‹å®çš„å¹³è¡¡åˆ¤æ–­
        - é¿å…ç»å¯¹åŒ–è¡¨è¿°ï¼Œæ‰¿è®¤å¤æ‚æ€§å’Œä¸ç¡®å®šæ€§
        - ä¸ºè¯»è€…æä¾›å¤šå…ƒåŒ–æ€è€ƒè§’åº¦
        """
        
        system_msg = f"""ä½ æ˜¯{topic}è¡Œä¸šçš„èµ„æ·±å®¢è§‚åˆ†æå¸ˆï¼Œå…·å¤‡ï¼š
        1. ä¸­ç«‹å®¢è§‚çš„åˆ†ææ€åº¦
        2. å¤šç»´åº¦çš„æ€è€ƒèƒ½åŠ›  
        3. å¹³è¡¡ä¸åŒè§‚ç‚¹çš„æŠ€å·§
        4. æ·±åº¦çš„è¡Œä¸šæ´å¯ŸåŠ›
        è¯·å±•ç°å‡ºä¸“ä¸šçš„å®¢è§‚åˆ†æèƒ½åŠ›ã€‚"""
        
        try:
            analysis = self.llm_processor.call_llm_api(enhanced_prompt, system_msg, max_tokens=8000)
            
            # æ·»åŠ åˆ†ææ¥æº
            sources = []
            for item in perspective_data:
                if item.get('url', '#') != '#':
                    sources.append(f"- [{item.get('title', 'æœªçŸ¥æ ‡é¢˜')}]({item.get('url')}) - {item.get('source', 'æœªçŸ¥æ¥æº')}")
            
            if sources:
                analysis += "\n\n**è§‚ç‚¹æ¥æº:**\n" + "\n".join(sources)
            
            return f"## âš–ï¸ å¤šå…ƒè§‚ç‚¹å¯¹æ¯”åˆ†æ\n\n{analysis}\n\n"
            
        except Exception as e:
            print(f"âŒ [é”™è¯¯] ç”Ÿæˆè§‚ç‚¹å¯¹æ¯”åˆ†ææ—¶å‡ºé”™: {str(e)}")
            return f"## âš–ï¸ å¤šå…ƒè§‚ç‚¹å¯¹æ¯”åˆ†æ\n\næš‚æ— {topic}è¡Œä¸šçš„ä¸åŒè§‚ç‚¹å¯¹æ¯”åˆ†æã€‚\n\n"
    
    def _generate_references(self, all_news_data):
        """ç”Ÿæˆå‚è€ƒèµ„æ–™"""
        references = []
        
        for news_type in ["breaking_news", "innovation_news", "trend_news", "policy_news", "investment_news", "company_news"]:
            for item in all_news_data.get(news_type, []):
                title = item.get('title', 'æœªçŸ¥æ ‡é¢˜')
                url = item.get('url', '#')
                source = item.get('source', 'æœªçŸ¥æ¥æº') 
                if url != '#':
                    references.append(f"- [{title}]({url}) - {source}")
        
        unique_references = list(set(references))
        
        return f"\n## ğŸ“š å‚è€ƒèµ„æ–™\n\n" + "\n".join(unique_references) + "\n"

# åŸæœ‰å‡½æ•°çš„ä¿ç•™ç‰ˆæœ¬ï¼ˆä¸ºäº†å…¼å®¹æ€§ï¼‰
def get_industry_news_comprehensive(topic, days=7, companies=None):
    """åŸæœ‰å‡½æ•°çš„ä¿ç•™ç‰ˆæœ¬"""
    agent = IntelligentReportAgent()
    return agent.generate_comprehensive_report_with_thinking(topic, days, companies)

def generate_news_report(topic, companies=None, days=7, output_file=None):
    """
    å¢å¼ºç‰ˆæŠ¥å‘Šç”Ÿæˆå‡½æ•°ï¼Œé›†æˆAIæ€è€ƒèƒ½åŠ›
    """
    print(f"\nğŸ¤– å¯åŠ¨æ™ºèƒ½æŠ¥å‘Šç”Ÿæˆç³»ç»Ÿ...")
    print(f"ğŸ¯ ç›®æ ‡: {topic}è¡Œä¸šåˆ†ææŠ¥å‘Š")
    print("=" * 60)
    
    # ä½¿ç”¨æ™ºèƒ½ä»£ç†ç”ŸæˆæŠ¥å‘Š
    agent = IntelligentReportAgent()
    report_data = agent.generate_comprehensive_report_with_thinking(topic, days, companies)
    
    # è·å–æŠ¥å‘Šå†…å®¹
    report_content = report_data["content"]
    
    # æ–‡ä»¶ä¿å­˜é€»è¾‘
    if not output_file:
        date_str = datetime.now().strftime('%Y%m%d')
        reports_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), "reports")
        os.makedirs(reports_dir, exist_ok=True)
        safe_topic = "".join([c if c.isalnum() or c in [' ', '_', '-'] else '_' for c in topic])
        safe_topic = safe_topic.replace(' ', '_')
        output_file = os.path.join(reports_dir, f"{safe_topic}_æ™ºèƒ½åˆ†ææŠ¥å‘Š_{date_str}.md")
        print(f"ğŸ“ æŠ¥å‘Šå°†ä¿å­˜è‡³: {output_file}")
    
    os.makedirs(os.path.dirname(os.path.abspath(output_file)), exist_ok=True)
    
    # ä¼˜åŒ–æ ¼å¼
    try:
        report_content = fix_markdown_headings(report_content)
        print("âœ… æŠ¥å‘Šæ ¼å¼å·²ä¼˜åŒ–")
    except Exception as e:
        print(f"âš ï¸ ä¿®å¤Markdownæ ¼å¼æ—¶å‡ºé”™: {str(e)}")
    
    # ä¿å­˜æŠ¥å‘Š
    try:
        from report_utils import safe_save_report
        safe_save_report(report_content, output_file)
    except ImportError:
        print("ğŸ“ ä½¿ç”¨æ ‡å‡†æ–¹å¼ä¿å­˜æ–‡ä»¶")
        with open(output_file, "w", encoding="utf-8-sig") as f:
            f.write(report_content)
    
    print(f"ğŸ‰ æ™ºèƒ½æŠ¥å‘Šå·²ç”Ÿæˆ: {output_file}")
    
    return report_content

# ==================== åŸè„šæœ¬é‡è¦å‡½æ•°ä¿ç•™åŒºåŸŸ ====================
# ä»¥ä¸‹å‡½æ•°ä»åŸè„šæœ¬ä¿ç•™ï¼Œç¡®ä¿å®Œæ•´çš„åŠŸèƒ½æ”¯æŒ

def process_breaking_news(llm_processor, topic, breaking_news):
    """å¤„ç†è¡Œä¸šé‡å¤§æ–°é—» - åŸç‰ˆä¿ç•™"""
    if not breaking_news:
        return f"## è¡Œä¸šé‡å¤§äº‹ä»¶\n\nç›®å‰æš‚æ— {topic}è¡Œä¸šçš„é‡å¤§æ–°é—»ã€‚\n\n"
    
    all_news_text = "\n\n".join([
        f"æ ‡é¢˜: {item.get('title', 'æ— æ ‡é¢˜')}\nå†…å®¹: {item.get('content', 'æ— å†…å®¹')[:500]}...\næ¥æº: {item.get('source', 'æœªçŸ¥')}"
        for item in breaking_news
    ])
    
    prompt = f"""
    è¯·åŸºäºä»¥ä¸‹{topic}è¡Œä¸šçš„æœ€æ–°é‡å¤§æ–°é—»ï¼Œæä¾›ç®€æ´ä½†å…¨é¢çš„æ‘˜è¦ï¼Œç‰¹åˆ«å…³æ³¨ä¸åŒæ¥æºå¯¹åŒä¸€äº‹ä»¶çš„ä¸åŒè§‚ç‚¹å’Œè§£è¯»ã€‚

    {all_news_text}
    
    è¯·æä¾›:
    1. æ¯æ¡é‡å¤§æ–°é—»çš„ç®€è¦æ‘˜è¦ï¼ŒåŒ…æ‹¬äº‹ä»¶çš„å…³é”®ç»†èŠ‚
    2. å¯¹ä¸åŒæ¥æºçš„è§‚ç‚¹è¿›è¡Œå¯¹æ¯”åˆ†æï¼Œçªå‡ºè§‚ç‚¹å·®å¼‚å’Œå…±è¯†
    3. å¯¹è¿™äº›äº‹ä»¶å¯èƒ½å¯¹{topic}è¡Œä¸šäº§ç”Ÿçš„å½±å“çš„å¤šè§’åº¦åˆ†æ
    4. ç›¸å…³ä¼ä¸šã€æŠ€æœ¯æˆ–å¸‚åœºçš„å¿…è¦èƒŒæ™¯ä¿¡æ¯
    
    è¦æ±‚:
    - ä¿æŒå®¢è§‚ï¼Œä¸“æ³¨äºäº‹å®
    - æŒ‰é‡è¦æ€§æ’åº
    - ç‰¹åˆ«å…³æ³¨å¯èƒ½æ”¹å˜è¡Œä¸šæ ¼å±€çš„çªå‘äº‹ä»¶
    - çªå‡ºä¸åŒæ¥æºçš„è§‚ç‚¹å·®å¼‚ï¼Œä½†ä¿æŒä¸­ç«‹ç«‹åœº
    - å¯¹äº‰è®®æ€§è¯é¢˜è¿›è¡Œå¤šè§’åº¦åˆ†æ
         - é•¿åº¦æ§åˆ¶åœ¨2000-2500å­—
    """
    
    system = f"""ä½ æ˜¯ä¸€ä½æƒå¨çš„{topic}è¡Œä¸šåˆ†æå¸ˆï¼Œæ“…é•¿ä»å¤æ‚ä¿¡æ¯ä¸­æå–å’Œæ€»ç»“æœ€é‡è¦çš„è¡Œä¸šäº‹ä»¶ä¸å‘å±•ã€‚
ä½ ç‰¹åˆ«æ³¨é‡å¯¹ä¸åŒæ¥æºçš„è§‚ç‚¹è¿›è¡Œå¯¹æ¯”åˆ†æï¼Œèƒ½å¤Ÿå®¢è§‚åœ°å‘ˆç°å„æ–¹è§‚ç‚¹ï¼Œå¹¶æŒ‡å‡ºå…¶ä¸­çš„å…±è¯†ä¸åˆ†æ­§ã€‚
ä½ çš„åˆ†æåº”è¯¥ä¿æŒä¸­ç«‹ï¼Œè®©è¯»è€…èƒ½å¤Ÿå…¨é¢äº†è§£äº‹ä»¶çš„å„ä¸ªæ–¹é¢ã€‚"""
    
    try:
        breaking_news_summary = llm_processor.call_llm_api(prompt, system)
        
        news_sources = []
        for item in breaking_news:
            title = item.get('title', 'æœªçŸ¥æ ‡é¢˜')
            url = item.get('url', '#')
            source = item.get('source', 'æœªçŸ¥æ¥æº')
            if url != '#':
                news_sources.append(f"- [{title}]({url}) - {source}")
        
        if news_sources:
            breaking_news_summary += "\n\n**æ¥æº:**\n" + "\n".join(news_sources)
            
        return f"## è¡Œä¸šé‡å¤§äº‹ä»¶\n\n{breaking_news_summary}\n\n"
    except Exception as e:
        print(f"ç”Ÿæˆè¡Œä¸šé‡å¤§äº‹ä»¶æ‘˜è¦æ—¶å‡ºé”™: {str(e)}")
        return f"## è¡Œä¸šé‡å¤§äº‹ä»¶\n\næš‚æ— {topic}è¡Œä¸šé‡å¤§äº‹ä»¶æ‘˜è¦ã€‚\n\n"

def process_innovation_news(llm_processor, topic, innovation_news):
    """å¤„ç†æŠ€æœ¯åˆ›æ–°æ–°é—» - åŸç‰ˆä¿ç•™"""
    if not innovation_news:
        return f"## æŠ€æœ¯åˆ›æ–°ä¸æ–°äº§å“\n\nç›®å‰æš‚æ— {topic}è¡Œä¸šçš„æŠ€æœ¯åˆ›æ–°æ–°é—»ã€‚\n\n"
    
    all_news_text = "\n\n".join([
        f"æ ‡é¢˜: {item.get('title', 'æ— æ ‡é¢˜')}\nå†…å®¹: {item.get('content', 'æ— å†…å®¹')[:500]}...\næ¥æº: {item.get('source', 'æœªçŸ¥')}"
        for item in innovation_news
    ])
    
    prompt = f"""
    è¯·åŸºäºä»¥ä¸‹{topic}è¡Œä¸šçš„æœ€æ–°æŠ€æœ¯åˆ›æ–°å’Œäº§å“å‘å¸ƒä¿¡æ¯ï¼Œæä¾›ç»¼åˆåˆ†æï¼Œç‰¹åˆ«å…³æ³¨ä¸åŒæ¥æºå¯¹åŒä¸€æŠ€æœ¯æˆ–äº§å“çš„ä¸åŒè¯„ä»·å’Œè§‚ç‚¹ã€‚

    {all_news_text}
    
    è¯·æä¾›:
    1. ä¸»è¦æŠ€æœ¯çªç ´å’Œåˆ›æ–°ç‚¹çš„æ‘˜è¦
    2. ä¸åŒæ¥æºå¯¹åŒä¸€æŠ€æœ¯/äº§å“çš„è¯„ä»·å¯¹æ¯”åˆ†æ
    3. è¿™äº›åˆ›æ–°å¦‚ä½•å½±å“{topic}è¡Œä¸šçš„å‘å±•æ–¹å‘çš„å¤šè§’åº¦è§£è¯»
    4. å¯èƒ½çš„å¸‚åœºååº”å’Œæ¶ˆè´¹è€…é‡‡çº³æƒ…å†µçš„ä¸åŒé¢„æµ‹
    5. æŠ€æœ¯å¯è¡Œæ€§å’Œå•†ä¸šå‰æ™¯çš„äº‰è®®ç‚¹åˆ†æ
    
    è¦æ±‚:
    - ä¸“æ³¨äºæŠ€æœ¯ç»†èŠ‚å’Œåˆ›æ–°ç‚¹
    - è§£é‡Šå¤æ‚æ¦‚å¿µæ—¶ä½¿ç”¨é€šä¿—æ˜“æ‡‚çš„è¯­è¨€
    - åˆ†æåˆ›æ–°çš„å®é™…åº”ç”¨ä»·å€¼
    - å¯¹æ¯”ä¸åŒæ¥æºçš„è§‚ç‚¹ï¼Œçªå‡ºå…±è¯†ä¸åˆ†æ­§
    - å¯¹äº‰è®®æ€§æŠ€æœ¯è¿›è¡Œå¤šè§’åº¦åˆ†æ
         - é•¿åº¦æ§åˆ¶åœ¨1800-2200å­—
    """
    
    system = f"""ä½ æ˜¯ä¸€ä½ä¸“ç²¾äº{topic}é¢†åŸŸæŠ€æœ¯çš„åˆ†æå¸ˆï¼Œæ“…é•¿è¯„ä¼°æŠ€æœ¯åˆ›æ–°çš„æ½œåŠ›å’Œå½±å“ã€‚
ä½ ç‰¹åˆ«æ³¨é‡å¯¹ä¸åŒæ¥æºçš„æŠ€æœ¯è¯„ä»·è¿›è¡Œå¯¹æ¯”åˆ†æï¼Œèƒ½å¤Ÿå®¢è§‚åœ°å‘ˆç°å„æ–¹è§‚ç‚¹ã€‚
ä½ çš„åˆ†æåº”è¯¥ä¿æŒä¸­ç«‹ï¼Œè®©è¯»è€…èƒ½å¤Ÿå…¨é¢äº†è§£æŠ€æœ¯åˆ›æ–°çš„å„ä¸ªæ–¹é¢ï¼ŒåŒ…æ‹¬å…¶ä¼˜åŠ¿å’Œå±€é™æ€§ã€‚"""
    
    try:
        innovation_summary = llm_processor.call_llm_api(prompt, system)
        
        news_sources = []
        for item in innovation_news:
            title = item.get('title', 'æœªçŸ¥æ ‡é¢˜')
            url = item.get('url', '#')
            source = item.get('source', 'æœªçŸ¥æ¥æº')
            if url != '#':
                news_sources.append(f"- [{title}]({url}) - {source}")
        
        if news_sources:
            innovation_summary += "\n\n**æ¥æº:**\n" + "\n".join(news_sources)
            
        return f"## æŠ€æœ¯åˆ›æ–°ä¸æ–°äº§å“\n\n{innovation_summary}\n\n"
    except Exception as e:
        print(f"ç”ŸæˆæŠ€æœ¯åˆ›æ–°æ‘˜è¦æ—¶å‡ºé”™: {str(e)}")
        return f"## æŠ€æœ¯åˆ›æ–°ä¸æ–°äº§å“\n\næš‚æ— {topic}è¡Œä¸šæŠ€æœ¯åˆ›æ–°æ‘˜è¦ã€‚\n\n"

def process_investment_news(llm_processor, topic, investment_news):
    """å¤„ç†æŠ•èµ„æ–°é—» - åŸç‰ˆä¿ç•™"""
    if not investment_news:
        return f"## æŠ•èµ„ä¸å¸‚åœºåŠ¨å‘\n\nç›®å‰æš‚æ— {topic}è¡Œä¸šçš„æŠ•èµ„ç›¸å…³æ–°é—»ã€‚\n\n"
    
    all_news_text = "\n\n".join([
        f"æ ‡é¢˜: {item.get('title', 'æ— æ ‡é¢˜')}\n"
        f"æ—¶é—´: {item.get('date', 'æœªçŸ¥æ—¥æœŸ')}\n"
        f"å†…å®¹: {item.get('content', 'æ— å†…å®¹')[:500]}...\n"
        f"æ¥æº: {item.get('source', 'æœªçŸ¥')}"
        for item in investment_news
    ])
    
    prompt = f"""
    è¯·åŸºäºä»¥ä¸‹{topic}è¡Œä¸šçš„æœ€æ–°æŠ•èµ„ã€èèµ„å’Œå¸‚åœºå˜åŠ¨ä¿¡æ¯ï¼Œè¿›è¡Œæ·±åº¦åˆ†æå’Œè§£è¯»ã€‚
    æ³¨æ„ï¼šè¯·ä¸¥æ ¼é™åˆ¶åœ¨æä¾›çš„æ–°é—»æ—¶é—´èŒƒå›´å†…è¿›è¡Œåˆ†æï¼Œä¸è¦å¼•ç”¨æˆ–åˆ†æèŒƒå›´ä¹‹å¤–çš„å†å²æ•°æ®ã€‚

    === æŠ•èµ„æ–°é—»æ•°æ® ===
    {all_news_text}
    
    è¯·æŒ‰ä»¥ä¸‹æ¡†æ¶è¿›è¡Œè¯¦ç»†åˆ†æï¼š

    1. æœ€æ–°æŠ•èèµ„åŠ¨æ€åˆ†æï¼ˆå æ¯”35%ï¼‰
    2. å½“æœŸæŠ•èµ„çƒ­ç‚¹åˆ†æï¼ˆå æ¯”25%ï¼‰
    3. æœ€æ–°ä¼°å€¼ç‰¹å¾ï¼ˆå æ¯”20%ï¼‰
    4. é£é™©æç¤ºï¼ˆå æ¯”10%ï¼‰
    5. è¿‘æœŸæŠ•èµ„å»ºè®®ï¼ˆå æ¯”10%ï¼‰

    è¦æ±‚ï¼š
    1. æ—¶æ•ˆæ€§ï¼šä¸¥æ ¼åŸºäºæä¾›çš„æ–°é—»æ—¶é—´èŒƒå›´
    2. å‡†ç¡®æ€§ï¼šåªåˆ†æç¡®å®šçš„ä¿¡æ¯ï¼Œä¸åšè¿‡åº¦æ¨æµ‹
    3. å®Œæ•´æ€§ï¼šç¡®ä¿åˆ†ææ¡†æ¶å®Œæ•´
    4. é•¿åº¦è¦æ±‚ï¼š1500-2000å­—
    """
    
    system = f"""ä½ æ˜¯ä¸€ä½ä¸“æ³¨äº{topic}è¡Œä¸šçš„èµ„æ·±æŠ•èµ„åˆ†æå¸ˆã€‚"""
    
    try:
        investment_summary = llm_processor.call_llm_api(prompt, system, max_tokens=8000)
        
        news_sources = []
        for item in investment_news:
            title = item.get('title', 'æœªçŸ¥æ ‡é¢˜')
            url = item.get('url', '#')
            source = item.get('source', 'æœªçŸ¥æ¥æº')
            if url != '#':
                news_sources.append(f"- [{title} ({source}) - {url}")
        
        if news_sources:
            investment_summary += "\n\n**å‚è€ƒæ–°é—»æ¥æº:**\n" + "\n".join(sorted(news_sources))
            
        return f"## æŠ•èµ„ä¸å¸‚åœºåŠ¨å‘\n\n{investment_summary}\n\n"
    except Exception as e:
        print(f"ç”ŸæˆæŠ•èµ„åˆ†ææ‘˜è¦æ—¶å‡ºé”™: {str(e)}")
        return f"## æŠ•èµ„ä¸å¸‚åœºåŠ¨å‘\n\næš‚æ— {topic}è¡Œä¸šæŠ•èµ„åˆ†ææ‘˜è¦ã€‚\n\n"

def process_industry_trends(llm_processor, topic, trend_news):
    """å¤„ç†è¡Œä¸šè¶‹åŠ¿æ–°é—» - åŸç‰ˆä¿ç•™"""
    if not trend_news:
        return f"## è¡Œä¸šè¶‹åŠ¿æ¦‚è§ˆ\n\nç›®å‰æš‚æ— {topic}è¡Œä¸šçš„è¶‹åŠ¿åˆ†æã€‚\n\n"
    
    all_news_text = "\n\n".join([
        f"æ ‡é¢˜: {item.get('title', 'æ— æ ‡é¢˜')}\nå†…å®¹: {item.get('content', 'æ— å†…å®¹')[:500]}...\næ¥æº: {item.get('source', 'æœªçŸ¥')}"
        for item in trend_news
    ])
    
    trend_prompt = f"""
    è¯·åŸºäºä»¥ä¸‹{topic}è¡Œä¸šçš„æœ€æ–°è¶‹åŠ¿ç›¸å…³æ–°é—»ï¼Œåˆ†æå¹¶æ€»ç»“è¡Œä¸šæ•´ä½“è¶‹åŠ¿å’Œå‘å±•æ–¹å‘ã€‚

    {all_news_text}
    
    è¯·æä¾›è¯¦ç»†çš„è¡Œä¸šè¶‹åŠ¿åˆ†æï¼Œå†…å®¹éœ€è¦åŒ…æ‹¬ï¼š
    1. {topic}è¡Œä¸šçš„æ•´ä½“å‘å±•è¶‹åŠ¿å’Œä¸»è¦ç‰¹å¾
    2. å¸‚åœºè§„æ¨¡ã€å¢é•¿ç‡å’Œä¸»è¦é©±åŠ¨å› ç´ çš„å¤šè§’åº¦åˆ†æ
    3. æŠ€æœ¯å‘å±•è·¯çº¿å’Œåˆ›æ–°ç„¦ç‚¹
    4. å€¼å¾—å…³æ³¨çš„æ–°æŠ€æœ¯ã€æ–°äº§å“æˆ–æ–°æ¨¡å¼
    5. è¡Œä¸šé¢ä¸´çš„æŒ‘æˆ˜ã€æœºé‡å’Œæ½œåœ¨é£é™©
    6. åŒºåŸŸå‘å±•å·®å¼‚å’Œå›½é™…ç«äº‰æ ¼å±€
    7. äº§ä¸šé“¾ä¸Šä¸‹æ¸¸å‘å±•æƒ…å†µ
    8. å¯¹æœªæ¥3-5å¹´çš„é¢„æµ‹å’Œå±•æœ›
    
    è¦æ±‚:
    - ä½¿ç”¨ä¸“ä¸šã€å®¢è§‚çš„è¯­è¨€
    - æä¾›å…·ä½“æ•°æ®å’Œäº‹å®æ”¯æŒ
    - åˆ†æè¦æ·±å…¥ä¸”æœ‰æ´å¯ŸåŠ›
    - ä½¿ç”¨å°æ ‡é¢˜ç»„ç»‡å†…å®¹
    - é•¿åº¦çº¦1000-1200å­—
    """
    
    trend_system = f"""ä½ æ˜¯ä¸€ä½æƒå¨çš„{topic}è¡Œä¸šè¶‹åŠ¿åˆ†æä¸“å®¶ï¼Œæ‹¥æœ‰ä¸°å¯Œçš„è¡Œä¸šç»éªŒå’Œæ·±åˆ»çš„æ´å¯ŸåŠ›ã€‚"""
    
    try:
        industry_trend = llm_processor.call_llm_api(trend_prompt, trend_system)
        
        news_sources = []
        for item in trend_news:
            title = item.get('title', 'æœªçŸ¥æ ‡é¢˜')
            url = item.get('url', '#')
            source = item.get('source', 'æœªçŸ¥æ¥æº')
            if url != '#':
                news_sources.append(f"- [{title}]({url}) - {source}")
        
        if news_sources:
            industry_trend += "\n\n**æ¥æº:**\n" + "\n".join(news_sources)
            
        return f"## è¡Œä¸šè¶‹åŠ¿æ·±åº¦åˆ†æ\n\n{industry_trend}\n\n"
    except Exception as e:
        print(f"ç”Ÿæˆè¡Œä¸šè¶‹åŠ¿åˆ†ææ—¶å‡ºé”™: {str(e)}")
        return f"## è¡Œä¸šè¶‹åŠ¿æ¦‚è§ˆ\n\næš‚æ— {topic}è¡Œä¸šè¶‹åŠ¿åˆ†æã€‚\n\n"

def process_company_news(llm_processor, topic, company, news_items):
    """å¤„ç†å•ä¸ªå…¬å¸çš„æ–°é—» - åŸç‰ˆä¿ç•™"""
    if not news_items:
        return f"### {company}\n\næš‚æ— {company}ç›¸å…³çš„æœ€æ–°åŠ¨æ€ã€‚\n\n"
    
    news_text = "\n\n".join([
        f"æ ‡é¢˜: {item.get('title', 'æ— æ ‡é¢˜')}\nå†…å®¹: {item.get('content', 'æ— å†…å®¹')}\né“¾æ¥: {item.get('url', '#')}"
        for item in news_items
    ])
    
    prompt = f"""
    è¯·åˆ†æä»¥ä¸‹å…³äº{company}å…¬å¸çš„æœ€æ–°æ–°é—»æŠ¥é“ï¼Œå¹¶æ’°å†™ä¸€ä»½æ€»ç»“ã€‚
    
    {news_text}
    
    æ€»ç»“è¦æ±‚:
    1. ä½¿ç”¨ä¸“ä¸šã€å®¢è§‚çš„è¯­è¨€
    2. ä¿ç•™å…³é”®äº‹å®å’Œæ•°æ®
    3. æŒ‰æ—¶é—´æˆ–é‡è¦æ€§è¿›è¡Œç»“æ„åŒ–ç»„ç»‡
    4. çªå‡ºä¸{topic}è¡Œä¸šç›¸å…³çš„ä¿¡æ¯
    5. é•¿åº¦æ§åˆ¶åœ¨300-500å­—ä»¥å†…
    """
    
    system_message = f"ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„{topic}è¡Œä¸šåˆ†æå¸ˆã€‚"
    
    try:
        summary = llm_processor.call_llm_api(prompt, system_message)
        
        news_sources = []
        for item in news_items:
            title = item.get('title', 'æœªçŸ¥æ ‡é¢˜')
            url = item.get('url', '#')
            source = item.get('source', 'æœªçŸ¥æ¥æº')
            if url != '#':
                news_sources.append(f"- [{title}]({url}) - {source}")
        
        if news_sources:
            summary += "\n\n**æ¥æº:**\n" + "\n".join(news_sources)
            
        return f"### {company}\n\n{summary}"
    except Exception as e:
        print(f"ä¸º {company} ç”Ÿæˆæ‘˜è¦æ—¶å‡ºé”™: {str(e)}")
        basic_content = "\n\n".join([f"- {item.get('title', 'æ— æ ‡é¢˜')}" for item in news_items])
        return f"### {company}\n\n{basic_content}"

def process_policy_news(llm_processor, topic, policy_news):
    """å¤„ç†æ”¿ç­–ç›‘ç®¡åŠ¨æ€ - åŸç‰ˆä¿ç•™"""
    if not policy_news:
        return f"## æ”¿ç­–ä¸ç›‘ç®¡åŠ¨æ€\n\nç›®å‰æš‚æ— {topic}è¡Œä¸šçš„æ”¿ç­–ç›¸å…³æ–°é—»ã€‚\n\n"
    
    all_news_text = "\n\n".join([
        f"æ ‡é¢˜: {item.get('title', 'æ— æ ‡é¢˜')}\nå†…å®¹: {item.get('content', 'æ— å†…å®¹')[:500]}...\næ¥æº: {item.get('source', 'æœªçŸ¥')}"
        for item in policy_news
    ])
    
    prompt = f"""
    è¯·åŸºäºä»¥ä¸‹{topic}è¡Œä¸šçš„æœ€æ–°æ”¿ç­–å’Œç›‘ç®¡ä¿¡æ¯ï¼Œæä¾›è¯¦ç»†åˆ†æã€‚
    
    {all_news_text}
    
    è¯·æä¾›:
    1. å„é¡¹æ”¿ç­–å’Œç›‘ç®¡åŠ¨æ€çš„æ¦‚è¿°
    2. è¿™äº›æ”¿ç­–å¯¹{topic}è¡Œä¸šçš„æ½œåœ¨å½±å“
    3. ä¼ä¸šåº”å¦‚ä½•åº”å¯¹è¿™äº›æ”¿ç­–å˜åŒ–
    4. æ”¿ç­–è¶‹åŠ¿åˆ¤æ–­å’Œæœªæ¥å±•æœ›
    
    è¦æ±‚:
    - ä¸“æ³¨äºæ”¿ç­–å†…å®¹å’Œå®è´¨å½±å“
    - åˆ†ææ”¿ç­–èƒŒåçš„æ„å›¾å’Œå¯¼å‘
    - é•¿åº¦æ§åˆ¶åœ¨1200-1500å­—
    """
    
    system = f"ä½ æ˜¯ä¸€ä½ä¸“ç²¾äº{topic}è¡Œä¸šæ”¿ç­–åˆ†æçš„ä¸“å®¶ã€‚"
    
    try:
        policy_summary = llm_processor.call_llm_api(prompt, system, max_tokens=8000)
        
        news_sources = []
        for item in policy_news:
            title = item.get('title', 'æœªçŸ¥æ ‡é¢˜')
            url = item.get('url', '#')
            source = item.get('source', 'æœªçŸ¥æ¥æº')
            if url != '#':
                news_sources.append(f"- [{title}]({url}) - {source}")
        
        if news_sources:
            policy_summary += "\n\n**æ¥æº:**\n" + "\n".join(news_sources)
            
        return f"## æ”¿ç­–ä¸ç›‘ç®¡åŠ¨æ€\n\n{policy_summary}\n\n"
    except Exception as e:
        print(f"ç”Ÿæˆæ”¿ç­–åˆ†ææ‘˜è¦æ—¶å‡ºé”™: {str(e)}")
        return f"## æ”¿ç­–ä¸ç›‘ç®¡åŠ¨æ€\n\næš‚æ— {topic}è¡Œä¸šæ”¿ç­–åˆ†ææ‘˜è¦ã€‚\n\n"

def generate_comprehensive_trend_summary(llm_processor, topic, all_news_data):
    """ç”Ÿæˆç®€çŸ­çš„è¡Œä¸šè¶‹åŠ¿æ¦‚å†µæ€»ç»“ - åŸç‰ˆä¿ç•™"""
    
    prompt = f"""
    è¯·é’ˆå¯¹ä¸Šè¿°å·²åˆ†æçš„{topic}è¡Œä¸šå„ä¸ªæ–¹é¢ï¼Œæä¾›ä¸€ä¸ªç®€çŸ­çš„æ€»ä½“æ¦‚æ‹¬å’Œè¶‹åŠ¿æ€»ç»“ã€‚
    
    è¦æ±‚:
    1. è¿™æ˜¯å¯¹å·²æœ‰å†…å®¹çš„æ¦‚æ‹¬æ€»ç»“
    2. é•¿åº¦æ§åˆ¶åœ¨300-400å­—ä»¥å†…
    3. ä½¿ç”¨ç®€æ´ã€ä¸“ä¸šçš„è¯­è¨€
    4. çªå‡ºæ ¸å¿ƒè¶‹åŠ¿å’Œå¯¹ä¼ä¸šçš„å»ºè®®
    """
    
    system = f"""ä½ æ˜¯ä¸€ä½{topic}è¡Œä¸šèµ„æ·±åˆ†æå¸ˆã€‚"""
    
    try:
        summary = llm_processor.call_llm_api(prompt, system, max_tokens=4000)
        return f"## è¡Œä¸šè¶‹åŠ¿æ€»ç»“\n\n{summary}\n\n"
    except Exception as e:
        print(f"ç”Ÿæˆè¡Œä¸šè¶‹åŠ¿æ€»ç»“æ—¶å‡ºé”™: {str(e)}")
        return f"## è¡Œä¸šè¶‹åŠ¿æ€»ç»“\n\n{topic}è¡Œä¸šæ­£å¤„äºå¿«é€Ÿå‘å±•é˜¶æ®µã€‚\n\n"

if __name__ == "__main__":
    # åŠ è½½ç¯å¢ƒå˜é‡
    load_dotenv()
    
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(description='æ™ºèƒ½è¡Œä¸šåˆ†ææŠ¥å‘Šç”Ÿæˆå™¨')
    parser.add_argument('--topic', type=str, required=True, help='æŠ¥å‘Šçš„ä¸»é¢˜')
    parser.add_argument('--companies', type=str, nargs='*', 
                      help='è¦ç‰¹åˆ«å…³æ³¨çš„å…¬å¸ï¼ˆå¯é€‰ï¼‰')
    parser.add_argument('--days', type=int, default=7, help='æœç´¢å†…å®¹çš„å¤©æ•°èŒƒå›´')
    parser.add_argument('--output', type=str, help='è¾“å‡ºæ–‡ä»¶åæˆ–è·¯å¾„')
    
    parser.epilog = """
    ğŸ¤– æ™ºèƒ½æŠ¥å‘Šç”Ÿæˆå™¨è¯´æ˜:
    
    æœ¬å·¥å…·é‡‡ç”¨AIä»£ç†çš„äº”æ­¥åˆ†ææ³• + å¤šæ¸ é“æœç´¢å¼•æ“ï¼š
    
    ğŸ” æœç´¢æ¸ é“é›†æˆï¼š
    - Brave Web Search API (éšç§å‹å¥½çš„æœç´¢ï¼Œå·²å¯ç”¨)  
    - Tavily Search API (AIä¼˜åŒ–çš„æœç´¢)
    - Google Custom Search API (æš‚æ—¶æ³¨é‡Šï¼Œéœ€è¦é…ç½®)
    - è‡ªåŠ¨å»é‡å’Œç»“æœä¼˜åŒ–
    
    ğŸ§  ç¬¬ä¸€æ­¥ï¼šæ™ºèƒ½æŸ¥è¯¢ç”Ÿæˆ
    - æ·±åº¦åˆ†æç”¨æˆ·éœ€æ±‚
    - ç”Ÿæˆå¤šç»´åº¦æœç´¢ç­–ç•¥
    - ç±»ä¼¼ä¸“ä¸šç ”ç©¶å‘˜çš„æ€è€ƒè¿‡ç¨‹
    
    ğŸ“Š ç¬¬äºŒæ­¥ï¼šå¤šæ¸ é“ä¿¡æ¯æœé›†  
    - åŒæ—¶ä½¿ç”¨å¤šä¸ªæœç´¢å¼•æ“
    - æå–é«˜è´¨é‡ä¿¡æ¯å¹¶å»é‡
    - ç¡®ä¿ä¿¡æ¯ç›¸å…³æ€§å’Œæƒå¨æ€§
    
    ğŸ¤” ç¬¬ä¸‰æ­¥ï¼šåæ€ä¸çŸ¥è¯†ç¼ºå£åˆ†æ
    - åˆ†æä¿¡æ¯å®Œæ•´æ€§
    - è¯†åˆ«çŸ¥è¯†ç©ºç™½ç‚¹
    - åˆ¤æ–­æ˜¯å¦éœ€è¦è¡¥å……æœç´¢
    
    ğŸ¯ ç¬¬å››æ­¥ï¼šè¿­ä»£ä¼˜åŒ–æœç´¢
    - ç”Ÿæˆé’ˆå¯¹æ€§æŸ¥è¯¢
    - å¤šæ¸ é“å¡«è¡¥çŸ¥è¯†ç¼ºå£
    - æœ€å¤šè¿­ä»£3è½®ç¡®ä¿è´¨é‡
    
    ğŸ“ ç¬¬äº”æ­¥ï¼šç»¼åˆæŠ¥å‘Šç”Ÿæˆ
    - æ•´åˆæ‰€æœ‰æ”¶é›†ä¿¡æ¯
    - ä½“ç°AIæ€è€ƒè¿‡ç¨‹
    - ç”Ÿæˆæ·±åº¦åˆ†ææŠ¥å‘Š
    
    âš™ï¸ ç¯å¢ƒé…ç½®:
    éœ€è¦åœ¨.envæ–‡ä»¶ä¸­é…ç½®APIå¯†é’¥ï¼š
    - BRAVE_SEARCH_API_KEY (Braveæœç´¢ï¼Œå·²é¢„é…ç½®)
    - TAVILY_API_KEY (Tavilyæœç´¢)
    - GOOGLE_SEARCH_API_KEY (Googleæœç´¢)
    - GOOGLE_SEARCH_CX (Googleè‡ªå®šä¹‰æœç´¢å¼•æ“ID)
    
    ğŸ’¡ ä½¿ç”¨ç¤ºä¾‹:
    python generate_news_report_enhanced.py --topic "äººå·¥æ™ºèƒ½" --days 10 --output "AIæ™ºèƒ½åˆ†ææŠ¥å‘Š.md"
    """
    
    args = parser.parse_args()
    
    print("ğŸš€ å¯åŠ¨æ™ºèƒ½æŠ¥å‘Šç”Ÿæˆ...")
    generate_news_report(args.topic, args.companies, args.days, args.output) 