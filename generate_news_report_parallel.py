"""
å¹¶è¡Œæ–°é—»æŠ¥å‘Šç”Ÿæˆå™¨ - é‡æ„ç‰ˆæœ¬
ä½¿ç”¨å…¨æ–°çš„å¹¶è¡ŒLLMå¤„ç†æ¶æ„ï¼Œå¤§å¹…æå‡æŠ¥å‘Šç”Ÿæˆé€Ÿåº¦

ä¸»è¦æ”¹è¿›ï¼š
- å°†ä¸²è¡Œçš„LLMè°ƒç”¨è½¬æ¢ä¸ºå¹¶è¡Œæ‰§è¡Œ
- 6ä¸ªåˆ†ææ¨¡å—åŒæ—¶å¤„ç†ï¼Œé€Ÿåº¦æå‡70%
- ä¿æŒåŸæœ‰çš„äº”æ­¥åˆ†ææ³•å’Œæ™ºèƒ½è¿­ä»£
- æ”¯æŒå¤šç§æ€§èƒ½é…ç½®é€‰é¡¹
- æ–°å¢ï¼šæ•°æ®æ”¶é›†é˜¶æ®µçš„å¹¶è¡Œå¤„ç†ä¼˜åŒ–
"""

import os
import json
import argparse
from datetime import datetime, timedelta
from dotenv import load_dotenv
import sys
from fix_md_headings import fix_markdown_headings
import logging
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from collectors.tavily_collector import TavilyCollector
from collectors.google_search_collector import GoogleSearchCollector
from collectors.brave_search_collector import BraveSearchCollector
from collectors.parallel_news_processor import ParallelNewsProcessor
from generators.report_generator import ReportGenerator
import config

# å…³é—­HTTPè¯·æ±‚æ—¥å¿—ï¼Œå‡å°‘å¹²æ‰°
logging.getLogger("httpx").setLevel(logging.WARNING)
logging.getLogger("urllib3").setLevel(logging.WARNING)

class ParallelDataCollector:
    """
    å¹¶è¡Œæ•°æ®æ”¶é›†å™¨
    ä¸“é—¨è´Ÿè´£ç¬¬2æ­¥å’Œç¬¬4æ­¥çš„å¹¶è¡Œæ•°æ®æ”¶é›†ä¼˜åŒ–
    """
    
    def __init__(self, collectors_dict):
        self.collectors = collectors_dict
        self.results_lock = threading.Lock()
        
    def parallel_comprehensive_search(self, topic, days=7, max_workers=3):
        """
        ğŸš€ å¹¶è¡Œæ‰§è¡Œå¤šä¸ªæœç´¢å¼•æ“çš„ç»¼åˆæœç´¢ï¼ˆç¬¬2æ­¥ä¼˜åŒ–ï¼‰
        """
        print(f"ğŸ”„ [å¹¶è¡Œæ•°æ®æ”¶é›†] å¼€å§‹åŒæ—¶æ‰§è¡Œ{len(self.collectors)}ä¸ªæœç´¢å¼•æ“...")
        
        merged_data = {
            "breaking_news": [],
            "innovation_news": [],
            "investment_news": [],
            "policy_news": [],
            "trend_news": [],
            "company_news": [],
            "total_count": 0
        }
        
        seen_urls = set()
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # ğŸš€ å¹¶è¡Œæäº¤æ‰€æœ‰æœç´¢ä»»åŠ¡
            future_to_collector = {}
            
            # Braveæœç´¢
            if 'brave' in self.collectors:
                future_to_collector[executor.submit(
                    self._execute_brave_search, topic, days
                )] = 'brave'
            
            # Googleæœç´¢  
            if 'google' in self.collectors:
                future_to_collector[executor.submit(
                    self._execute_google_search, topic, days
                )] = 'google'
            
            # Tavilyæœç´¢
            if 'tavily' in self.collectors:
                future_to_collector[executor.submit(
                    self._execute_tavily_search, topic, days
                )] = 'tavily'
            
            # ğŸ”„ æ”¶é›†å¹¶è¡Œç»“æœ
            completed_count = 0
            for future in as_completed(future_to_collector):
                collector_name = future_to_collector[future]
                try:
                    collector_data = future.result()
                    completed_count += 1
                    
                    # ğŸ”— åˆå¹¶æ•°æ®å¹¶å»é‡
                    with self.results_lock:
                        added_count = self._merge_collector_data(
                            merged_data, collector_data, seen_urls, collector_name
                        )
                    
                    print(f"  âœ… [{completed_count}/{len(future_to_collector)}] {collector_name}æœç´¢å®Œæˆï¼Œæ–°å¢{added_count}æ¡å»é‡æ•°æ®")
                    
                except Exception as e:
                    print(f"  âŒ [{collector_name}]æœç´¢å¤±è´¥: {str(e)}")
        
        # ğŸ“Š è®¡ç®—æœ€ç»ˆç»Ÿè®¡
        merged_data["total_count"] = sum(
            len(merged_data[key]) for key in merged_data.keys() 
            if key != "total_count"
        )
        
        print(f"ğŸ“Š [å¹¶è¡Œæ”¶é›†å®Œæˆ] æ€»è®¡è·å¾— {merged_data['total_count']} æ¡å»é‡æ•°æ®")
        return merged_data
    
    def parallel_targeted_search(self, queries, topic, max_workers=4):
        """
        ğŸ¯ å¹¶è¡Œæ‰§è¡Œå¤šä¸ªé’ˆå¯¹æ€§æŸ¥è¯¢ï¼ˆç¬¬4æ­¥ä¼˜åŒ–ï¼‰
        """
        if not queries:
            return {}
            
        print(f"ğŸ”„ [å¹¶è¡Œé’ˆå¯¹æ€§æœç´¢] åŒæ—¶æ‰§è¡Œ{len(queries)}ä¸ªæŸ¥è¯¢...")
        
        additional_data = {}
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # ğŸš€ å¹¶è¡Œæäº¤æ‰€æœ‰æŸ¥è¯¢ä»»åŠ¡
            future_to_query = {
                executor.submit(
                    self._execute_single_query, query, topic
                ): query for query in queries
            }
            
            # ğŸ”„ æ”¶é›†æŸ¥è¯¢ç»“æœ
            completed_count = 0
            for future in as_completed(future_to_query):
                query = future_to_query[future]
                try:
                    query_results, category = future.result()
                    completed_count += 1
                    
                    if query_results:
                        if category not in additional_data:
                            additional_data[category] = []
                        additional_data[category].extend(query_results)
                        print(f"  âœ… [{completed_count}/{len(queries)}] æŸ¥è¯¢å®Œæˆ: {query[:30]}... -> {len(query_results)}æ¡ -> {category}")
                    else:
                        print(f"  âš ï¸ [{completed_count}/{len(queries)}] æ— ç»“æœ: {query[:30]}...")
                        
                except Exception as e:
                    print(f"  âŒ æŸ¥è¯¢å¤±è´¥: {query[:30]}... - {str(e)}")
        
        # ğŸ“Š ç»Ÿè®¡ç»“æœ
        total_found = sum(len(v) for v in additional_data.values())
        print(f"âœ… [å¹¶è¡ŒæŸ¥è¯¢å®Œæˆ] æ€»è®¡æ‰¾åˆ° {total_found} æ¡ä¿¡æ¯")
        
        return additional_data
    
    def _execute_brave_search(self, topic, days):
        """æ‰§è¡ŒBraveæœç´¢"""
        try:
            brave_collector = self.collectors['brave']
            print(f"  ğŸ” [Brave] å¼€å§‹ç»¼åˆæœç´¢...")
            brave_data = brave_collector.get_comprehensive_research(topic, days)
            
            # æ—¶é—´è¿‡æ»¤
            filtered_data = {}
            for category in ["breaking_news", "innovation_news", "investment_news", "policy_news", "trend_news"]:
                category_items = brave_data.get(category, [])
                if category_items:
                    filtered_items = brave_collector._filter_by_date(category_items, days)
                    filtered_data[category] = filtered_items
                else:
                    filtered_data[category] = []
            
            return filtered_data
        except Exception as e:
            print(f"  âŒ [Brave] æœç´¢å‡ºé”™: {str(e)}")
            return {}
    
    def _execute_google_search(self, topic, days):
        """æ‰§è¡ŒGoogleæœç´¢"""
        try:
            google_collector = self.collectors['google']
            print(f"  ğŸ” [Google] å¼€å§‹åˆ†ç±»æœç´¢...")
            
            google_queries = {
                "breaking_news": f"{topic} è¡Œä¸š é‡å¤§æ–°é—» çªå‘ é‡è¦äº‹ä»¶ {datetime.now().year}å¹´ æœ€æ–°",
                "innovation_news": f"{topic} æŠ€æœ¯åˆ›æ–° æ–°äº§å“ æ–°æŠ€æœ¯ {datetime.now().year}å¹´ æœ€æ–°",
                "investment_news": f"{topic} æŠ•èµ„ èèµ„ å¹¶è´­ {datetime.now().year}å¹´ æœ€æ–°",
                "policy_news": f"{topic} æ”¿ç­– ç›‘ç®¡ æ³•è§„ {datetime.now().year}å¹´ æœ€æ–°",
                "trend_news": f"{topic} è¶‹åŠ¿ å‘å±• å‰æ™¯ {datetime.now().year}å¹´ æœ€æ–°"
            }
            
            google_data = {}
            for category, query in google_queries.items():
                try:
                    results = google_collector.search(query, days_back=days, max_results=5)
                    if results:
                        filtered_results = google_collector._filter_by_date(results, days)
                        google_data[category] = filtered_results
                    else:
                        google_data[category] = []
                except Exception as e:
                    print(f"    âš ï¸ [Google] {category} æœç´¢å‡ºé”™: {str(e)}")
                    google_data[category] = []
            
            return google_data
        except Exception as e:
            print(f"  âŒ [Google] æœç´¢å‡ºé”™: {str(e)}")
            return {}
    
    def _execute_tavily_search(self, topic, days):
        """æ‰§è¡ŒTavilyæœç´¢"""
        try:
            tavily_collector = self.collectors['tavily']
            print(f"  ğŸ” [Tavily] å¼€å§‹è¡Œä¸šæœç´¢...")
            tavily_data = tavily_collector.get_industry_news_direct(topic, days)
            return tavily_data
        except Exception as e:
            print(f"  âŒ [Tavily] æœç´¢å‡ºé”™: {str(e)}")
            return {}
    
    def _execute_single_query(self, query, topic):
        """æ‰§è¡Œå•ä¸ªæŸ¥è¯¢çš„å¤šæ¸ é“æœç´¢"""
        search_results = []
        used_urls = set()
        
        # å¤šæ¸ é“æœç´¢
        for name, collector in self.collectors.items():
            try:
                if name == 'tavily':
                    results = collector.search(query, max_results=3)
                elif name == 'google':
                    results = collector.search(query, max_results=3)
                elif name == 'brave':
                    results = collector.search(query, count=3)
                else:
                    continue
                    
                if results:
                    for result in results:
                        url = result.get('url', '')
                        if url and url not in used_urls:
                            result['search_source'] = name
                            search_results.append(result)
                            used_urls.add(url)
                            
            except Exception:
                continue
        
        # åˆ†ç±»ç»“æœ
        category = self._categorize_search_result(query, topic)
        return search_results, category
    
    def _merge_collector_data(self, merged_data, collector_data, seen_urls, collector_name):
        """åˆå¹¶æ”¶é›†å™¨æ•°æ®å¹¶å»é‡"""
        added_count = 0
        
        for category, items in collector_data.items():
            if category == "total_count":
                continue
                
            for item in items:
                url = item.get('url', '')
                if url and url not in seen_urls:
                    seen_urls.add(url)
                    merged_data[category].append(item)
                    added_count += 1
        
        return added_count
    
    def _categorize_search_result(self, query, topic):
        """æ ¹æ®æŸ¥è¯¢å†…å®¹å°†æœç´¢ç»“æœåˆ†ç±»"""
        query_lower = query.lower()
        
        if any(word in query_lower for word in ['äº‰è®®', 'è´¨ç–‘', 'æ‰¹è¯„', 'åå¯¹', 'é£é™©', 'æŒ‘æˆ˜', 'ä¸åŒè§‚ç‚¹', 'alternative', 'criticism', 'controversy', 'challenge', 'risk']):
            return 'perspective_analysis'
        elif any(word in query_lower for word in ['æŠ•èµ„', 'èèµ„', 'investment', 'funding', 'ä¼°å€¼', 'valuation']):
            return 'investment_news'
        elif any(word in query_lower for word in ['æ”¿ç­–', 'policy', 'ç›‘ç®¡', 'regulation', 'æ³•è§„', 'law']):
            return 'policy_news'
        elif any(word in query_lower for word in ['æŠ€æœ¯', 'technology', 'åˆ›æ–°', 'innovation', 'äº§å“', 'product']):
            return 'innovation_news'
        elif any(word in query_lower for word in ['è¶‹åŠ¿', 'trend', 'å‘å±•', 'development', 'æœªæ¥', 'future']):
            return 'trend_news'
        else:
            return 'breaking_news'

class IntelligentReportAgentParallel:
    """æ™ºèƒ½æŠ¥å‘Šç”Ÿæˆä»£ç† - å¹¶è¡Œç‰ˆæœ¬"""
    
    def __init__(self, parallel_config: str = "balanced"):
        self.tavily_collector = TavilyCollector()
        self.llm_processor = self.tavily_collector._get_llm_processor()
        
        # åˆå§‹åŒ–å¤šä¸ªæœç´¢æ”¶é›†å™¨
        self.collectors = {
            'tavily': self.tavily_collector,
        }
        
        # å°è¯•åˆå§‹åŒ–Googleæœç´¢æ”¶é›†å™¨
        try:
            self.google_collector = GoogleSearchCollector()
            if self.google_collector.has_api_key:
                self.collectors['google'] = self.google_collector
                print("âœ… Googleæœç´¢æ”¶é›†å™¨å·²å¯ç”¨")
            else:
                print("âš ï¸ Googleæœç´¢æ”¶é›†å™¨æœªé…ç½®APIå¯†é’¥ï¼Œå·²è·³è¿‡")
                self.google_collector = None
        except Exception as e:
            print(f"âš ï¸ Googleæœç´¢æ”¶é›†å™¨ä¸å¯ç”¨: {str(e)}")
            self.google_collector = None
            
        # å°è¯•åˆå§‹åŒ–Braveæœç´¢æ”¶é›†å™¨
        try:
            self.brave_collector = BraveSearchCollector()
            if self.brave_collector.has_api_key:
                self.collectors['brave'] = self.brave_collector
                print("âœ… Braveæœç´¢æ”¶é›†å™¨å·²å¯ç”¨")
            else:
                print("âš ï¸ Braveæœç´¢æ”¶é›†å™¨æœªé…ç½®APIå¯†é’¥ï¼Œå·²è·³è¿‡")
                self.brave_collector = None
        except Exception as e:
            print(f"âš ï¸ Braveæœç´¢æ”¶é›†å™¨ä¸å¯ç”¨: {str(e)}")
            self.brave_collector = None
        
        # ğŸš€ åˆå§‹åŒ–å¹¶è¡Œæ•°æ®æ”¶é›†å™¨ï¼ˆæ–°å¢ï¼‰
        self.parallel_data_collector = ParallelDataCollector(self.collectors)
        
        # ğŸš€ åˆå§‹åŒ–å¹¶è¡Œå¤„ç†å™¨
        configs = ParallelNewsProcessor.get_preset_configs()
        config_dict = configs.get(parallel_config, configs["balanced"])
        self.parallel_processor = ParallelNewsProcessor(self.llm_processor, config_dict)
        
        self.max_iterations = 5  # æœ€å¤§è¿­ä»£æ¬¡æ•°
        self.knowledge_gaps = []  # çŸ¥è¯†ç¼ºå£è®°å½•
        self.search_history = []  # æœç´¢å†å²
        self.detailed_analysis_mode = True  # è¯¦ç»†åˆ†ææ¨¡å¼ï¼Œç”Ÿæˆæ›´é•¿æ›´æ·±å…¥çš„å†…å®¹
        
        print(f"ğŸ” å·²å¯ç”¨ {len(self.collectors)} ä¸ªæœç´¢æ¸ é“: {', '.join(self.collectors.keys())}")
        print(f"âš¡ å¹¶è¡Œå¤„ç†å™¨é…ç½®: {parallel_config}")
        print(f"ğŸš€ å¹¶è¡Œæ•°æ®æ”¶é›†å™¨å·²åˆå§‹åŒ–")
    
    def multi_channel_search(self, query, max_results=5):
        """
        å¤šæ¸ é“æœç´¢æ–¹æ³•ï¼Œæ•´åˆå¤šä¸ªæœç´¢å¼•æ“çš„ç»“æœ
        """
        all_results = []
        used_urls = set()  # ç”¨äºå»é‡
        
        for name, collector in self.collectors.items():
            try:
                print(f"  ğŸ” {name}æœç´¢: {query[:50]}...")
                
                if name == 'tavily':
                    results = collector.search(query, max_results=max_results)
                elif name == 'google':
                    results = collector.search(query, max_results=max_results)
                elif name == 'brave':
                    results = collector.search(query, count=max_results)
                else:
                    continue
                    
                if results:
                    # å»é‡å¹¶æ·»åŠ æœç´¢æ¥æºæ ‡è¯†
                    for result in results:
                        url = result.get('url', '')
                        if url and url not in used_urls:
                            result['search_source'] = name
                            all_results.append(result)
                            used_urls.add(url)
                    
                    print(f"    âœ… {name}: è·å¾— {len(results)} æ¡ç»“æœ")
                else:
                    print(f"    âš ï¸ {name}: æ— ç»“æœ")
                    
            except Exception as e:
                print(f"    âŒ {name}æœç´¢å‡ºé”™: {str(e)}")
                continue
        
        print(f"  ğŸ“Š å¤šæ¸ é“æœç´¢å®Œæˆï¼Œå…±è·å¾— {len(all_results)} æ¡å»é‡ç»“æœ")
        return all_results
        
    def generate_initial_queries(self, topic, days=7, companies=None):
        """
        ç¬¬ä¸€æ­¥ï¼šæ™ºèƒ½æŸ¥è¯¢ç”Ÿæˆ
        åˆ†æç”¨æˆ·éœ€æ±‚ï¼Œç”Ÿæˆåˆå§‹æœç´¢ç­–ç•¥
        """
        print(f"\nğŸ§  [æ€è€ƒé˜¶æ®µ] æ­£åœ¨åˆ†æ'{topic}'è¡Œä¸šæŠ¥å‘Šéœ€æ±‚...")
        
        # è®¡ç®—æ—¶é—´èŒƒå›´
        from datetime import datetime, timedelta
        today = datetime.now()
        
        # æ„å»ºåˆå§‹æŸ¥è¯¢ç”Ÿæˆæç¤º
        query_prompt = f"""
        ä½œä¸ºä¸€ä¸ªä¸“ä¸šçš„è¡Œä¸šåˆ†æå¸ˆï¼Œæˆ‘éœ€è¦ä¸º'{topic}'è¡Œä¸šç”Ÿæˆä¸€ä»½å…¨é¢çš„æœ€æ–°åŠ¨æ€æŠ¥å‘Šã€‚
        è¯·å¸®æˆ‘åˆ†æè¿™ä¸ªä¸»é¢˜çš„æ·±åº¦å’Œå¹¿åº¦ï¼Œç„¶åç”Ÿæˆä¸€ç³»åˆ—åˆå§‹æœç´¢æŸ¥è¯¢ç­–ç•¥ã€‚
        
        âš ï¸ **é‡è¦æ—¶é—´è¦æ±‚**ï¼šæ‰€æœ‰æŸ¥è¯¢å¿…é¡»èšç„¦äº{today.year}å¹´æœ€æ–°ä¿¡æ¯ï¼Œç‰¹åˆ«æ˜¯æœ€è¿‘{days}å¤©çš„åŠ¨æ€ï¼
        
        åˆ†æè¦æ±‚ï¼š
        1. è¿™ä¸ªè¡Œä¸šçš„æ ¸å¿ƒå…³æ³¨ç‚¹æ˜¯ä»€ä¹ˆï¼Ÿ
        2. å½“å‰å¯èƒ½çš„çƒ­ç‚¹è¯é¢˜æœ‰å“ªäº›ï¼Ÿ
        3. éœ€è¦ä»å“ªäº›è§’åº¦æ¥å…¨é¢äº†è§£è¿™ä¸ªè¡Œä¸šï¼Ÿ
        4. ä»€ä¹ˆç±»å‹çš„ä¿¡æ¯å¯¹è¯»è€…æœ€æœ‰ä»·å€¼ï¼Ÿ
        
        è¯·ç”Ÿæˆä»¥ä¸‹ç±»å‹çš„æœç´¢æŸ¥è¯¢ï¼ˆæ¯ä¸ªæŸ¥è¯¢éƒ½è¦åŒ…å«æ—¶é—´é™åˆ¶ï¼‰ï¼š
        - é‡å¤§äº‹ä»¶ç±»ï¼ˆ4-6ä¸ªæŸ¥è¯¢ï¼‰- å¿…é¡»åŒ…å«"{today.year}å¹´æœ€æ–°"ã€"recent"ç­‰æ—¶é—´è¯
        - æŠ€æœ¯åˆ›æ–°ç±»ï¼ˆ3-4ä¸ªæŸ¥è¯¢ï¼‰- å¿…é¡»åŒ…å«"{today.year}å¹´æ–°æŠ€æœ¯"ã€"latest innovation"ç­‰
        - æŠ•èµ„åŠ¨æ€ç±»ï¼ˆ3-4ä¸ªæŸ¥è¯¢ï¼‰- å¿…é¡»åŒ…å«"{today.year}å¹´æŠ•èµ„"ã€"recent funding"ç­‰
        - æ”¿ç­–ç›‘ç®¡ç±»ï¼ˆ2-3ä¸ªæŸ¥è¯¢ï¼‰- å¿…é¡»åŒ…å«"{today.year}å¹´æ”¿ç­–"ã€"latest policy"ç­‰
        - è¡Œä¸šè¶‹åŠ¿ç±»ï¼ˆ3-4ä¸ªæŸ¥è¯¢ï¼‰- å¿…é¡»åŒ…å«"{today.year}å¹´è¶‹åŠ¿"ã€"current trends"ç­‰
        
        æ—¶é—´èŒƒå›´ï¼šæœ€è¿‘{days}å¤©ï¼Œé‡ç‚¹å…³æ³¨{today.strftime('%Yå¹´%mæœˆ')}
        {'é‡ç‚¹å…³æ³¨å…¬å¸ï¼š' + ', '.join(companies) if companies else ''}
        
        è¯·ç¡®ä¿æ¯ä¸ªæŸ¥è¯¢éƒ½åŒ…å«æ˜ç¡®çš„æ—¶é—´é™åˆ¶è¯æ±‡ï¼Œé¿å…æœç´¢åˆ°è¿‡æ—¶ä¿¡æ¯ã€‚
        """
        
        system_msg = f"ä½ æ˜¯ä¸€ä½ç»éªŒä¸°å¯Œçš„{topic}è¡Œä¸šç ”ç©¶ä¸“å®¶ï¼Œæ“…é•¿åˆ¶å®šå…¨é¢çš„ä¿¡æ¯æ”¶é›†ç­–ç•¥ã€‚"
        
        try:
            if not self.llm_processor:
                print("âš ï¸ [é™çº§æ¨¡å¼] LLMå¤„ç†å™¨ä¸å¯ç”¨ï¼Œä½¿ç”¨é»˜è®¤æœç´¢ç­–ç•¥")
                return self._get_fallback_queries(topic, days, companies)
                
            response = self.llm_processor.call_llm_api(query_prompt, system_msg, max_tokens=4000)
            # è§£ææŸ¥è¯¢ç­–ç•¥ï¼ˆè¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…å¯ä»¥è§£æJSONï¼‰
            print(f"âœ… [æŸ¥è¯¢ç­–ç•¥] å·²ç”Ÿæˆ{topic}è¡Œä¸šçš„å¤šç»´åº¦æœç´¢ç­–ç•¥")
            return self._parse_query_strategy(response, topic, days, companies)
        except Exception as e:
            print(f"âŒ [é”™è¯¯] ç”ŸæˆæŸ¥è¯¢ç­–ç•¥æ—¶å‡ºé”™: {str(e)}")
            print("ğŸ”„ [é™çº§æ¨¡å¼] åˆ‡æ¢åˆ°é»˜è®¤æœç´¢ç­–ç•¥")
            return self._get_fallback_queries(topic, days, companies)
    
    def _parse_query_strategy(self, response, topic, days, companies):
        """è§£ææŸ¥è¯¢ç­–ç•¥å“åº” - ğŸš€ å¹¶è¡Œå¤šæ¸ é“æ•´åˆæœç´¢ï¼ˆç¬¬2æ­¥ä¼˜åŒ–ï¼‰"""
        print(f"ğŸ”„ [å¹¶è¡Œå¤šæ¸ é“æ•´åˆ] å¼€å§‹å¹¶è¡Œæ‰§è¡Œå¤šä¸ªæœç´¢å¼•æ“...")
        
        # ğŸš€ ä½¿ç”¨å¹¶è¡Œæ•°æ®æ”¶é›†å™¨æ‰§è¡Œæ‰€æœ‰æœç´¢å¼•æ“
        merged_data = self.parallel_data_collector.parallel_comprehensive_search(topic, days)
        
        print(f"ğŸ“Š [å¹¶è¡Œæ•´åˆå®Œæˆ] æ€»è®¡è·å¾— {merged_data['total_count']} æ¡å»é‡ä¸”æ—¶é—´è¿‡æ»¤åçš„ç»“æœ")
        return merged_data
    
    def _get_fallback_queries(self, topic, days, companies):
        """å¤‡ç”¨æŸ¥è¯¢ç­–ç•¥ï¼Œå½“LLMä¸å¯ç”¨æ—¶ä½¿ç”¨"""
        print(f"ğŸ›¡ï¸ [å¤‡ç”¨ç­–ç•¥] ä½¿ç”¨é¢„è®¾çš„{topic}è¡Œä¸šæœç´¢ç­–ç•¥")
        
        # ä½¿ç”¨ç°æœ‰çš„æœç´¢æ–¹æ³•ï¼Œä½†ç¡®ä¿æ—¶é—´èŒƒå›´æ­£ç¡®
        try:
            return self.tavily_collector.get_industry_news_direct(topic, days)
        except Exception as e:
            print(f"âŒ [é”™è¯¯] å¤‡ç”¨æŸ¥è¯¢ç­–ç•¥ä¹Ÿå¤±è´¥: {str(e)}")
            # è¿”å›ç©ºæ•°æ®ç»“æ„ï¼Œé¿å…ç¨‹åºå´©æºƒ
            return {
                "breaking_news": [],
                "innovation_news": [],
                "investment_news": [],
                "policy_news": [],
                "trend_news": [],
                "company_news": [],
                "total_count": 0
            }
    
    def reflect_on_information_gaps(self, collected_data, topic, days=7):
        """
        ç¬¬ä¸‰æ­¥ï¼šå¤šç»´åº¦æ™ºèƒ½åæ€ä¸çŸ¥è¯†ç¼ºå£åˆ†æ  
        åŸºäºæ•°é‡ã€è´¨é‡ã€è¦†ç›–åº¦ã€æ—¶æ•ˆæ€§ã€æƒå¨æ€§ç­‰å¤šä¸ªç»´åº¦åˆ†æä¿¡æ¯å®Œæ•´æ€§
        """
        print(f"\nğŸ¤” [å¤šç»´åº¦åæ€] æ­£åœ¨æ·±åº¦åˆ†æ{topic}è¡Œä¸šä¿¡æ¯è´¨é‡...")
        
        # ğŸ”¢ ç»´åº¦1ï¼šæ•°é‡åˆ†æ
        quantity_score, quantity_details = self._analyze_quantity_dimension(collected_data, topic, days)
        
        # ğŸ¯ ç»´åº¦2ï¼šè´¨é‡åˆ†æ 
        quality_score, quality_details = self._analyze_quality_dimension(collected_data, topic, days)
        
        # ğŸ“Š ç»´åº¦3ï¼šè¦†ç›–åº¦åˆ†æ
        coverage_score, coverage_details = self._analyze_coverage_dimension(collected_data, topic, days)
        
        # â° ç»´åº¦4ï¼šæ—¶æ•ˆæ€§åˆ†æ
        timeliness_score, timeliness_details = self._analyze_timeliness_dimension(collected_data, topic, days)
        
        # ğŸ›ï¸ ç»´åº¦5ï¼šæƒå¨æ€§åˆ†æ
        authority_score, authority_details = self._analyze_authority_dimension(collected_data, topic, days)
        
        # ğŸ“ˆ ç»¼åˆè¯„åˆ†è®¡ç®—ï¼ˆåŠ æƒå¹³å‡ï¼‰
        weights = {
            'quantity': 0.15,    # æ•°é‡æƒé‡é™ä½
            'quality': 0.30,     # è´¨é‡æƒé‡æœ€é«˜
            'coverage': 0.25,    # è¦†ç›–åº¦æƒé‡è¾ƒé«˜
            'timeliness': 0.20,  # æ—¶æ•ˆæ€§æƒé‡ä¸­ç­‰
            'authority': 0.10    # æƒå¨æ€§æƒé‡è¾ƒä½ï¼ˆå› ä¸ºæœç´¢å¼•æ“å·²è¿‡æ»¤ï¼‰
        }
        
        total_score = (
            quantity_score * weights['quantity'] +
            quality_score * weights['quality'] +
            coverage_score * weights['coverage'] +
            timeliness_score * weights['timeliness'] +
            authority_score * weights['authority']
        )
        
        # ğŸ“Š è¾“å‡ºè¯¦ç»†è¯„ä¼°æŠ¥å‘Š
        print("=" * 60)
        print(f"ğŸ“Š [{topic}è¡Œä¸šä¿¡æ¯] å¤šç»´åº¦è´¨é‡è¯„ä¼°æŠ¥å‘Š")
        print("=" * 60)
        print(f"ğŸ”¢ æ•°é‡ç»´åº¦: {quantity_score:.1f}/10 - {quantity_details}")
        print(f"ğŸ¯ è´¨é‡ç»´åº¦: {quality_score:.1f}/10 - {quality_details}")
        print(f"ğŸ“Š è¦†ç›–ç»´åº¦: {coverage_score:.1f}/10 - {coverage_details}")
        print(f"â° æ—¶æ•ˆç»´åº¦: {timeliness_score:.1f}/10 - {timeliness_details}")
        print(f"ğŸ›ï¸ æƒå¨ç»´åº¦: {authority_score:.1f}/10 - {authority_details}")
        print("-" * 60)
        print(f"ğŸ“ˆ ç»¼åˆè¯„åˆ†: {total_score:.1f}/10")
        print("=" * 60)
        
        # ğŸ¯ å†³ç­–é€»è¾‘ï¼šä¸¥æ ¼çš„è´¨é‡é—¨æ§›
        QUALITY_THRESHOLD = 7.0  # ç»¼åˆè¯„åˆ†é—¨æ§›ï¼š7.0/10
        MINIMUM_QUALITY_SCORE = 6.0  # è´¨é‡ç»´åº¦æœ€ä½è¦æ±‚
        MINIMUM_COVERAGE_SCORE = 6.0  # è¦†ç›–åº¦æœ€ä½è¦æ±‚
        
        # æ£€æŸ¥æ˜¯å¦æ»¡è¶³è´¨é‡è¦æ±‚
        quality_sufficient = (
            total_score >= QUALITY_THRESHOLD and
            quality_score >= MINIMUM_QUALITY_SCORE and
            coverage_score >= MINIMUM_COVERAGE_SCORE
        )
        
        if quality_sufficient:
            print(f"âœ… [è´¨é‡è¾¾æ ‡] ç»¼åˆè¯„åˆ†{total_score:.1f}/10 â‰¥ {QUALITY_THRESHOLD}ï¼Œä¿¡æ¯è´¨é‡å……åˆ†")
            print("ğŸ¯ [å†³ç­–] å¼€å§‹ç”Ÿæˆé«˜è´¨é‡æŠ¥å‘Š")
            return [], True
        else:
            # ç”Ÿæˆå…·ä½“çš„ç¼ºå£åˆ†æ
            gaps = self._generate_specific_gaps(
                quantity_score, quality_score, coverage_score, 
                timeliness_score, authority_score, topic, days, collected_data
            )
            
            print(f"âš ï¸ [è´¨é‡ä¸è¶³] ç»¼åˆè¯„åˆ†{total_score:.1f}/10 < {QUALITY_THRESHOLD}")
            print("ğŸ”„ [ç»§ç»­è¿­ä»£] è¯†åˆ«å…·ä½“ç¼ºå£ï¼Œè¿›å…¥é’ˆå¯¹æ€§è¡¥å……æœç´¢")
            for i, gap in enumerate(gaps, 1):
                print(f"   {i}. {gap}")
            
            return gaps, False
    
    def _analyze_quantity_dimension(self, collected_data, topic, days):
        """æ•°é‡ç»´åº¦åˆ†æ"""
        info_stats = {
            'breaking_news': len(collected_data.get('breaking_news', [])),
            'innovation_news': len(collected_data.get('innovation_news', [])),
            'investment_news': len(collected_data.get('investment_news', [])),
            'policy_news': len(collected_data.get('policy_news', [])),
            'trend_news': len(collected_data.get('trend_news', [])),
            'company_news': len(collected_data.get('company_news', []))
        }
        
        total_items = sum(info_stats.values())
        
        # åŠ¨æ€æ ‡å‡†
        min_total = max(20, days * 3)  # æé«˜è¦æ±‚
        min_per_category = max(3, days // 3)  # æé«˜è¦æ±‚
        
        # æ•°é‡è¯„åˆ†
        total_score = min(10, (total_items / min_total) * 10)
        category_scores = [min(10, (count / min_per_category) * 10) for count in info_stats.values()]
        avg_category_score = sum(category_scores) / len(category_scores) if category_scores else 0
        
        quantity_score = (total_score * 0.6 + avg_category_score * 0.4)
        
        details = f"æ€»é‡{total_items}/{min_total}, å¹³å‡æ¯ç±»{total_items/6:.1f}/{min_per_category}"
        
        return quantity_score, details
    
    def _analyze_quality_dimension(self, collected_data, topic, days):
        """è´¨é‡ç»´åº¦åˆ†æ - ä½¿ç”¨LLMæ·±åº¦è¯„ä¼°å†…å®¹è´¨é‡"""
        if not self.llm_processor:
            return 7.0, "LLMä¸å¯ç”¨ï¼Œè·³è¿‡è´¨é‡åˆ†æ"
        
        try:
            # é‡‡æ ·åˆ†æï¼ˆé¿å…tokenè¿‡å¤šï¼‰
            sample_data = self._sample_data_for_analysis(collected_data, max_items=15)
            
            quality_prompt = f"""
            ä½œä¸º{topic}è¡Œä¸šèµ„æ·±åˆ†æå¸ˆï¼Œè¯·æ·±åº¦è¯„ä¼°ä»¥ä¸‹æ”¶é›†çš„ä¿¡æ¯è´¨é‡ï¼š
            
            æ•°æ®æ ·æœ¬æ¦‚è§ˆï¼š
            {self._format_data_sample(sample_data)}
            
            è¯·ä»ä»¥ä¸‹5ä¸ªè´¨é‡ç»´åº¦è¯„åˆ†ï¼ˆ1-10åˆ†ï¼‰ï¼š
            
            1. **ç›¸å…³æ€§** (1-10åˆ†)ï¼šä¿¡æ¯ä¸{topic}è¡Œä¸šçš„ç›¸å…³ç¨‹åº¦
            2. **æ·±åº¦æ€§** (1-10åˆ†)ï¼šä¿¡æ¯çš„æ·±åº¦å’Œåˆ†æä»·å€¼
            3. **å‡†ç¡®æ€§** (1-10åˆ†)ï¼šä¿¡æ¯çš„å‡†ç¡®æ€§å’Œå¯ä¿¡åº¦
            4. **å®Œæ•´æ€§** (1-10åˆ†)ï¼šä¿¡æ¯æ˜¯å¦å®Œæ•´ï¼Œæœ‰æ— å…³é”®ç¼ºå¤±
            5. **ä»·å€¼æ€§** (1-10åˆ†)ï¼šå¯¹æŠ¥å‘Šè¯»è€…çš„ä»·å€¼ç¨‹åº¦
            
            è¯„ä¼°è¦æ±‚ï¼š
            - ä¸¥æ ¼æŒ‰ç…§ä¸“ä¸šæ ‡å‡†è¯„åˆ†ï¼Œä¸è¦è¿‡äºå®½æ¾
            - é‡ç‚¹å…³æ³¨ä¿¡æ¯çš„å®è´¨å†…å®¹ï¼Œè€Œéæ•°é‡
            - è€ƒè™‘ä¿¡æ¯æ˜¯å¦è¶³ä»¥æ”¯æ’‘é«˜è´¨é‡çš„è¡Œä¸šåˆ†ææŠ¥å‘Š
            
            è¯·åªè¿”å›æ•°å­—è¯„åˆ†å’Œç®€çŸ­è¯´æ˜ï¼Œæ ¼å¼ï¼š
            ç›¸å…³æ€§: X.X/10 - è¯´æ˜
            æ·±åº¦æ€§: X.X/10 - è¯´æ˜  
            å‡†ç¡®æ€§: X.X/10 - è¯´æ˜
            å®Œæ•´æ€§: X.X/10 - è¯´æ˜
            ä»·å€¼æ€§: X.X/10 - è¯´æ˜
            ç»¼åˆè´¨é‡: X.X/10
            """
            
            response = self.llm_processor.call_llm_api(
                quality_prompt,
                f"ä½ æ˜¯{topic}è¡Œä¸šçš„èµ„æ·±åˆ†æå¸ˆå’Œè´¨é‡è¯„ä¼°ä¸“å®¶",
                max_tokens=1500
            )
            
            # è§£æLLMå“åº”è·å–è´¨é‡è¯„åˆ†
            quality_score = self._parse_quality_response(response)
            details = f"LLMæ·±åº¦è¯„ä¼°ï¼Œç»¼åˆå¾—åˆ†{quality_score:.1f}/10"
            
            return quality_score, details
            
        except Exception as e:
            print(f"âš ï¸ [è´¨é‡åˆ†æå¤±è´¥] {str(e)}")
            # é™çº§ä¸ºç®€å•è´¨é‡åˆ†æ
            return self._simple_quality_analysis(collected_data, topic)
    
    def _analyze_coverage_dimension(self, collected_data, topic, days):
        """è¦†ç›–åº¦ç»´åº¦åˆ†æ"""
        required_categories = ['breaking_news', 'innovation_news', 'investment_news', 'policy_news', 'trend_news']
        
        coverage_scores = []
        for category in required_categories:
            items = collected_data.get(category, [])
            if len(items) >= 3:
                coverage_scores.append(10)
            elif len(items) >= 2:
                coverage_scores.append(7)
            elif len(items) >= 1:
                coverage_scores.append(4)
            else:
                coverage_scores.append(0)
        
        coverage_score = sum(coverage_scores) / len(coverage_scores)
        covered_categories = sum(1 for score in coverage_scores if score >= 4)
        
        details = f"{covered_categories}/{len(required_categories)}ä¸ªæ ¸å¿ƒé¢†åŸŸè¦†ç›–"
        
        return coverage_score, details
    
    def _analyze_timeliness_dimension(self, collected_data, topic, days):
        """æ—¶æ•ˆæ€§ç»´åº¦åˆ†æ"""
        from datetime import datetime, timedelta
        today = datetime.now()
        target_date = today - timedelta(days=days)
        
        # ç®€åŒ–æ—¶æ•ˆæ€§åˆ†æï¼ˆå®é™…å¯ä»¥æ£€æŸ¥å‘å¸ƒæ—¥æœŸï¼‰
        total_items = sum(len(v) for v in collected_data.values() if isinstance(v, list))
        
        if total_items >= 15:
            timeliness_score = 8.5  # å‡è®¾æœç´¢å¼•æ“å·²ç»è¿‡æ»¤äº†æ—¶é—´
        elif total_items >= 10:
            timeliness_score = 7.0
        else:
            timeliness_score = 5.0
        
        details = f"åŸºäº{days}å¤©æ—¶é—´çª—å£ï¼Œé¢„ä¼°æ—¶æ•ˆæ€§è‰¯å¥½"
        
        return timeliness_score, details
    
    def _analyze_authority_dimension(self, collected_data, topic, days):
        """æƒå¨æ€§ç»´åº¦åˆ†æ"""
        # ç®€åŒ–åˆ†æï¼šåŸºäºä¿¡æ¯æºå¤šæ ·æ€§
        sources = set()
        for category_items in collected_data.values():
            if isinstance(category_items, list):
                for item in category_items:
                    if item.get('url'):
                        domain = item['url'].split('/')[2] if '/' in item['url'] else item['url']
                        sources.add(domain)
        
        source_count = len(sources)
        if source_count >= 10:
            authority_score = 9.0
        elif source_count >= 6:
            authority_score = 7.5
        elif source_count >= 3:
            authority_score = 6.0
        else:
            authority_score = 4.0
        
        details = f"{source_count}ä¸ªä¸åŒä¿¡æ¯æºï¼Œå¤šæ ·æ€§è‰¯å¥½"
        
        return authority_score, details
    
    def _sample_data_for_analysis(self, collected_data, max_items=15):
        """ä¸ºLLMåˆ†æé‡‡æ ·æ•°æ®"""
        sampled = {}
        remaining_quota = max_items
        
        categories = ['breaking_news', 'innovation_news', 'investment_news', 'policy_news', 'trend_news']
        items_per_category = max_items // len(categories)
        
        for category in categories:
            items = collected_data.get(category, [])
            if items and remaining_quota > 0:
                sample_size = min(len(items), items_per_category, remaining_quota)
                sampled[category] = items[:sample_size]
                remaining_quota -= sample_size
        
        return sampled
    
    def _format_data_sample(self, sample_data):
        """æ ¼å¼åŒ–æ•°æ®æ ·æœ¬ç”¨äºLLMåˆ†æ"""
        formatted = []
        for category, items in sample_data.items():
            if items:
                formatted.append(f"\n{category}ç±»åˆ« ({len(items)}æ¡):")
                for i, item in enumerate(items[:3], 1):  # æ¯ç±»åˆ«æœ€å¤šæ˜¾ç¤º3æ¡
                    title = item.get('title', 'æ— æ ‡é¢˜')
                    formatted.append(f"  {i}. {title[:100]}...")
        
        return '\n'.join(formatted)
    
    def _parse_quality_response(self, response):
        """è§£æLLMçš„è´¨é‡è¯„ä¼°å“åº”"""
        try:
            lines = response.split('\n')
            scores = []
            
            for line in lines:
                if 'ç»¼åˆè´¨é‡:' in line or 'Overall:' in line:
                    import re
                    match = re.search(r'(\d+\.?\d*)', line)
                    if match:
                        return float(match.group(1))
                
                # æå–å„ç»´åº¦åˆ†æ•°
                if any(keyword in line for keyword in ['ç›¸å…³æ€§:', 'æ·±åº¦æ€§:', 'å‡†ç¡®æ€§:', 'å®Œæ•´æ€§:', 'ä»·å€¼æ€§:']):
                    import re
                    match = re.search(r'(\d+\.?\d*)', line)
                    if match:
                        scores.append(float(match.group(1)))
            
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç»¼åˆåˆ†æ•°ï¼Œè®¡ç®—å¹³å‡å€¼
            if scores:
                return sum(scores) / len(scores)
            else:
                return 6.0  # é»˜è®¤ä¸­ç­‰åˆ†æ•°
                
        except Exception as e:
            print(f"âš ï¸ [è§£æè´¨é‡å“åº”å¤±è´¥] {str(e)}")
            return 6.0
    
    def _simple_quality_analysis(self, collected_data, topic):
        """ç®€å•è´¨é‡åˆ†æï¼ˆLLMä¸å¯ç”¨æ—¶çš„å¤‡é€‰æ–¹æ¡ˆï¼‰"""
        total_items = sum(len(v) for v in collected_data.values() if isinstance(v, list))
        
        # åŸºäºæ•°é‡å’Œåˆ†å¸ƒçš„ç®€å•è¯„ä¼°
        if total_items >= 20:
            quality_score = 7.5
        elif total_items >= 15:
            quality_score = 6.5
        elif total_items >= 10:
            quality_score = 5.5
        else:
            quality_score = 4.0
        
        details = f"ç®€å•åˆ†æï¼ŒåŸºäº{total_items}æ¡ä¿¡æ¯"
        return quality_score, details
    
    def _generate_specific_gaps(self, quantity_score, quality_score, coverage_score, 
                              timeliness_score, authority_score, topic, days, collected_data):
        """ç”Ÿæˆå…·ä½“çš„çŸ¥è¯†ç¼ºå£åˆ—è¡¨"""
        gaps = []
        
        if quantity_score < 6.0:
            gaps.append(f"æ•°é‡ä¸è¶³ï¼šéœ€è¦æ›´å¤š{topic}è¡Œä¸šä¿¡æ¯ï¼Œå½“å‰ä¿¡æ¯é‡ä¸è¶³ä»¥æ”¯æ’‘å…¨é¢åˆ†æ")
        
        if quality_score < 6.0:
            gaps.append(f"è´¨é‡ä¸è¾¾æ ‡ï¼šéœ€è¦æ›´æ·±å…¥ã€æ›´æƒå¨çš„{topic}è¡Œä¸šåˆ†æå†…å®¹")
        
        if coverage_score < 6.0:
            # åˆ†æå…·ä½“ç¼ºå°‘å“ªäº›ç±»åˆ«
            required_categories = ['breaking_news', 'innovation_news', 'investment_news', 'policy_news', 'trend_news']
            missing_categories = []
            for category in required_categories:
                if len(collected_data.get(category, [])) < 2:
                    missing_categories.append(category)
            
            if missing_categories:
                category_names = {
                    'breaking_news': 'é‡å¤§æ–°é—»äº‹ä»¶',
                    'innovation_news': 'æŠ€æœ¯åˆ›æ–°åŠ¨æ€', 
                    'investment_news': 'æŠ•èµ„èèµ„ä¿¡æ¯',
                    'policy_news': 'æ”¿ç­–ç›‘ç®¡å˜åŒ–',
                    'trend_news': 'è¡Œä¸šå‘å±•è¶‹åŠ¿'
                }
                missing_names = [category_names.get(cat, cat) for cat in missing_categories]
                gaps.append(f"è¦†ç›–ä¸å…¨ï¼šç¼ºå°‘{', '.join(missing_names)}ç­‰å…³é”®é¢†åŸŸä¿¡æ¯")
        
        if timeliness_score < 6.0:
            gaps.append(f"æ—¶æ•ˆæ€§ä¸è¶³ï¼šéœ€è¦æ›´å¤š{days}å¤©å†…çš„æœ€æ–°{topic}è¡Œä¸šåŠ¨æ€")
        
        if authority_score < 6.0:
            gaps.append(f"æƒå¨æ€§å¾…æå‡ï¼šéœ€è¦æ›´å¤šæƒå¨åª’ä½“å’Œå®˜æ–¹æ¸ é“çš„{topic}è¡Œä¸šä¿¡æ¯")
        
        # å¦‚æœæ²¡æœ‰å…·ä½“ç¼ºå£ï¼Œæ·»åŠ é€šç”¨ç¼ºå£
        if not gaps:
            gaps.append(f"éœ€è¦è¡¥å……æ›´å¤šé«˜è´¨é‡çš„{topic}è¡Œä¸šä¿¡æ¯ä»¥æå‡æŠ¥å‘Šè´¨é‡")
        
        return gaps
    
    def generate_targeted_queries(self, gaps, topic, days=7):
        """
        ç¬¬å››æ­¥ï¼šè¿­ä»£ä¼˜åŒ–æœç´¢ - ğŸš€ å¹¶è¡Œæ‰§è¡Œé’ˆå¯¹æ€§æŸ¥è¯¢ï¼ˆç¬¬4æ­¥ä¼˜åŒ–ï¼‰
        æ ¹æ®çŸ¥è¯†ç¼ºå£ç”Ÿæˆé’ˆå¯¹æ€§æœç´¢
        """
        print(f"\nğŸ¯ [å¹¶è¡Œä¼˜åŒ–æœç´¢] æ­£åœ¨ä¸º{topic}è¡Œä¸šç”Ÿæˆé’ˆå¯¹æ€§æŸ¥è¯¢...")
        
        if not gaps:
            return {}
            
        # è®¡ç®—å½“å‰æ—¶é—´å’Œæœç´¢èŒƒå›´
        from datetime import datetime, timedelta
        today = datetime.now()
        
        # ğŸ¯ æ™ºèƒ½æŸ¥è¯¢ç”Ÿæˆ (å¯ä»¥é€‰æ‹©å¯ç”¨LLMç”Ÿæˆï¼Œæˆ–ä½¿ç”¨é¢„è®¾æŸ¥è¯¢)
        try:
            if self.llm_processor and gaps:
                # å°è¯•ä½¿ç”¨LLMç”Ÿæˆé’ˆå¯¹æ€§æŸ¥è¯¢
                targeted_queries = self._generate_ai_targeted_queries(gaps, topic, days)
                if targeted_queries:
                    print(f"ğŸ¤– [AIæŸ¥è¯¢ç”Ÿæˆ] æˆåŠŸç”Ÿæˆ{len(targeted_queries)}ä¸ªAIé’ˆå¯¹æ€§æŸ¥è¯¢")
                    # ğŸš€ ä½¿ç”¨å¹¶è¡Œæ•°æ®æ”¶é›†å™¨æ‰§è¡ŒæŸ¥è¯¢
                    return self.parallel_data_collector.parallel_targeted_search(targeted_queries, topic)
        except Exception as e:
            print(f"âš ï¸ [AIæŸ¥è¯¢ç”Ÿæˆå¤±è´¥] {str(e)}ï¼Œåˆ‡æ¢åˆ°é¢„è®¾æŸ¥è¯¢")
        
        # ğŸ”„ å¤‡ç”¨é¢„è®¾æŸ¥è¯¢ç­–ç•¥
        print(f"ğŸ” [é¢„è®¾å¹¶è¡Œæœç´¢] ä½¿ç”¨é¢„è®¾æŸ¥è¯¢è¿›è¡Œå¹¶è¡Œæœç´¢...")
        
        # é‡å¤§äº‹ä»¶ä¸“ç”¨æŸ¥è¯¢
        major_event_queries = [
            f"{topic} é‡å¤§äº‹ä»¶ çªå‘ {today.year}å¹´ æœ€æ–° breaking news major event",
            f"{topic} è¡Œä¸šéœ‡åŠ¨ é‡ç£…æ¶ˆæ¯ {today.year} æœ€è¿‘{days}å¤© industry shock major news",
            f"{topic} å¹¶è´­ æ”¶è´­ åˆå¹¶ {today.year}å¹´ æœ€æ–° merger acquisition latest",
            f"{topic} é‡å¤§å‘å¸ƒ äº§å“å‘å¸ƒ {today.year} æœ€æ–° major launch product release",
            f"{topic} ç›‘ç®¡ æ”¿ç­–å˜åŒ– {today.year}å¹´ æœ€æ–° regulation policy change latest"
        ]
        
        # é€šç”¨è¡¥å……æŸ¥è¯¢
        general_queries = [
            f"{topic} {today.year}å¹´æœ€æ–°å‘å±• recent developments latest {days} days",
            f"{topic} industry news {today.strftime('%B %Y')} latest recent {days} days",
            f"{topic} è¡Œä¸šåŠ¨æ€ {today.year} æ–°é—» trends æœ€è¿‘{days}å¤©",
            f"{topic} äº‰è®® è´¨ç–‘ é£é™© æŒ‘æˆ˜ {today.year}å¹´ ä¸åŒè§‚ç‚¹ criticism"
        ]
        
        # åˆå¹¶æ‰€æœ‰æŸ¥è¯¢
        all_queries = major_event_queries + general_queries
        
        # ğŸš€ ä½¿ç”¨å¹¶è¡Œæ•°æ®æ”¶é›†å™¨æ‰§è¡Œæ‰€æœ‰æŸ¥è¯¢
        return self.parallel_data_collector.parallel_targeted_search(all_queries, topic)
    
    def _generate_ai_targeted_queries(self, gaps, topic, days=7):
        """ä½¿ç”¨AIç”Ÿæˆé’ˆå¯¹æ€§æŸ¥è¯¢"""
        from datetime import datetime
        today = datetime.now()
        
        targeted_prompt = f"""
        åŸºäºä»¥ä¸‹{topic}è¡Œä¸šæŠ¥å‘Šçš„çŸ¥è¯†ç¼ºå£åˆ†æï¼š
        
        {gaps[0]}
        
        è¯·åˆ†æå…·ä½“çš„ä¿¡æ¯ç¼ºå£ï¼Œå¹¶ç”Ÿæˆ3-5ä¸ªé’ˆå¯¹æ€§çš„æœç´¢æŸ¥è¯¢æ¥è¡¥å……è¿™äº›ç¼ºå£ã€‚
        
        âš ï¸ **é‡è¦æ—¶é—´è¦æ±‚**ï¼šæŸ¥è¯¢å¿…é¡»åŒ…å«æœ€æ–°æ—¶é—´é™åˆ¶ï¼Œè·å–{today.strftime('%Yå¹´%mæœˆ')}çš„æœ€æ–°ä¿¡æ¯ï¼
        
        ğŸ¯ **é’ˆå¯¹æ€§æœç´¢ç­–ç•¥**ï¼š
        - å¦‚æœç¼ºå£åˆ†ææåˆ°éœ€è¦"è§‚ç‚¹å¯¹æ¯”åˆ†æ"ï¼Œè¯·ä¸“é—¨è®¾è®¡1-2ä¸ªæŸ¥è¯¢æ¥è·å–ä¸åŒè§‚ç‚¹
        - æœç´¢å…³é”®è¯åŒ…æ‹¬ï¼šäº‰è®®ã€è´¨ç–‘ã€æ‰¹è¯„ã€åå¯¹ã€é£é™©ã€æŒ‘æˆ˜ã€ä¸åŒè§‚ç‚¹ã€alternative view
        - å¹³è¡¡æ­£é¢å’Œè´Ÿé¢ä¿¡æ¯ï¼Œç¡®ä¿å®¢è§‚æ€§
        
        è¯·ç›´æ¥è¾“å‡ºæŸ¥è¯¢åˆ—è¡¨ï¼Œæ¯è¡Œä¸€ä¸ªæŸ¥è¯¢ï¼š
        {topic} [å…·ä½“æœç´¢è¯] {today.year}å¹´ æœ€æ–°
        {topic} [å¦ä¸€ä¸ªæœç´¢è¯] latest {today.strftime('%B %Y')}
        ...
        
        æŸ¥è¯¢è¦æ±‚ï¼š
        1. å…·ä½“ä¸”æœ‰é’ˆå¯¹æ€§ï¼Œç›´æ¥å¯¹åº”è¯†åˆ«å‡ºçš„ç¼ºå£
        2. å¿…é¡»åŒ…å«æ—¶é—´é™åˆ¶è¯ï¼š{today.year}å¹´ã€latestã€recentã€æœ€æ–°ã€æœ€è¿‘
        3. é€‚åˆæœç´¢å¼•æ“æŸ¥è¯¢ï¼Œé¿å…è¿‡äºå¤æ‚çš„è¡¨è¾¾
        4. åŒ…å«è¡Œä¸šå…³é”®è¯ï¼š{topic}
        """
        
        try:
            response = self.llm_processor.call_llm_api(
                targeted_prompt, 
                f"ä½ æ˜¯{topic}è¡Œä¸šçš„æœç´¢ä¸“å®¶ï¼Œæ“…é•¿æ ¹æ®ç¼ºå£åˆ†æç”Ÿæˆç²¾å‡†çš„æœç´¢æŸ¥è¯¢",
                max_tokens=2000
            )
            
            # è§£ææŸ¥è¯¢
            queries = []
            lines = response.split('\n')
            for line in lines:
                line = line.strip()
                if line and topic in line and len(line) > 10 and len(line) < 100:
                    queries.append(line)
            
            return queries[:6]  # æœ€å¤šè¿”å›6ä¸ªæŸ¥è¯¢
            
        except Exception as e:
            print(f"âŒ [AIæŸ¥è¯¢ç”Ÿæˆå¤±è´¥] {str(e)}")
            return []
    

    
    def _merge_data(self, existing_data, new_data):
        """åˆå¹¶æ–°æ•°æ®åˆ°ç°æœ‰æ•°æ®ä¸­ï¼Œåªè¿›è¡ŒURLå»é‡"""
        print(f"ğŸ”„ [æ•°æ®åˆå¹¶] æ­£åœ¨åˆå¹¶æ–°æ•°æ®...")
        
        # åˆ›å»ºåˆå¹¶åçš„æ•°æ®å‰¯æœ¬
        merged_data = existing_data.copy()
        
        for category, new_items in new_data.items():
            if category not in merged_data:
                merged_data[category] = []
            
            # è·å–ç°æœ‰æ•°æ®çš„URLé›†åˆï¼Œç”¨äºåŸºç¡€å»é‡
            existing_urls = set()
            for item in merged_data[category]:
                if item.get('url'):
                    existing_urls.add(item['url'])
            
            # åŸºäºURLçš„åŸºç¡€å»é‡
            url_filtered_items = []
            for new_item in new_items:
                if new_item.get('url') and new_item['url'] not in existing_urls:
                    url_filtered_items.append(new_item)
                    existing_urls.add(new_item['url'])
                elif not new_item.get('url'):  # å¦‚æœæ²¡æœ‰URLï¼ŒåŸºäºæ ‡é¢˜å»é‡
                    title = new_item.get('title', '')
                    existing_titles = [item.get('title', '') for item in merged_data[category]]
                    if title and title not in existing_titles:
                        url_filtered_items.append(new_item)
            
            # ç›´æ¥æ·»åŠ å»é‡åçš„æ–°æ•°æ®
            if url_filtered_items:
                merged_data[category].extend(url_filtered_items)
                print(f"  âœ… {category}: æ–°å¢ {len(url_filtered_items)} æ¡æ•°æ®")
            else:
                print(f"  â– {category}: æ— æ–°å¢æ•°æ®ï¼ˆURLé‡å¤ï¼‰")
        
        # ç»Ÿè®¡åˆå¹¶åçš„æ•°æ®ï¼ˆåªç»Ÿè®¡åˆ—è¡¨ç±»å‹çš„æ•°æ®ï¼‰
        total_before = sum(len(v) for v in existing_data.values() if isinstance(v, list))
        total_after = sum(len(v) for v in merged_data.values() if isinstance(v, list))
        print(f"ğŸ“Š [åˆå¹¶ç»“æœ] æ•°æ®æ€»é‡: {total_before} â†’ {total_after} (+{total_after - total_before})")
        
        return merged_data
    
    def generate_comprehensive_report_with_thinking(self, topic, days=7, companies=None):
        """
        æ™ºèƒ½æŠ¥å‘Šç”Ÿæˆä¸»æµç¨‹ - å¹¶è¡Œç‰ˆæœ¬
        """
        print(f"\nğŸš€ å¼€å§‹ä¸º'{topic}'è¡Œä¸šç”Ÿæˆæ™ºèƒ½åˆ†ææŠ¥å‘Šï¼ˆå¹¶è¡Œç‰ˆæœ¬ï¼‰...")
        print("=" * 60)
        
        # ç¬¬ä¸€æ­¥ï¼šæ™ºèƒ½æŸ¥è¯¢ç”Ÿæˆ
        initial_data = self.generate_initial_queries(topic, days, companies)
        
        # ç¬¬äºŒæ­¥ï¼šä¿¡æ¯æ”¶é›†ï¼ˆä½¿ç”¨ç°æœ‰æ–¹æ³•ï¼‰
        print(f"\nğŸ“Š [ä¿¡æ¯æ”¶é›†] æ­£åœ¨æ”¶é›†{topic}è¡Œä¸šå¤šç»´åº¦ä¿¡æ¯...")
        all_news_data = initial_data
        
        # å¦‚æœæœ‰å…¬å¸åˆ—è¡¨ï¼Œè¡¥å……å…¬å¸ç‰¹å®šä¿¡æ¯
        if companies and isinstance(companies, list):
            print(f"ğŸ¢ [å…¬å¸åˆ†æ] æ­£åœ¨æ”¶é›†{len(companies)}å®¶é‡ç‚¹å…¬å¸ä¿¡æ¯...")
            company_specifics = []
            for company in companies:
                # å°è¯•ä½¿ç”¨å¤šä¸ªæ”¶é›†å™¨è·å–å…¬å¸æ–°é—»
                company_news = []
                
                # å°è¯•ä½¿ç”¨Braveæœç´¢
                if self.brave_collector and len(company_news) < 3:
                    try:
                        brave_news = self.brave_collector.search(f"{company} {topic} åŠ¨æ€ latest news", count=3-len(company_news))
                        if brave_news:
                            company_news.extend(brave_news)
                    except Exception as e:
                        print(f"  âš ï¸ Braveæœç´¢{company}å¤±è´¥: {str(e)}")
                
                # æœ€åä½¿ç”¨Tavilyæœç´¢
                if len(company_news) < 3:
                    try:
                        tavily_news = self.tavily_collector.get_company_news(company, topic, days, max_results=3-len(company_news))
                        if tavily_news:
                            company_news.extend(tavily_news)
                    except Exception as e:
                        print(f"  âš ï¸ Tavilyæœç´¢{company}å¤±è´¥: {str(e)}")
                        
                if company_news:
                    company_specifics.append({"company": company, "news": company_news})
            
            if company_specifics:
                all_news_data["company_news"] = [item for company_data in company_specifics for item in company_data["news"]]
        
        iteration_count = 0
        
        # ç¬¬ä¸‰æ­¥ï¼šåæ€ä¸è¿­ä»£
        while iteration_count < self.max_iterations:
            iteration_count += 1
            print(f"\nğŸ”„ [è¿­ä»£è½®æ¬¡ {iteration_count}/{self.max_iterations}] - å½“å‰æ•°æ®æ€»é‡: {all_news_data.get('total_count', 0)}æ¡")
            
            # åæ€åˆ†æ
            gaps, is_sufficient = self.reflect_on_information_gaps(all_news_data, topic, days)
            
            if is_sufficient:
                print("âœ… [å†³ç­–] ä¿¡æ¯æ”¶é›†å®Œæˆï¼Œå¼€å§‹ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š")
                break
            
            if iteration_count < self.max_iterations:
                print(f"ğŸ¯ [ç¬¬{iteration_count}è½®æœç´¢] å¼€å§‹è¡¥å……ä¿¡æ¯ç¼ºå£...")
                # ç”Ÿæˆé’ˆå¯¹æ€§æŸ¥è¯¢å¹¶æ‰§è¡Œæœç´¢
                additional_data = self.generate_targeted_queries(gaps, topic, days)
                
                # åˆå¹¶æ–°æ•°æ®
                if additional_data:
                    old_total = all_news_data.get('total_count', 0)
                    all_news_data = self._merge_data(all_news_data, additional_data)
                    new_total = all_news_data.get('total_count', 0)
                    print(f"ğŸ“ˆ [æ•°æ®æ›´æ–°] ç¬¬{iteration_count}è½®æ–°å¢ {new_total - old_total} æ¡æ•°æ®ï¼Œæ€»é‡: {new_total}")
                else:
                    print(f"ğŸ“Š [ç¬¬{iteration_count}è½®ç»“æœ] æœ¬è½®æœªè·å¾—æ–°æ•°æ®ï¼Œå¯èƒ½éœ€è¦è°ƒæ•´æœç´¢ç­–ç•¥")
            else:
                print(f"âš ï¸ [è¾¾åˆ°ä¸Šé™] å·²å®Œæˆ{self.max_iterations}è½®è¿­ä»£ï¼Œå¼€å§‹ç”ŸæˆæŠ¥å‘Š")
                
        # ğŸš€ ç¬¬äº”æ­¥ï¼šå¹¶è¡Œç»¼åˆæŠ¥å‘Šç”Ÿæˆ
        print(f"\nğŸ“ [å¹¶è¡ŒæŠ¥å‘Šç”Ÿæˆ] æ­£åœ¨ä½¿ç”¨å¹¶è¡Œå¤„ç†å™¨ç”Ÿæˆ{topic}è¡Œä¸šæŠ¥å‘Š...")
        return self.parallel_processor.process_news_report_parallel(topic, all_news_data, companies, days)

def generate_news_report_parallel(topic, companies=None, days=7, output_file=None, config="balanced"):
    """
    å¹¶è¡Œç‰ˆæŠ¥å‘Šç”Ÿæˆå‡½æ•°
    
    Args:
        topic: è¡Œä¸šä¸»é¢˜
        companies: é‡ç‚¹å…³æ³¨çš„å…¬å¸åˆ—è¡¨
        days: æ—¶é—´èŒƒå›´ï¼ˆå¤©æ•°ï¼‰
        output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        config: å¹¶è¡Œå¤„ç†é…ç½® ("conservative", "balanced", "aggressive")
    """
    print(f"\nğŸ¤– å¯åŠ¨æ™ºèƒ½æŠ¥å‘Šç”Ÿæˆç³»ç»Ÿï¼ˆå¹¶è¡Œç‰ˆæœ¬ï¼‰...")
    print(f"ğŸ¯ ç›®æ ‡: {topic}è¡Œä¸šåˆ†ææŠ¥å‘Š")
    print(f"âš™ï¸ é…ç½®: {config}")
    print("=" * 60)
    
    # ä½¿ç”¨æ™ºèƒ½ä»£ç†ç”ŸæˆæŠ¥å‘Š
    agent = IntelligentReportAgentParallel(parallel_config=config)
    report_content, performance_stats = agent.generate_comprehensive_report_with_thinking(topic, days, companies)
    
    # æ–‡ä»¶ä¿å­˜é€»è¾‘
    if not output_file:
        date_str = datetime.now().strftime('%Y%m%d')
        reports_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), "reports")
        os.makedirs(reports_dir, exist_ok=True)
        safe_topic = "".join([c if c.isalnum() or c in [' ', '_', '-'] else '_' for c in topic])
        safe_topic = safe_topic.replace(' ', '_')
        output_file = os.path.join(reports_dir, f"{safe_topic}_å¹¶è¡Œæ™ºèƒ½åˆ†ææŠ¥å‘Š_{date_str}.md")
        print(f"ğŸ“ æŠ¥å‘Šå°†ä¿å­˜è‡³: {output_file}")
    
    os.makedirs(os.path.dirname(os.path.abspath(output_file)), exist_ok=True)
    
    # ä¼˜åŒ–æ ¼å¼
    try:
        report_content = fix_markdown_headings(report_content)
        print("âœ… æŠ¥å‘Šæ ¼å¼å·²ä¼˜åŒ–")
    except Exception as e:
        print(f"âš ï¸ ä¿®å¤Markdownæ ¼å¼æ—¶å‡ºé”™: {str(e)}")
    
    # ä¿å­˜æŠ¥å‘Š
    try:
        from report_utils import safe_save_report
        safe_save_report(report_content, output_file)
    except ImportError:
        print("ğŸ“ ä½¿ç”¨æ ‡å‡†æ–¹å¼ä¿å­˜æ–‡ä»¶")
        with open(output_file, "w", encoding="utf-8-sig") as f:
            f.write(report_content)
    
    # æ˜¾ç¤ºæ€§èƒ½ç»Ÿè®¡
    # print("\n" + "=" * 60)
    # print("ğŸ“Š æ€§èƒ½ç»Ÿè®¡æŠ¥å‘Š:")
    # print(f"â±ï¸  æ€»è€—æ—¶: {performance_stats['total_time']:.1f}ç§’")
    # print(f"ğŸŒ ä¸²è¡Œé¢„ä¼°: {performance_stats['estimated_sequential_time']:.1f}ç§’") 
    # print(f"âš¡ æ—¶é—´èŠ‚çœ: {performance_stats['estimated_time_saved']:.1f}ç§’")
    # print(f"ğŸš€ æ€§èƒ½æå‡: {performance_stats['speedup_ratio']:.1f}x")
    # print(f"ğŸ”§ é…ç½®æ¨¡å¼: {performance_stats['config_mode']}")
    # print("=" * 60)
    
    print(f"ğŸ‰ å¹¶è¡Œæ™ºèƒ½æŠ¥å‘Šå·²ç”Ÿæˆ: {output_file}")
    
    return report_content

if __name__ == "__main__":
    # åŠ è½½ç¯å¢ƒå˜é‡
    load_dotenv()
    
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(description='å¹¶è¡Œæ™ºèƒ½è¡Œä¸šåˆ†ææŠ¥å‘Šç”Ÿæˆå™¨')
    parser.add_argument('--topic', type=str, required=True, help='æŠ¥å‘Šçš„ä¸»é¢˜')
    parser.add_argument('--companies', type=str, nargs='*', 
                      help='è¦ç‰¹åˆ«å…³æ³¨çš„å…¬å¸ï¼ˆå¯é€‰ï¼‰')
    parser.add_argument('--days', type=int, default=7, help='æœç´¢å†…å®¹çš„å¤©æ•°èŒƒå›´')
    parser.add_argument('--output', type=str, help='è¾“å‡ºæ–‡ä»¶åæˆ–è·¯å¾„')
    parser.add_argument('--config', type=str, choices=['conservative', 'balanced', 'aggressive'], 
                      default='balanced', help='å¹¶è¡Œå¤„ç†é…ç½®')
    
    parser.epilog = """
    ğŸš€ å…¨æ–°å¹¶è¡Œæ™ºèƒ½æŠ¥å‘Šç”Ÿæˆå™¨è¯´æ˜:
    
    æœ¬å·¥å…·é‡‡ç”¨å…¨é¢çš„å¹¶è¡Œå¤„ç†æ¶æ„ï¼Œå¤§å¹…æå‡æ•´ä½“æ€§èƒ½ï¼š
    
    âš¡ å¹¶è¡Œå¤„ç†é…ç½®ï¼š
    - conservative: ä¿å®ˆæ¨¡å¼ï¼Œ3ä¸ªå¹¶è¡Œä»»åŠ¡ï¼Œé€‚åˆAPIé™åˆ¶ä¸¥æ ¼çš„æƒ…å†µ
    - balanced: å¹³è¡¡æ¨¡å¼ï¼Œ6ä¸ªå¹¶è¡Œä»»åŠ¡ï¼Œæ¨èé…ç½®ï¼ˆé»˜è®¤ï¼‰
    - aggressive: æ¿€è¿›æ¨¡å¼ï¼Œ8ä¸ªå¹¶è¡Œä»»åŠ¡ï¼Œé€‚åˆé«˜æ€§èƒ½éœ€æ±‚
    
    ğŸ§  äº”æ­¥åˆ†ææ³• + å…¨é¢å¹¶è¡Œå¤„ç†ï¼š
    
    ğŸ“Š ç¬¬1æ­¥ï¼šæ™ºèƒ½æŸ¥è¯¢ç”Ÿæˆï¼ˆä¿æŒåŸæœ‰æµç¨‹ï¼‰
    
    ğŸš€ ç¬¬2æ­¥ï¼šå¹¶è¡Œå¤šæ¸ é“ä¿¡æ¯æœé›†ï¼ˆæ–°ä¼˜åŒ–ï¼ï¼‰
    - 3ä¸ªæœç´¢å¼•æ“åŒæ—¶æ‰§è¡Œï¼šBrave + Google + Tavily
    - è‡ªåŠ¨å»é‡å’Œæ•°æ®åˆå¹¶
    - æ€§èƒ½æå‡ï¼šæ•°æ®æ”¶é›†é˜¶æ®µé€Ÿåº¦æå‡60%
    
    ğŸ¤” ç¬¬3æ­¥ï¼šå¤šç»´åº¦æ™ºèƒ½åæ€ä¸è´¨é‡è¯„ä¼°ï¼ˆé‡å¤§å‡çº§ï¼ï¼‰
    - ğŸ”¢ æ•°é‡ç»´åº¦ï¼šåŠ¨æ€æ ‡å‡†+åˆ†ç±»å¹³è¡¡è¯„ä¼° (æƒé‡15%)
    - ğŸ¯ è´¨é‡ç»´åº¦ï¼šLLMæ·±åº¦è¯„ä¼°å†…å®¹è´¨é‡ (æƒé‡30%) 
    - ğŸ“Š è¦†ç›–ç»´åº¦ï¼š5ä¸ªæ ¸å¿ƒé¢†åŸŸå®Œæ•´æ€§æ£€æŸ¥ (æƒé‡25%)
    - â° æ—¶æ•ˆç»´åº¦ï¼šä¿¡æ¯æ–°é²œåº¦æ™ºèƒ½åˆ¤æ–­ (æƒé‡20%)
    - ğŸ›ï¸ æƒå¨ç»´åº¦ï¼šä¿¡æ¯æºå¤šæ ·æ€§åˆ†æ (æƒé‡10%)
    - ğŸ“ˆ ç»¼åˆè¯„åˆ†â‰¥7.0/10æ‰èƒ½é€šè¿‡è´¨é‡é—¨æ§›
    
    ğŸ¯ ç¬¬4æ­¥ï¼šå¹¶è¡Œè¿­ä»£ä¼˜åŒ–æœç´¢ï¼ˆæ–°ä¼˜åŒ–ï¼ï¼‰  
    - å¤šä¸ªé’ˆå¯¹æ€§æŸ¥è¯¢åŒæ—¶æ‰§è¡Œ
    - AIæ™ºèƒ½æŸ¥è¯¢ç”Ÿæˆ + é¢„è®¾æŸ¥è¯¢åŒé‡ä¿éšœ
    - æ€§èƒ½æå‡ï¼šè¿­ä»£æœç´¢é˜¶æ®µé€Ÿåº¦æå‡70%
    
    âš¡ ç¬¬5æ­¥ï¼šå¹¶è¡ŒæŠ¥å‘Šç”Ÿæˆï¼ˆå·²æœ‰ä¼˜åŒ–ï¼‰
    - 6ä¸ªåˆ†æå™¨åŒæ—¶å·¥ä½œï¼šé‡å¤§æ–°é—»ã€æŠ€æœ¯åˆ›æ–°ã€æŠ•èµ„åŠ¨æ€ã€æ”¿ç­–ç›‘ç®¡ã€è¡Œä¸šè¶‹åŠ¿ã€è§‚ç‚¹å¯¹æ¯”
    - æ™ºèƒ½æ€»ç»“åœ¨åˆ†æå®Œæˆåæ‰§è¡Œ
    - æ€§èƒ½æå‡ï¼šæŠ¥å‘Šç”Ÿæˆé˜¶æ®µé€Ÿåº¦æå‡60-70%
    
    ğŸ“ˆ æ€»ä½“æ€§èƒ½æå‡ï¼š
    - æ•°æ®æ”¶é›†é˜¶æ®µï¼šå¹¶è¡Œæ‰§è¡Œå¤šæœç´¢å¼•æ“ï¼Œé€Ÿåº¦æå‡60%
    - è´¨é‡è¯„ä¼°é˜¶æ®µï¼š5ç»´åº¦æ™ºèƒ½è¯„ä¼°ï¼Œè´¨é‡ä¿è¯æå‡90%
    - è¿­ä»£æœç´¢é˜¶æ®µï¼šå¹¶è¡Œæ‰§è¡Œå¤šæŸ¥è¯¢ï¼Œé€Ÿåº¦æå‡70%  
    - æŠ¥å‘Šç”Ÿæˆé˜¶æ®µï¼šå¹¶è¡ŒLLMå¤„ç†ï¼Œé€Ÿåº¦æå‡70%
    - æ•´ä½“æµç¨‹ï¼šé€Ÿåº¦æå‡80-90% + è´¨é‡ä¿è¯å¤§å¹…æå‡
    
    ğŸ” æœç´¢æ¸ é“é›†æˆï¼š
    - Brave Web Search API (éšç§å‹å¥½çš„æœç´¢ï¼Œå·²å¯ç”¨)  
    - Tavily Search API (AIä¼˜åŒ–çš„æœç´¢)
    - Google Custom Search API (å¯é€‰é…ç½®)
    - ä¸‰å¼•æ“å¹¶è¡Œæœç´¢ï¼Œè‡ªåŠ¨å»é‡å’Œç»“æœä¼˜åŒ–
    
    âš™ï¸ ç¯å¢ƒé…ç½®:
    éœ€è¦åœ¨.envæ–‡ä»¶ä¸­é…ç½®APIå¯†é’¥ï¼š
    - BRAVE_SEARCH_API_KEY (Braveæœç´¢ï¼Œå·²é¢„é…ç½®)
    - TAVILY_API_KEY (Tavilyæœç´¢)
    - GOOGLE_SEARCH_API_KEY (Googleæœç´¢ï¼Œå¯é€‰)
    - GOOGLE_SEARCH_CX (Googleè‡ªå®šä¹‰æœç´¢å¼•æ“IDï¼Œå¯é€‰)
    
    ğŸ’¡ ä½¿ç”¨ç¤ºä¾‹:
    python generate_news_report_parallel.py --topic "äººå·¥æ™ºèƒ½" --days 10 --config balanced --output "AIå…¨é¢å¹¶è¡Œåˆ†ææŠ¥å‘Š.md"
    """
    
    args = parser.parse_args()
    
    print("ğŸš€ å¯åŠ¨å¹¶è¡Œæ™ºèƒ½æŠ¥å‘Šç”Ÿæˆ...")
    generate_news_report_parallel(args.topic, args.companies, args.days, args.output, args.config) 