# streaming_orchestrator.py
"""
æµå¼MCPè°ƒåº¦å™¨ - æ”¯æŒå®æ—¶SSEæ¨é€çš„MCPå·¥å…·è°ƒåº¦å™¨
åŸºäºMCPå·¥å…·çš„çº¯å‡€å®ç°ï¼Œæä¾›æµå¼æ•°æ®æ¨é€èƒ½åŠ›
"""

import json
import asyncio
from typing import Dict, Any, AsyncGenerator
from datetime import datetime

from main import (
    analysis_mcp, query_generation_mcp, outline_writer_mcp, 
    summary_writer_mcp, content_writer_mcp, search,
    orchestrator_mcp, llm_processor
)

class StreamingOrchestrator:
    """æ”¯æŒå®æ—¶SSEæ¨é€çš„MCPè°ƒåº¦å™¨ - åŸºäºMCPå·¥å…·çš„çº¯å‡€å®ç°"""
    
    def __init__(self):
        self.tool_name = None
        self.current_step = 0
        self.total_steps = 0
        
    def _create_heartbeat_message(self) -> str:
        """åˆ›å»ºå¿ƒè·³æ¶ˆæ¯"""
        try:
            return json.dumps({
                "type": "heartbeat",
                "timestamp": datetime.now().isoformat(),
                "data": {
                    "status": "alive",
                    "tool": self.tool_name,
                    "step": f"{self.current_step}/{self.total_steps}" if self.total_steps > 0 else "0/0"
                }
            })
        except Exception as e:
            return json.dumps({"type": "error", "error": f"å¿ƒè·³æ¶ˆæ¯ç”Ÿæˆå¤±è´¥: {str(e)}"})

    async def _safe_yield(self, message: str, description: str = "æ¶ˆæ¯"):
        """å®‰å…¨çš„æ¶ˆæ¯æ¨é€ï¼ŒåŒ…å«é”™è¯¯å¤„ç†"""
        try:
            if not message:
                message = json.dumps({
                    "type": "error",
                    "error": f"ç©º{description}",
                    "timestamp": datetime.now().isoformat()
                })
            yield message
        except Exception as e:
            error_message = json.dumps({
                "type": "error", 
                "error": f"{description}æ¨é€å¤±è´¥: {str(e)}",
                "timestamp": datetime.now().isoformat()
            })
            yield error_message

    async def _call_content_writer_with_usage(self, **kwargs):
        """è°ƒç”¨content_writer_mcpå¹¶è¿”å›å†…å®¹å’Œç”¨é‡ä¿¡æ¯"""
        try:
            result = await asyncio.to_thread(content_writer_mcp, **kwargs)
            print(f"ğŸ” [è°ƒè¯•] content_writer_mcpè¿”å›ç»“æœ: {str(result)[:200]}...")
            
            try:
                import json
                # å¤„ç†è¿”å›ç»“æœ - generate_insight_reportè¿”å›çš„æ˜¯å­—ç¬¦ä¸²ï¼Œä¸æ˜¯JSON
                if isinstance(result, str) and result.startswith('{'):
                    try:
                        result_data = json.loads(result)
                    except json.JSONDecodeError:
                        result_data = {"report_content": result, "status": "completed"}
                else:
                    result_data = {"report_content": result, "status": "completed"}
                
                content = result_data.get('content', result)  # æå–çº¯æ–‡æœ¬å†…å®¹
                usage = result_data.get('usage', None)
                print(f"ğŸ” [è°ƒè¯•] è§£æJSONæˆåŠŸï¼Œæå–åˆ°{len(content)}å­—ç¬¦çš„å†…å®¹ï¼Œusage: {usage}")
                
                # æ ¼å¼åŒ–å†…å®¹
                if isinstance(content, str):
                    # å¤„ç†è½¬ä¹‰å­—ç¬¦
                    content = content.replace('\\n', '\n').replace('\\t', '\t')
                    print(f"ğŸ” [è°ƒè¯•] å†…å®¹æ ¼å¼åŒ–å®Œæˆï¼Œæœ€ç»ˆé•¿åº¦: {len(content)}å­—ç¬¦")
                
                return content, usage
            except (json.JSONDecodeError, TypeError) as json_error:
                print(f"âš ï¸ [è°ƒè¯•] JSONè§£æå¤±è´¥: {json_error}ï¼Œä½¿ç”¨fallbackæ–¹å¼")
                # å¦‚æœJSONè§£æå¤±è´¥ï¼Œç›´æ¥è¿”å›åŸå§‹ç»“æœ
                usage = None
                if hasattr(llm_processor, 'last_usage') and llm_processor.last_usage:
                    usage = llm_processor.last_usage
                    print(f"ğŸ” [è°ƒè¯•] ä»llm_processorè·å–åˆ°çš„usage: {usage}")
                return result, usage
        except Exception as e:
            print(f"âŒ [è°ƒè¯•] content_writer_mcpè°ƒç”¨å¤±è´¥: {str(e)}")
            raise e

    # åˆ é™¤äº†æ— ç”¨çš„å§”æ‰˜æ–¹æ³•ï¼Œç›´æ¥ä½¿ç”¨ stream_insight_report ç­‰æ ¸å¿ƒæ–¹æ³•

    async def stream_insight_report(self, **kwargs) -> AsyncGenerator[Dict[str, Any], None]:
        """æµå¼ç”Ÿæˆæ´å¯ŸæŠ¥å‘Š"""
        self.tool_name = "orchestrator_mcp"
        
        try:
            topic = kwargs.get("topic", "æœªæŒ‡å®šä¸»é¢˜")
            task = kwargs.get("task", f"ç”Ÿæˆå…³äº{topic}çš„æŠ¥å‘Š")
            task_type = kwargs.get("task_type", "insights")
            depth_level = kwargs.get("depth_level", "detailed")
            target_audience = kwargs.get("target_audience", "è¡Œä¸šä¸“å®¶")
            
            # æ ¹æ®ä»»åŠ¡ç±»å‹ç¡®å®šæŠ¥å‘Šç±»å‹åç§°
            report_type_names = {
                "insights": "æ´å¯ŸæŠ¥å‘Š",
                "industry": "è¡Œä¸šåŠ¨æ€æŠ¥å‘Š",
                "academic": "å­¦æœ¯ç ”ç©¶æŠ¥å‘Š",
                "comprehensive": "ç»¼åˆæŠ¥å‘Š"
            }
            report_name = report_type_names.get(task_type, "æŠ¥å‘Š")
            
            # å‘é€å¼€å§‹æ¶ˆæ¯
            yield self._create_progress_message("started", f"å¼€å§‹ç”Ÿæˆ{report_name}", f"æ­£åœ¨åˆå§‹åŒ–{report_name}åˆ†ææµç¨‹...")
            await asyncio.sleep(0.1)
            
            # å‘é€è¿›åº¦æ›´æ–°
            yield self._create_progress_message("processing", "åˆ†æä¸»é¢˜éœ€æ±‚", f"æ­£åœ¨åˆ†æ{topic}çš„{task_type}éœ€æ±‚...")
            await asyncio.sleep(0.1)
            
            # ç›´æ¥ç”Ÿæˆå®Œæ•´æŠ¥å‘Šï¼ˆåŒ…æ‹¬å¤§çº²ï¼‰
            yield self._create_progress_message("processing", "ç”ŸæˆæŠ¥å‘Šå¤§çº²", f"æ­£åœ¨ä¸º{topic}ç”Ÿæˆè¯¦ç»†çš„æŠ¥å‘Šç»“æ„...")
            await asyncio.sleep(0.1)
            
            # ç”Ÿæˆå®Œæ•´æŠ¥å‘Šå†…å®¹ï¼ˆorchestrator_mcpä¼šå¤„ç†å¤§çº²ç”Ÿæˆï¼‰
            yield self._create_progress_message("processing", "ç”ŸæˆæŠ¥å‘Šå†…å®¹", f"æ­£åœ¨åŸºäºå¤§çº²ç”Ÿæˆè¯¦ç»†çš„{report_name}å†…å®¹...")
            await asyncio.sleep(0.1)
            
            # æ¸…ç©ºä¸Šæ¬¡ç”¨é‡ï¼Œç¡®ä¿è·å–æœ¬æ¬¡çœŸå®ç”¨é‡
            if hasattr(llm_processor, 'last_usage'):
                llm_processor.last_usage = None
            
            # ç›´æ¥è°ƒç”¨orchestrator_mcpç”Ÿæˆæ´å¯ŸæŠ¥å‘Š
            result = await asyncio.to_thread(
                orchestrator_mcp,
                task=task,
                task_type=task_type,
                topic=topic,
                depth_level=depth_level,
                target_audience=target_audience
            )
            
            # å¤„ç†è¿”å›ç»“æœ - orchestrator_mcpè¿”å›çš„æ˜¯å­—ç¬¦ä¸²ï¼Œä¸æ˜¯JSON
            if isinstance(result, str) and result.startswith('{'):
                try:
                    result_data = json.loads(result)
                except json.JSONDecodeError:
                    result_data = {"report_content": result, "status": "completed"}
            else:
                result_data = {"report_content": result, "status": "completed"}
            
            # ä»result_dataä¸­æå–usageä¿¡æ¯å¹¶å‘é€æ¨¡å‹ç”¨é‡æ¶ˆæ¯
            if result_data and 'usage' in result_data:
                usage_info = result_data['usage']
                yield self._create_model_usage_message(usage_data=usage_info)
                await asyncio.sleep(0.1)
            
            # æ£€æŸ¥æ˜¯å¦æœ‰æŠ¥å‘Šå†…å®¹ - ä¿®å¤å­—æ®µååŒ¹é…é—®é¢˜
            if result_data and result_data.get('status') == 'success' and 'report' in result_data:
                report_content = result_data['report']
                yield self._create_progress_message("completed", "æ´å¯ŸæŠ¥å‘Šç”Ÿæˆå®Œæˆ", "æˆåŠŸç”Ÿæˆæ·±åº¦æ´å¯Ÿåˆ†ææŠ¥å‘Š")
                await asyncio.sleep(0.1)
                
                # å‘é€æœ€ç»ˆç»“æœ
                final_result = {
                    "jsonrpc": "2.0",
                    "result": {
                        "tool": "orchestrator_mcp",
                        "content": report_content
                    }
                }
                
                # å‘é€ç”¨é‡ä¿¡æ¯
                if result_data and 'usage' in result_data:
                    yield self._create_model_usage_message(usage_data=result_data['usage'])
                elif hasattr(llm_processor, 'last_usage') and llm_processor.last_usage:
                    yield self._create_model_usage_message(usage_data=llm_processor.last_usage)
                
                yield final_result
            else:
                # å¤„ç†å¤±è´¥æƒ…å†µ
                error_msg = result_data.get('error', 'æŠ¥å‘Šç”Ÿæˆå¤±è´¥ï¼ŒæœªçŸ¥åŸå› ')
                yield self._create_error_message(f"æ´å¯ŸæŠ¥å‘Šç”Ÿæˆå¤±è´¥: {error_msg}")
                
        except Exception as e:
            print(f"âŒ æ´å¯ŸæŠ¥å‘Šç”Ÿæˆè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
            yield self._create_error_message(f"æ´å¯ŸæŠ¥å‘Šç”Ÿæˆè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")

    def _create_progress_message(self, status: str, message: str, content: str, details: Dict = None) -> Dict[str, Any]:
        """åˆ›å»ºç¬¦åˆMCPæ ‡å‡†çš„è¿›åº¦æ¶ˆæ¯"""
        msg_data = {
            "status": status,
            "message": message
        }
        
        if details:
            msg_data["details"] = details
        elif content:
            msg_data["details"] = {
                "content": content,
                "step": self.current_step,
                "total_steps": self.total_steps
            }
        
        return {
            "method": "notifications/message",
            "params": {
                "level": "info",
                "data": {
                    "msg": msg_data,
                    "extra": None
                }
            },
            "jsonrpc": "2.0"
        }

    def _create_model_usage_message(self, provider: str = None, model: str = None, input_tokens: int = None, output_tokens: int = None, total_tokens: int = None, usage_data: dict = None) -> Dict[str, Any]:
        """åˆ›å»ºç¬¦åˆMCPæ ‡å‡†çš„æ¨¡å‹ç”¨é‡æ¶ˆæ¯"""
        if usage_data:
            # ä½¿ç”¨ä¼ å…¥çš„usage_data
            usage_info = usage_data
        else:
            # æ„å»ºusageä¿¡æ¯
            usage_info = {
                "model_provider": provider or "unknown",
                "model_name": model or "unknown", 
                "input_tokens": input_tokens or 0,
                "output_tokens": output_tokens or 0
            }
        
        return {
            "jsonrpc": "2.0",
            "method": "notifications/message",
            "params": {
                "data": {
                    "msg": {
                        "type": "model_usage",
                        "data": usage_info
                    }
                }
            }
        }

    def _create_error_message(self, error: str) -> Dict[str, Any]:
        """åˆ›å»ºç¬¦åˆMCPæ ‡å‡†çš„é”™è¯¯æ¶ˆæ¯"""
        return {
            "jsonrpc": "2.0",
            "error": {
                "code": -32603,
                "message": "Internal error",
                "data": {
                    "type": "streaming_error",
                    "message": error
                }
            }
        }
