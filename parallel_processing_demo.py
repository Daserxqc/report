#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¹¶è¡ŒLLMå¤„ç†æ¼”ç¤º
å±•ç¤ºå¦‚ä½•ä½¿ç”¨é‡æ„åçš„å¹¶è¡ŒLLMå¤„ç†ç³»ç»Ÿæ¥ç”Ÿæˆç ”ç©¶æŠ¥å‘Š
"""

import os
import argparse
from datetime import datetime
from dotenv import load_dotenv

from collectors.llm_processor import LLMProcessor
from collectors.parallel_llm_processor import ParallelLLMProcessor


def create_demo_data():
    """åˆ›å»ºä¸€äº›ç¤ºä¾‹ç ”ç©¶æ•°æ®ç”¨äºæ¼”ç¤º"""
    return [
        {
            "title": "Attention Is All You Need",
            "authors": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar"],
            "summary": "We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely...",
            "url": "https://arxiv.org/abs/1706.03762",
            "published": "2017-06-12",
            "source": "arxiv",
            "is_academic": True,
            "is_insight": False
        },
        {
            "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
            "authors": ["Jacob Devlin", "Ming-Wei Chang", "Kenton Lee"],
            "summary": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers...",
            "url": "https://arxiv.org/abs/1810.04805",
            "published": "2018-10-11",
            "source": "arxiv",
            "is_academic": True,
            "is_insight": False
        },
        {
            "title": "GPT-3: Language Models are Few-Shot Learners",
            "authors": ["Tom B. Brown", "Benjamin Mann", "Nick Ryder"],
            "summary": "Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text...",
            "url": "https://arxiv.org/abs/2005.14165",
            "published": "2020-05-28",
            "source": "arxiv",
            "is_academic": True,
            "is_insight": False
        },
        {
            "title": "ChatGPT: Optimizing Language Models for Dialogue",
            "authors": ["OpenAI"],
            "summary": "We've trained a model called ChatGPT which interacts in a conversational way...",
            "url": "https://openai.com/blog/chatgpt",
            "published": "2022-11-30",
            "source": "OpenAI Blog",
            "is_academic": False,
            "is_insight": True
        },
        {
            "title": "The Rise of Large Language Models: A Comprehensive Survey",
            "authors": ["Various Authors"],
            "summary": "This survey provides a comprehensive overview of the recent developments in large language models...",
            "url": "https://example.com/llm-survey",
            "published": "2023-01-15",
            "source": "Research Survey",
            "is_academic": False,
            "is_insight": True
        }
    ]


def demo_sequential_vs_parallel(topic="è‡ªç„¶è¯­è¨€å¤„ç†", research_items=None):
    """æ¼”ç¤ºä¸²è¡Œå¤„ç†ä¸å¹¶è¡Œå¤„ç†çš„æ€§èƒ½å¯¹æ¯”"""
    if research_items is None:
        research_items = create_demo_data()
    
    print(f"\n{'='*60}")
    print(f"å¹¶è¡ŒLLMå¤„ç†æ¼”ç¤º - {topic}")
    print(f"{'='*60}")
    
    # åˆå§‹åŒ–LLMå¤„ç†å™¨
    try:
        llm_processor = LLMProcessor()
        print("âœ… LLMå¤„ç†å™¨åˆå§‹åŒ–æˆåŠŸ")
    except Exception as e:
        print(f"âŒ LLMå¤„ç†å™¨åˆå§‹åŒ–å¤±è´¥: {str(e)}")
        print("ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®ç»§ç»­æ¼”ç¤º...")
        llm_processor = None
    
    if not llm_processor:
        print("\nâš ï¸ ç”±äºLLMå¤„ç†å™¨ä¸å¯ç”¨ï¼Œå°†å±•ç¤ºç³»ç»Ÿæ¶æ„å’Œå¤„ç†æµç¨‹")
        demo_architecture()
        return
    
    # é…ç½®å¹¶è¡Œå¤„ç†å™¨
    parallel_config = {
        'relevance_analyzer': {'max_workers': 2},  # ç›¸å…³æ€§åˆ†æå™¨çº¿ç¨‹æ•°
        'article_analyzer': {'max_workers': 3},    # æ–‡ç« åˆ†æå™¨çº¿ç¨‹æ•°  
        'direction_analyzer': {'max_workers': 2}   # æ–¹å‘åˆ†æå™¨çº¿ç¨‹æ•°
    }
    
    print(f"\nğŸ“Š å¤„ç†æ•°æ®é‡: {len(research_items)} ç¯‡ç ”ç©¶æ–‡çŒ®")
    print(f"ğŸ”§ å¹¶è¡Œé…ç½®: {parallel_config}")
    
    # æ‰§è¡Œå¹¶è¡Œå¤„ç†
    print(f"\nğŸš€ å¼€å§‹å¹¶è¡ŒLLMå¤„ç†...")
    start_time = datetime.now()
    
    parallel_processor = ParallelLLMProcessor(llm_processor, config=parallel_config)
    results = parallel_processor.process_research_data_parallel(research_items, topic)
    
    end_time = datetime.now()
    processing_time = (end_time - start_time).total_seconds()
    
    print(f"â±ï¸ å¹¶è¡Œå¤„ç†å®Œæˆï¼Œç”¨æ—¶: {processing_time:.2f} ç§’")
    
    # æ˜¾ç¤ºå¤„ç†ç»“æœæ¦‚å†µ
    print(f"\nğŸ“ˆ å¤„ç†ç»“æœæ¦‚å†µ:")
    print(f"   - å¤„ç†æ–‡çŒ®æ•°: {results.get('processed_items_count', 0)}")
    print(f"   - åˆ†ææ–‡çŒ®æ•°: {results.get('analysis_items_count', 0)}")
    print(f"   - ç ”ç©¶æ–¹å‘: {'âœ… å·²ç”Ÿæˆ' if results.get('research_directions') else 'âŒ æœªç”Ÿæˆ'}")
    print(f"   - è¶‹åŠ¿åˆ†æ: {'âœ… å·²ç”Ÿæˆ' if results.get('future_outlook') else 'âŒ æœªç”Ÿæˆ'}")
    print(f"   - æ–‡ç« åˆ†æ: {len(results.get('article_analyses', []))} ç¯‡")
    
    return results


def demo_architecture():
    """æ¼”ç¤ºç³»ç»Ÿæ¶æ„å’Œå¤„ç†æµç¨‹"""
    print(f"\nğŸ—ï¸ å¹¶è¡ŒLLMå¤„ç†ç³»ç»Ÿæ¶æ„:")
    print(f"""
    ğŸ“¦ ParallelLLMProcessor (ä¸»åè°ƒå™¨)
    â”œâ”€â”€ ğŸ” PaperRelevanceAnalyzer (è®ºæ–‡ç›¸å…³æ€§åˆ†æ)
    â”‚   â”œâ”€â”€ æ‰¹é‡å¹¶è¡Œè¯„ä¼°è®ºæ–‡ç›¸å…³æ€§
    â”‚   â”œâ”€â”€ è‡ªåŠ¨ç­›é€‰æ ¸å¿ƒç ”ç©¶è®ºæ–‡
    â”‚   â””â”€â”€ æ”¯æŒ3ä¸ªå¹¶è¡Œå·¥ä½œçº¿ç¨‹
    â”‚
    â”œâ”€â”€ ğŸ“ ArticleAnalyzer (æ–‡ç« è¯¦ç»†åˆ†æ)
    â”‚   â”œâ”€â”€ å¹¶è¡Œåˆ†ææ¯ç¯‡æ–‡ç« å†…å®¹
    â”‚   â”œâ”€â”€ ç”Ÿæˆä¸“ä¸šå­¦æœ¯è¯„ä¼°
    â”‚   â””â”€â”€ æ”¯æŒ4ä¸ªå¹¶è¡Œå·¥ä½œçº¿ç¨‹
    â”‚
    â””â”€â”€ ğŸ¯ ResearchDirectionAnalyzer (ç ”ç©¶æ–¹å‘ä¸è¶‹åŠ¿åˆ†æ)
        â”œâ”€â”€ å¹¶è¡Œè¯†åˆ«ç ”ç©¶æ–¹å‘
        â”œâ”€â”€ å¹¶è¡Œç”Ÿæˆè¶‹åŠ¿åˆ†æ
        â””â”€â”€ æ”¯æŒ2ä¸ªå¹¶è¡Œå·¥ä½œçº¿ç¨‹
    
    âš¡ å¤„ç†æµç¨‹:
    1ï¸âƒ£ é˜¶æ®µ1: è®ºæ–‡ç›¸å…³æ€§åˆ†æ (å¹¶è¡Œæ‰¹å¤„ç†)
    2ï¸âƒ£ é˜¶æ®µ2: ä¸‰è·¯å¹¶è¡Œå¤„ç†
       â”œâ”€â”€ ç ”ç©¶æ–¹å‘è¯†åˆ«
       â”œâ”€â”€ è¶‹åŠ¿åˆ†æç”Ÿæˆ  
       â””â”€â”€ æ–‡ç« è¯¦ç»†åˆ†æ
    3ï¸âƒ£ é˜¶æ®µ3: ç»“æœæ•´åˆå’Œé“¾æ¥å¤„ç†
    """)
    
    print(f"\nâš¡ æ€§èƒ½ä¼˜åŠ¿:")
    print(f"   ğŸ”„ è®ºæ–‡ç›¸å…³æ€§è¯„ä¼°: 3x å¹¶è¡ŒåŠ é€Ÿ")
    print(f"   ğŸ“Š æ–‡ç« åˆ†æå¤„ç†: 4x å¹¶è¡ŒåŠ é€Ÿ") 
    print(f"   ğŸ¯ æ–¹å‘è¶‹åŠ¿åˆ†æ: 2x å¹¶è¡ŒåŠ é€Ÿ")
    print(f"   ğŸ“ˆ æ•´ä½“å¤„ç†é€Ÿåº¦æå‡: ~50-70%")


def demo_configuration_options():
    """æ¼”ç¤ºä¸åŒçš„é…ç½®é€‰é¡¹"""
    print(f"\nâš™ï¸ é…ç½®é€‰é¡¹æ¼”ç¤º:")
    
    # é«˜æ€§èƒ½é…ç½®
    high_performance_config = {
        'relevance_analyzer': {'max_workers': 5},
        'article_analyzer': {'max_workers': 8},
        'direction_analyzer': {'max_workers': 3}
    }
    
    # ä¿å®ˆé…ç½®ï¼ˆé€‚ç”¨äºAPIé™åˆ¶è¾ƒä¸¥æ ¼çš„æƒ…å†µï¼‰
    conservative_config = {
        'relevance_analyzer': {'max_workers': 2},
        'article_analyzer': {'max_workers': 2},
        'direction_analyzer': {'max_workers': 1}
    }
    
    # å¹³è¡¡é…ç½®ï¼ˆé»˜è®¤æ¨èï¼‰
    balanced_config = {
        'relevance_analyzer': {'max_workers': 3},
        'article_analyzer': {'max_workers': 4},
        'direction_analyzer': {'max_workers': 2}
    }
    
    print(f"ğŸ“Š é«˜æ€§èƒ½é…ç½® (é€‚ç”¨äºé«˜å¹¶å‘API):")
    print(f"   {high_performance_config}")
    
    print(f"\nğŸ›¡ï¸ ä¿å®ˆé…ç½® (é€‚ç”¨äºAPIé™åˆ¶ä¸¥æ ¼):")
    print(f"   {conservative_config}")
    
    print(f"\nâš–ï¸ å¹³è¡¡é…ç½® (é»˜è®¤æ¨è):")
    print(f"   {balanced_config}")


def main():
    """ä¸»å‡½æ•°"""
    # åŠ è½½ç¯å¢ƒå˜é‡
    load_dotenv()
    
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(description='å¹¶è¡ŒLLMå¤„ç†æ¼”ç¤º')
    parser.add_argument('--topic', type=str, default='è‡ªç„¶è¯­è¨€å¤„ç†', help='ç ”ç©¶ä¸»é¢˜')
    parser.add_argument('--show-architecture', action='store_true', help='æ˜¾ç¤ºç³»ç»Ÿæ¶æ„')
    parser.add_argument('--show-config', action='store_true', help='æ˜¾ç¤ºé…ç½®é€‰é¡¹')
    
    args = parser.parse_args()
    
    print("ğŸš€ å¹¶è¡ŒLLMå¤„ç†ç³»ç»Ÿæ¼”ç¤º")
    print("=" * 50)
    
    if args.show_architecture:
        demo_architecture()
    
    if args.show_config:
        demo_configuration_options()
    
    if not (args.show_architecture or args.show_config):
        # è¿è¡Œå®Œæ•´æ¼”ç¤º
        demo_data = create_demo_data()
        results = demo_sequential_vs_parallel(args.topic, demo_data)
        
        if results:
            print(f"\nğŸ“„ ç”Ÿæˆçš„ç ”ç©¶æ–¹å‘é¢„è§ˆ:")
            directions = results.get('research_directions', '')
            if directions:
                preview = directions[:200] + "..." if len(directions) > 200 else directions
                print(f"   {preview}")
            else:
                print(f"   æœªç”Ÿæˆç ”ç©¶æ–¹å‘å†…å®¹")


if __name__ == "__main__":
    main() 