import asyncio
import aiohttp
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime, timedelta
from typing import List, Dict, Optional, Union
from dataclasses import dataclass, asdict
from urllib.parse import urlparse
import threading

# å¯¼å…¥ç°æœ‰çš„æ”¶é›†å™¨
from collectors.tavily_collector import TavilyCollector
from collectors.brave_search_collector import BraveSearchCollector
from collectors.google_search_collector import GoogleSearchCollector
from collectors.arxiv_collector import ArxivCollector
from collectors.academic_collector import AcademicCollector
from collectors.news_collector import NewsCollector


@dataclass
class Document:
    """
    ç»Ÿä¸€çš„æ–‡æ¡£æ•°æ®ç»“æ„
    æ ‡å‡†åŒ–ä¸åŒæ•°æ®æºçš„è¿”å›æ ¼å¼
    """
    title: str
    content: str
    url: str
    source: str
    source_type: str  # 'web', 'academic', 'news', 'arxiv'
    publish_date: Optional[str] = None
    authors: List[str] = None
    venue: Optional[str] = None  # å­¦æœ¯æœŸåˆŠ/ä¼šè®®åç§°
    score: Optional[float] = None  # ç›¸å…³æ€§è¯„åˆ†
    language: Optional[str] = None
    
    def __post_init__(self):
        if self.authors is None:
            self.authors = []
    
    def to_dict(self) -> Dict:
        """è½¬æ¢ä¸ºå­—å…¸æ ¼å¼"""
        return asdict(self)
    
    @property
    def domain(self) -> str:
        """æå–åŸŸåç”¨äºå»é‡"""
        try:
            return urlparse(self.url).netloc.lower()
        except:
            return ""


class SearchMcp:
    """
    æœç´¢MCP (Model Context Protocol)
    
    ç”¨é€”ï¼šå¹¶è¡ŒåŒ–åœ°ä»å¤šä¸ªWebæœç´¢å¼•æ“å’Œå­¦æœ¯æ•°æ®åº“ä¸­è·å–ã€èšåˆå’Œå»é‡ä¿¡æ¯ã€‚
    
    èŒè´£ï¼š
    - ç®¡ç†åˆ°å¤šä¸ªæ•°æ®æºAPIçš„è¿æ¥ï¼ˆTavily, Brave, Google, Arxiv, Semantic Scholarç­‰ï¼‰
    - æ¥æ”¶æŸ¥è¯¢åˆ—è¡¨ï¼Œå¹¶è¡Œæ‰§è¡Œæ‰€æœ‰æœç´¢
    - å°†ä¸åŒæ¥æºçš„ç»“æœæ ‡å‡†åŒ–ä¸ºç»Ÿä¸€çš„Documentå¯¹è±¡è¡¨
    - åŸºäºURLè¿›è¡Œç»“æœå»é‡
    
    è¾“å…¥ï¼šqueries: list[str], sources: list[str], max_results_per_query: int = 5
    è¾“å‡ºï¼šlist[Document]
    
    å®ç°è¦ç‚¹ï¼šä½¿ç”¨asyncioå’Œaiohttpä¸ºæ¯ä¸ªæ•°æ®æºå®ç°ä¸€ä¸ªç‹¬ç«‹çš„Collectorç±»ï¼Œ
            SearchMcpè´Ÿè´£è°ƒåº¦è¿™äº›Collectorã€‚
    """
    
    def __init__(self):
        """åˆå§‹åŒ–SearchMcpï¼Œè®¾ç½®æ‰€æœ‰å¯ç”¨çš„æ”¶é›†å™¨"""
        self.collectors = {}
        self.results_lock = threading.Lock()
        
        # åˆå§‹åŒ–æ‰€æœ‰æ”¶é›†å™¨
        self._initialize_collectors()
        
        # æ”¯æŒçš„æ•°æ®æºç±»å‹
        self.source_types = {
            'web': ['tavily', 'brave', 'google'],
            'academic': ['arxiv', 'semantic_scholar', 'ieee', 'springer', 'core'],
            'news': ['news_api', 'google_news', 'brave_news', 'rss']
        }
        
        print(f"âœ… SearchMcpåˆå§‹åŒ–å®Œæˆï¼Œå¯ç”¨æ”¶é›†å™¨: {list(self.collectors.keys())}")
    
    def _initialize_collectors(self):
        """åˆå§‹åŒ–æ‰€æœ‰æ”¶é›†å™¨"""
        # Webæœç´¢æ”¶é›†å™¨
        try:
            tavily = TavilyCollector()
            if tavily.has_api_key:
                self.collectors['tavily'] = tavily
                print("âœ… Tavilyæ”¶é›†å™¨å·²å¯ç”¨")
        except Exception as e:
            print(f"âš ï¸ Tavilyæ”¶é›†å™¨åˆå§‹åŒ–å¤±è´¥: {str(e)}")
        
        try:
            brave = BraveSearchCollector()
            if brave.has_api_key:
                self.collectors['brave'] = brave
                print("âœ… Braveæ”¶é›†å™¨å·²å¯ç”¨")
        except Exception as e:
            print(f"âš ï¸ Braveæ”¶é›†å™¨åˆå§‹åŒ–å¤±è´¥: {str(e)}")
        
        try:
            google = GoogleSearchCollector()
            if google.has_api_key:
                self.collectors['google'] = google
                print("âœ… Googleæ”¶é›†å™¨å·²å¯ç”¨")
        except Exception as e:
            print(f"âš ï¸ Googleæ”¶é›†å™¨åˆå§‹åŒ–å¤±è´¥: {str(e)}")
        
        # å­¦æœ¯æœç´¢æ”¶é›†å™¨
        try:
            arxiv = ArxivCollector()
            self.collectors['arxiv'] = arxiv
            print("âœ… ArXivæ”¶é›†å™¨å·²å¯ç”¨")
        except Exception as e:
            print(f"âš ï¸ ArXivæ”¶é›†å™¨åˆå§‹åŒ–å¤±è´¥: {str(e)}")
        
        try:
            academic = AcademicCollector()
            self.collectors['academic'] = academic
            print("âœ… Academicæ”¶é›†å™¨å·²å¯ç”¨")
        except Exception as e:
            print(f"âš ï¸ Academicæ”¶é›†å™¨åˆå§‹åŒ–å¤±è´¥: {str(e)}")
        
        # æ–°é—»æ”¶é›†å™¨
        try:
            news = NewsCollector()
            self.collectors['news'] = news
            print("âœ… Newsæ”¶é›†å™¨å·²å¯ç”¨")
        except Exception as e:
            print(f"âš ï¸ Newsæ”¶é›†å™¨åˆå§‹åŒ–å¤±è´¥: {str(e)}")
    
    def parallel_search(self, 
                       queries: List[str], 
                       sources: List[str] = None, 
                       max_results_per_query: int = 5,
                       days_back: int = 7,
                       max_workers: int = 6) -> List[Document]:
        """
        å¹¶è¡Œæœç´¢å¤šä¸ªæŸ¥è¯¢å’Œæ•°æ®æº
        
        Args:
            queries: æœç´¢æŸ¥è¯¢åˆ—è¡¨
            sources: æŒ‡å®šçš„æ•°æ®æºåˆ—è¡¨ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨æ‰€æœ‰å¯ç”¨æº
            max_results_per_query: æ¯ä¸ªæŸ¥è¯¢çš„æœ€å¤§ç»“æœæ•°
            days_back: æœç´¢å¤šå°‘å¤©å†…çš„å†…å®¹
            max_workers: æœ€å¤§å¹¶è¡Œå·¥ä½œçº¿ç¨‹æ•°
            
        Returns:
            List[Document]: å»é‡åçš„æœç´¢ç»“æœåˆ—è¡¨
        """
        if not queries:
            return []
        
        # å¦‚æœæ²¡æœ‰æŒ‡å®šæ•°æ®æºï¼Œä½¿ç”¨æ‰€æœ‰å¯ç”¨çš„
        if sources is None:
            sources = list(self.collectors.keys())
        else:
            # è¿‡æ»¤åªä¿ç•™å¯ç”¨çš„æ•°æ®æº
            sources = [s for s in sources if s in self.collectors]
        
        if not sources:
            print("âŒ æ²¡æœ‰å¯ç”¨çš„æ•°æ®æº")
            return []
        
        print(f"ğŸš€ å¼€å§‹å¹¶è¡Œæœç´¢ {len(queries)} ä¸ªæŸ¥è¯¢ï¼Œä½¿ç”¨ {len(sources)} ä¸ªæ•°æ®æº")
        print(f"ğŸ“ æŸ¥è¯¢: {queries[:3]}{'...' if len(queries) > 3 else ''}")
        print(f"ğŸ” æ•°æ®æº: {sources}")
        
        all_results = []
        seen_urls = set()
        
        start_time = time.time()
        
        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œæ‰§è¡Œæœç´¢
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # ä¸ºæ¯ä¸ªæŸ¥è¯¢å’Œæ•°æ®æºç»„åˆåˆ›å»ºä»»åŠ¡
            future_to_info = {}
            
            for query in queries:
                for source in sources:
                    if source in self.collectors:
                        future = executor.submit(
                            self._execute_single_search,
                            query, source, max_results_per_query, days_back
                        )
                        future_to_info[future] = (query, source)
            
            # æ”¶é›†ç»“æœ
            completed_tasks = 0
            total_tasks = len(future_to_info)
            
            for future in as_completed(future_to_info):
                query, source = future_to_info[future]
                completed_tasks += 1
                
                try:
                    results = future.result()
                    
                    # å»é‡å¹¶åˆå¹¶ç»“æœ
                    new_count = 0
                    for doc in results:
                        if doc.url not in seen_urls:
                            seen_urls.add(doc.url)
                            all_results.append(doc)
                            new_count += 1
                    
                    print(f"  âœ… [{completed_tasks}/{total_tasks}] {source}({query}): {len(results)}æ¡ç»“æœ, {new_count}æ¡æ–°å¢")
                    
                except Exception as e:
                    print(f"  âŒ [{completed_tasks}/{total_tasks}] {source}({query}) æœç´¢å¤±è´¥: {str(e)}")
        
        # æŒ‰ç›¸å…³æ€§å’Œæ—¶é—´æ’åº
        all_results.sort(key=lambda doc: (
            doc.score or 0,  # ç›¸å…³æ€§è¯„åˆ†
            self._parse_date(doc.publish_date) if doc.publish_date else datetime.min
        ), reverse=True)
        
        total_time = time.time() - start_time
        print(f"ğŸ“Š æœç´¢å®Œæˆ: è·å¾— {len(all_results)} æ¡å»é‡ç»“æœï¼Œè€—æ—¶ {total_time:.1f}ç§’")
        
        return all_results
    
    def _execute_single_search(self, query: str, source: str, max_results: int, days_back: int) -> List[Document]:
        """æ‰§è¡Œå•ä¸ªæœç´¢ä»»åŠ¡"""
        collector = self.collectors.get(source)
        if not collector:
            return []
        
        try:
            # æ ¹æ®ä¸åŒçš„æ”¶é›†å™¨è°ƒç”¨ç›¸åº”çš„æœç´¢æ–¹æ³•
            raw_results = []
            
            if source == 'tavily':
                raw_results = collector.search(query, max_results=max_results)
                
            elif source == 'brave':
                raw_results = collector.search(query, count=max_results)
                
            elif source == 'google':
                raw_results = collector.search(query, days_back=days_back, max_results=max_results)
                
            elif source == 'arxiv':
                raw_results = collector.search(query, days_back=days_back)
                
            elif source == 'academic':
                # Academicæ”¶é›†å™¨æœ‰å¤šä¸ªæ–¹æ³•ï¼Œè¿™é‡Œä½¿ç”¨Semantic Scholar
                raw_results = collector.search_semantic_scholar(query, days_back=days_back)
                
            elif source == 'news':
                raw_results = collector.search_news_api(query, days_back=days_back)
            
            # æ ‡å‡†åŒ–ç»“æœä¸ºDocumentæ ¼å¼
            documents = self._standardize_results(raw_results, source)
            
            return documents[:max_results]  # é™åˆ¶ç»“æœæ•°é‡
            
        except Exception as e:
            print(f"æœç´¢æ‰§è¡Œå¤±è´¥ {source}({query}): {str(e)}")
            return []
    
    def _standardize_results(self, raw_results: List[Dict], source: str) -> List[Document]:
        """å°†åŸå§‹æœç´¢ç»“æœæ ‡å‡†åŒ–ä¸ºDocumentæ ¼å¼"""
        documents = []
        
        for result in raw_results:
            try:
                # ç¡®å®šæ•°æ®æºç±»å‹
                source_type = self._determine_source_type(source, result)
                
                # æå–æ ‡å‡†å­—æ®µ
                title = result.get('title', 'Untitled')
                
                # å†…å®¹å­—æ®µå¯èƒ½æœ‰å¤šç§åç§°
                content = (result.get('content') or 
                          result.get('summary') or 
                          result.get('abstract') or 
                          result.get('snippet') or 
                          result.get('description') or '')
                
                url = result.get('url', '#')
                
                # ä½œè€…å¤„ç†
                authors = []
                if 'authors' in result:
                    if isinstance(result['authors'], list):
                        authors = result['authors']
                    elif isinstance(result['authors'], str):
                        authors = [result['authors']]
                
                # å‘å¸ƒæ—¥æœŸå¤„ç†
                publish_date = self._extract_publish_date(result)
                
                # æœŸåˆŠ/ä¼šè®®åç§°
                venue = result.get('venue') or result.get('source')
                
                # åˆ›å»ºDocumentå¯¹è±¡
                doc = Document(
                    title=title,
                    content=content,
                    url=url,
                    source=source,
                    source_type=source_type,
                    publish_date=publish_date,
                    authors=authors,
                    venue=venue,
                    score=result.get('score'),
                    language=result.get('language')
                )
                
                documents.append(doc)
                
            except Exception as e:
                print(f"æ ‡å‡†åŒ–ç»“æœå¤±è´¥: {str(e)}")
                continue
        
        return documents
    
    def _determine_source_type(self, source: str, result: Dict) -> str:
        """ç¡®å®šæ•°æ®æºç±»å‹"""
        if source in ['arxiv', 'academic']:
            return 'academic'
        elif source == 'news' or 'news' in source:
            return 'news'
        else:
            return 'web'
    
    def _extract_publish_date(self, result: Dict) -> Optional[str]:
        """æå–å‘å¸ƒæ—¥æœŸ"""
        date_fields = ['publish_date', 'published', 'date', 'year', 'publication_date']
        
        for field in date_fields:
            if field in result and result[field]:
                date_value = result[field]
                
                # å¦‚æœæ˜¯å¹´ä»½æ•´æ•°
                if isinstance(date_value, int):
                    return f"{date_value}-01-01"
                
                # å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œå°è¯•è§£æ
                if isinstance(date_value, str):
                    try:
                        # å°è¯•è§£æISOæ ¼å¼æ—¥æœŸ
                        parsed_date = datetime.fromisoformat(date_value.replace('Z', '+00:00'))
                        return parsed_date.strftime('%Y-%m-%d')
                    except:
                        return date_value
        
        return None
    
    def _parse_date(self, date_str: Optional[str]) -> datetime:
        """è§£ææ—¥æœŸå­—ç¬¦ä¸²ä¸ºdatetimeå¯¹è±¡"""
        if not date_str:
            return datetime.min
        
        try:
            return datetime.fromisoformat(date_str.replace('Z', '+00:00'))
        except:
            try:
                return datetime.strptime(date_str, '%Y-%m-%d')
            except:
                return datetime.min
    
    def search_by_category(self, 
                          queries: List[str], 
                          category: str = 'web',
                          max_results_per_query: int = 5,
                          days_back: int = 7,
                          max_workers: int = 4) -> List[Document]:
        """
        æŒ‰ç±»åˆ«æœç´¢
        
        Args:
            queries: æœç´¢æŸ¥è¯¢åˆ—è¡¨
            category: æœç´¢ç±»åˆ« ('web', 'academic', 'news')
            max_results_per_query: æ¯ä¸ªæŸ¥è¯¢çš„æœ€å¤§ç»“æœæ•°
            days_back: æœç´¢å¤šå°‘å¤©å†…çš„å†…å®¹
            max_workers: æœ€å¤§å¹¶è¡Œå·¥ä½œçº¿ç¨‹æ•°
            
        Returns:
            List[Document]: æœç´¢ç»“æœåˆ—è¡¨
        """
        if category not in self.source_types:
            raise ValueError(f"ä¸æ”¯æŒçš„æœç´¢ç±»åˆ«: {category}")
        
        # è·å–è¯¥ç±»åˆ«ä¸‹çš„å¯ç”¨æ•°æ®æº
        sources = [s for s in self.source_types[category] if s in self.collectors]
        
        if not sources:
            print(f"âŒ ç±»åˆ« {category} ä¸‹æ²¡æœ‰å¯ç”¨çš„æ•°æ®æº")
            return []
        
        return self.parallel_search(
            queries=queries,
            sources=sources,
            max_results_per_query=max_results_per_query,
            days_back=days_back,
            max_workers=max_workers
        )
    
    def get_available_sources(self) -> Dict[str, List[str]]:
        """è·å–æ‰€æœ‰å¯ç”¨çš„æ•°æ®æº"""
        available = {}
        for category, sources in self.source_types.items():
            available[category] = [s for s in sources if s in self.collectors]
        return available
    
    def search_with_fallback(self, 
                           queries: List[str],
                           preferred_sources: List[str] = None,
                           fallback_sources: List[str] = None,
                           max_results_per_query: int = 5,
                           days_back: int = 7) -> List[Document]:
        """
        å¸¦é™çº§çš„æœç´¢ï¼Œå¦‚æœé¦–é€‰æ•°æ®æºå¤±è´¥ï¼Œè‡ªåŠ¨ä½¿ç”¨å¤‡é€‰æ•°æ®æº
        
        Args:
            queries: æœç´¢æŸ¥è¯¢åˆ—è¡¨
            preferred_sources: é¦–é€‰æ•°æ®æº
            fallback_sources: å¤‡é€‰æ•°æ®æº
            max_results_per_query: æ¯ä¸ªæŸ¥è¯¢çš„æœ€å¤§ç»“æœæ•°
            days_back: æœç´¢å¤šå°‘å¤©å†…çš„å†…å®¹
            
        Returns:
            List[Document]: æœç´¢ç»“æœåˆ—è¡¨
        """
        # è®¾ç½®é»˜è®¤çš„é¦–é€‰å’Œå¤‡é€‰æ•°æ®æº
        if preferred_sources is None:
            preferred_sources = ['tavily', 'brave']
        
        if fallback_sources is None:
            fallback_sources = ['google', 'arxiv', 'academic']
        
        # é¦–å…ˆå°è¯•é¦–é€‰æ•°æ®æº
        results = self.parallel_search(
            queries=queries,
            sources=preferred_sources,
            max_results_per_query=max_results_per_query,
            days_back=days_back
        )
        
        # å¦‚æœç»“æœä¸å¤Ÿï¼Œä½¿ç”¨å¤‡é€‰æ•°æ®æºè¡¥å……
        if len(results) < len(queries) * max_results_per_query // 2:
            print("ğŸ”„ é¦–é€‰æ•°æ®æºç»“æœä¸è¶³ï¼Œå¯ç”¨å¤‡é€‰æ•°æ®æº...")
            
            fallback_results = self.parallel_search(
                queries=queries,
                sources=fallback_sources,
                max_results_per_query=max_results_per_query,
                days_back=days_back
            )
            
            # åˆå¹¶ç»“æœå¹¶å»é‡
            seen_urls = {doc.url for doc in results}
            for doc in fallback_results:
                if doc.url not in seen_urls:
                    results.append(doc)
                    seen_urls.add(doc.url)
        
        return results