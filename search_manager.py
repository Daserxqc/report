#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æœç´¢ç®¡ç†æ¨¡å—
ç»Ÿä¸€ç®¡ç†æ‰€æœ‰æœç´¢ç›¸å…³åŠŸèƒ½ï¼ŒåŒ…æ‹¬å¤šå¼•æ“å¹¶è¡Œæœç´¢
"""

import json
import threading
from datetime import datetime
from typing import Dict, List, Optional, Any, Union
from concurrent.futures import ThreadPoolExecutor, as_completed
from dataclasses import dataclass

from collectors.web_content_collector import WebContentCollector

from config_manager import config_manager, apis, search_config


@dataclass
class SearchResult:
    """æœç´¢ç»“æœæ•°æ®ç»“æ„"""
    title: str
    content: str
    url: str
    source: str
    category: str = ""
    published_date: str = ""
    relevance_score: float = 0.0


class SearchEngineManager:
    """æœç´¢å¼•æ“ç®¡ç†å™¨"""
    
    def __init__(self):
        self.search_available = False
        self.multi_search_available = False
        self.orchestrator = None
        self.multi_collectors = {}
        self.parallel_data_collector = None
        
        self._initialize_search_components()
        self._initialize_multi_collectors()
    
    def _initialize_search_components(self):
        """åˆå§‹åŒ–æœç´¢ç»„ä»¶"""
        try:
            # ä½¿ç”¨åŠ¨æ€å¯¼å…¥æ¥é¿å…æ¨¡å—è·¯å¾„é—®é¢˜
            import sys
            import importlib.util
            
            search_mcp_path = config_manager.paths.search_mcp_path
            config_file = search_mcp_path / 'search_mcp' / 'config.py'
            generators_file = search_mcp_path / 'search_mcp' / 'generators.py'
            
            print(f"ğŸ” å°è¯•ä»è·¯å¾„å¯¼å…¥: {search_mcp_path}")
            print(f"ğŸ” search_mcpç›®å½•å­˜åœ¨: {search_mcp_path.exists()}")
            print(f"ğŸ” config.pyæ–‡ä»¶å­˜åœ¨: {config_file.exists()}")
            print(f"ğŸ” generators.pyæ–‡ä»¶å­˜åœ¨: {generators_file.exists()}")
            
            # æ·»åŠ search_mcpè·¯å¾„åˆ°sys.pathä»¥æ”¯æŒç›¸å¯¹å¯¼å…¥
            search_mcp_src = str(search_mcp_path)
            if search_mcp_src not in sys.path:
                sys.path.insert(0, search_mcp_src)
            
            # åŠ¨æ€å¯¼å…¥configæ¨¡å—
            config_spec = importlib.util.spec_from_file_location("search_mcp.config", config_file)
            config_module = importlib.util.module_from_spec(config_spec)
            sys.modules['search_mcp.config'] = config_module  # æ³¨å†Œåˆ°sys.modules
            config_spec.loader.exec_module(config_module)
            SearchConfig = config_module.SearchConfig
            
            # å¯¼å…¥modelså’Œloggeræ¨¡å—ä»¥æ”¯æŒgeneratorsçš„ä¾èµ–
            models_file = search_mcp_path / 'search_mcp' / 'models.py'
            logger_file = search_mcp_path / 'search_mcp' / 'logger.py'
            
            models_spec = importlib.util.spec_from_file_location("search_mcp.models", models_file)
            models_module = importlib.util.module_from_spec(models_spec)
            sys.modules['search_mcp.models'] = models_module
            models_spec.loader.exec_module(models_module)
            
            logger_spec = importlib.util.spec_from_file_location("search_mcp.logger", logger_file)
            logger_module = importlib.util.module_from_spec(logger_spec)
            sys.modules['search_mcp.logger'] = logger_module
            logger_spec.loader.exec_module(logger_module)
            
            # åŠ¨æ€å¯¼å…¥generatorsæ¨¡å—
            generators_spec = importlib.util.spec_from_file_location("search_mcp.generators", generators_file)
            generators_module = importlib.util.module_from_spec(generators_spec)
            sys.modules['search_mcp.generators'] = generators_module  # æ³¨å†Œåˆ°sys.modules
            generators_spec.loader.exec_module(generators_module)
            SearchOrchestrator = generators_module.SearchOrchestrator
            
            config = SearchConfig()
            print(f"ğŸ” é…ç½®åˆ›å»ºæˆåŠŸï¼ŒAPIå¯†é’¥çŠ¶æ€: {config.get_api_keys()}")
            
            self.orchestrator = SearchOrchestrator(config)
            self.search_available = True
            print(f"âœ… æœç´¢ç»„ä»¶åˆå§‹åŒ–æˆåŠŸï¼Œå¯ç”¨æ•°æ®æº: {config.get_enabled_sources()}")
            
        except Exception as e:
            print(f"âš ï¸ æœç´¢ç»„ä»¶åˆå§‹åŒ–å¤±è´¥: {e}")
            print(f"   é”™è¯¯ç±»å‹: {type(e).__name__}")
            print(f"   æœç´¢ç»„ä»¶è·¯å¾„: {config_manager.paths.search_mcp_path}")
            import traceback
            traceback.print_exc()
            
            self.orchestrator = None
            self.search_available = False
    
    def _initialize_multi_collectors(self):
        """åˆå§‹åŒ–å¤šæœç´¢å¼•æ“æ”¶é›†å™¨"""
        try:
            from collectors.tavily_collector import TavilyCollector
            from collectors.google_search_collector import GoogleSearchCollector
            from collectors.brave_search_collector import BraveSearchCollector
            
            # åˆå§‹åŒ–Tavilyæ”¶é›†å™¨ï¼ˆé»˜è®¤å¯ç”¨ï¼‰
            self.multi_collectors = {
                'tavily': TavilyCollector(),
            }
            
            # å°è¯•åˆå§‹åŒ–Googleæœç´¢æ”¶é›†å™¨
            try:
                google_collector = GoogleSearchCollector()
                if google_collector.has_api_key:
                    self.multi_collectors['google'] = google_collector
                    print("âœ… Googleæœç´¢æ”¶é›†å™¨å·²å¯ç”¨")
                else:
                    print("âš ï¸ Googleæœç´¢æ”¶é›†å™¨æœªé…ç½®APIå¯†é’¥ï¼Œå·²è·³è¿‡")
            except Exception as e:
                print(f"âš ï¸ Googleæœç´¢æ”¶é›†å™¨ä¸å¯ç”¨: {str(e)}")
            
            # å°è¯•åˆå§‹åŒ–Braveæœç´¢æ”¶é›†å™¨
            try:
                brave_collector = BraveSearchCollector()
                if brave_collector.has_api_key:
                    self.multi_collectors['brave'] = brave_collector
                    print("âœ… Braveæœç´¢æ”¶é›†å™¨å·²å¯ç”¨")
                else:
                    print("âš ï¸ Braveæœç´¢æ”¶é›†å™¨æœªé…ç½®APIå¯†é’¥ï¼Œå·²è·³è¿‡")
            except Exception as e:
                print(f"âš ï¸ Braveæœç´¢æ”¶é›†å™¨ä¸å¯ç”¨: {str(e)}")
            
            self.multi_search_available = True
            print(f"ğŸ” å·²å¯ç”¨ {len(self.multi_collectors)} ä¸ªæœç´¢æ¸ é“: {', '.join(self.multi_collectors.keys())}")
            
            # åˆå§‹åŒ–å¹¶è¡Œæ•°æ®æ”¶é›†å™¨
            if self.multi_search_available:
                self.parallel_data_collector = ParallelDataCollector(self.multi_collectors)
                print("ğŸš€ å¹¶è¡Œæ•°æ®æ”¶é›†å™¨å·²åˆå§‹åŒ–")
            
        except Exception as e:
            self.multi_collectors = {}
            self.multi_search_available = False
            self.parallel_data_collector = None
            print(f"âš ï¸ å¤šæœç´¢å¼•æ“æ”¶é›†å™¨åˆå§‹åŒ–å¤±è´¥: {str(e)}")
    
    def reinitialize_search(self):
        """é‡æ–°åˆå§‹åŒ–æœç´¢æœåŠ¡"""
        if not self.search_available:
            try:
                from search_mcp.config import SearchConfig
                from search_mcp.generators import SearchOrchestrator
                
                config = SearchConfig()
                self.orchestrator = SearchOrchestrator(config)
                self.search_available = True
                print(f"ğŸ”„ æœç´¢æœåŠ¡é‡æ–°åˆå§‹åŒ–æˆåŠŸï¼Œå¯ç”¨æ•°æ®æº: {config.get_enabled_sources()}")
                return True
                
            except Exception as e:
                print(f"âŒ æœç´¢æœåŠ¡é‡æ–°åˆå§‹åŒ–å¤±è´¥: {str(e)}")
                return False
        return True
    
    def get_available_engines(self) -> List[str]:
        """è·å–å¯ç”¨çš„æœç´¢å¼•æ“åˆ—è¡¨"""
        engines = []
        if self.search_available and self.orchestrator:
            engines.append('search_mcp')
        if self.multi_search_available:
            engines.extend(list(self.multi_collectors.keys()))
        return engines
    
    def get_status(self) -> Dict[str, Any]:
        """è·å–æœç´¢ç®¡ç†å™¨çŠ¶æ€"""
        return {
            'search_available': self.search_available,
            'multi_search_available': self.multi_search_available,
            'enabled_collectors': list(self.multi_collectors.keys()),
            'parallel_collector_available': self.parallel_data_collector is not None,
            'api_status': apis.validate_search_apis()
        }


class ParallelDataCollector:
    """å¹¶è¡Œæ•°æ®æ”¶é›†å™¨"""
    
    def __init__(self, collectors_dict):
        self.collectors = collectors_dict
        print(f"ğŸš€ å¹¶è¡Œæ•°æ®æ”¶é›†å™¨åˆå§‹åŒ–ï¼Œæ”¯æŒ {len(collectors_dict)} ä¸ªæœç´¢å¼•æ“")
    
    def parallel_comprehensive_search(self, topic, days=7, max_workers=3):
        """å¹¶è¡Œç»¼åˆæœç´¢"""
        print(f"ğŸ¯ å¼€å§‹å¯¹'{topic}'è¿›è¡Œå¤šå¼•æ“å¹¶è¡Œç»¼åˆæœç´¢ï¼Œæ—¶é—´èŒƒå›´ï¼šæœ€è¿‘{days}å¤©")
        
        # ä½¿ç”¨AIç”Ÿæˆåˆ†ç±»æŸ¥è¯¢
        categorized_queries = query_generator.generate_intelligent_queries(topic, days)
        
        merged_data = {
            "breaking_news": [],
            "innovation_news": [],
            "investment_news": [],
            "policy_news": [],
            "trend_news": [],
            "company_news": []
        }
        
        seen_urls = set()
        results_lock = threading.Lock()
        
        def execute_collector_search(collector_name, collector):
            """æ‰§è¡Œå•ä¸ªæ”¶é›†å™¨çš„æœç´¢"""
            try:
                print(f"  ğŸ” [{collector_name}] å¼€å§‹æœç´¢...")
                collector_data = {category: [] for category in merged_data.keys()}

                for query_item in categorized_queries:
                    query = query_item["query"]
                    category = query_item["category"]
                    
                    if category not in collector_data:
                        continue

                    try:
                        if collector_name == 'tavily':
                            results = collector.search(query, max_results=3)
                        elif collector_name == 'google':
                            results = collector.search(query, max_results=3)
                        elif collector_name == 'brave':
                            results = collector.search(query, count=3)
                        else:
                            continue
                        
                        if results:
                            for result in results:
                                if isinstance(result, dict):
                                    result['category'] = category
                                    result['search_source'] = collector_name
                                    collector_data[category].append(result)

                    except Exception as e:
                        print(f"    âš ï¸ [{collector_name}] æŸ¥è¯¢ '{query}' å¤±è´¥: {str(e)}")
                
                print(f"  âœ… [{collector_name}] æœç´¢å®Œæˆ")
                return collector_name, collector_data
                
            except Exception as e:
                print(f"  âŒ [{collector_name}] æœç´¢å¤±è´¥: {str(e)}")
                return collector_name, {}
        
        # å¹¶è¡Œæ‰§è¡Œæ‰€æœ‰æ”¶é›†å™¨
        with ThreadPoolExecutor(max_workers=min(len(self.collectors), max_workers)) as executor:
            future_to_collector = {
                executor.submit(execute_collector_search, name, collector): name
                for name, collector in self.collectors.items()
            }
            
            for future in as_completed(future_to_collector):
                collector_name = future_to_collector[future]
                try:
                    returned_name, collector_data = future.result()
                    
                    # çº¿ç¨‹å®‰å…¨çš„æ•°æ®åˆå¹¶
                    with results_lock:
                        self._merge_collector_data(merged_data, collector_data, seen_urls, returned_name)
                        
                except Exception as e:
                    print(f"  âŒ [{collector_name}] å¤„ç†ç»“æœå¤±è´¥: {str(e)}")
        
        # è®¡ç®—æ€»æ•°
        total_count = sum(len(merged_data[category]) for category in merged_data.keys())
        merged_data["total_count"] = total_count
        
        print(f"ğŸ¯ å¤šå¼•æ“å¹¶è¡Œç»¼åˆæœç´¢å®Œæˆï¼Œæ€»è®¡ {total_count} æ¡ç»“æœ")

        # Enrich results with full content
        print("ğŸŒ å¼€å§‹æŠ“å–ç½‘é¡µå…¨æ–‡å†…å®¹...")
        web_collector = WebContentCollector()
        
        all_results_to_enrich = []
        for category in merged_data:
            if category != "total_count" and isinstance(merged_data[category], list):
                all_results_to_enrich.extend(merged_data[category])
        
        if all_results_to_enrich:
            enriched_results = web_collector.enrich_search_results(all_results_to_enrich)
            
            # Re-organize enriched results back into categories
            enriched_merged_data = {key: [] for key in merged_data if key != "total_count"}
            
            for result in enriched_results:
                category = result.get('category')
                if category and category in enriched_merged_data:
                    enriched_merged_data[category].append(result)
            
            # Update merged_data with enriched content
            for category in enriched_merged_data:
                merged_data[category] = enriched_merged_data[category]
                
            print(f"âœ… ç½‘é¡µå…¨æ–‡å†…å®¹æŠ“å–å®Œæˆï¼Œå…±å¤„ç† {len(enriched_results)} æ¡ç»“æœ")

        return merged_data
    
    def parallel_targeted_search(self, queries, topic, max_workers=4):
        """å¹¶è¡Œé’ˆå¯¹æ€§æœç´¢"""
        print(f"ğŸ¯ å¼€å§‹å¯¹{len(queries)}ä¸ªæŸ¥è¯¢è¿›è¡Œå¤šå¼•æ“å¹¶è¡Œé’ˆå¯¹æ€§æœç´¢")
        
        additional_data = {
            'perspective_analysis': [],
            'investment_news': [],
            'policy_news': [],
            'innovation_news': [],
            'trend_news': [],
            'breaking_news': []
        }
        
        seen_urls = set()
        results_lock = threading.Lock()
        
        def execute_single_query(query):
            """æ‰§è¡Œå•ä¸ªæŸ¥è¯¢çš„æœç´¢"""
            try:
                print(f"  ğŸ” æ‰§è¡ŒæŸ¥è¯¢: {query[:50]}...")
                query_results = []
                
                # å¯¹æ¯ä¸ªæ”¶é›†å™¨æ‰§è¡ŒæŸ¥è¯¢
                for collector_name, collector in self.collectors.items():
                    try:
                        if collector_name == 'tavily':
                            results = collector.search(query, max_results=3, days=7)
                        elif collector_name == 'google':
                            results = collector.search(query, max_results=3)
                        elif collector_name == 'brave':
                            results = collector.search(query, count=3)
                        else:
                            continue
                        
                        if results:
                            for result in results:
                                if isinstance(result, dict):
                                    result['search_source'] = collector_name
                                    result['query'] = query
                                    query_results.append(result)
                    
                    except Exception as e:
                        print(f"    âš ï¸ [{collector_name}] æŸ¥è¯¢å¤±è´¥: {str(e)}")
                
                print(f"  âœ… æŸ¥è¯¢å®Œæˆï¼Œè·å¾— {len(query_results)} æ¡ç»“æœ")
                return query, query_results
                
            except Exception as e:
                print(f"  âŒ æŸ¥è¯¢æ‰§è¡Œå¤±è´¥: {str(e)}")
                return query, []
        
        # å¹¶è¡Œæ‰§è¡Œæ‰€æœ‰æŸ¥è¯¢
        with ThreadPoolExecutor(max_workers=min(len(queries), max_workers)) as executor:
            future_to_query = {
                executor.submit(execute_single_query, query): query
                for query in queries
            }
            
            for future in as_completed(future_to_query):
                query = future_to_query[future]
                try:
                    returned_query, query_results = future.result()
                    
                    # çº¿ç¨‹å®‰å…¨çš„æ•°æ®åˆ†ç±»å’Œåˆå¹¶
                    with results_lock:
                        for result in query_results:
                            category = self._categorize_search_result(returned_query, topic)
                            if category in additional_data:
                                url = result.get('url', '')
                                if url and url not in seen_urls:
                                    seen_urls.add(url)
                                    additional_data[category].append(result)
                        
                except Exception as e:
                    print(f"  âŒ [{query}] å¤„ç†ç»“æœå¤±è´¥: {str(e)}")
        
        total_count = sum(len(v) for v in additional_data.values())
        print(f"ğŸ¯ å¤šå¼•æ“å¹¶è¡Œé’ˆå¯¹æ€§æœç´¢å®Œæˆï¼Œæ€»è®¡ {total_count} æ¡ç»“æœ")
        return additional_data
    
    
    def _categorize_search_result(self, query, topic):
        """å¯¹æœç´¢ç»“æœè¿›è¡Œåˆ†ç±»"""
        query_lower = query.lower()
        
        if any(keyword in query_lower for keyword in ['breaking', 'çªå‘', 'é‡å¤§', 'major', 'urgent']):
            return 'breaking_news'
        elif any(keyword in query_lower for keyword in ['investment', 'æŠ•èµ„', 'èèµ„', 'funding', 'acquisition']):
            return 'investment_news'
        elif any(keyword in query_lower for keyword in ['policy', 'æ”¿ç­–', 'ç›‘ç®¡', 'regulation', 'compliance']):
            return 'policy_news'
        elif any(keyword in query_lower for keyword in ['innovation', 'åˆ›æ–°', 'æŠ€æœ¯', 'technology', 'breakthrough']):
            return 'innovation_news'
        elif any(keyword in query_lower for keyword in ['trend', 'è¶‹åŠ¿', 'å‘å±•', 'development', 'outlook']):
            return 'trend_news'
        else:
            return 'perspective_analysis'
    
    def _merge_collector_data(self, merged_data, collector_data, seen_urls, collector_name):
        """åˆå¹¶æ”¶é›†å™¨æ•°æ®"""
        added_count = 0
        
        for category, items in collector_data.items():
            if category in merged_data and items:
                for item in items:
                    url = item.get('url', '')
                    if url and url not in seen_urls:
                        seen_urls.add(url)
                        merged_data[category].append(item)
                        added_count += 1
        
        print(f"  ğŸ“Š [{collector_name}] æ–°å¢ {added_count} æ¡å»é‡æ•°æ®")


class SearchQueryGenerator:
    """æœç´¢æŸ¥è¯¢ç”Ÿæˆå™¨"""
    
    @staticmethod
    def generate_intelligent_queries(topic: str, days: int = 7, context: str = "") -> List[Dict[str, str]]:
        """ç”Ÿæˆæ™ºèƒ½æœç´¢æŸ¥è¯¢"""
        try:
            from collectors.llm_processor import LLMProcessor
            llm = LLMProcessor()
            
            today = datetime.now()
            intelligent_prompt = f"""
            ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„{topic}è¡Œä¸šç ”ç©¶åˆ†æå¸ˆã€‚è¯·ä¸º"{topic}"é¢†åŸŸç”Ÿæˆ8-10ä¸ªé«˜è´¨é‡çš„æœç´¢æŸ¥è¯¢ï¼Œç”¨äºæ”¶é›†æœ€è¿‘{days}å¤©çš„é‡è¦ä¿¡æ¯ã€‚

            è¦æ±‚ï¼š
            1. æŸ¥è¯¢åº”æ¶µç›–ä»¥ä¸‹5ä¸ªæ ¸å¿ƒç»´åº¦ï¼Œå¹¶ä¸ºæ¯ä¸ªæŸ¥è¯¢æŒ‡å®šå¯¹åº”çš„ç±»åˆ«ï¼š
               - é‡å¤§æ–°é—»äº‹ä»¶ (breaking_news)
               - æŠ€æœ¯åˆ›æ–°çªç ´ (innovation_news) 
               - æŠ•èµ„èèµ„åŠ¨æ€ (investment_news)
               - æ”¿ç­–ç›‘ç®¡å˜åŒ– (policy_news)
               - è¡Œä¸šå‘å±•è¶‹åŠ¿ (trend_news)

            2. æŸ¥è¯¢ç‰¹ç‚¹ï¼š
               - åŒ…å«æ—¶é—´é™å®šè¯ï¼ˆæœ€è¿‘ã€{today.year}å¹´ã€{days}å¤©å†…ç­‰ï¼‰
               - ç»“åˆä¸­è‹±æ–‡å…³é”®è¯æé«˜è¦†ç›–ç‡
               - é’ˆå¯¹{topic}é¢†åŸŸçš„ä¸“ä¸šæœ¯è¯­
               - å…³æ³¨çªå‘æ€§ã€é‡è¦æ€§ã€å½±å“åŠ›å¤§çš„äº‹ä»¶

            3. è¾“å‡ºæ ¼å¼ï¼š
               è¿”å›ä¸€ä¸ªJSONå¯¹è±¡ï¼Œå…¶ä¸­åŒ…å«ä¸€ä¸ªåä¸º "queries" çš„æ•°ç»„ã€‚æ•°ç»„ä¸­çš„æ¯ä¸ªå¯¹è±¡éƒ½åº”åŒ…å« "query" å’Œ "category" ä¸¤ä¸ªå­—æ®µã€‚
               
            ç¤ºä¾‹è¾“å‡ºï¼š
            {{
                "queries": [
                    {{
                        "query": "{topic} é‡å¤§çªç ´ {today.year}å¹´ æœ€æ–° breakthrough major development",
                        "category": "innovation_news"
                    }},
                    {{
                        "query": "{topic} æŠ•èµ„ èèµ„ å¹¶è´­ {today.year}å¹´ æœ€è¿‘{days}å¤© investment funding acquisition",
                        "category": "investment_news"
                    }}
                ]
            }}

            ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼š{context}
            
            è¯·ç”ŸæˆæŸ¥è¯¢ï¼š
            """
            
            response = llm.call_llm_api(
                prompt=intelligent_prompt,
                system_message=f"ä½ æ˜¯{topic}é¢†åŸŸçš„ä¸“ä¸šåˆ†æå¸ˆï¼Œæ“…é•¿ç”Ÿæˆé«˜è´¨é‡çš„æœç´¢æŸ¥è¯¢ã€‚",
                max_tokens=1500,
                temperature=0.7
            )
            
            if response:
                import re
                json_match = re.search(r'\{\s*"queries":\s*\[.*?\]\s*\}', response, re.DOTALL)
                if json_match:
                    queries_json = json_match.group()
                    queries_data = json.loads(queries_json)
                    queries = queries_data.get("queries", [])
                    if queries:
                        print(f"ğŸ¤– [AIæŸ¥è¯¢ç”Ÿæˆ] æˆåŠŸç”Ÿæˆ{len(queries)}ä¸ªæ™ºèƒ½æŸ¥è¯¢")
                        return queries
        
        except Exception as e:
            print(f"âš ï¸ [AIæŸ¥è¯¢ç”Ÿæˆå¤±è´¥] {str(e)}")
        
        # å¤‡ç”¨é¢„è®¾æŸ¥è¯¢ç­–ç•¥
        print(f"ğŸ” [å¤‡ç”¨ç­–ç•¥] ä½¿ç”¨é¢„è®¾{topic}æŸ¥è¯¢æ¨¡æ¿")
        today = datetime.now()
        fallback_queries = [
            {{"query": f"{topic} é‡å¤§äº‹ä»¶ çªå‘ {today.year}å¹´ æœ€æ–° breaking news major event", "category": "breaking_news"}},
            {{"query": f"{topic} æŠ€æœ¯åˆ›æ–° çªç ´ {today.year}å¹´ æœ€è¿‘{days}å¤© innovation breakthrough technology", "category": "innovation_news"}},
            {{"query": f"{topic} æŠ•èµ„ èèµ„ å¹¶è´­ {today.year}å¹´ æœ€æ–° investment funding acquisition merger", "category": "investment_news"}},
            {{"query": f"{topic} æ”¿ç­– ç›‘ç®¡ æ³•è§„ {today.year}å¹´ æœ€æ–° policy regulation compliance", "category": "policy_news"}},
            {{"query": f"{topic} å‘å±•è¶‹åŠ¿ å‰æ™¯ {today.year}å¹´ æœ€æ–° trend development future outlook", "category": "trend_news"}},
            {{"query": f"{topic} è¡Œä¸šéœ‡åŠ¨ é‡ç£…æ¶ˆæ¯ {today.year}å¹´ æœ€è¿‘{days}å¤© industry shock major news", "category": "breaking_news"}},
            {{"query": f"{topic} å¸‚åœºå˜åŒ– åŠ¨æ€ {today.year}å¹´ æœ€æ–° market change dynamics", "category": "trend_news"}},
            {{"query": f"{topic} ç«äº‰æ ¼å±€ é¢†å…ˆä¼ä¸š {today.year}å¹´ æœ€æ–° competition landscape leading companies", "category": "company_news"}}
        ]
        
        return fallback_queries


# å…¨å±€æœç´¢ç®¡ç†å™¨å®ä¾‹
search_manager = SearchEngineManager()

# ä¾¿æ·è®¿é—®
parallel_collector = search_manager.parallel_data_collector
query_generator = SearchQueryGenerator()

if __name__ == "__main__":
    # æ‰“å°æœç´¢ç®¡ç†å™¨çŠ¶æ€
    status = search_manager.get_status()
    print("\nğŸ” æœç´¢ç®¡ç†å™¨çŠ¶æ€æŠ¥å‘Š:")
    print(f"   ğŸ” åŸºç¡€æœç´¢: {'âœ…' if status['search_available'] else 'âŒ'}")
    print(f"   ğŸš€ å¤šå¼•æ“æœç´¢: {'âœ…' if status['multi_search_available'] else 'âŒ'}")
    print(f"   ğŸ“¦ å¯ç”¨æ”¶é›†å™¨: {', '.join(status['enabled_collectors']) if status['enabled_collectors'] else 'æ— '}")
    print(f"   âš¡ å¹¶è¡Œæ”¶é›†å™¨: {'âœ…' if status['parallel_collector_available'] else 'âŒ'}")
else:
    print("âœ… æœç´¢ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ")