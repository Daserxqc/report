"""
Search MCP æ ¸å¿ƒæœç´¢ç”Ÿæˆå™¨

å‚è€ƒç»˜æœ¬ç”Ÿæˆæ¨¡å—çš„æ¶æ„ï¼Œåˆ›å»ºå„ç§æœç´¢Agentç±»
"""

import asyncio
import time
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime, timedelta
from typing import List, Dict, Optional, Union, Any, Tuple
from pathlib import Path
import sys
import json

# å¯¼å…¥é…ç½®å’Œæ¨¡å‹
from .config import SearchConfig
from .models import Document, SearchResult, SearchMetrics, CollectorInfo
from .logger import SearchLogger

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„ä»¥å¯¼å…¥ç°æœ‰æ”¶é›†å™¨
parent_dir = Path(__file__).parent.parent.parent.parent
sys.path.insert(0, str(parent_dir))

# å¯¼å…¥ç°æœ‰çš„æ”¶é›†å™¨
try:
    from collectors.tavily_collector import TavilyCollector
    from collectors.brave_search_collector import BraveSearchCollector  
    from collectors.google_search_collector import GoogleSearchCollector
    from collectors.arxiv_collector import ArxivCollector
    from collectors.academic_collector import AcademicCollector
    from collectors.news_collector import NewsCollector
except ImportError as e:
    print(f"âš ï¸ å¯¼å…¥æ”¶é›†å™¨å¤±è´¥: {e}")
    # åˆ›å»ºç©ºç±»ä»¥é˜²å¯¼å…¥å¤±è´¥
    class TavilyCollector: pass
    class BraveSearchCollector: pass
    class GoogleSearchCollector: pass
    class ArxivCollector: pass
    class AcademicCollector: pass
    class NewsCollector: pass


class BaseSearchAgent:
    """
    å°è£…æœç´¢ç›¸å…³æ“ä½œçš„åŸºç¡€Agentç±»
    æ‰€æœ‰å…·ä½“çš„æœç´¢Agentéƒ½ç»§æ‰¿è‡ªæ­¤ç±»
    """
    
    def __init__(self, config: SearchConfig):
        """
        åˆå§‹åŒ–åŸºç¡€æœç´¢Agent
        
        Args:
            config (SearchConfig): æœç´¢é…ç½®å®ä¾‹
        """
        self.config = config
        self.logger = SearchLogger(config.to_dict())
        self.results_lock = threading.Lock()
        
    def _parse_date(self, date_str: Optional[str]) -> datetime:
        """è§£ææ—¥æœŸå­—ç¬¦ä¸²ä¸ºdatetimeå¯¹è±¡"""
        if not date_str:
            return datetime.min
        
        try:
            return datetime.fromisoformat(date_str.replace('Z', '+00:00'))
        except:
            try:
                return datetime.strptime(date_str, '%Y-%m-%d')
            except:
                return datetime.min
    
    def _extract_publish_date(self, result: Dict) -> Optional[str]:
        """æå–å‘å¸ƒæ—¥æœŸ"""
        date_fields = ['publish_date', 'published', 'date', 'year', 'publication_date']
        
        for field in date_fields:
            if field in result and result[field]:
                date_value = result[field]
                
                # å¦‚æœæ˜¯å¹´ä»½æ•´æ•°
                if isinstance(date_value, int):
                    return f"{date_value}-01-01"
                
                # å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œå°è¯•è§£æ
                if isinstance(date_value, str):
                    try:
                        # å°è¯•è§£æISOæ ¼å¼æ—¥æœŸ
                        parsed_date = datetime.fromisoformat(date_value.replace('Z', '+00:00'))
                        return parsed_date.strftime('%Y-%m-%d')
                    except:
                        return date_value
        
        return None
    
    def _determine_source_type(self, source: str, result: Dict) -> str:
        """ç¡®å®šæ•°æ®æºç±»å‹"""
        if source in ['arxiv', 'academic']:
            return 'academic'
        elif source == 'news' or 'news' in source:
            return 'news'
        else:
            return 'web'


class CollectorInitializationAgent(BaseSearchAgent):
    """è´Ÿè´£åˆå§‹åŒ–å’Œç®¡ç†å„ç§æ•°æ®æ”¶é›†å™¨çš„Agent"""
    
    def __init__(self, config: SearchConfig):
        super().__init__(config)
        self.collectors = {}
        self.source_types = {
            'web': ['tavily', 'brave', 'google'],
            'academic': ['arxiv', 'academic', 'semantic_scholar', 'ieee', 'springer', 'core'],
            'news': ['news_api', 'google_news', 'brave_news', 'rss']
        }
        self.initialize_all_collectors()
    
    def initialize_all_collectors(self):
        """åˆå§‹åŒ–æ‰€æœ‰å¯ç”¨çš„æ”¶é›†å™¨"""
        self.logger.logger.info("ğŸ”§ å¼€å§‹åˆå§‹åŒ–æ•°æ®æ”¶é›†å™¨...")
        
        # Webæœç´¢æ”¶é›†å™¨
        self._init_web_collectors()
        
        # å­¦æœ¯æœç´¢æ”¶é›†å™¨  
        self._init_academic_collectors()
        
        # æ–°é—»æ”¶é›†å™¨
        self._init_news_collectors()
        
        self.logger.logger.info(f"âœ… æ”¶é›†å™¨åˆå§‹åŒ–å®Œæˆï¼Œå¯ç”¨æ”¶é›†å™¨: {list(self.collectors.keys())}")
    
    def _init_web_collectors(self):
        """åˆå§‹åŒ–Webæœç´¢æ”¶é›†å™¨"""
        # Tavilyæ”¶é›†å™¨
        if self.config.has_api_key('tavily'):
            try:
                tavily = TavilyCollector()
                if hasattr(tavily, 'has_api_key') and tavily.has_api_key:
                    self.collectors['tavily'] = tavily
                    self.logger.logger.info("âœ… Tavilyæ”¶é›†å™¨å·²å¯ç”¨")
            except Exception as e:
                self.logger.logger.error(f"âš ï¸ Tavilyæ”¶é›†å™¨åˆå§‹åŒ–å¤±è´¥: {str(e)}")
        
        # Braveæ”¶é›†å™¨
        if self.config.has_api_key('brave'):
            try:
                brave = BraveSearchCollector()
                if hasattr(brave, 'has_api_key') and brave.has_api_key:
                    self.collectors['brave'] = brave
                    self.logger.logger.info("âœ… Braveæ”¶é›†å™¨å·²å¯ç”¨")
            except Exception as e:
                self.logger.logger.error(f"âš ï¸ Braveæ”¶é›†å™¨åˆå§‹åŒ–å¤±è´¥: {str(e)}")
        
        # Googleæ”¶é›†å™¨
        if self.config.has_api_key('google') and getattr(self.config, 'GOOGLE_SEARCH_ENABLED', True):
            try:
                google = GoogleSearchCollector()
                if hasattr(google, 'has_api_key') and google.has_api_key:
                    self.collectors['google'] = google
                    self.logger.logger.info("âœ… Googleæ”¶é›†å™¨å·²å¯ç”¨")
            except Exception as e:
                self.logger.logger.error(f"âš ï¸ Googleæ”¶é›†å™¨åˆå§‹åŒ–å¤±è´¥: {str(e)}")
        else:
            self.logger.logger.info("âš ï¸ Googleæ”¶é›†å™¨å·²ç¦ç”¨")
    
    def _init_academic_collectors(self):
        """åˆå§‹åŒ–å­¦æœ¯æœç´¢æ”¶é›†å™¨"""
        # ArXivæ”¶é›†å™¨ï¼ˆæ— éœ€APIå¯†é’¥ï¼‰
        try:
            arxiv = ArxivCollector()
            self.collectors['arxiv'] = arxiv
            self.logger.logger.info("âœ… ArXivæ”¶é›†å™¨å·²å¯ç”¨")
        except Exception as e:
            self.logger.logger.error(f"âš ï¸ ArXivæ”¶é›†å™¨åˆå§‹åŒ–å¤±è´¥: {str(e)}")
        
        # Academicæ”¶é›†å™¨ï¼ˆæ— éœ€APIå¯†é’¥ï¼‰
        try:
            academic = AcademicCollector()
            self.collectors['academic'] = academic
            self.logger.logger.info("âœ… Academicæ”¶é›†å™¨å·²å¯ç”¨")
        except Exception as e:
            self.logger.logger.error(f"âš ï¸ Academicæ”¶é›†å™¨åˆå§‹åŒ–å¤±è´¥: {str(e)}")
    
    def _init_news_collectors(self):
        """åˆå§‹åŒ–æ–°é—»æ”¶é›†å™¨"""
        if self.config.has_api_key('news'):
            try:
                news = NewsCollector()
                self.collectors['news'] = news
                self.logger.logger.info("âœ… Newsæ”¶é›†å™¨å·²å¯ç”¨")
            except Exception as e:
                self.logger.logger.error(f"âš ï¸ Newsæ”¶é›†å™¨åˆå§‹åŒ–å¤±è´¥: {str(e)}")
    
    def get_available_sources(self) -> Dict[str, List[str]]:
        """è·å–æ‰€æœ‰å¯ç”¨çš„æ•°æ®æº"""
        available = {}
        for category, sources in self.source_types.items():
            available[category] = [s for s in sources if s in self.collectors]
        return available
    
    def get_collector_info(self) -> List[CollectorInfo]:
        """è·å–æ‰€æœ‰æ”¶é›†å™¨çš„ä¿¡æ¯"""
        info_list = []
        all_sources = ['tavily', 'brave', 'google', 'arxiv', 'academic', 'news']
        
        for source in all_sources:
            is_available = source in self.collectors
            api_key_required = source not in ['arxiv', 'academic']
            has_api_key = self.config.has_api_key(source)
            
            # ç¡®å®šæ•°æ®æºç±»å‹
            source_type = 'web'
            if source in ['arxiv', 'academic']:
                source_type = 'academic'
            elif source == 'news':
                source_type = 'news'
            
            info = CollectorInfo(
                name=source,
                source_type=source_type,
                is_available=is_available,
                api_key_required=api_key_required,
                has_api_key=has_api_key,
                description=f"{source.title()}æœç´¢æ”¶é›†å™¨"
            )
            
            info_list.append(info)
        
        return info_list


class SearchExecutionAgent(BaseSearchAgent):
    """è´Ÿè´£æ‰§è¡Œå…·ä½“æœç´¢æ“ä½œçš„Agent"""
    
    def __init__(self, config: SearchConfig, collectors: Dict):
        super().__init__(config)
        self.collectors = collectors
    
    def execute_single_search(self, query: str, source: str, max_results: int, days_back: int) -> List[Document]:
        """æ‰§è¡Œå•ä¸ªæœç´¢ä»»åŠ¡"""
        collector = self.collectors.get(source)
        if not collector:
            return []
        
        try:
            # æ ¹æ®ä¸åŒçš„æ”¶é›†å™¨è°ƒç”¨ç›¸åº”çš„æœç´¢æ–¹æ³•
            raw_results = []
            
            if source == 'tavily':
                if hasattr(collector, 'search'):
                    raw_results = collector.search(query, max_results=max_results)
                
            elif source == 'brave':
                if hasattr(collector, 'search'):
                    raw_results = collector.search(query, count=max_results)
                
            elif source == 'google':
                if hasattr(collector, 'search'):
                    raw_results = collector.search(query, days_back=days_back, max_results=max_results)
                
            elif source == 'arxiv':
                if hasattr(collector, 'search'):
                    raw_results = collector.search(query, days_back=days_back)
                
            elif source == 'academic':
                # Academicæ”¶é›†å™¨æœ‰å¤šä¸ªæ–¹æ³•ï¼Œè¿™é‡Œä½¿ç”¨Semantic Scholar
                if hasattr(collector, 'search_semantic_scholar'):
                    raw_results = collector.search_semantic_scholar(query, days_back=days_back)
                
            elif source == 'news':
                if hasattr(collector, 'search_news_api'):
                    raw_results = collector.search_news_api(query, days_back=days_back)
            
            # æ ‡å‡†åŒ–ç»“æœä¸ºDocumentæ ¼å¼
            documents = self.standardize_results(raw_results, source)
            
            return documents[:max_results]  # é™åˆ¶ç»“æœæ•°é‡
            
        except Exception as e:
            self.logger.logger.error(f"æœç´¢æ‰§è¡Œå¤±è´¥ {source}({query}): {str(e)}")
            return []
    
    def standardize_results(self, raw_results: List[Dict], source: str) -> List[Document]:
        """å°†åŸå§‹æœç´¢ç»“æœæ ‡å‡†åŒ–ä¸ºDocumentæ ¼å¼"""
        documents = []
        
        for result in raw_results:
            try:
                # ç¡®å®šæ•°æ®æºç±»å‹
                source_type = self._determine_source_type(source, result)
                
                # æå–æ ‡å‡†å­—æ®µ
                title = result.get('title', 'Untitled')
                
                # å†…å®¹å­—æ®µå¯èƒ½æœ‰å¤šç§åç§°
                content = (result.get('content') or 
                          result.get('summary') or 
                          result.get('abstract') or 
                          result.get('snippet') or 
                          result.get('description') or '')
                
                url = result.get('url', '#')
                
                # ä½œè€…å¤„ç†
                authors = []
                if 'authors' in result:
                    if isinstance(result['authors'], list):
                        authors = result['authors']
                    elif isinstance(result['authors'], str):
                        authors = [result['authors']]
                
                # å‘å¸ƒæ—¥æœŸå¤„ç†
                publish_date = self._extract_publish_date(result)
                
                # æœŸåˆŠ/ä¼šè®®åç§°
                venue = result.get('venue') or result.get('source')
                
                # åˆ›å»ºDocumentå¯¹è±¡
                doc = Document(
                    title=title,
                    content=content,
                    url=url,
                    source=source,
                    source_type=source_type,
                    publish_date=publish_date,
                    authors=authors,
                    venue=venue,
                    score=result.get('score'),
                    language=result.get('language')
                )
                
                documents.append(doc)
                
            except Exception as e:
                self.logger.logger.error(f"æ ‡å‡†åŒ–ç»“æœå¤±è´¥: {str(e)}")
                continue
        
        return documents


class ParallelSearchAgent(BaseSearchAgent):
    """è´Ÿè´£å¹¶è¡Œæœç´¢æ‰§è¡Œå’Œç»“æœèšåˆçš„Agent"""
    
    def __init__(self, config: SearchConfig, collectors: Dict, execution_agent: SearchExecutionAgent):
        super().__init__(config)
        self.collectors = collectors
        self.execution_agent = execution_agent
    
    def parallel_search(self, 
                       queries: List[str], 
                       sources: List[str] = None, 
                       max_results_per_query: int = 5,
                       days_back: int = 7,
                       max_workers: int = 6) -> List[Document]:
        """
        å¹¶è¡Œæœç´¢å¤šä¸ªæŸ¥è¯¢å’Œæ•°æ®æº
        """
        start_time = time.time()
        
        if not queries:
            return []
        
        # å¦‚æœæ²¡æœ‰æŒ‡å®šæ•°æ®æºï¼Œä½¿ç”¨æ‰€æœ‰å¯ç”¨çš„
        if sources is None:
            sources = list(self.collectors.keys())
        else:
            # è¿‡æ»¤åªä¿ç•™å¯ç”¨çš„æ•°æ®æº
            sources = [s for s in sources if s in self.collectors]
        
        if not sources:
            self.logger.logger.error("âŒ æ²¡æœ‰å¯ç”¨çš„æ•°æ®æº")
            return []
        
        # è®°å½•æœç´¢å¼€å§‹
        self.logger.log_search_start(queries, sources, {
            'max_results_per_query': max_results_per_query,
            'days_back': days_back,
            'max_workers': max_workers
        })
        
        all_results = []
        seen_urls = set()
        
        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œæ‰§è¡Œæœç´¢
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # ä¸ºæ¯ä¸ªæŸ¥è¯¢å’Œæ•°æ®æºç»„åˆåˆ›å»ºä»»åŠ¡
            future_to_info = {}
            
            for query in queries:
                for source in sources:
                    if source in self.collectors:
                        future = executor.submit(
                            self.execution_agent.execute_single_search,
                            query, source, max_results_per_query, days_back
                        )
                        future_to_info[future] = (query, source)
            
            # æ”¶é›†ç»“æœ
            completed_tasks = 0
            total_tasks = len(future_to_info)
            errors = []
            
            for future in as_completed(future_to_info):
                query, source = future_to_info[future]
                completed_tasks += 1
                
                try:
                    results = future.result()
                    
                    # å»é‡å¹¶åˆå¹¶ç»“æœ
                    new_count = 0
                    for doc in results:
                        if doc.url not in seen_urls:
                            seen_urls.add(doc.url)
                            all_results.append(doc)
                            new_count += 1
                    
                    # è®°å½•å•ä¸ªæºçš„ç»“æœ
                    self.logger.log_source_result(source, query, len(results), True)
                    self.logger.logger.info(f"  âœ… [{completed_tasks}/{total_tasks}] {source}({query}): {len(results)}æ¡ç»“æœ, {new_count}æ¡æ–°å¢")
                    
                except Exception as e:
                    error_msg = f"{source}({query}) æœç´¢å¤±è´¥: {str(e)}"
                    errors.append(error_msg)
                    self.logger.log_source_result(source, query, 0, False, str(e))
                    self.logger.logger.error(f"  âŒ [{completed_tasks}/{total_tasks}] {error_msg}")
        
        # æŒ‰ç›¸å…³æ€§å’Œæ—¶é—´æ’åº
        all_results.sort(key=lambda doc: (
            doc.score or 0,  # ç›¸å…³æ€§è¯„åˆ†
            self._parse_date(doc.publish_date) if doc.publish_date else datetime.min
        ), reverse=True)
        
        total_time = time.time() - start_time
        
        # è®°å½•æœç´¢å®Œæˆ
        self.logger.log_search_complete(len(all_results), total_time, sources, errors)
        
        return all_results
    
    def search_by_category(self, 
                          queries: List[str], 
                          category: str = 'web',
                          max_results_per_query: int = 5,
                          days_back: int = 7,
                          max_workers: int = 4) -> List[Document]:
        """
        æŒ‰ç±»åˆ«æœç´¢
        """
        source_types = {
            'web': ['tavily', 'brave', 'google'],
            'academic': ['arxiv', 'academic', 'semantic_scholar', 'ieee', 'springer', 'core'],
            'news': ['news_api', 'google_news', 'brave_news', 'rss']
        }
        
        if category not in source_types:
            raise ValueError(f"ä¸æ”¯æŒçš„æœç´¢ç±»åˆ«: {category}")
        
        # è·å–è¯¥ç±»åˆ«ä¸‹çš„å¯ç”¨æ•°æ®æº
        sources = [s for s in source_types[category] if s in self.collectors]
        
        if not sources:
            self.logger.logger.error(f"âŒ ç±»åˆ« {category} ä¸‹æ²¡æœ‰å¯ç”¨çš„æ•°æ®æº")
            return []
        
        return self.parallel_search(
            queries=queries,
            sources=sources,
            max_results_per_query=max_results_per_query,
            days_back=days_back,
            max_workers=max_workers
        )
    
    def search_with_fallback(self, 
                           queries: List[str],
                           preferred_sources: List[str] = None,
                           fallback_sources: List[str] = None,
                           max_results_per_query: int = 5,
                           days_back: int = 7) -> List[Document]:
        """
        å¸¦é™çº§çš„æœç´¢ï¼Œå¦‚æœé¦–é€‰æ•°æ®æºå¤±è´¥ï¼Œè‡ªåŠ¨ä½¿ç”¨å¤‡é€‰æ•°æ®æº
        """
        # è®¾ç½®é»˜è®¤çš„é¦–é€‰å’Œå¤‡é€‰æ•°æ®æº
        if preferred_sources is None:
            preferred_sources = ['tavily', 'brave']
        
        if fallback_sources is None:
            fallback_sources = ['google', 'arxiv', 'academic']
        
        # é¦–å…ˆå°è¯•é¦–é€‰æ•°æ®æº
        results = self.parallel_search(
            queries=queries,
            sources=preferred_sources,
            max_results_per_query=max_results_per_query,
            days_back=days_back
        )
        
        # å¦‚æœç»“æœä¸å¤Ÿï¼Œä½¿ç”¨å¤‡é€‰æ•°æ®æºè¡¥å……
        if len(results) < len(queries) * max_results_per_query // 2:
            self.logger.logger.info("ğŸ”„ é¦–é€‰æ•°æ®æºç»“æœä¸è¶³ï¼Œå¯ç”¨å¤‡é€‰æ•°æ®æº...")
            
            fallback_results = self.parallel_search(
                queries=queries,
                sources=fallback_sources,
                max_results_per_query=max_results_per_query,
                days_back=days_back
            )
            
            # åˆå¹¶ç»“æœå¹¶å»é‡
            seen_urls = {doc.url for doc in results}
            for doc in fallback_results:
                if doc.url not in seen_urls:
                    results.append(doc)
                    seen_urls.add(doc.url)
        
        return results


class SearchOrchestrator:
    """
    æœç´¢æœåŠ¡çš„æ€»è°ƒåº¦ä¸­å¿ƒï¼ˆOrchestratorï¼‰
    è´Ÿè´£åè°ƒå„ä¸ªSearchAgentï¼Œç®¡ç†æ•´ä¸ªç«¯åˆ°ç«¯çš„æœç´¢æµç¨‹
    """
    
    def __init__(self, config: SearchConfig):
        """
        åˆå§‹åŒ–æœç´¢ç¼–æ’å™¨ï¼Œåˆ›å»ºæ‰€éœ€çš„Agentå®ä¾‹
        
        Args:
            config (SearchConfig): æœç´¢é…ç½®å®ä¾‹
        """
        self.config = config
        self.logger = SearchLogger(config.to_dict())
        
        # åˆå§‹åŒ–å„ä¸ªAgent
        self.collector_agent = CollectorInitializationAgent(config)
        self.execution_agent = SearchExecutionAgent(config, self.collector_agent.collectors)
        self.parallel_agent = ParallelSearchAgent(config, self.collector_agent.collectors, self.execution_agent)
        
        self.logger.logger.info("ğŸš€ SearchOrchestratoråˆå§‹åŒ–å®Œæˆ")
    
    def parallel_search(self, 
                       queries: List[str], 
                       sources: List[str] = None, 
                       max_results_per_query: int = 5,
                       days_back: int = 7,
                       max_workers: int = 6) -> List[Document]:
        """
        æ‰§è¡Œå¹¶è¡Œæœç´¢
        """
        return self.parallel_agent.parallel_search(
            queries=queries,
            sources=sources,
            max_results_per_query=max_results_per_query,
            days_back=days_back,
            max_workers=max_workers
        )
    
    def search_by_category(self, 
                          queries: List[str], 
                          category: str = 'web',
                          max_results_per_query: int = 5,
                          days_back: int = 7,
                          max_workers: int = 4) -> List[Document]:
        """
        æŒ‰ç±»åˆ«æœç´¢
        """
        return self.parallel_agent.search_by_category(
            queries=queries,
            category=category,
            max_results_per_query=max_results_per_query,
            days_back=days_back,
            max_workers=max_workers
        )
    
    def search_with_fallback(self, 
                           queries: List[str],
                           preferred_sources: List[str] = None,
                           fallback_sources: List[str] = None,
                           max_results_per_query: int = 5,
                           days_back: int = 7) -> List[Document]:
        """
        å¸¦é™çº§çš„æœç´¢
        """
        return self.parallel_agent.search_with_fallback(
            queries=queries,
            preferred_sources=preferred_sources,
            fallback_sources=fallback_sources,
            max_results_per_query=max_results_per_query,
            days_back=days_back
        )
    
    def get_available_sources(self) -> Dict[str, List[str]]:
        """è·å–æ‰€æœ‰å¯ç”¨çš„æ•°æ®æº"""
        return self.collector_agent.get_available_sources()
    
    def get_collector_info(self) -> List[CollectorInfo]:
        """è·å–æ‰€æœ‰æ”¶é›†å™¨çš„ä¿¡æ¯"""
        return self.collector_agent.get_collector_info()
    
    def get_search_metrics(self, search_results: List[Document], execution_time: float, 
                          queries: List[str], sources_used: List[str]) -> SearchMetrics:
        """ç”Ÿæˆæœç´¢æ€§èƒ½æŒ‡æ ‡"""
        errors = []  # è¿™é‡Œå¯ä»¥æ”¶é›†æ‰§è¡Œè¿‡ç¨‹ä¸­çš„é”™è¯¯
        
        metrics = SearchMetrics(
            total_queries=len(queries),
            successful_queries=len(queries),  # ç®€åŒ–å¤„ç†ï¼Œå®é™…åº”è¯¥ç»Ÿè®¡æˆåŠŸçš„æŸ¥è¯¢æ•°
            failed_queries=0,
            total_results=len(search_results),
            execution_time=execution_time,
            average_time_per_query=execution_time / len(queries) if queries else 0,
            sources_used=sources_used,
            errors=errors
        )
        
        return metrics


# For backward compatibility - ä¸ºäº†ä¿æŒå‘åå…¼å®¹æ€§
SearchGenerator = SearchOrchestrator 