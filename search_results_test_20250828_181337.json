{
    "breaking_news": [
        {
            "title": "通过GCR 恢复共和国报告｜截至2025 年8 月26 日星期二更新– 准备 ...",
            "content": "2 days ago ... 我鼓励你自己做研究，并自己决定在这场伟大的善恶战争中发生了什么。 Compiled Tues. 26 August 2025 12:01 am EST by Judy Byington, MSW, LCSW, Therapist ret, ...",
            "url": "https://www.pfcchina.org/dajielou/71548.html",
            "source": "www.pfcchina.org",
            "published_date": "2025-08-28",
            "search_engine": "Google",
            "query": "人工智能在医疗诊断中的应用与发展趋势 重大新闻 突发事件 最近7天 breaking news emergency AI medical diagnosis",
            "description": "2 days ago ... 我鼓励你自己做研究，并自己决定在这场伟大的善恶战争中发生了什么。 Compiled Tues. 26 August 2025 12:01 am EST by Judy Byington, MSW, LCSW, Therapist ret, ...",
            "image": "",
            "category": "breaking_news",
            "search_source": "google",
            "full_content": "2 days ago ... 我鼓励你自己做研究，并自己决定在这场伟大的善恶战争中发生了什么。 Compiled Tues. 26 August 2025 12:01 am EST by Judy Byington, MSW, LCSW, Therapist ret, ..."
        },
        {
            "title": "医疗领域人工智能的应用、风险及对策",
            "content": "用微信扫码二维码 · 分享至好友和朋友圈",
            "url": "https://aj.xhu.edu.cn/xhdxxbzskb/article/doi/10.12189/j.issn.1672-8505.2021.01.004",
            "source": "aj.xhu.edu.cn",
            "published_date": "January 26, 2021",
            "search_engine": "Brave",
            "query": "人工智能在医疗诊断中的应用与发展趋势 重大新闻 突发事件 最近7天 breaking news emergency AI medical diagnosis",
            "language": "unk",
            "family_friendly": true,
            "category": "breaking_news",
            "search_source": "brave",
            "full_content": "1672-8505\nCN 51-1675/C\n\n \n \n\n\n\n/\n\n用微信扫码二维码\n分享至好友和朋友圈"
        },
        {
            "title": "AI 医疗设备的飞速发展推动医疗行业的变革 | NVIDIA 英伟达博客",
            "content": "软件定义和 <strong>AI</strong> 赋能是医疗行业的两大未来趋势。目前，市场上大约有 700 种 FDA 认证的 <strong>AI</strong> 医疗设备，是 2020 年的 10 倍多。 推动这一领域蓬勃发展的众多创新企业近期在硅谷举行的 NVIDIA GTC 全球大会上发布了他们的最新 <strong>AI</strong> ...",
            "url": "https://blogs.nvidia.cn/blog/ai-medical-devices-gtc-2024/",
            "source": "blogs.nvidia.cn",
            "published_date": "April 15, 2024",
            "search_engine": "Brave",
            "query": "人工智能在医疗诊断中的应用与发展趋势 重大新闻 突发事件 最近7天 breaking news emergency AI medical diagnosis",
            "language": "zh",
            "family_friendly": true,
            "category": "breaking_news",
            "search_source": "brave",
            "full_content": "扫描二维码将网页分享至朋友圈\n软件定义和 AI 赋能是医疗行业的两大未来趋势。目前，市场上大约有 700 种 FDA 认证的 AI 医疗设备，是 2020 年的 10 倍多。\n推动这一领域蓬勃发展的众多创新企业近期在硅谷举行的 NVIDIA GTC 全球大会上发布了他们的最新 AI 赋能的解决方案。本届 GTC 不仅吸引了 16000 多名商界领袖、开发者和研究者亲临现场，并且还吸引了许多线上参加者。\n这些新技术旨在提高医疗健康行业的效率和患者的治疗效果，例如可以加速超声波分析的基础模型、用于心脏成像的增强和虚拟现实解决方案以及为外科医生提供支持的生成式 AI 软件等。\n长期以来，医疗设备一直以硬件为中心并依赖于复杂的设计和精密的工程。现在，它们正在向可以通过软件更新不断增强功能的软件定义设备转变。这将使医疗设备也能像智能手机一样可以连续多年升级到新的应用和功能，而用户在此期间无需更换新的设备。\n在 NVIDIA 专用实时加速计算平台的加持下，这一新模式因其在改变患者护理、提高效率、增强临床医师体验和改善疗效方面的潜力而备受瞩目。\nGE 医疗等医疗科技领军企业正在使用 NVIDIA 技术开发、微调和部署适用于软件定义医疗影像应用的 AI。\nGE 医疗在 GTC 上宣布使用 NVIDIA 工具（包括 TensorRT 软件开发套件）开发并优化了 SonoSAMTrack。这个最新的研究基础模型只需点击几下即可在医学影像中划定并追踪器官、结构或病变。该研究模型有望简化并加快医疗专业人员的超声波分析。\n医疗设备企业正在借助 NVIDIA IGX 边缘计算平台和 NVIDIA Holoscan 医疗级边缘 AI 平台，加速在手术室中开发并部署 AI 创新。\n强生医疗科技正与 NVIDIA 一起测试该公司手术互联数字生态系统的新 AI 功能。其目标是实现开放式创新并大规模加快实时洞察的交付，为医疗专业人员提供术前、术中和术后支持。\n总部位于巴黎的机器人手术公司 Moon Surgical 是 NVIDIA 初创加速计划（一项面向前沿初创公司的计划）会员。该公司正在使用 Holoscan 和 IGX 为其 Maestro System 腹腔镜手术系统提供助力。腹腔镜手术是一种外科医生经小切口将摄像头和器械伸入到患者体内进行操作的手术。\nMaestro 的 ScoPilot 可以让外科医生在手术过程中无需将手从其他手术工具上移开就能控制腹腔镜。迄今为止，它已成功治疗了 200 多名患者。\nMoon Surgical 也与 NVIDIA 一同利用 Maestro 和 Holoscan 将生成式 AI 功能应用于手术室。\n越来越多的医疗技术公司和解决方案提供商正在帮助客户更加轻松地采用 NVIDIA 边缘 AI 平台推动和加速医疗健康行业的发展。\n艾睿电子以订阅式平台即服务的形式将 IGX 提供给工业和医疗客户。Kaliber AI 是采用艾睿电子商业模式加速应用部署的客户之一。这家 NVIDIA 初创加速计划会员正在开发辅助微创手术的 AI 工具。在 GTC 上，Kaliber 展示了 AI 生成的外科洞察以及用于回答患者问题的一个大语言模型。\n全球可视化领域的领军企业 Barco 正在采用 Holoscan 和 IGX 构建一个交钥匙手术 AI 平台，通过为客户提供此类现成产品，使他们能够将工程资源集中于应用开发工作。该公司正在与 SoftAcuity 联合开发两款基于 Holoscan 的产品，这两款产品将加入生成式 AI 语音控制和 AI 数据分析功能。\nMagic Leap 已将 Holoscan 集成到其扩展现实软件堆栈中，为 Medical iSight（一家专门为中风和神经血管疾病微创治疗提供实时术中支持的软件开发商）等客户提供更加强大的功能。\n进一步了解 NVIDIA 加速的医疗科技。\n开始使用 NVIDIA NGC 或在 ai.nvidia.com 上试用超过二十种医疗微服务。\n订阅 NVIDIA 医疗行业新闻\n\n\n\t\t\t\tNVIDIA Jetson Thor 为通用机器人和物理 AI 解锁实时推理能力\t\t\t\n\n\n\n\t\t\t\t基于 NVIDIA Blackwell 的 Jetson Thor 现已发售，加速通用机器人时代的到来\t\t\t\n\n\n\n\t\t\t\tNVIDIA 推出 Spectrum-XGS 以太网，助力分布式数据中心迈入十亿瓦级 AI 超级工厂\t\t\t\n\n\n\n\t\t\t\tNVIDIA GR00T-Dreams 助力光轮智能革新合成数据，推动具身 AI 现实场景落地\t\t\t\n\n\n\n\t\t\t\tGamescom 2025 多款今年的游戏新作将支持NVIDIA DLSS 4 和光线追踪\t\t\t\n\n\n收件人的邮箱地址\n\n\n\n您的名字\n\n\n\n您的邮箱地址\n\n\n\nComments\n\n\n\n 发送邮件\n"
        },
        {
            "title": "焦点访谈丨辅助诊断、AI问答、健康管理……AI正重塑医疗服务链条_中央网络安全和信息化委员会办公室",
            "content": "当前位置：首页&gt;正文 · 在科技飞速发展的今天，人工智能正在以前所未有的方式深刻改变着我们的生活，比如现在有些医院就接入了人工智能，在取药、问诊，甚至包括病理筛查、重症监护等多个环节发挥着作用。",
            "url": "https://www.cac.gov.cn/2025-03/17/c_1743916011485636.htm",
            "source": "www.cac.gov.cn",
            "published_date": "",
            "search_engine": "Brave",
            "query": "人工智能在医疗诊断中的应用与发展趋势 重大新闻 突发事件 最近7天 breaking news emergency AI medical diagnosis",
            "language": "zh",
            "family_friendly": true,
            "category": "breaking_news",
            "search_source": "brave",
            "full_content": "当前位置：首页&gt;正文 · 在科技飞速发展的今天，人工智能正在以前所未有的方式深刻改变着我们的生活，比如现在有些医院就接入了人工智能，在取药、问诊，甚至包括病理筛查、重症监护等多个环节发挥着作用。"
        }
    ],
    "innovation_news": [
        {
            "title": "rising repo",
            "content": "3 hours ago ... Project management system for Claude Code using GitHub Issues and Git worktrees for parallel agent execution. ai-agents. ai-coding. claude. claude-code. project ...",
            "url": "https://yanggggjie.github.io/rising-repo/",
            "source": "yanggggjie.github.io",
            "published_date": "2025-08-28",
            "search_engine": "Google",
            "query": "2025年 人工智能在医学影像识别 最新技术突破 medical imaging AI breakthrough 深度学习算法",
            "description": "3 hours ago ... Project management system for Claude Code using GitHub Issues and Git worktrees for parallel agent execution. ai-agents. ai-coding. claude. claude-code. project ...",
            "image": "",
            "category": "innovation_news",
            "search_source": "google",
            "full_content": "Stars\n+1875\n+1510\n+1422\n+1130\n+1120\n+1091\n+575\n+514\n+499\n+470\n+399\n+380\n+363\n+354\n+323\n+317\n+292\n+284\n+280\n+278\n+273\n+261\n+259\n+247\n+247\n+244\n+237\n+231\n+222\n+214\n+213\n+209\n+208\n+203\n+203\n+203\n+195\n+189\n+183\n+182\n+177\n+176\n+175\n+174\n+173\n+173\n+172\n+171\n+169\n+168\n+162\n+161\n+160\n+160\n+160\n+159\n+157\n+153\n+150\n+149\n+145\n+145\n+141\n+141\n+140\n+138\n+137\n+132\n+132\n+132\n+130\n+130\n+130\n+128\n+128\n+127\n+127\n+122\n+121\n+120\n+120\n+119\n+118\n+118\n+118\n+118\n+116\n+115\n+114\n+114\n+114\n+113\n+113\n+112\n+111\n+110\n+110\n+109\n+108\n+108\n+108\n+107\n+107\n+105\n+104\n+104\n+103\n+103\n+102\n+102\n+101\n+99\n+98\n+97\n+96\n+94\n+94\n+93\n+91\n+91\n+90\n+89\n+89\n+88\n+87\n+87\n+87\n+87\n+87\n+85\n+85\n+84\n+84\n+84\n+83\n+83\n+83\n+83\n+82\n+82\n+81\n+81\n+81\n+79\n+79\n+79\n+78\n+78\n+77\n+77\n+77\n+76\n+75\n+74\n+74\n+74\n+73\n+73\n+73\n+72\n+70\n+70\n+69\n+69\n+69\n+69\n+69\n+69\n+68\n+68\n+68\n+68\n+68\n+68\n+68\n+67\n+67\n+67\n+67\n+67\n+66\n+66\n+66\n+66\n+65\n+65\n+65\n+65\n+65\n+64\n+64\n+64\n+64\n+64\n+63\n+63\n+63\n+63\n+63\n+62\n+62\n+62\n+62\n+61\n+60\n+60\n+59\n+59\n+59\n+59\n+58\n+58\n+58\n+58\n+57\n+57\n+57\n+56\n+56\n+56\n+56\n+56\n+56\n+56\n+55\n+55\n+55\n+54\n+54\n+54\n+54\n+54\n+54\n+54\n+54\n+54\n+53\n+53\n+53\n+53\n+53\n+53\n+52\n+52\n+52\n+52\n+52\n+52\n+52\n+52\n+52\n+52\n+51\n+51\n+51\n+51\n+51\n+51\n+51\n+51\n+51\n+50\n+50\n+50\n+50\n+50\n+50\n+50\n+49\n+49\n+49\n+49\n+49\n+49\n+49\n+48\n+48\n+48\n+47\n+47\n+47\n+47\n+47\n+47\n+47\n+47\n+47\n+46\n+46\n+46\n+46\n+46\n+46\n+46\n+46\n+46\n+46\n+46\n+46\n+46\n+45\n+45\n+45\n+45\n+45\n+45\n+45\n+45\n+45\n+45\n+45\n+45\n+44\n+44\n+44\n+44\n+44\n+44\n+44\n+44\n+44\n+44\n+44\n+44\n+44\n+43\n+43\n+43\n+43\n+43\n+43\n+43\n+43\n+43\n+43\n+43\n+43\n+43\n+42\n+42\n+42\n+42\n+42\n+42\n+42\n+42\n+42\n+42\n+41\n+41\n+41\n+41\n+41\n+41\n+41\n+41\n+41\n+41\n+40\n+40\n+40\n+40\n+40\n+40\n+40\n+40\n+40\n+40\n+40\n+40\n+40\n+40\n+40\n+39\n+39\n+39\n+39\n+39\n+39\n+39\n+39\n+39\n+39\n+39\n+39\n+38\n+38\n+38\n+38\n+38\n+38\n+38\n+38\n+38\n+38\n+38\n+38\n+38\n+38\n+38\n+38\n+38\n+38\n+37\n+37\n+37\n+37\n+37\n+37\n+37\n+36\n+36\n+36\n+36\n+36\n+36\n+36\n+36\n+36\n+36\n+36\n+36\n+36\n+35\n+35\n+35\n+35\n+35\n+35\n+35\n+35\n+35\n+35\n+35\n+35\n+35\n+35\n+35\n+35\n+35\n+35\n+35\n+35\n+35\n+34\n+34\n+34\n+34\n+34\n+34\n+34\n+34\n+34\n+34\n+34\n+34\n+34\n+34\n+34\n+34\n+34\n+34\n+34\n+34\n+33\n+33\n+33\n+33\n+33\n+33\n+33\n+33\n+33\n+33\n+33\n+33\n+32\n+32\n+32\n+32\n+32\n+32\n+32\n+32\n+32\n+32\n+32\n+32\n+32\n+32\n+32\n+32\n+32\n+32\n+32\n+32\n+32\n+32\n+32\n+32\n+32\n+32\n+32\n+32\n+32\n+32\n+31\n+31\n+31\n+31\n+31\n+31\n+31\n+31\n+31\n+31\n+31\n+31\n+31\n+31\n+31\n+31\n+31\n+31\n+31\n+31\n+31\n+31\n+31\n+31\n+31\n+30\n+30\n+30\n+30\n+30\n+30\n+30\n+30\n+30\n+30\n+30\n+30\n+30\n+30\n+30\n+30\n+30\n+30\n+30\n+30\n+30\n+30\n+29\n+29\n+29\n+29\n+29\n+29\n+29\n+29\n+29\n+29\n+29\n+29\n+29\n+29\n+29\n+29\n+29\n+29\n+29\n+29\n+29\n+29\n+29\n+29\n+29\n+29\n+29\n+29\n+29\n+29\n+29\n+28\n+28\n+28\n+28\n+28\n+28\n+28\n+28\n+28\n+28\n+28\n+28\n+28\n+28\n+28\n+28\n+28\n+28\n+28\n+28\n+28\n+28\n+28\n+28\n+28\n+27\n+27\n+27\n+27\n+27\n+27\n+27\n+27\n+27\n+27\n+27\n+27\n+27\n+27\n+27\n+27\n+27\n+27\n+27\n+27\n+27\n+27\n+27\n+27\n+27\n+27\n+27\n+27\n+26\n+26\n+26\n+26\n+26\n+26\n+26\n+26\n+26\n+26\n+26\n+26\n+26\n+26\n+26\n+26\n+26\n+26\n+26\n+26\n+26\n+26\n+26\n+26\n+26\n+25\n+25\n+25\n+25\n+25\n+25\n+25\n+25\n+25\n+25\n+25\n+25\n+25\n+25\n+25\n+25\n+25\n+25\n+25\n+25\n+25\n+25\n+25\n+25\n+25\n+25\n+24\n+24\n+24\n+24\n+24\n+24\n+24\n+24\n+24\n+24\n+24\n+24\n+24\n+24\n+24\n+24\n+24\n+24\n+24\n+24\n+24\n+24\n+24\n+24\n+24\n+24\n+24\n+24\n+24\n+24\n+24\n+24\n+24\n+24\n+24\n+24\n+24\n+24\n+24\n+24\n+24\n+24\n+23\n+23\n+23\n+23\n+23\n+23\n+23\n+23\n+23\n+23\n+23\n+23\n+23\n+23\n+23\n+23\n+23\n+23\n+23\n+23\n+23\n+23\n+23\n+23\n+23\n+23\n+23\n+23\n+23\n+23\n+23\n+23\n+23\n+23\n+23\n+23\n+23\n+23\n+23\n+23\n+23\n+23\n+23\n+23\n+23\n+23\n+23\n+23\n+23\n+23\n+23\n+23\n+23\n+22\n+22\n+22\n+22\n+22\n+22\n+22\n+22\n+22\n+22\n+22\n+22\n+22\n+22\n+22\n+22\n+22\n+22\n+22\n+22\n+22\n+22\n+22\n+22\n+22\n+22\n+22\n+22\n+22\n+22\n+22\n+22\n+22\n+22\n+22\n+22\n+22\n+22\n+22\n+22\n+22\n+22\n+22\n+22\n+22\n+22\n+22\n+22\n+22\n+22\n+21\n+21\n+21\n+21\n+21\n+21\n+21\n+21\n+21\n+21\n+21\n+21\n+21\n+21\n+21\n+21\n+21\n+21\n+21\n+21\n+21\n+21\n+21\n+21\n+21\n+21\n+21\n+21\n+21\n+21\n+21\n+21\n+21\n+21\n+21\n+21\n+21\n+21\n+21\n+21\n+21\n+21\n+21\n+21\n+21\n+21\n+20\n+20\n+20\n+20\n+20\n+20\n+20\n+20\n+20\n+20\n+20\n+20\n+20\n+20\n+20\n+20\n+20\n+20\n+20\n+20\n+20\n+20\n+20\n+20\n+20\n+20\n+20\n+20\n+20\n+20\n+20\n+20\n+20\n+20\n+20\n+20\n+20\n+20\n+20\n+20\n+20\n+20\n+20\n+20\n+20\n+20\n+20\n+20\n+20\n+20\n+20\n+20\n+20\n+20\n+20\n+20\n+20\n+20\n+20\n+20\n+20\n+20\n+20\n+20\n+20\n+20\n+20\n+20\n+20\n+20\n+19\n+19\n+19\n+19\n+19\n+19\n+19\n+19\n+19\n+19\n+19\n+19\n+19\n+19\n+19\n+19\n+19\n+19\n+19\n+19\n+19\n+19\n+19\n+19\n+19\n+19\n+19\n+19\n+19\n+19\n+19\n+19\n+19\n+19\n+19\n+19\n+19\n+19\n+19\n+19\n+19\n+19\n+19\n+19"
        },
        {
            "title": "网易有道词典-工作学习必备",
            "content": "6 days ago ... 小米应用商店是由小米推出的手机软件商店合集，致力于成为全面、优质的手机游戏应用商店，在这里用户可以找到最新最好玩的手机app和游戏应用，并提供软件商店app下载， ...",
            "url": "https://app.xiaomi.com/details?id=com.youdao.dict",
            "source": "app.xiaomi.com",
            "published_date": "2025-08-28",
            "search_engine": "Google",
            "query": "2025年 人工智能在医学影像识别 最新技术突破 medical imaging AI breakthrough 深度学习算法",
            "description": "6 days ago ... 小米应用商店是由小米推出的手机软件商店合集，致力于成为全面、优质的手机游戏应用商店，在这里用户可以找到最新最好玩的手机app和游戏应用，并提供软件商店app下载， ...",
            "image": "",
            "category": "innovation_news",
            "search_source": "google",
            "full_content": "\n分类：学习教育|支持：手机、平板\n\n\n\n听全球发音，上有道词典！支持中文、英语、日语、韩语、法语、德语、俄语、西班牙语、葡萄牙语、藏语、西语等109种语言翻译。拍照翻译、语音翻译、对话翻译、在线翻译、离线翻译更顺畅。【权威词典】完整收录学习型牛津词典、《新牛津英汉双解大词典》、《韦氏大学英语词典》、《柯林斯COBUILD高级英汉双解词典》、《新世纪日汉双解大辞典》、龙朝《韩中词典》《中韩词典》等权威词典，海量英汉、汉英词汇和例句，满足商务英语、四六级、考研、GRE、托福、雅思、初中、高中、小学等语言翻译学习需求。  【拍照翻译】拍照就能翻译，无需输入也能查词翻译的词典，支持英日韩离线拍照翻译。 【对话翻译】语音翻译升级为对话翻译，即说即译，与外国人交流0障碍。 【离线翻译】英语、日语、韩语等多种语言的离线翻译词库，没有网络也能查单词和翻译。 【文本翻译】输入文本即可翻译，支持109种语言的全球翻译，支持英日韩离线文本翻译。【文档翻译】一键翻译Word和PDF文档，支持整篇翻译，结果快速导出。 【热词翻译】囊括中文、英语、日语等互联网热词翻译。 【汉语词典】支持汉语拼音、手写、语音输入，中文读音、中文生僻字轻松查。 【日语词典】收录近17万条中文日语双语词汇、例句，支持日语单词语法查词、日语翻译、日语发音、日语手写输入、日语罗马音检索、人名地名缩略语查询，助力日语学习。 【韩语词典】收录龙朝《韩中词典》《中韩词典》两部韩语学习者必备词典，超过20万余词汇。【作文批改】基于AI技术的英语作文批改功能。一键上传照片，自动识别作文内容；自动检测拼写、语法和搭配错误并提供修改建议。【地道发音】支持英语、日语等的全球和本地发音。\n重磅升级！【AI 同传翻译大升级​】- 覆盖 112 种语言，粤语、印度口音英语都能翻​- 新增 AI 总结 + 脑图，课堂笔记、会议纪要一键生成​- 识别翻译更精准，专业术语也能搞定​【拍照翻译全新优化​】- 分屏翻译新上线，菜单、路牌等场景核对更直观​- 段落原文译文同屏对照，深度学习更高效​- 多段文字一键提取，复制超便捷\n你所举报的信息将被发送至工信部12321举报平台\n开发者正在努力完善中...\n很抱歉，部分应用下载服务目前正在升级维护中，您可以前往小米手机的应用商店下载该应用。\n\n违法和不良信息举报电话：171-5104-4404\n举报邮箱：csc-report@xiaomi.com\n互联网违法和不良信息举报中心：www.12377.cn\n\n\n隐私协议\n京ICP备10046444号\n京公网安备11010802020134号、11010802000100号\nMIUI.com All Rights Reserved\n            2025\n\n          \n\n\n公司名：小米科技有限责任公司\n\n\n北京地址：北京市昌平区安居路小米智慧产业园\n联系电话：010-60606666\n\n\n南京地址：江苏省南京市建邺区新城科技园互联网产业园\n\n您需要登陆小米帐号才可以安装应用到手机\n已经将“新浪微博”推送到您的手机请在手机上完成安装"
        },
        {
            "title": "Sparse",
            "content": "3 days ago ... Abstract. Text-to-image (T2I) diffusion models have made significant strides in generating high-quality images. However, progressively manipulating certain ...",
            "url": "https://paperreading.club/category?cate=Sparse",
            "source": "paperreading.club",
            "published_date": "2025-08-28",
            "search_engine": "Google",
            "query": "2025年 人工智能在医学影像识别 最新技术突破 medical imaging AI breakthrough 深度学习算法",
            "description": "3 days ago ... Abstract. Text-to-image (T2I) diffusion models have made significant strides in generating high-quality images. However, progressively manipulating certain ...",
            "image": "",
            "category": "innovation_news",
            "search_source": "google",
            "full_content": "Current methods for 3D semantic segmentation propose training models with limited annotations to address the difficulty of annotating large, irregular, and unordered 3D point cloud data. They usually focus on the 3D domain only, without leveraging the complementary nature of 2D and 3D data. Besides, some methods extend original labels or generate pseudo labels to guide the training, but they often fail to fully use these labels or address the noise within them. Meanwhile, the emergence of comprehensive and adaptable foundation models has offered effective solutions for segmenting 2D data. Leveraging this advancement, we present a novel approach that maximizes the utility of sparsely available 3D annotations by incorporating segmentation masks generated by 2D foundation models. We further propagate the 2D segmentation masks into the 3D space by establishing geometric correspondences between 3D scenes and 2D views. We extend the highly sparse annotations to encompass the areas delineated by 3D masks, thereby substantially augmenting the pool of available labels. Furthermore, we apply confidence- and uncertainty-based consistency regularization on augmentations of the 3D point cloud and select the reliable pseudo labels, which are further spread on the 3D masks to generate more labels. This innovative strategy bridges the gap between limited 3D annotations and the powerful capabilities of 2D foundation models, ultimately improving the performance of 3D weakly supervised segmentation.\n当前的三维语义分割方法通过使用有限注释的数据来训练模型，以解决对大量、不规则且无序的三维点云数据进行标注的困难。这些方法通常只专注于三维领域，而没有利用二维和三维数据之间的互补性。此外，一些方法扩展原始标签或生成伪标签以指导训练，但它们往往未能充分利用这些标签或有效处理其中的噪声。与此同时，全面且适应性强的基础模型的出现为分割二维数据提供了有效的解决方案。借助这一进展，我们提出了一种新方法，通过引入由二维基础模型生成的分割掩码来最大化稀疏可用三维注释的效用。进一步地，我们将二维分割掩码传播到三维空间中，通过在三维场景和二维视图之间建立几何对应关系实现这一点。我们将高度稀疏的标注扩展到由三维掩码界定的区域，从而大幅增加了可用标签的数量。此外，我们对三维点云的数据增强应用了基于置信度和不确定性的一致性正则化，并选择可靠的伪标签，这些伪标签进一步被传播在三维掩码上以生成更多标签。这一创新策略弥合了有限的三维注释与二维基础模型的强大能力之间的差距，最终提升了弱监督下三维语义分割的表现。\nhttps://arxiv.org/abs/2508.19909\nhttps://arxiv.org/pdf/2508.19909.pdf\nEvent-based camera has emerged as a promising paradigm for robot perception, offering advantages with high temporal resolution, high dynamic range, and robustness to motion blur. However, existing deep learning-based event processing methods often fail to fully leverage the sparse nature of event data, complicating their integration into resource-constrained edge applications. While neuromorphic computing provides an energy-efficient alternative, spiking neural networks struggle to match of performance of state-of-the-art models in complex event-based vision tasks, like object detection and optical flow. Moreover, achieving high activation sparsity in neural networks is still difficult and often demands careful manual tuning of sparsity-inducing loss terms. Here, we propose Context-aware Sparse Spatiotemporal Learning (CSSL), a novel framework that introduces context-aware thresholding to dynamically regulate neuron activations based on the input distribution, naturally reducing activation density without explicit sparsity constraints. Applied to event-based object detection and optical flow estimation, CSSL achieves comparable or superior performance to state-of-the-art methods while maintaining extremely high neuronal sparsity. Our experimental results highlight CSSL's crucial role in enabling efficient event-based vision for neuromorphic processing.\n基于事件的相机已成为机器人感知领域的一个有前景的范式，它提供了高时间分辨率、宽动态范围以及对运动模糊的强大鲁棒性。然而，现有的基于深度学习的事件处理方法往往未能充分利用事件数据的稀疏特性，从而使其难以在资源受限的应用中集成。虽然神经形态计算提供了一种节能替代方案，但脉冲神经网络（SNN）在复杂基于事件的视觉任务，如物体检测和光流估计上，仍难以达到当前最佳模型的表现水平。此外，在神经网络中实现高激活稀疏性仍然具有挑战性，并且通常需要对诱导稀疏性的损失项进行仔细的手动调整。\n\n为此，我们提出了一种新颖的框架——感知上下文的稀疏时空学习（CSSL），该框架引入了基于输入分布动态调节神经元激活的感知上下文阈值法。这种方法自然减少了激活密度，而无需明确的稀疏约束条件。在基于事件的物体检测和光流估计的应用中，CSSL实现了与现有最佳方法相当或更优的表现水平，并且保持了极高的神经元稀疏性。\n\n实验结果强调了CSSL在实现高效的基于事件视觉处理以支持神经形态计算中的关键作用。\nhttps://arxiv.org/abs/2508.19806\nhttps://arxiv.org/pdf/2508.19806.pdf\nThe rapid advance of deep generative models such as GANs and diffusion networks now produces images that are virtually indistinguishable from genuine photographs, undermining media forensics and biometric security. Supervised detectors quickly lose effectiveness on unseen generators or after adversarial post-processing, while existing unsupervised methods that rely on low-level statistical cues remain fragile. We introduce a physics-inspired, model-agnostic detector that treats synthetic-image identification as a community-detection problem on a sparse weighted graph. Image features are first extracted with pretrained CNNs and reduced to 32 dimensions, each feature vector becomes a node of a Multi-Edge Type QC-LDPC graph. Pairwise similarities are transformed into edge couplings calibrated at the Nishimori temperature, producing a Random Bond Ising Model (RBIM) whose Bethe-Hessian spectrum exhibits a characteristic gap when genuine community structure (real images) is present. Synthetic images violate the Nishimori symmetry and therefore lack such gaps. We validate the approach on binary tasks cat versus dog and male versus female using real photos from Flickr-Faces-HQ and CelebA and synthetic counterparts generated by GANs and diffusion models. Without any labeled synthetic data or retraining of the feature extractor, the detector achieves over 94% accuracy. Spectral analysis shows multiple well separated gaps for real image sets and a collapsed spectrum for generated ones. Our contributions are threefold: a novel LDPC graph construction that embeds deep image features, an analytical link between Nishimori temperature RBIM and the Bethe-Hessian spectrum providing a Bayes optimal detection criterion; and a practical, unsupervised synthetic image detector robust to new generative architectures. Future work will extend the framework to video streams and multi-class anomaly detection.\n深度生成模型（如GAN和扩散网络）的迅速发展现在能够生成几乎无法与真实照片区别的图像，这对媒体取证和生物识别安全构成了威胁。监督检测器在面对未见过的生成器或经过对抗性后处理时会快速失去效果，而现有的依赖低级统计线索的无监督方法仍然脆弱。我们提出了一种受物理启发、模型不可知的检测器，它将合成图像的识别视为稀疏加权图上的社区检测问题。\n\n首先使用预训练的CNN提取图像特征，并将其缩减为32维，每个特征向量成为一个多边类型QC-LDPC图中的节点。配对相似性被转换成在Nishimori温度下校准的边缘耦合，生成一个随机键伊辛模型（RBIM），其Bethe-Hessian谱在存在真实的社区结构（真实图像）时表现出特有的间隙。合成图像破坏了Nishimori对称性，因此缺乏此类间隙。\n\n我们通过使用Flickr-Faces-HQ和CelebA中的真实照片与GAN和扩散模型生成的合成对应物进行二元任务猫对狗以及男性对女性的任务来验证该方法的有效性。在不使用任何标记的合成数据或重新训练特征提取器的情况下，检测器实现了超过94%的准确率。谱分析显示了真实图像集中的多个清晰间隔和生成图像集的坍缩谱。\n\n我们的贡献有三点：一种新颖的LDPC图构建方式，它嵌入了深度图像特征；将Nishimori温度下的RBIM与Bethe-Hessian谱之间建立了一个理论联系，提供了一个贝叶斯最优检测准则；以及一个实用、无监督的合成图像检测器，对新的生成架构具有鲁棒性。未来的研究工作将扩展该框架以处理视频流和多类异常检测任务。\nhttps://arxiv.org/abs/2508.19698\nhttps://arxiv.org/pdf/2508.19698.pdf\nThe rapid evolution of specialized large language models (LLMs) has transitioned from simple domain adaptation to sophisticated native architectures, marking a paradigm shift in AI development. This survey systematically examines this progression across healthcare, finance, legal, and technical domains. Besides the wide use of specialized LLMs, technical breakthrough such as the emergence of domain-native designs beyond fine-tuning, growing emphasis on parameter efficiency through sparse computation and quantization, increasing integration of multimodal capabilities and so on are applied to recent LLM agent. Our analysis reveals how these innovations address fundamental limitations of general-purpose LLMs in professional applications, with specialized models consistently performance gains on domain-specific benchmarks. The survey further highlights the implications for E-Commerce field to fill gaps in the field.\n专业大型语言模型（LLMs）的快速演化已经从简单的领域适应发展到复杂的原生架构，标志着人工智能开发范式的转变。本综述系统地考察了这一进展在医疗、金融、法律和技术领域的应用情况。除了广泛使用专门化的LLMs之外，技术突破如超越微调的领域原生设计的出现、通过稀疏计算和量化提高参数效率的日益重视、多模态能力集成的增加等都被应用于最近的LLM代理中。我们的分析揭示了这些创新如何解决了通用目的LLMs在专业应用中的根本限制，并且专用模型在特定领域的基准测试中持续表现出性能优势。综述进一步强调了这些发展对电子商务领域填补空白的意义。\nhttps://arxiv.org/abs/2508.19667\nhttps://arxiv.org/pdf/2508.19667.pdf\nVision-Language Models (VLMs) often suffer from visual hallucinations, saying things that are not actually in the image, and language shortcuts, where they skip the visual part and just rely on text priors. These issues arise because most post-training methods for VLMs rely on simple verifiable answer matching and supervise only final outputs, leaving intermediate visual reasoning without explicit guidance. As a result, VLMs receive sparse visual signals and often learn to prioritize language-based reasoning over visual perception. To mitigate this, some existing methods add visual supervision using human annotations or distilled labels from external large models. However, human annotations are labor-intensive and costly, and because external signals cannot adapt to the evolving policy, they cause distributional shifts that can lead to reward hacking. In this paper, we introduce Vision-SR1, a self-rewarding method that improves visual reasoning without relying on external visual supervisions via reinforcement learning. Vision-SR1 decomposes VLM reasoning into two stages: visual perception and language reasoning. The model is first prompted to produce self-contained visual perceptions that are sufficient to answer the question without referring back the input image. To validate this self-containment, the same VLM model is then re-prompted to perform language reasoning using only the generated perception as input to compute reward. This self-reward is combined with supervision on final outputs, providing a balanced training signal that strengthens both visual perception and language reasoning. Our experiments demonstrate that Vision-SR1 improves visual reasoning, mitigates visual hallucinations, and reduces reliance on language shortcuts across diverse vision-language tasks.\n视觉-语言模型（VLMs）经常出现视觉幻觉，即描述图像中不存在的内容，以及依赖于文本先验的语言捷径问题。这些问题产生的原因是大多数针对VLM的后期训练方法仅依靠简单的可验证答案匹配，并且仅仅监督最终输出而忽视了中间的视觉推理过程，从而导致模型接收到稀疏的视觉信号并倾向于优先使用基于语言的推理而非图像感知。为解决这个问题，现有的一些方法通过添加来自人类标注或外部大型模型蒸馏标签的人类视觉监督来缓解这一问题。然而，人工标注成本高昂且耗时，并且因为外部信号无法适应不断变化的需求，它们会导致分布偏移并导致奖励欺骗。\n\n在本文中，我们提出了一种名为Vision-SR1的新方法，该方法通过强化学习改善VLM的视觉推理过程而不依赖于外部视觉监督。Vision-SR1将VLM的推理过程分为两个阶段：视觉感知和语言推理。首先，模型被提示生成能够自我包含回答问题的视觉感知结果，无需再参考输入图像。然后，使用相同的VLM模型重新进行提示，仅以生成的感知作为输入来进行语言推理并计算奖励，以此来验证这一自我包容性。这种自激励信号与对最终输出的监督相结合，提供了一个平衡的训练信号，同时强化了视觉感知和语言推理的能力。\n\n我们的实验表明，Vision-SR1能够改进视觉推理、减少视觉幻觉的发生，并且在多种视觉-语言任务中减少了对语言捷径的依赖。\nhttps://arxiv.org/abs/2508.19652\nhttps://arxiv.org/pdf/2508.19652.pdf\nThis work proposes an energy-efficient, learning-based beamforming scheme for integrated sensing and communication (ISAC)-enabled V2X networks. Specifically, we first model the dynamic and uncertain nature of V2X environments as a Markov Decision Process. This formulation allows the roadside unit to generate beamforming decisions based solely on current sensing information, thereby eliminating the need for frequent pilot transmissions and extensive channel state information acquisition. We then develop a deep reinforcement learning (DRL) algorithm to jointly optimize beamforming and power allocation, ensuring both communication throughput and sensing accuracy in highly dynamic scenario. To address the high energy demands of conventional learning-based schemes, we embed spiking neural networks (SNNs) into the DRL framework. Leveraging their event-driven and sparsely activated architecture, SNNs significantly enhance energy efficiency while maintaining robust performance. Simulation results confirm that the proposed method achieves substantial energy savings and superior communication performance, demonstrating its potential to support green and sustainable connectivity in future V2X systems.\n这项工作提出了一种用于集成感测与通信（ISAC）赋能的车联网（V2X）网络的能量高效、基于学习的波束成形方案。具体来说，我们首先将V2X环境中的动态和不确定性建模为马尔可夫决策过程。这种形式化使得路边单元能够仅根据当前的感测信息生成波束成形决策，从而消除了频繁发送导频信号和广泛获取信道状态信息的需求。\n\n接着，我们开发了一种深度强化学习（DRL）算法，以同时优化波束成形和功率分配，确保在高度动态场景下的通信吞吐量和感知准确性。为了解决传统基于学习方案的高能耗需求，我们将脉冲神经网络（SNNs）嵌入到DRL框架中。利用其事件驱动和稀疏激活架构的优势，SNN能够显著提高能量效率并保持强大的性能。\n\n仿真结果证实了所提出的方法在实现显著节能的同时还能提供优越的通信性能，展示了它支持未来V2X系统绿色可持续连接的巨大潜力。\nhttps://arxiv.org/abs/2508.19566\nhttps://arxiv.org/pdf/2508.19566.pdf\nDigital twin applications offered transformative potential by enabling real-time monitoring and robotic simulation through accurate virtual replicas of physical assets. The key to these systems is 3D reconstruction with high geometrical fidelity. However, existing methods struggled under field conditions, especially with sparse and occluded views. This study developed a two-stage framework (DATR) for the reconstruction of apple trees from sparse views. The first stage leverages onboard sensors and foundation models to semi-automatically generate tree masks from complex field images. Tree masks are used to filter out background information in multi-modal data for the single-image-to-3D reconstruction at the second stage. This stage consists of a diffusion model and a large reconstruction model for respective multi view and implicit neural field generation. The training of the diffusion model and LRM was achieved by using realistic synthetic apple trees generated by a Real2Sim data generator. The framework was evaluated on both field and synthetic datasets. The field dataset includes six apple trees with field-measured ground truth, while the synthetic dataset featured structurally diverse trees. Evaluation results showed that our DATR framework outperformed existing 3D reconstruction methods across both datasets and achieved domain-trait estimation comparable to industrial-grade stationary laser scanners while improving the throughput by $\\sim$360 times, demonstrating strong potential for scalable agricultural digital twin systems.\n数字孪生应用通过使用物理资产的精确虚拟副本实现了实时监控和机器人模拟，从而提供了变革性的潜力。这些系统的关键在于具有高几何精度的三维重建技术。然而，现有的方法在实地条件下尤其是面对稀疏和遮挡视角时遇到了挑战。\n\n本研究开发了一种两阶段框架（DATR），用于从稀疏视图中重构苹果树。第一阶段利用车载传感器和基础模型来半自动地生成复杂的现场图像中的树木掩模。这些树木掩模被用来在第二阶段的单幅图像到3D重建过程中过滤掉多模式数据中的背景信息。第二阶段包括一个扩散模型和一个大型重建模型，分别用于多视角和隐式神经场的生成。\n\n通过使用由Real2Sim数据生成器产生的逼真的合成苹果树来训练扩散模型和LRM（大尺度重建模型）。该框架在实地和合成数据集上进行了评估。实地数据集中包括六棵具有现场测量真实情况的苹果树，而合成数据集则涵盖了结构多样的树木。\n\n评估结果显示，我们的DATR框架在两个数据集上的三维重构方法中表现优于现有技术，并且在结构特征估计方面达到了工业级静态激光扫描仪的水平，同时将吞吐量提高了大约360倍。这证明了可扩展农业数字孪生系统的巨大潜力。\nhttps://arxiv.org/abs/2508.19508\nhttps://arxiv.org/pdf/2508.19508.pdf\nText-to-image (T2I) diffusion models have made significant strides in generating high-quality images. However, progressively manipulating certain attributes of generated images to meet the desired user expectations remains challenging, particularly for content with rich details, such as human faces. Some studies have attempted to address this by training slider modules. However, they follow a One-for-One manner, where an independent slider is trained for each attribute, requiring additional training whenever a new attribute is introduced. This not only results in parameter redundancy accumulated by sliders but also restricts the flexibility of practical applications and the scalability of attribute manipulation. To address this issue, we introduce the All-in-One Slider, a lightweight module that decomposes the text embedding space into sparse, semantically meaningful attribute directions. Once trained, it functions as a general-purpose slider, enabling interpretable and fine-grained continuous control over various attributes. Moreover, by recombining the learned directions, the All-in-One Slider supports zero-shot manipulation of unseen attributes (e.g., races and celebrities) and the composition of multiple attributes. Extensive experiments demonstrate that our method enables accurate and scalable attribute manipulation, achieving notable improvements compared to previous methods. Furthermore, our method can be extended to integrate with the inversion framework to perform attribute manipulation on real images, broadening its applicability to various real-world scenarios. The code and trained model will be released at: this https URL.\n文本到图像（T2I）扩散模型在生成高质量图片方面取得了显著进展。然而，逐步调整生成图像中特定属性以满足用户期望仍然是一个挑战，尤其是在细节丰富的场景下，例如人物面部特征的处理。一些研究尝试通过训练滑块模块来解决这个问题，但这些方法采用了一对一的方式，即为每个属性单独训练一个独立的滑块，在引入新属性时需要额外进行训练。这种方式不仅导致了参数冗余，并且限制了实际应用中的灵活性和多属性操作的可扩展性。\n\n为了应对这一挑战，我们提出了All-in-One Slider模块，这是一个轻量级的解决方案，它将文本嵌入空间分解为稀疏而语义意义明确的属性方向。一旦训练完成，该滑块即可作为一个通用工具使用，实现对各种属性的解释性和精细控制。此外，通过重新组合学习到的方向，All-in-One Slider支持零样本操作未见过的属性（例如种族和名人特征）以及多个属性的组合。\n\n广泛的实验表明，我们的方法能够实现准确而可扩展的属性调整，在与先前的方法相比时取得了显著的进步。此外，该方法还可以拓展以集成到反转框架中，用于对真实图像进行属性调整，从而将其应用范围扩大到了各种现实场景中。相关的代码和训练好的模型将在以下链接发布：[此URL]（请将“this https URL”替换为实际的网址）。\nhttps://arxiv.org/abs/2508.19195\nhttps://arxiv.org/pdf/2508.19195.pdf\nFederated fine-tuning of Mixture-of-Experts (MoE)-based large language models (LLMs) is challenging due to their massive computational requirements and the resource constraints of participants. Existing working attempts to fill this gap through model quantization, computation offloading, or expert pruning. However, they cannot achieve desired performance due to impractical system assumptions and a lack of consideration for MoE-specific characteristics. In this paper, we propose FLUX, a system designed to enable federated fine-tuning of MoE-based LLMs across participants with constrained computing resources (e.g., consumer-grade GPUs), aiming to minimize time-to-accuracy. FLUX introduces three key innovations: (1) quantization-based local profiling to estimate expert activation with minimal overhead, (2) adaptive layer-aware expert merging to reduce resource consumption while preserving accuracy, and (3) dynamic expert role assignment using an exploration-exploitation strategy to balance tuning and non-tuning experts. Extensive experiments on LLaMA-MoE and DeepSeek-MoE with multiple benchmark datasets demonstrate that FLUX significantly outperforms existing methods, achieving up to 4.75X speedup in time-to-accuracy.\n联邦微调混合专家（MoE）模型的大型语言模型（LLM）具有挑战性，因为这些模型需要庞大的计算资源，并且参与者通常受到资源限制。现有的尝试通过模型量化、计算卸载或专家修剪来解决这一问题。然而，由于它们不切实际的系统假设以及未能充分考虑MoE特有的特性，现有方法无法实现预期性能。\n\n在本文中，我们提出了FLUX系统，该系统旨在支持具有有限计算资源（例如消费级GPU）的参与者之间的混合专家基础大型语言模型的联邦微调，并且旨在将时间至准确度最小化。FLUX引入了三个关键创新：\n\n1. **基于量化的本地分析**：使用最少的开销来估计专家激活。\n2. **自适应分层感知专家合并**：在减少资源消耗的同时保持准确性。\n3. **动态专家角色分配**：采用探索与利用策略平衡调优和非调优专家的作用。\n\n在LLaMA-MoE和DeepSeek-MoE上进行的广泛实验，并使用多个基准数据集，证明FLUX显著优于现有方法，在时间至准确度方面实现了高达4.75倍的速度提升。\nhttps://arxiv.org/abs/2508.19078\nhttps://arxiv.org/pdf/2508.19078.pdf\nVisual localization (VL) is the task of estimating the camera pose in a known scene. VL methods, a.o., can be distinguished based on how they represent the scene, e.g., explicitly through a (sparse) point cloud or a collection of images or implicitly through the weights of a neural network. Recently, NeRF-based methods have become popular for VL. While NeRFs offer high-quality novel view synthesis, they inadvertently encode fine scene details, raising privacy concerns when deployed in cloud-based localization services as sensitive information could be recovered. In this paper, we tackle this challenge on two ends. We first propose a new protocol to assess privacy-preservation of NeRF-based representations. We show that NeRFs trained with photometric losses store fine-grained details in their geometry representations, making them vulnerable to privacy attacks, even if the head that predicts colors is removed. Second, we propose ppNeSF (Privacy-Preserving Neural Segmentation Field), a NeRF variant trained with segmentation supervision instead of RGB images. These segmentation labels are learned in a self-supervised manner, ensuring they are coarse enough to obscure identifiable scene details while remaining discriminativeness in 3D. The segmentation space of ppNeSF can be used for accurate visual localization, yielding state-of-the-art results.\n视觉定位（VL）的任务是估计已知场景中相机的位置和姿态。基于它们表示场景的方式，视觉定位方法可以被区分，例如通过显式的点云或图像集合，或者隐式地通过神经网络的权重来表示。最近，基于NeRF的方法在视觉定位领域变得非常流行。尽管NeRF能够提供高质量的新视角合成，但它们无意中编码了精细的场景细节，在云端定位服务中部署时可能会引发隐私问题，因为敏感信息可能被恢复出来。\n\n在这篇论文中，我们从两个方面解决了这个挑战。首先，我们提出了一种新的评估协议来衡量基于NeRF表示形式的隐私保护能力。研究表明，即使移除了预测颜色的头部（head），那些使用光度损失训练得到的NeRF也会在其几何表示中存储详细的场景信息，使其容易遭受隐私攻击。\n\n其次，我们提出了ppNeSF（Privacy-Preserving Neural Segmentation Field），这是一种基于NeRF的变体，它通过分割监督而非RGB图像进行训练。这些分割标签是自监督学习得到的，在保证3D区分性的前提下足够粗略以掩盖可识别的场景细节。ppNeSF的分割空间可以用于准确的视觉定位，并且能够达到当前最先进的性能水平。\nhttps://arxiv.org/abs/2508.18971\nhttps://arxiv.org/pdf/2508.18971.pdf\nTrustworthy AI is mandatory for the broad deployment of autonomous vehicles. Although end-to-end approaches derive control commands directly from raw data, interpreting these decisions remains challenging, especially in complex urban scenarios. This is mainly attributed to very deep neural networks with non-linear decision boundaries, making it challenging to grasp the logic behind AI-driven decisions. This paper presents a method to enhance interpretability while optimizing control commands in autonomous driving. To address this, we propose loss functions that promote the interpretability of our model by generating sparse and localized feature maps. The feature activations allow us to explain which image regions contribute to the predicted control command. We conduct comprehensive ablation studies on the feature extraction step and validate our method on the CARLA benchmarks. We also demonstrate that our approach improves interpretability, which correlates with reducing infractions, yielding a safer, high-performance driving model. Notably, our monocular, non-ensemble model surpasses the top-performing approaches from the CARLA Leaderboard by achieving lower infraction scores and the highest route completion rate, all while ensuring interpretability.\n可信的人工智能对于自主驾驶车辆的广泛部署是必要的。尽管端到端方法可以直接从原始数据中推导出控制命令，但在复杂的城市环境中解释这些决策仍然是一个挑战。这主要归因于非常深的神经网络具有非线性的决策边界，使得理解AI驱动决策背后的逻辑变得困难。本文提出了一种在自主驾驶中增强可解释性的同时优化控制指令的方法。为此，我们提出了促进模型可解释性的损失函数，通过生成稀疏且局部化的特征图来实现这一目标。激活的特征允许我们解释哪些图像区域对预测的控制命令有贡献。我们在CARLA基准测试上进行了全面的消融研究，并验证了我们的方法的有效性。此外，我们展示了我们的方法可以提高可解释性，这与减少违规行为相关联，从而产生一个更安全、高性能的驾驶模型。值得注意的是，我们的单目非集成模型在降低违规得分和实现最高的路线完成率方面超过了CARLA排行榜上的顶级表现方法，并且同时保证了可解释性。\nhttps://arxiv.org/abs/2508.18898\nhttps://arxiv.org/pdf/2508.18898.pdf\nModern time series analysis demands frameworks that are flexible, efficient, and extensible. However, many existing Python libraries exhibit limitations in modularity and in their native support for irregular, multi-source, or sparse data. We introduce pyFAST, a research-oriented PyTorch framework that explicitly decouples data processing from model computation, fostering a cleaner separation of concerns and facilitating rapid experimentation. Its data engine is engineered for complex scenarios, supporting multi-source loading, protein sequence handling, efficient sequence- and patch-level padding, dynamic normalization, and mask-based modeling for both imputation and forecasting. pyFAST integrates LLM-inspired architectures for the alignment-free fusion of sparse data sources and offers native sparse metrics, specialized loss functions, and flexible exogenous data fusion. Training utilities include batch-based streaming aggregation for evaluation and device synergy to maximize computational efficiency. A comprehensive suite of classical and deep learning models (Linears, CNNs, RNNs, Transformers, and GNNs) is provided within a modular architecture that encourages extension. Released under the MIT license at GitHub, pyFAST provides a compact yet powerful platform for advancing time series research and applications.\n现代的时间序列分析需要灵活、高效且可扩展的框架。然而，许多现有的Python库在模块化和对不规则数据、多源数据或稀疏数据的原生支持方面存在局限性。我们推出了pyFAST，这是一个面向研究的PyTorch框架，它明确地将数据处理与模型计算解耦，促进了清晰的关注分离，并便于快速实验。其数据引擎为复杂场景而设计，支持多源加载、蛋白质序列处理、高效的序列和块级别的填充、动态归一化以及用于插补和预测的基于掩码的建模。\n\npyFAST集成了受大型语言模型启发的架构，能够实现稀疏数据来源的无对齐融合，并提供了原生稀疏度量、专门的损失函数以及灵活的外源数据融合。训练工具包包括用于评估的批量流式聚合和设备协同以最大化计算效率的功能。该框架提供了一个模块化架构，内置了经典模型（如线性回归）和深度学习模型（卷积神经网络(CNN)、循环神经网络(RNN)、变换器(Transformer)及图神经网络(GNN)），并鼓励扩展。\n\npyFAST在GitHub上以MIT许可证发布，为推进时间序列研究和应用提供了一个紧凑而强大的平台。\nhttps://arxiv.org/abs/2508.18891\nhttps://arxiv.org/pdf/2508.18891.pdf\nWhile modern recommender systems are instrumental in navigating information abundance, they remain fundamentally limited by static user modeling and reactive decision-making paradigms. Current large language model (LLM)-based agents inherit these shortcomings through their overreliance on heuristic pattern matching, yielding recommendations prone to shallow correlation bias, limited causal inference, and brittleness in sparse-data scenarios. We introduce STARec, a slow-thinking augmented agent framework that endows recommender systems with autonomous deliberative reasoning capabilities. Each user is modeled as an agent with parallel cognitions: fast response for immediate interactions and slow reasoning that performs chain-of-thought rationales. To cultivate intrinsic slow thinking, we develop anchored reinforcement training - a two-stage paradigm combining structured knowledge distillation from advanced reasoning models with preference-aligned reward shaping. This hybrid approach scaffolds agents in acquiring foundational capabilities (preference summarization, rationale generation) while enabling dynamic policy adaptation through simulated feedback loops. Experiments on MovieLens 1M and Amazon CDs benchmarks demonstrate that STARec achieves substantial performance gains compared with state-of-the-art baselines, despite using only 0.4% of the full training data.\n尽管现代推荐系统在应对信息泛滥方面起到了重要作用，但它们仍然受限于静态用户建模和反应式决策模式。当前基于大型语言模型（LLM）的代理通过过度依赖启发式的模式匹配继承了这些缺点，导致其产生的推荐结果容易受到浅层相关性偏差、因果推理能力有限以及在稀疏数据场景中表现脆弱的影响。\n\n我们引入STARec框架，这是一种慢思考增强型代理框架，赋予推荐系统自主审慎推理的能力。每个用户都被建模为一个拥有平行认知的代理：快速响应以处理即时互动，并通过链式思维过程进行深层次思考。为了培养内在的慢思考能力，我们开发了锚定强化训练——一种两阶段范式，结合高级推理模型的知识蒸馏和与偏好对齐的奖励塑形。这种混合方法为代理提供了获取基础能力（如偏好总结、论据生成）的同时，也支持通过模拟反馈循环实现动态策略调整。\n\n在MovieLens 1M 和 Amazon CDs 数据集上的实验表明，STARec即使仅使用完整训练数据的0.4%，也能显著超越最先进的基准模型。\nhttps://arxiv.org/abs/2508.18812\nhttps://arxiv.org/pdf/2508.18812.pdf\nWe present a unified framework combining statistical physics, coding theory, and algebraic topology for efficient multi-class image classification. High-dimensional feature vectors from a frozen MobileNetV2 backbone are interpreted as spins on a sparse Multi-Edge Type quasi-cyclic LDPC (MET-QC-LDPC) graph, forming a Random-Bond Ising Model (RBIM). We operate this RBIM at its Nishimori temperature, $\\beta_N$, where the smallest eigenvalue of the Bethe-Hessian matrix vanishes, maximizing class separability. Our theoretical contribution establishes a correspondence between local trapping sets in the code's graph and topological invariants (Betti numbers, bordism classes) of the feature manifold. A practical algorithm estimates $\\beta_N$ efficiently with a quadratic interpolant and Newton correction, achieving a six-fold speed-up over bisection. Guided by topology, we design spherical and toroidal MET-QC-LDPC graph ensembles, using permanent bounds to suppress harmful trapping sets. This compresses 1280-dimensional features to 32 or 64 dimensions for ImageNet-10 and -100 subsets. Despite massive compression (40x fewer parameters), we achieve 98.7% accuracy on ImageNet-10 and 82.7% on ImageNet-100, demonstrating that topology-guided graph design yields highly efficient, physics-inspired embeddings with state-of-the-art performance.\n我们提出了一种结合统计物理、编码理论和代数拓扑的统一框架，用于高效的多类图像分类。从冻结的MobileNetV2骨干网络提取的高维特征向量被视为稀疏Multi-Edge Type准循环LDPC（MET-QC-LDPC）图上的自旋，形成了随机键伊辛模型（RBIM）。我们在该RBIM上以Nishimori温度$\\beta_N$操作，在此温度下Bethe-Hessian矩阵的最小特征值消失，从而最大化类别的可分离性。我们的理论贡献建立了一个对应关系：编码图中的局部陷井集与特征流形上的拓扑不变量（贝蒂数、边缘类）之间存在联系。\n\n一个实用算法通过二次插值和牛顿校正有效地估计$\\beta_N$，这比二分法快六倍。根据拓扑设计了球面和环面MET-QC-LDPC图集合，并利用永久性界限抑制有害的陷井集。这一方法将1280维特征压缩为32或64维度用于ImageNet-10及ImageNet-100子集的数据处理。\n\n尽管存在大量压缩（参数减少了40倍），我们在ImageNet-10上实现了98.7%的准确率，在ImageNet-100上达到了82.7%，证明了基于拓扑设计引导图结构的方法可以生成高效、物理启发式的嵌入，性能达到业界领先水平。\nhttps://arxiv.org/abs/2508.18717\nhttps://arxiv.org/pdf/2508.18717.pdf\nAs the embodiment gap between a robot and a human narrows, new opportunities arise to leverage datasets of humans interacting with their surroundings for robot learning. We propose a novel technique for training sensorimotor policies with reinforcement learning by imitating predictive models of human motions. Our key insight is that the motion of keypoints on human-inspired robot end-effectors closely mirrors the motion of corresponding human body keypoints. This enables us to use a model trained to predict future motion on human data \\emph{zero-shot} on robot data. We train sensorimotor policies to track the predictions of such a model, conditioned on a history of past robot states, while optimizing a relatively sparse task reward. This approach entirely bypasses gradient-based kinematic retargeting and adversarial losses, which limit existing methods from fully leveraging the scale and diversity of modern human-scene interaction datasets. Empirically, we find that our approach can work across robots and tasks, outperforming existing baselines by a large margin. In addition, we find that tracking a human motion model can substitute for carefully designed dense rewards and curricula in manipulation tasks. Code, data and qualitative results available at this https URL.\n随着机器人与人类之间的表现差距逐渐缩小，利用人类与其环境互动的数据集来训练机器人的机会也随之增多。我们提出了一种新颖的技术，通过模仿人类运动预测模型来进行强化学习中传感器-电机策略的训练。我们的关键见解是，在受人启发设计的机器人末端执行器上的关键点运动与对应的人体关键点运动极为相似。这使我们可以将一个用于预测人体未来动作的数据模型直接应用于机器人数据上（无需额外调整）。我们通过条件化历史机器人的状态，训练传感器-电机策略来追踪这种模型的预测，并在此过程中优化相对稀疏的任务奖励。这种方法完全绕过了基于梯度的动力学重定位和对抗损失，这些限制因素使得现有的方法无法充分利用现代人类场景互动数据集的规模和多样性。从经验上来看，我们发现我们的方法可以在不同的机器人和任务中有效运行，并且大幅超越现有基准。此外，我们还发现了追踪一个运动模型可以替代精细设计的密集奖励和课程训练，在操控任务中特别有效。代码、数据及定性结果可在此处获得：[提供链接]。\n\n请直接用提供的URL来替换上述文字中的\"this https URL\"以查看完整信息。\nhttps://arxiv.org/abs/2508.18691\nhttps://arxiv.org/pdf/2508.18691.pdf\nEmpirical scaling laws have driven the evolution of large language models (LLMs), yet their coefficients shift whenever the model architecture or data pipeline changes. Mixture-of-Experts (MoE) models, now standard in state-of-the-art systems, introduce a new sparsity dimension that current dense-model frontiers overlook. We investigate how MoE sparsity influences two distinct capability regimes: memorization and reasoning. We train families of MoE Transformers that systematically vary total parameters, active parameters, and top-$k$ routing while holding the compute budget fixed. For every model we record pre-training loss, downstream task loss, and task accuracy, allowing us to separate the train-test generalization gap from the loss-accuracy gap. Memorization benchmarks improve monotonically with total parameters, mirroring training loss. By contrast, reasoning performance saturates and can even regress despite continued gains in both total parameters and training loss. Altering top-$k$ alone has little effect when active parameters are constant, and classic hyperparameters such as learning rate and initialization modulate the generalization gap in the same direction as sparsity. Neither post-training reinforcement learning (GRPO) nor extra test-time compute rescues the reasoning deficit of overly sparse models. Our model checkpoints, code and logs are open-source at this https URL.\n实证缩放定律推动了大型语言模型（LLMs）的发展，然而每当模型架构或数据流水线发生变化时，它们的系数也会随之改变。混合专家（MoE）模型现在已经成为最先进的系统中的标准组成部分，引入了一个新的稀疏维度，而现有的密集模型前沿却忽视了这一点。我们研究了MoE稀疏性如何影响两个不同的能力范围：记忆和推理。我们训练了一系列系统的MoE变压器，这些系统在总参数、活动参数和top-$k$路由方面有系统性的变化，同时保持计算预算不变。对于每个模型，我们都记录了预训练损失、下游任务损失以及任务准确率，从而能够将训练-测试泛化差距与损失-准确性差距分开。记忆基准随着总参数的增加而单调改善，并且反映了训练损失的变化。相比之下，推理性能会饱和甚至退步，即使在总参数和训练损失继续增长的情况下也是如此。当活动参数保持不变时，仅改变top-$k$几乎没有影响，传统的超参数如学习率和初始化则以与稀疏性相同的方向调节泛化差距。无论是经过训练后的强化学习（GRPO）还是额外的测试时间计算都不能补救过于稀疏模型的推理不足问题。我们的模型检查点、代码和日志可以在[此处](https://this-is-the-url-to-access-the-open-source-materials)公开访问。\nhttps://arxiv.org/abs/2508.18672\nhttps://arxiv.org/pdf/2508.18672.pdf\nAs FMs drive progress toward Artificial General Intelligence (AGI), fine-tuning them under privacy and resource constraints has become increasingly critical particularly when highquality training data resides on distributed edge devices. Federated Learning (FL) offers a compelling solution through Federated Fine-Tuning (FFT), which enables collaborative model adaptation without sharing raw data. Recent approaches incorporate Parameter-Efficient Fine-Tuning (PEFT) techniques such as Low Rank Adaptation (LoRA) to reduce computational overhead. However, LoRA-based FFT faces two major limitations in heterogeneous FL environments: structural incompatibility across clients with varying LoRA configurations and limited adaptability to non-IID data distributions, which hinders convergence and generalization. To address these challenges, we propose FFT MoE, a novel FFT framework that replaces LoRA with sparse Mixture of Experts (MoE) adapters. Each client trains a lightweight gating network to selectively activate a personalized subset of experts, enabling fine-grained adaptation to local resource budgets while preserving aggregation compatibility. To further combat the expert load imbalance caused by device and data heterogeneity, we introduce a heterogeneity-aware auxiliary loss that dynamically regularizes the routing distribution to ensure expert diversity and balanced utilization. Extensive experiments spanning both IID and non-IID conditions demonstrate that FFT MoE consistently outperforms state of the art FFT baselines in generalization performance and training efficiency.\n随着大型模型（FMs）向通用人工智能（AGI）迈进，隐私和资源限制下的微调变得越来越重要，尤其是在高质量训练数据分布在分布式边缘设备上的情况下。联邦学习（FL）通过联邦微调（FFT）提供了一种有吸引力的解决方案，这种方案允许在不共享原始数据的情况下进行模型协作适应。最近的方法结合了参数高效微调（PEFT）技术，如低秩适配（LoRA），以减少计算开销。然而，在异构FL环境中，基于LoRA的FFT面临两个主要限制：不同客户端之间由于不同的LoRA配置而导致的结构不兼容性，以及对非独立同分布（non-IID）数据分布的适应能力有限，这些因素阻碍了模型的收敛性和泛化能力。\n\n为了解决这些问题，我们提出了一种新的联邦微调框架FFT MoE，该框架用稀疏专家混合（MoE）适配器替换了LoRA。每个客户端训练一个轻量级门控网络，以选择性地激活一组个性化的专家子集，这使得模型可以进行细粒度的本地资源预算适应，并保持聚合兼容性。为了进一步解决由于设备和数据异构导致的专业负荷不平衡问题，我们引入了一个感知异质性的辅助损失，该损失动态地规范了路由分布，以确保专家多样性并平衡其利用。\n\n广泛的实验涵盖了独立同分布（IID）和非独立同分布（non-IID）条件下的测试结果表明，在泛化性能和训练效率方面，FFT MoE始终优于现有的最先进的联邦微调基线方法。\nhttps://arxiv.org/abs/2508.18663\nhttps://arxiv.org/pdf/2508.18663.pdf\nOracle Bone Inscriptions (OBIs), play a crucial role in understanding ancient Chinese civilization. The automated detection of OBIs from rubbing images represents a fundamental yet challenging task in digital archaeology, primarily due to various degradation factors including noise and cracks that limit the effectiveness of conventional detection networks. To address these challenges, we propose a novel clustering-based feature space representation learning method. Our approach uniquely leverages the Oracle Bones Character (OBC) font library dataset as prior knowledge to enhance feature extraction in the detection network through clustering-based representation learning. The method incorporates a specialized loss function derived from clustering results to optimize feature representation, which is then integrated into the total network loss. We validate the effectiveness of our method by conducting experiments on two OBIs detection dataset using three mainstream detection frameworks: Faster R-CNN, DETR, and Sparse R-CNN. Through extensive experimentation, all frameworks demonstrate significant performance improvements.\n甲骨文（OBIs）的翻译在理解古代中国文明中扮演着关键角色。从拓印图像中自动检测甲骨文是一项基础但颇具挑战性的数字考古任务，主要是因为诸如噪音和裂缝等退化因素限制了传统检测网络的有效性。为了应对这些挑战，我们提出了一种基于聚类的特征空间表示学习方法。我们的方法独特地利用了甲骨文字（OBC）字体库数据集作为先验知识，在检测网络中通过基于聚类的表示学习增强特征提取能力。该方法将从聚类结果衍生出的专业损失函数集成到总网络损失中，以优化特征表达。\n\n为了验证我们方法的有效性，我们在两个OBIs检测数据集上使用三种主流检测框架（Faster R-CNN、DETR和Sparse R-CNN）进行了实验。通过广泛的实验，所有三个框架都显示出了显著的性能改进。\nhttps://arxiv.org/abs/2508.18641\nhttps://arxiv.org/pdf/2508.18641.pdf\nRecent advancements in code generation have shown remarkable success across software domains, yet hardware description languages (HDLs) such as Verilog remain underexplored due to their concurrency semantics, syntactic rigidity, and simulation complexity. In this work, we address these challenges by introducing a reinforcement learning (RL) framework tailored for Verilog code generation. We first construct Veribench-53K, a high-quality dataset curated from over 700K Verilog problems, enriched with structured prompts, complexity labels, and diverse testbenches. To tackle the problem of sparse and noisy reward signals, we propose a Trace-back based Rescore mechanism that leverages reasoning paths and iterative refinement to enhance feedback reliability and support reward model training. Furthermore, to mitigate catastrophic forgetting and overfitting during RL fine-tuning, we introduce a sample-balanced weighting strategy that adaptively balances learning dynamics based on reward-probability distributions. These innovations are integrated into an iterative RL pipeline that co-evolves the policy and reward models. In contrast to recent work such as CraftRTL, which relies on large-scale closed-source model distillation, and DeepSeek-style approaches that struggle with sparse feedback, our method demonstrates superior performance using a smaller but high-quality dataset combined with RL optimization. Experiments on Verilog generation tasks demonstrate state-of-the-art performance, with substantial gains in test pass rate, functional correctness, and compilation robustness. Our findings highlight the potential of RL-driven approaches for structured code generation in hardware-centric domains. VERIRL is publicly available at this https URL.\n近期在代码生成领域的进展显示了其在软件领域各个方面的显著成功，然而硬件描述语言（如Verilog）由于其并发语义、语法刚性和模拟复杂性等原因仍未得到充分探索。为应对这些挑战，我们引入了一种针对Verilog代码生成的强化学习（RL）框架。\n\n首先，我们构建了一个高质量的数据集——Veribench-53K，该数据集中包含了超过70万个问题，并通过结构化的提示、复杂度标签和多样化的测试基准进行了丰富。为解决稀疏且嘈杂的奖励信号问题，我们提出了一种基于回溯的重新评分机制（Trace-back based Rescore mechanism），利用推理路径和迭代细化来增强反馈可靠性并支持奖励模型训练。\n\n此外，为了在强化学习微调过程中缓解灾难性遗忘和过拟合的问题，我们引入了样本均衡加权策略，该策略根据奖励-概率分布自适应地平衡学习动态。这些创新被整合到一个迭代的RL流水线中，实现了策略和奖励模型的同时进化。\n\n与最近的工作（如依赖大规模闭源模型蒸馏的CraftRTL）或在稀疏反馈方面挣扎的DeepSeek风格方法相比，我们的方法通过使用一个小但高质量的数据集结合RL优化，展示了卓越的性能。实验结果表明，在Verilog生成任务中，我们的方法在测试通过率、功能正确性和编译鲁棒性方面取得了显著提升。\n\n这些发现强调了基于强化学习的方法在硬件为中心领域中的结构化代码生成方面的潜力。VERIRL软件可在此网址上公开获取。\nhttps://arxiv.org/abs/2508.18462\nhttps://arxiv.org/pdf/2508.18462.pdf\nVision foundation models (FMs) have become the predominant architecture in computer vision, providing highly transferable representations learned from large-scale, multimodal corpora. Nonetheless, they exhibit persistent limitations on tasks that require explicit reasoning over entities, roles, and spatio-temporal relations. Such relational competence is indispensable for fine-grained human activity recognition, egocentric video understanding, and multimodal medical image analysis, where spatial, temporal, and semantic dependencies are decisive for performance. We advance the position that next-generation FMs should incorporate explicit relational interfaces, instantiated as dynamic relational graphs (graphs whose topology and edge semantics are inferred from the input and task context). We illustrate this position with cross-domain evidence from recent systems in human manipulation action recognition and brain tumor segmentation, showing that augmenting FMs with lightweight, context-adaptive graph-reasoning modules improves fine-grained semantic fidelity, out of distribution robustness, interpretability, and computational efficiency relative to FM only baselines. Importantly, by reasoning sparsely over semantic nodes, such hybrids also achieve favorable memory and hardware efficiency, enabling deployment under practical resource constraints. We conclude with a targeted research agenda for FM graph hybrids, prioritizing learned dynamic graph construction, multi-level relational reasoning (e.g., part object scene in activity understanding, or region organ in medical imaging), cross-modal fusion, and evaluation protocols that directly probe relational competence in structured vision tasks.\n视觉基础模型（FMs）已经成为计算机视觉领域的主流架构，它们从大规模多模态语料库中学习到的高度可转移表示为各种任务提供了强大的支持。然而，在需要明确推理实体、角色和时空关系的任务上，这些模型仍然表现出持续的局限性。这种关系能力对于细粒度的人体活动识别、第一人称视角视频理解以及跨模态医学图像分析至关重要，因为空间、时间及语义依赖性对性能有着决定性的影响。\n\n我们主张下一代FMs应该融入明确的关系接口，这些接口可以被视作动态关系图（其拓扑结构和边的语义是从输入和任务上下文中推断出来的）。通过最近的人类操作动作识别系统和脑肿瘤分割系统的跨领域证据来说明这一观点，证明了在基础模型中加入轻量级、上下文自适应的关系推理模块可以提升细粒度语义精度，在分布外数据上的鲁棒性、可解释性和计算效率方面超越纯FM基线。\n\n重要的是，通过稀疏地对语义节点进行推理，这种混合系统还实现了更佳的记忆和硬件效率，使得在实际资源限制下部署成为可能。我们最后提出了一个针对FMs图谱混合体的有针对性的研究议程，优先考虑学习动态图构建、多层次关系推理（例如，在活动理解中的部分-对象-场景，或者医学成像中的区域-器官），跨模态融合以及直接探究结构化视觉任务中关系能力的评估协议。\nhttps://arxiv.org/abs/2508.18421\nhttps://arxiv.org/pdf/2508.18421.pdf\n\n            Recent Papers\n        \n\n            Contact me at:\n            \n\n\n\n\n\n\n京ICP备18036300号-1\n"
        },
        {
            "title": "灯塔索引- 为您的科研、创新、梦想导航引路",
            "content": "16 hours ago ... 科研工具箱 · 收藏 · 生活服务 · AI助手 · 科学研究 · 学术发表 · 临床医学 · 医技科室 · 中医中药.",
            "url": "https://www.dotaindex.com/",
            "source": "www.dotaindex.com",
            "published_date": "2025-08-28",
            "search_engine": "Google",
            "query": "人工智能癌症早期诊断 重大研究进展 最近7天 breakthrough cancer detection machine learning",
            "description": "16 hours ago ... 科研工具箱 · 收藏 · 生活服务 · AI助手 · 科学研究 · 学术发表 · 临床医学 · 医技科室 · 中医中药.",
            "image": "",
            "category": "innovation_news",
            "search_source": "google",
            "full_content": "16 hours ago ... 科研工具箱 · 收藏 · 生活服务 · AI助手 · 科学研究 · 学术发表 · 临床医学 · 医技科室 · 中医中药."
        },
        {
            "title": "图像和视频处理2025_8_22",
            "content": "6 days ago ... 在最小训练数据集上进行血管分割的准确性达到了最先进的结果。它帮助我们基于IXI数据集（注释200张图像）创建大型、半手动注释的脑部MRA图像脑血管数据集。应用HessNet后，由 ...",
            "url": "https://www.arxivdaily.com/thread/70831",
            "source": "www.arxivdaily.com",
            "published_date": "2025-08-28",
            "search_engine": "Google",
            "query": "生成式AI在医疗诊断中的应用 最新研究 最近7天 generative AI medical diagnosis research paper",
            "description": "6 days ago ... 在最小训练数据集上进行血管分割的准确性达到了最先进的结果。它帮助我们基于IXI数据集（注释200张图像）创建大型、半手动注释的脑部MRA图像脑血管数据集。应用HessNet后，由 ...",
            "image": "",
            "category": "innovation_news",
            "search_source": "google",
            "full_content": "6 days ago ... 在最小训练数据集上进行血管分割的准确性达到了最先进的结果。它帮助我们基于IXI数据集（注释200张图像）创建大型、半手动注释的脑部MRA图像脑血管数据集。应用HessNet后，由 ..."
        },
        {
            "title": "Paper Reading",
            "content": "6 days ago ... Abstract. In this paper, we describe and benchmark a competitor-discovery component used within an agentic AI system for fast drug asset due diligence. A ...",
            "url": "http://paperreading.club/",
            "source": "paperreading.club",
            "published_date": "2025-08-28",
            "search_engine": "Google",
            "query": "生成式AI在医疗诊断中的应用 最新研究 最近7天 generative AI medical diagnosis research paper",
            "description": "6 days ago ... Abstract. In this paper, we describe and benchmark a competitor-discovery component used within an agentic AI system for fast drug asset due diligence. A ...",
            "image": "",
            "category": "innovation_news",
            "search_source": "google",
            "full_content": "Autonomous agents for Graphical User Interfaces (GUIs) face significant challenges in specialized domains such as scientific computing, where both long-horizon planning and precise execution are required. Existing approaches suffer from a trade-off: generalist agents excel at planning but perform poorly in execution, while specialized agents demonstrate the opposite weakness. Recent compositional frameworks attempt to bridge this gap by combining a planner and an actor, but they are typically static and non-trainable, which prevents adaptation from experience. This is a critical limitation given the scarcity of high-quality data in scientific domains. To address these limitations, we introduce CODA, a novel and trainable compositional framework that integrates a generalist planner (Cerebrum) with a specialist executor (Cerebellum), trained via a dedicated two-stage pipeline. In the first stage, Specialization, we apply a decoupled GRPO approach to train an expert planner for each scientific application individually, bootstrapping from a small set of task trajectories. In the second stage, Generalization, we aggregate all successful trajectories from the specialized experts to build a consolidated dataset, which is then used for supervised fine-tuning of the final planner. This equips CODA with both robust execution and cross-domain generalization. Evaluated on four challenging applications from the ScienceBoard benchmark, CODA significantly outperforms baselines and establishes a new state of the art among open-source models.\n自动代理在图形用户界面（GUI）中的特定领域，如科学计算中面临着重大挑战。这些领域需要长期规划和精确执行的结合能力。现有的方法存在权衡问题：通才型代理擅长于计划但执行表现不佳；而专门型代理则正好相反，在执行方面表现出色但在规划上却显得不足。最近的一些组合框架试图通过结合一个规划器和一个行动者来弥补这一差距，但是这些框架通常是静态的且不可训练，这阻碍了从经验中的适应性学习能力。考虑到科学领域高质量数据的稀缺性，这种局限性尤为重要。\n\n为了克服这些限制，我们提出了CODA（Cerebrum and Cerebellum for GUI Automation），这是一个新型且可训练的组合框架，它将通用规划器（称为大脑皮层）与专业执行者（小脑）结合在一起，并通过一个专门设计的两阶段管道进行训练。\n\n在第一阶段，即专业化阶段，我们采用解耦GRPO方法为每个科学应用单独训练专家级规划器，初始训练基于少量任务轨迹。第二阶段是泛化阶段，在这个阶段中，我们将所有专业领域专家的成功轨迹聚集起来构建一个综合数据集，然后利用该数据集进行监督微调以完善最终的规划器。\n\n通过这种方式，CODA不仅具备了稳健的执行能力，还具有跨领域的泛化性能。在ScienceBoard基准测试中的四个挑战性应用上评估后，CODA显著超越基线模型，并且在开源模型中确立了一个新的最佳状态。\nhttps://arxiv.org/abs/2508.20096\nhttps://arxiv.org/pdf/2508.20096.pdf\nMulti-Robot Motion Planning (MRMP) involves generating collision-free trajectories for multiple robots operating in a shared continuous workspace. While discrete multi-agent path finding (MAPF) methods are broadly adopted due to their scalability, their coarse discretization severely limits trajectory quality. In contrast, continuous optimization-based planners offer higher-quality paths but suffer from the curse of dimensionality, resulting in poor scalability with respect to the number of robots. This paper tackles the limitations of these two approaches by introducing a novel framework that integrates discrete MAPF solvers with constrained generative diffusion models. The resulting framework, called Discrete-Guided Diffusion (DGD), has three key characteristics: (1) it decomposes the original nonconvex MRMP problem into tractable subproblems with convex configuration spaces, (2) it combines discrete MAPF solutions with constrained optimization techniques to guide diffusion models capture complex spatiotemporal dependencies among robots, and (3) it incorporates a lightweight constraint repair mechanism to ensure trajectory feasibility. The proposed method sets a new state-of-the-art performance in large-scale, complex environments, scaling to 100 robots while achieving planning efficiency and high success rates.\n多机器人路径规划（MRMP）涉及为在共享连续工作空间中运行的多个机器人生成无碰撞轨迹。虽然离散多智能体路径查找（MAPF）方法由于其可扩展性而被广泛采用，但其粗略离散化严重限制了轨迹质量。相比之下，基于连续优化的规划器提供了更高质量的路径，但由于维度灾难问题，它们在机器人数量增加时表现出较差的可扩展性。本文通过引入一种将离散MAPF求解器与约束生成扩散模型相结合的新框架，解决了上述两种方法的局限性。该框架称为离散引导扩散（DGD），具有三个关键特性：(1) 它将原始非凸MRMP问题分解为具有凸配置空间的可处理子问题；(2) 它结合了离散MAPF解决方案与约束优化技术，以指导扩散模型捕捉机器人之间的复杂时空依赖关系；(3) 它集成了轻量级约束修复机制，确保轨迹可行性。该方法在大规模、复杂的环境中表现出了新的最先进的性能，在涉及多达100个机器人的场景中实现了规划效率和高成功率的结合。\nhttps://arxiv.org/abs/2508.20095\nhttps://arxiv.org/pdf/2508.20095.pdf\nLabelling images of Lepidoptera (moths) from automated camera systems is vital for understanding insect declines. However, accurate species identification is challenging due to domain shifts between curated images and noisy field imagery. We propose a lightweight classification approach, combining limited expert-labelled field data with knowledge distillation from the high-performance BioCLIP2 foundation model into a ConvNeXt-tiny architecture. Experiments on 101 Danish moth species from AMI camera systems demonstrate that BioCLIP2 substantially outperforms other methods and that our distilled lightweight model achieves comparable accuracy with significantly reduced computational cost. These insights offer practical guidelines for the development of efficient insect monitoring systems and bridging domain gaps for fine-grained classification.\n对自动化相机系统拍摄的鳞翅目（蛾类）图像进行标注对于理解昆虫数量减少至关重要。然而，由于精心整理的图片与现场杂乱影像之间的领域差异，准确的物种识别变得颇具挑战性。我们提出了一种轻量级分类方法，该方法结合了有限的专家标注现场数据和从高性能BioCLIP2基础模型中提取的知识蒸馏技术，并将其应用到ConvNeXt-tiny架构上。在对来自AMI相机系统的101种丹麦蛾类物种进行实验后，我们发现BioCLIP2大幅优于其他方法，而我们的轻量级模型虽然计算成本显著降低，但依然能达到相似的准确性。这些研究结果为开发高效的昆虫监测系统以及解决细粒度分类领域的差距提供了实用指导。\nhttps://arxiv.org/abs/2508.20089\nhttps://arxiv.org/pdf/2508.20089.pdf\nRecent advances in text-to-audio (TTA) generation excel at synthesizing short audio clips but struggle with long-form narrative audio, which requires temporal coherence and compositional reasoning. To address this gap, we propose AudioStory, a unified framework that integrates large language models (LLMs) with TTA systems to generate structured, long-form audio narratives. AudioStory possesses strong instruction-following reasoning generation capabilities. It employs LLMs to decompose complex narrative queries into temporally ordered sub-tasks with contextual cues, enabling coherent scene transitions and emotional tone consistency. AudioStory has two appealing features: (1) Decoupled bridging mechanism: AudioStory disentangles LLM-diffuser collaboration into two specialized components, i.e., a bridging query for intra-event semantic alignment and a residual query for cross-event coherence preservation. (2) End-to-end training: By unifying instruction comprehension and audio generation within a single end-to-end framework, AudioStory eliminates the need for modular training pipelines while enhancing synergy between components. Furthermore, we establish a benchmark AudioStory-10K, encompassing diverse domains such as animated soundscapes and natural sound narratives. Extensive experiments show the superiority of AudioStory on both single-audio generation and narrative audio generation, surpassing prior TTA baselines in both instruction-following ability and audio fidelity. Our code is available at this https URL\n近期在文本到音频（TTA）生成方面的进展虽然擅长合成短音频片段，但在处理长篇叙事音频时却面临挑战。长篇叙事音频需要时间连贯性和组合推理能力。为了解决这一差距，我们提出了AudioStory框架，该框架将大规模语言模型（LLMs）与TTA系统集成在一起，用于生成结构化、长篇的音频叙述。AudioStory具备强大的指令跟随式推理和生成能力。\n\nAudioStory采用大型语言模型来解析复杂的叙事查询，并将其分解为带有上下文提示的时间顺序子任务，从而确保场景之间的连贯转换以及情感基调的一致性。此外，它具有两个显著特点：\n\n1. **解耦桥梁机制**：AudioStory将LLM与扩散器的合作过程分解为两个专门的组件，即用于事件内语义对齐的桥梁查询和用于跨事件一致性保留的残差查询。\n2. **端到端训练**：通过在单一端到端框架中统一指令理解与音频生成的过程，AudioStory消除了模块化训练管道的需求，并增强了各个部分之间的协同作用。\n\n此外，我们建立了一个基准测试集AudioStory-10K，涵盖动画音景和自然声音叙述等多样化领域。广泛的实验表明，无论是单个音频的生成还是叙事音频的生成，AudioStory都优于之前的TTA基线模型，在指令跟随能力和音频保真度方面均表现出色。\n\n我们的代码可在此链接获取：[此URL]（请将\"this https URL\"替换为实际链接地址）。\nhttps://arxiv.org/abs/2508.20088\nhttps://arxiv.org/pdf/2508.20088.pdf\nLeveraging human motion data to impart robots with versatile manipulation skills has emerged as a promising paradigm in robotic manipulation. Nevertheless, translating multi-source human hand motions into feasible robot behaviors remains challenging, particularly for robots equipped with multi-fingered dexterous hands characterized by complex, high-dimensional action spaces. Moreover, existing approaches often struggle to produce policies capable of adapting to diverse environmental conditions. In this paper, we introduce HERMES, a human-to-robot learning framework for mobile bimanual dexterous manipulation. First, HERMES formulates a unified reinforcement learning approach capable of seamlessly transforming heterogeneous human hand motions from multiple sources into physically plausible robotic behaviors. Subsequently, to mitigate the sim2real gap, we devise an end-to-end, depth image-based sim2real transfer method for improved generalization to real-world scenarios. Furthermore, to enable autonomous operation in varied and unstructured environments, we augment the navigation foundation model with a closed-loop Perspective-n-Point (PnP) localization mechanism, ensuring precise alignment of visual goals and effectively bridging autonomous navigation and dexterous manipulation. Extensive experimental results demonstrate that HERMES consistently exhibits generalizable behaviors across diverse, in-the-wild scenarios, successfully performing numerous complex mobile bimanual dexterous manipulation tasks. Project Page:https:/gemcollector.github.io/HERMES/.\n利用人类动作数据来赋予机器人多样化的操作技能已成为机器人操纵领域的一种有前景的范式。然而，将来自多源的人类手部动作转化为可行的机器人行为仍然颇具挑战性，尤其是在具有复杂高维动作空间的多指灵巧手中更是如此。此外，现有的方法往往难以生成能够适应各种环境条件的策略。\n\n在本文中，我们介绍了HERMES——一种用于移动双臂灵巧操作的人机学习框架。首先，HERMES提出了一种统一的强化学习方法，可以无缝地将来自多个来源的异质人类手部动作转化为物理上合理的机器人行为。其次，为了缩小仿真与现实之间的差距，我们设计了一种端到端、基于深度图像的仿真到真实场景转移的方法，以提高在实际环境中的泛化能力。此外，为了使机器人能够在各种未结构化的环境中自主操作，我们将导航基础模型增强了一个闭环透视n点（PnP）定位机制，确保视觉目标的精确对齐，并有效地连接了自主导航与灵巧操作。\n\n大量的实验结果表明，HERMES在多种现实世界场景中表现出一致且通用的行为模式，成功执行了许多复杂的移动双臂灵巧操纵任务。项目页面：https://gemcollector.github.io/HERMES/.\nhttps://arxiv.org/abs/2508.20085\nhttps://arxiv.org/pdf/2508.20085.pdf\nRetrieval-Augmented Generation (RAG) has become a standard approach for improving the reliability of large language models (LLMs). Prior work demonstrates the vulnerability of RAG systems by misleading them into generating attacker-chosen outputs through poisoning the knowledge base. However, this paper uncovers that such attacks could be mitigated by the strong \\textit{self-correction ability (SCA)} of modern LLMs, which can reject false context once properly configured. This SCA poses a significant challenge for attackers aiming to manipulate RAG systems. In contrast to previous poisoning methods, which primarily target the knowledge base, we introduce \\textsc{DisarmRAG}, a new poisoning paradigm that compromises the retriever itself to suppress the SCA and enforce attacker-chosen outputs. This compromisation enables the attacker to straightforwardly embed anti-SCA instructions into the context provided to the generator, thereby bypassing the SCA. To this end, we present a contrastive-learning-based model editing technique that performs localized and stealthy edits, ensuring the retriever returns a malicious instruction only for specific victim queries while preserving benign retrieval behavior. To further strengthen the attack, we design an iterative co-optimization framework that automatically discovers robust instructions capable of bypassing prompt-based defenses. We extensively evaluate DisarmRAG across six LLMs and three QA benchmarks. Our results show near-perfect retrieval of malicious instructions, which successfully suppress SCA and achieve attack success rates exceeding 90\\% under diverse defensive prompts. Also, the edited retriever remains stealthy under several detection methods, highlighting the urgent need for retriever-centric defenses.\n基于检索的生成（Retrieval-Augmented Generation，RAG）已经成为提高大型语言模型（LLMs）可靠性的标准方法。先前的研究表明，通过污染知识库可以误导RAG系统产生攻击者选择的输出，从而暴露出其脆弱性。然而，本文揭示了现代LLM的强大自我纠正能力（Self-Correction Ability, SCA），只要正确配置，就可以拒绝虚假上下文，因此这种攻击可以通过这一机制被减轻。这种SCA对试图操纵RAG系统的攻击者构成了重大挑战。\n\n与之前的中毒方法主要针对知识库不同，我们提出了一个新的中毒范式——**DisarmRAG**，该方法通过破坏检索器本身来抑制SCA，并强制生成器输出攻击者选定的内容。这种妥协允许攻击者直接将对抗性-SCA指令嵌入提供给生成器的上下文中，从而绕过SCA。\n\n为此，我们提出了一种基于对比学习的模型编辑技术，执行局部且隐蔽的修改，确保检索器仅对特定受害查询返回恶意指令，同时保持正常的检索行为。为进一步加强攻击效果，我们设计了一个迭代协同优化框架，能够自动发现可以绕过提示防御的强大指令。\n\n我们在六种不同的LLM和三个问答基准上进行了广泛的评估。我们的结果显示，在各种防御性提示下，DisarmRAG几乎可以完美地检索出恶意指令，成功抑制SCA，并且攻击成功率超过90%。此外，经过修改的检索器在几种检测方法中仍然保持隐蔽，这突显了对以检索器为中心的防御措施进行紧急研究的需求。\nhttps://arxiv.org/abs/2508.20083\nhttps://arxiv.org/pdf/2508.20083.pdf\n360-degree visual content is widely shared on platforms such as YouTube and plays a central role in virtual reality, robotics, and autonomous navigation. However, consumer-grade dual-fisheye systems consistently yield imperfect panoramas due to inherent lens separation and angular distortions. In this work, we introduce a novel calibration framework that incorporates a dual-fisheye camera model into the 3D Gaussian splatting pipeline. Our approach not only simulates the realistic visual artifacts produced by dual-fisheye cameras but also enables the synthesis of seamlessly rendered 360-degree images. By jointly optimizing 3D Gaussian parameters alongside calibration variables that emulate lens gaps and angular distortions, our framework transforms imperfect omnidirectional inputs into flawless novel view synthesis. Extensive evaluations on real-world datasets confirm that our method produces seamless renderings-even from imperfect images-and outperforms existing 360-degree rendering models.\n360度视觉内容在YouTube等平台上广泛分享，并在虚拟现实、机器人技术和自主导航中发挥核心作用。然而，消费者级的双鱼眼系统始终会产生不完美的全景图，这是由于镜头之间的固有分离和角度失真造成的。在这项工作中，我们引入了一个新颖的校准框架，该框架将双鱼眼相机模型整合到3D高斯点喷射（Gaussian splatting）管道中。我们的方法不仅模拟了由双鱼眼摄像头产生的现实视觉伪影，还能够合成无缝渲染的360度图像。通过同时优化3D高斯参数以及模拟镜头间隙和角度失真的校准变量，我们的框架可以将不完美的全方位输入转换为完美无瑕的新视角合成效果。在真实数据集上的广泛评估确认了我们方法产生的渲染结果具有连续性（即使是从不完美的图片中），并且优于现有的360度渲染模型。\nhttps://arxiv.org/abs/2508.20080\nhttps://arxiv.org/pdf/2508.20080.pdf\nVision-Language-Action (VLA) models adapt large vision-language backbones to map images and instructions to robot actions. However, prevailing VLA decoders either generate actions autoregressively in a fixed left-to-right order or attach continuous diffusion or flow matching heads outside the backbone, demanding specialized training and iterative sampling that hinder a unified, scalable architecture. We present Discrete Diffusion VLA, a single-transformer policy that models discretized action chunks with discrete diffusion and is trained with the same cross-entropy objective as the VLM backbone. The design retains diffusion's progressive refinement paradigm while remaining natively compatible with the discrete token interface of VLMs. Our method achieves an adaptive decoding order that resolves easy action elements before harder ones and uses secondary remasking to revisit uncertain predictions across refinement rounds, which improves consistency and enables robust error correction. This unified decoder preserves pretrained vision language priors, supports parallel decoding, breaks the autoregressive bottleneck, and reduces the number of function evaluations. Discrete Diffusion VLA achieves 96.3% avg. SR on LIBERO, 71.2% visual matching on SimplerEnv Fractal and 49.3% overall on SimplerEnv Bridge, improving over both autoregressive and continuous diffusion baselines. These findings indicate that discrete-diffusion action decoder supports precise action modeling and consistent training, laying groundwork for scaling VLA to larger models and datasets.\nVision-Language-Action (VLA) 模型通过调整大型视觉语言骨干网络来将图像和指令映射到机器人动作。然而，现有的VLA解码器要么以固定的从左至右顺序自回归生成动作，要么在骨干网外部附加连续扩散或流匹配头部，这需要专门的训练和迭代采样过程，阻碍了统一、可扩展架构的发展。我们提出了离散扩散VLA（Discrete Diffusion VLA），这是一种单一变压器策略，它使用离散扩散来建模离散的动作块，并且该模型在相同的交叉熵目标下与视觉语言骨干网一起进行训练。此设计保留了扩散的渐进式细化范例，同时天然兼容视觉语言模型的离散令牌接口。我们的方法实现了自适应解码顺序，在解决简单动作元素之前先处理复杂的动作部分，并通过二次掩模技术在多次细化迭代中重新访问不确定预测，这提高了结果的一致性并增强了错误修正能力。这种统一的解码器保留了预训练的视觉语言先验知识，支持并行解码，打破了自回归瓶颈，并减少了函数评估次数。离散扩散VLA模型在LIBERO数据集上达到了96.3%的平均准确率，在SimplerEnv Fractal上的视觉匹配任务中达到71.2%，以及在SimplerEnv Bridge任务中总评分为49.3%，这些成绩均超越了自回归和连续扩散基线。这些发现表明，离散扩散动作解码器支持精确的动作建模和一致的训练，并为将VLA扩展到更大规模模型和数据集奠定了基础。\nhttps://arxiv.org/abs/2508.20072\nhttps://arxiv.org/pdf/2508.20072.pdf\nFor human cognitive process, spatial reasoning and perception are closely entangled, yet the nature of this interplay remains underexplored in the evaluation of multimodal large language models (MLLMs). While recent MLLM advancements show impressive performance on reasoning, their capacity for human-like spatial cognition remains an open question. In this work, we introduce a systematic evaluation framework to assess the spatial reasoning abilities of state-of-the-art MLLMs relative to human performance. Central to our work is 11Plus-Bench, a high-quality benchmark derived from realistic standardized spatial aptitude tests. 11Plus-Bench also features fine-grained expert annotations of both perceptual complexity and reasoning process, enabling detailed instance-level analysis of model behavior. Through extensive experiments across 14 MLLMs and human evaluation, we find that current MLLMs exhibit early signs of spatial cognition. Despite a large performance gap compared to humans, MLLMs' cognitive profiles resemble those of humans in that cognitive effort correlates strongly with reasoning-related complexity. However, instance-level performance in MLLMs remains largely random, whereas human correctness is highly predictable and shaped by abstract pattern complexity. These findings highlight both emerging capabilities and limitations in current MLLMs' spatial reasoning capabilities and provide actionable insights for advancing model design.\n对于人类的认知过程而言，空间推理和感知紧密交织在一起，但在评估多模态大型语言模型（MLLMs）时，这种相互作用的本质仍然被忽视。尽管最近的MLLM进展在推理任务上表现出色，但它们是否具备类似人类的空间认知能力仍是一个未解之谜。在这项工作中，我们引入了一个系统性评估框架，用于评估最新状态的多模态大型语言模型（MLLMs）在空间推理方面的能力，并将其与人类的表现进行比较。我们的研究核心是11Plus-Bench，这是一个从现实中的标准化空间能力测试中提取出的高质量基准数据集。11Plus-Bench还包括了专家对感知复杂度和推理过程进行了详细的标注，这使得可以进行详细实例级别的模型行为分析。\n\n通过在14种MLLMs和人类评估上的广泛实验，我们发现当前的MLLM已经展现出了一些初步的空间认知迹象。尽管与人类相比存在较大的性能差距，但MLLM的认知特征类似于人类：认知努力与推理相关的复杂度密切相关。然而，在MLLM中的实例级表现仍然很大程度上是随机的，而人类正确率高度可预测，并受到抽象模式复杂性的塑造。\n\n这些发现不仅揭示了当前MLLM在空间推理能力方面的新兴能力和局限性，而且还为推进模型设计提供了实际可行的见解。\nhttps://arxiv.org/abs/2508.20068\nhttps://arxiv.org/pdf/2508.20068.pdf\nCross-view geo-localization is a critical task for UAV navigation, event detection, and aerial surveying, as it enables matching between drone-captured and satellite imagery. Most existing approaches embed multi-modal data into a joint feature space to maximize the similarity of paired images. However, these methods typically assume perfect alignment of image pairs during training, which rarely holds true in real-world scenarios. In practice, factors such as urban canyon effects, electromagnetic interference, and adverse weather frequently induce GPS drift, resulting in systematic alignment shifts where only partial correspondences exist between pairs. Despite its prevalence, this source of noisy correspondence has received limited attention in current research. In this paper, we formally introduce and address the Noisy Correspondence on Cross-View Geo-Localization (NC-CVGL) problem, aiming to bridge the gap between idealized benchmarks and practical applications. To this end, we propose PAUL (Partition and Augmentation by Uncertainty Learning), a novel framework that partitions and augments training data based on estimated data uncertainty through uncertainty-aware co-augmentation and evidential co-training. Specifically, PAUL selectively augments regions with high correspondence confidence and utilizes uncertainty estimation to refine feature learning, effectively suppressing noise from misaligned pairs. Distinct from traditional filtering or label correction, PAUL leverages both data uncertainty and loss discrepancy for targeted partitioning and augmentation, thus providing robust supervision for noisy samples. Comprehensive experiments validate the effectiveness of individual components in PAUL,which consistently achieves superior performance over other competitive noisy-correspondence-driven methods in various noise ratios.\n跨视角地理定位是无人机导航、事件检测和航空测绘中的关键任务，因为它能够将无人机拍摄的图像与卫星图像进行匹配。现有大多数方法都将多模态数据嵌入到一个共同特征空间中以最大化配对图像之间的相似性。然而，这些方法通常假设在训练过程中图像对具有完美的对齐情况，在实际场景中这种理想状态很少出现。实际上，诸如城市峡谷效应、电磁干扰和恶劣天气等因素经常导致GPS漂移，造成系统性的定位偏差，使得图像对之间仅存在部分对应关系。尽管这种情况普遍存在于实践中，但在当前研究中这一类噪声对应问题却受到了较少的关注。\n\n本文正式引入并解决了跨视角地理定位中的噪声对应问题（NC-CVGL），旨在弥合理想化基准与实际应用之间的差距。为此，我们提出了PAUL（不确定性学习的分区和增强）框架，通过利用不确定性感知协同增强和证据协同训练来估计数据的不确定性，并据此对训练数据进行分区和增强。具体而言，PAUL选择性地增强具有高对应置信度的区域，并利用不确定性估计来改进特征学习，有效地抑制了来自错位图像对中的噪声。不同于传统的过滤或标签修正方法，PAUL同时使用数据不确定性和损失差异来进行有针对性的数据分割和增强，从而为带有噪声样本提供稳健的监督。\n\n全面的实验验证了PAUL各个组成部分的有效性，在不同的噪声比例下，PAUL始终优于其他竞争性的基于噪声对应的方法。\nhttps://arxiv.org/abs/2508.20066\nhttps://arxiv.org/pdf/2508.20066.pdf\nAge-related Macular Degeneration (AMD) is a prevalent eye condition affecting visual acuity. Anti-vascular endothelial growth factor (anti-VEGF) treatments have been effective in slowing the progression of neovascular AMD, with better outcomes achieved through timely diagnosis and consistent monitoring. Tracking the progression of neovascular activity in OCT scans of patients with exudative AMD allows for the development of more personalized and effective treatment plans. This was the focus of the Monitoring Age-related Macular Degeneration Progression in Optical Coherence Tomography (MARIO) challenge, in which we participated. In Task 1, which involved classifying the evolution between two pairs of 2D slices from consecutive OCT acquisitions, we employed a fusion CNN network with model ensembling to further enhance the model's performance. For Task 2, which focused on predicting progression over the next three months based on current exam data, we proposed the Patch Progression Masked Autoencoder that generates an OCT for the next exam and then classifies the evolution between the current OCT and the one generated using our solution from Task 1. The results we achieved allowed us to place in the Top 10 for both tasks. Some team members are part of the same organization as the challenge organizers; therefore, we are not eligible to compete for the prize.\n年龄相关性黄斑变性（AMD）是一种影响视力清晰度的常见眼疾。抗血管内皮生长因子（anti-VEGF）治疗在减缓新生血管型AMD的发展方面取得了成效，及早诊断和持续监测可以取得更好的治疗效果。通过跟踪具有渗出性AMD患者的光学相干断层扫描（OCT）图像中的新生血管活动进展，能够为患者制定更加个性化且有效的治疗方案。这正是Optical Coherence Tomography (MARIO)挑战赛的重点，在这个比赛中我们进行了参与。\n\n在任务1中，我们的目标是通过对两次连续OCT获取的2D切片进行分类来判断其演化情况。为此，我们采用了一种融合CNN网络与模型集成的方法以进一步提升模型的表现能力。对于任务2，则要求基于当前检查数据预测接下来三个月的发展趋势。针对这一挑战，我们提出了一种“补丁进展掩码自编码器”，该方法可以生成未来的OCT图像，并对当前的OCT和使用我们的解决方案所生成的未来OCT进行演化分类。\n\n我们在两项任务中的表现使我们进入了前10名的位置。然而，部分团队成员与比赛主办方隶属于同一组织，因此我们不符合参加此次竞赛赢取奖项的资格。\nhttps://arxiv.org/abs/2508.20064\nhttps://arxiv.org/pdf/2508.20064.pdf\nOpen-vocabulary (OV) 3D object detection is an emerging field, yet its exploration through image-based methods remains limited compared to 3D point cloud-based methods. We introduce OpenM3D, a novel open-vocabulary multi-view indoor 3D object detector trained without human annotations. In particular, OpenM3D is a single-stage detector adapting the 2D-induced voxel features from the ImGeoNet model. To support OV, it is jointly trained with a class-agnostic 3D localization loss requiring high-quality 3D pseudo boxes and a voxel-semantic alignment loss requiring diverse pre-trained CLIP features. We follow the training setting of OV-3DET where posed RGB-D images are given but no human annotations of 3D boxes or classes are available. We propose a 3D Pseudo Box Generation method using a graph embedding technique that combines 2D segments into coherent 3D structures. Our pseudo-boxes achieve higher precision and recall than other methods, including the method proposed in OV-3DET. We further sample diverse CLIP features from 2D segments associated with each coherent 3D structure to align with the corresponding voxel feature. The key to training a highly accurate single-stage detector requires both losses to be learned toward high-quality targets. At inference, OpenM3D, a highly efficient detector, requires only multi-view images for input and demonstrates superior accuracy and speed (0.3 sec. per scene) on ScanNet200 and ARKitScenes indoor benchmarks compared to existing methods. We outperform a strong two-stage method that leverages our class-agnostic detector with a ViT CLIP-based OV classifier and a baseline incorporating multi-view depth estimator on both accuracy and speed.\n开放词汇（Open-vocabulary, OV）的3D物体检测是一个新兴的研究领域，但与基于3D点云的方法相比，通过图像方法进行探索仍然有限。我们引入了OpenM3D，这是一种新颖的、无需人工标注的开放式多视角室内3D物体检测器。特别地，OpenM3D是一种单阶段检测器，它借鉴了ImGeoNet模型中的2D诱导体素特征。为了支持OV，它与一个类无关的3D定位损失和一个体素语义对齐损失一起联合训练，前者需要高质量的3D伪框，后者需要多样化的预训练CLIP特征。\n\n我们遵循OV-3DET的训练设置，在该设置中给定了具有姿态信息的RGB-D图像，但没有提供3D边界框或类别的人工标注。我们提出了一种基于图嵌入技术生成3D伪框的方法，该方法将2D片段组合成一致的3D结构。我们的伪框在精度和召回率方面优于其他现有方法，包括OV-3DET中提出的方法。\n\n此外，为了与相应的体素特征对齐，我们在每个一致的3D结构关联的2D片段上采样多样化的CLIP特征。训练一个高度准确的单阶段检测器的关键在于两个损失函数都需要朝着高质量的目标学习。在推理过程中，高效且轻量级的OpenM3D只需要多视角图像作为输入，并在ScanNet200和ARKitScenes室内基准测试中展示了优于现有方法的精度和速度（每场景0.3秒）。\n\n我们还超越了一种强大的两阶段方法，在这种方法中，我们的类无关检测器与基于ViT CLIP的OV分类器以及多视角深度估计基线相结合，无论是在准确性还是速度方面都表现出色。\nhttps://arxiv.org/abs/2508.20063\nhttps://arxiv.org/pdf/2508.20063.pdf\nWe introduce {AraHealthQA 2025}, the {Comprehensive Arabic Health Question Answering Shared Task}, held in conjunction with {ArabicNLP 2025} (co-located with EMNLP 2025). This shared task addresses the paucity of high-quality Arabic medical QA resources by offering two complementary tracks: {MentalQA}, focusing on Arabic mental health Q\\&A (e.g., anxiety, depression, stigma reduction), and {MedArabiQ}, covering broader medical domains such as internal medicine, pediatrics, and clinical decision making. Each track comprises multiple subtasks, evaluation datasets, and standardized metrics, facilitating fair benchmarking. The task was structured to promote modeling under realistic, multilingual, and culturally nuanced healthcare contexts. We outline the dataset creation, task design and evaluation framework, participation statistics, baseline systems, and summarize the overall outcomes. We conclude with reflections on the performance trends observed and prospects for future iterations in Arabic health QA.\n我们介绍了{AraHealthQA 2025}，即{综合阿拉伯语健康问答共享任务}，该活动与{ArabicNLP 2025}（与EMNLP 2025联合举办）同期举行。此共享任务旨在解决高质量阿拉伯医学问答资源匮乏的问题，并提供了两个互补的赛道：专注于阿拉伯语心理健康问答（如焦虑、抑郁和减少污名化问题的）的{MentalQA}，以及涵盖内科、儿科和临床决策等更广泛医疗领域的{MedArabiQ}。每个赛道包括多个子任务、评估数据集和标准化指标，以促进公平基准测试。该任务的设计旨在推动在现实的多语言及文化语境下的建模工作。我们概述了数据集创建流程、任务设计与评估框架、参与统计数据、基线系统，并总结了总体结果。最后，我们对观察到的表现趋势进行了反思，并展望了未来阿拉伯语健康问答领域的前景。\nhttps://arxiv.org/abs/2508.20047\nhttps://arxiv.org/pdf/2508.20047.pdf\nThe growing adoption of foundation models calls for a paradigm shift from Data Science to Model Science. Unlike data-centric approaches, Model Science places the trained model at the core of analysis, aiming to interact, verify, explain, and control its behavior across diverse operational contexts. This paper introduces a conceptual framework for a new discipline called Model Science, along with the proposal for its four key pillars: Verification, which requires strict, context-aware evaluation protocols; Explanation, which is understood as various approaches to explore of internal model operations; Control, which integrates alignment techniques to steer model behavior; and Interface, which develops interactive and visual explanation tools to improve human calibration and decision-making. The proposed framework aims to guide the development of credible, safe, and human-aligned AI systems.\n随着基础模型的广泛采用，我们需要从数据科学转向模型科学这一范式转变。与以数据为中心的方法不同，模型科学将训练好的模型置于分析的核心位置，旨在通过多种操作上下文与其互动、验证、解释和控制其行为。本文介绍了一种新的学科——模型科学的概念框架，并提出了该学科的四大支柱：验证（需要严格且情境感知的评估协议）、解释（被视为探索内部模型操作的各种方法）、控制（整合对齐技术以引导模型行为）以及界面（开发交互式及可视化解释工具，以改善人类校准和决策）。所提出的框架旨在指导可信、安全和与人为本的人工智能系统的开发。\nhttps://arxiv.org/abs/2508.20040\nhttps://arxiv.org/pdf/2508.20040.pdf\nDespite advances in improving large language model(LLM) to refuse to answer malicious instructions, widely used LLMs remain vulnerable to jailbreak attacks where attackers generate instructions with distributions differing from safety alignment corpora. New attacks expose LLMs' inability to recognize unseen malicious instructions, highlighting a critical distributional mismatch between training data and real-world attacks that forces developers into reactive patching cycles. To tackle this challenge, we propose IMAGINE, a synthesis framework that leverages embedding space distribution analysis to generate jailbreak-like instructions. This approach effectively fills the distributional gap between authentic jailbreak patterns and safety alignment corpora. IMAGINE follows an iterative optimization process that dynamically evolves text generation distributions across iterations, thereby augmenting the coverage of safety alignment data distributions through synthesized data examples. Based on the safety-aligned corpus enhanced through IMAGINE, our framework demonstrates significant decreases in attack success rate on Qwen2.5, Llama3.1, and Llama3.2 without compromising their utility.\n尽管在改进大型语言模型（LLM）以拒绝回答恶意指令方面取得了进展，但广泛使用的LLM仍然容易受到 jailbreak 攻击。在这种攻击中，攻击者生成与安全对齐语料库分布不同的指令。新的攻击方法揭示了 LLM 无法识别未见过的恶意指令的能力不足，突显了训练数据和现实世界攻击之间的关键分布差异，迫使开发人员进入被动修补周期。为了解决这一挑战，我们提出了 IMAGINE 框架，这是一个利用嵌入空间分布分析来生成类似 jailbreak 的指令的合成框架。这种方法有效地填补了真实 jailbreak 模式与安全对齐语料库之间的分布差距。IMAGINE 采用迭代优化过程，在每次迭代中动态演化文本生成分布，通过合成数据示例增强安全对齐数据分布的覆盖范围。基于经过 IMAGINE 增强的安全对齐语料库，我们的框架在 Qwen2.5、Llama3.1 和 Llama3.2 上显著降低了攻击成功率，同时不影响其效用。\nhttps://arxiv.org/abs/2508.20038\nhttps://arxiv.org/pdf/2508.20038.pdf\nThe paper presents a visio-verbal teleimpedance interface for commanding 3D stiffness ellipsoids to the remote robot with a combination of the operator's gaze and verbal interaction. The gaze is detected by an eye-tracker, allowing the system to understand the context in terms of what the operator is currently looking at in the scene. Along with verbal interaction, a Visual Language Model (VLM) processes this information, enabling the operator to communicate their intended action or provide corrections. Based on these inputs, the interface can then generate appropriate stiffness matrices for different physical interaction actions. To validate the proposed visio-verbal teleimpedance interface, we conducted a series of experiments on a setup including a Force Dimension Sigma.7 haptic device to control the motion of the remote Kuka LBR iiwa robotic arm. The human operator's gaze is tracked by Tobii Pro Glasses 2, while human verbal commands are processed by a VLM using GPT-4o. The first experiment explored the optimal prompt configuration for the interface. The second and third experiments demonstrated different functionalities of the interface on a slide-in-the-groove task.\n这篇论文提出了一种结合操作者目光与口头互动的视听说觉遥阻抗界面，用于远程机器人控制3D刚度椭球。通过眼动追踪器检测目光，系统能够理解操作者当前在场景中关注的内容。配合口头交流，一个视觉语言模型（VLM）处理这些信息，使操作员可以传达其意图或进行修正。基于此输入，该界面能生成适用于不同物理交互动作的刚度矩阵。\n\n为了验证所提出的视听说觉遥阻抗界面的有效性，我们在一套包括力维度Sigma.7力反馈设备和远程Kuka LBR iiwa机器人臂的实验装置上进行了系列测试。操作者的目光由Tobii Pro Glasses 2追踪记录，而人类口头指令则通过使用GPT-4o处理的VLM进行解析。\n\n第一个实验探索了界面的最佳提示配置；第二和第三个实验展示了该界面对滑槽任务的不同功能应用。\nhttps://arxiv.org/abs/2508.20037\nhttps://arxiv.org/pdf/2508.20037.pdf\nThe ability to research and synthesize knowledge is central to human expertise and progress. An emerging class of systems promises these exciting capabilities through generative research synthesis, performing retrieval over the live web and synthesizing discovered sources into long-form, cited summaries. However, evaluating such systems remains an open challenge: existing question-answering benchmarks focus on short-form factual responses, while expert-curated datasets risk staleness and data contamination. Both fail to capture the complexity and evolving nature of real research synthesis tasks. In this work, we introduce DeepScholar-bench, a live benchmark and holistic, automated evaluation framework designed to evaluate generative research synthesis. DeepScholar-bench draws queries from recent, high-quality ArXiv papers and focuses on a real research synthesis task: generating the related work sections of a paper by retrieving, synthesizing, and citing prior research. Our evaluation framework holistically assesses performance across three key dimensions, knowledge synthesis, retrieval quality, and verifiability. We also develop DeepScholar-base, a reference pipeline implemented efficiently using the LOTUS API. Using the DeepScholar-bench framework, we perform a systematic evaluation of prior open-source systems, search AI's, OpenAI's DeepResearch, and DeepScholar-base. We find that DeepScholar-base establishes a strong baseline, attaining competitive or higher performance than each other method. We also find that DeepScholar-bench remains far from saturated, with no system exceeding a score of $19\\%$ across all metrics. These results underscore the difficulty of DeepScholar-bench, as well as its importance for progress towards AI systems capable of generative research synthesis. We make our code available at this https URL.\n研究和综合知识的能力是人类专业知识和进步的核心。一类新兴系统通过生成式研究综述提供了这些令人兴奋的功能，即在实时网络上检索信息，并将发现的来源综合成带有引用的长篇总结。然而，评估此类系统的性能仍然是一个开放性挑战：现有的问答基准主要关注短格式的事实回答，而专家策划的数据集则面临时效性和数据污染的风险。两者都无法捕捉到真实研究综述任务的复杂性和演变性质。\n\n在本工作中，我们介绍了DeepScholar-bench，这是一个实时基准和全面、自动化的评估框架，旨在评估生成式研究综合系统的性能。DeepScholar-bench 从最近高质量的ArXiv论文中提取查询，并专注于一个实际的研究综合任务：通过检索、综合并引用先前的研究成果来生成一篇论文的相关工作部分。我们的评估框架全方位地评估了三个关键维度的表现：知识综合能力，检索质量以及验证性。\n\n我们还开发了DeepScholar-base，这是一个使用LOTUS API高效实现的参考流程。借助于DeepScholar-bench 框架，我们对先前开源系统、搜索引擎AI、OpenAI的DeepResearch及DeepScholar-base进行了系统的评估。结果显示，DeepScholar-base 建立了一个强有力的基础，在知识综合能力方面达到了与其它方法相当或更高的性能表现。\n\n此外，我们发现 DeepScholar-bench 远未达到饱和点，所有系统在各个度量标准上的得分均未超过19%。这些结果强调了评估此类系统的难度及其对开发能够进行生成式研究综述的人工智能系统的重要性。我们的代码可以在[此处](https://example.com)获取（请注意替换实际的网址）。\nhttps://arxiv.org/abs/2508.20033\nhttps://arxiv.org/pdf/2508.20033.pdf\nBackdoor attacks are a significant threat to the performance and integrity of pre-trained language models. Although such models are routinely fine-tuned for downstream NLP tasks, recent work shows they remain vulnerable to backdoor attacks that survive vanilla fine-tuning. These attacks are difficult to defend because end users typically lack knowledge of the attack triggers. Such attacks consist of stealthy malicious triggers introduced through subtle syntactic or stylistic manipulations, which can bypass traditional detection and remain in the model, making post-hoc purification essential. In this study, we explore whether attention-head pruning can mitigate these threats without any knowledge of the trigger or access to a clean reference model. To this end, we design and implement six pruning-based strategies: (i) gradient-based pruning, (ii) layer-wise variance pruning, (iii) gradient-based pruning with structured L1/L2 sparsification, (iv) randomized ensemble pruning, (v) reinforcement-learning-guided pruning, and (vi) Bayesian uncertainty pruning. Each method iteratively removes the least informative heads while monitoring validation accuracy to avoid over-pruning. Experimental evaluation shows that gradient-based pruning performs best while defending the syntactic triggers, whereas reinforcement learning and Bayesian pruning better withstand stylistic attacks.\n后门攻击对预训练语言模型的性能和完整性构成了重大威胁。尽管这些模型通常会经过微调以适应下游自然语言处理任务，但近期研究表明它们仍然容易受到能够经受常规微调的后门攻击的影响。由于终端用户通常不了解此类攻击的触发机制，因此难以防御这些攻击。这类攻击包括通过细微的语法或风格调整引入的隐蔽恶意触发器，这使得传统的检测方法失效，并且让这些攻击得以在模型中存活下来，从而需要进行事后净化处理。\n\n在这项研究中，我们探讨了是否可以通过注意力头剪枝技术来减轻这些威胁，而无需了解触发机制或者访问干净的参考模型。为此，我们设计并实现了六种基于剪枝的方法：(i) 基于梯度的剪枝；(ii) 分层方差剪枝；(iii) 结构化L1/L2稀疏化的基于梯度的剪枝；(iv) 随机集成剪枝；(v) 强化学习引导的剪枝；以及(vi) 贝叶斯不确定性剪枝。每种方法都会迭代地移除最不重要的头部，并通过监测验证准确性来避免过度剪枝。\n\n实验评估表明，基于梯度的剪枝在防御语法触发器方面表现最佳，而强化学习和贝叶斯剪枝则更擅长抵御风格攻击。\nhttps://arxiv.org/abs/2508.20032\nhttps://arxiv.org/pdf/2508.20032.pdf\nWith the growing complexity of modern integrated circuits, hardware engineers are required to devote more effort to the full design-to-manufacturing workflow. This workflow involves numerous iterations, making it both labor-intensive and error-prone. Therefore, there is an urgent demand for more efficient Electronic Design Automation (EDA) solutions to accelerate hardware development. Recently, large language models (LLMs) have shown remarkable advancements in contextual comprehension, logical reasoning, and generative capabilities. Since hardware designs and intermediate scripts can be represented as text, integrating LLM for EDA offers a promising opportunity to simplify and even automate the entire workflow. Accordingly, this paper provides a comprehensive overview of incorporating LLMs into EDA, with emphasis on their capabilities, limitations, and future opportunities. Three case studies, along with their outlook, are introduced to demonstrate the capabilities of LLMs in hardware design, testing, and optimization. Finally, future directions and challenges are highlighted to further explore the potential of LLMs in shaping the next-generation EDA, providing valuable insights for researchers interested in leveraging advanced AI technologies for EDA.\n随着现代集成电路复杂性的增加，硬件工程师必须在完整的从设计到制造的工作流程上投入更多精力。这一工作流程涉及多次迭代，既耗时又容易出错。因此，迫切需要更高效的电子设计自动化（EDA）解决方案来加速硬件开发进程。最近，大型语言模型（LLMs）在上下文理解、逻辑推理和生成能力方面取得了显著进展。由于硬件设计及中间脚本可以表示为文本形式，将LLM集成到EDA中提供了一个简化甚至自动化工序的前景。因此，本文全面概述了将LLM整合进EDA的方法，并强调其功能、限制以及未来机遇。文章还介绍了三个案例研究及其展望，以展示LLMs在硬件设计、测试和优化中的能力。最后，论文指出未来的方向与挑战，进一步探索LLMs对下一代EDA的潜在影响，为有兴趣利用先进AI技术进行EDA的研究人员提供宝贵的见解。\nhttps://arxiv.org/abs/2508.20030\nhttps://arxiv.org/pdf/2508.20030.pdf\nIn dynamic environments, unfamiliar objects and distribution shifts are often encountered, which challenge the generalization abilities of the deployed trained models. This work addresses Incremental Test Time Adaptation of Vision Language Models, tackling scenarios where unseen classes and unseen domains continuously appear during testing. Unlike traditional Test Time Adaptation approaches, where the test stream comes only from a predefined set of classes, our framework allows models to adapt simultaneously to both covariate and label shifts, actively incorporating new classes as they emerge. Towards this goal, we establish a new benchmark for ITTA, integrating single image TTA methods for VLMs with active labeling techniques that query an oracle for samples potentially representing unseen classes during test time. We propose a segmentation assisted active labeling module, termed SegAssist, which is training free and repurposes the segmentation capabilities of VLMs to refine active sample selection, prioritizing samples likely to belong to unseen classes. Extensive experiments on several benchmark datasets demonstrate the potential of SegAssist to enhance the performance of VLMs in real world scenarios, where continuous adaptation to emerging data is essential. Project-page:this https URL\n在动态环境中，经常会遇到不熟悉的对象和分布变化，这对部署的训练模型的泛化能力提出了挑战。这项工作解决了视觉语言模型（Vision Language Models，VLM）的增量测试时间适应问题（Incremental Test Time Adaptation, ITTA），应对那些在测试过程中持续出现未见类别和未见领域的场景。与传统的测试时间适应方法不同，传统方法中测试流仅来自预定义的一组类别，我们的框架允许模型同时适应协变量变化和标签变化，并主动整合新出现的类别。\n\n为了实现这一目标，我们为ITTA建立了一个新的基准，将单张图像TTA方法应用于VLM，并结合了主动标记技术，在测试过程中通过查询专家以获取可能代表未见类别的样本。我们提出了一种名为SegAssist的分割辅助主动标签模块，该模块无需训练且重新利用了VLM的分割能力来优化积极样本的选择过程，优先考虑可能是新类别成员的样本。\n\n在多个基准数据集上进行的大量实验表明，SegAssist有潜力增强VLM在现实场景中的性能，在这些场景中，持续适应不断出现的数据至关重要。项目页面：[此链接](https://this-url)\nhttps://arxiv.org/abs/2508.20029\nhttps://arxiv.org/pdf/2508.20029.pdf\n\n            Recent Papers\n        \n\n            Contact me at:\n            \n\n\n\n\n\n\n京ICP备18036300号-1\n"
        },
        {
            "title": "Dialog",
            "content": "3 days ago ... Abstract (translated). 尽管大型语言模型（LLM）在临床对话系统中的应用日益增多，但现有的评估方法主要关注任务完成和 ...",
            "url": "https://paperreading.club/category?cate=Dialog",
            "source": "paperreading.club",
            "published_date": "2025-08-28",
            "search_engine": "Google",
            "query": "生成式AI在医疗诊断中的应用 最新研究 最近7天 generative AI medical diagnosis research paper",
            "description": "3 days ago ... Abstract (translated). 尽管大型语言模型（LLM）在临床对话系统中的应用日益增多，但现有的评估方法主要关注任务完成和 ...",
            "image": "",
            "category": "innovation_news",
            "search_source": "google",
            "full_content": "Fine-tuning multi-turn dialogue systems requires high-quality supervision but often suffers from degraded performance when exposed to low-quality data. Supervision errors in early turns can propagate across subsequent turns, undermining coherence and response quality. Existing methods typically address data quality via static prefiltering, which decouples quality control from training and fails to mitigate turn-level error propagation. In this context, we propose ReSURE (Regularizing Supervision UnREliability), an adaptive learning method that dynamically down-weights unreliable supervision without explicit filtering. ReSURE estimates per-turn loss distributions using Welford's online statistics and reweights sample losses on the fly accordingly. Experiments on both single-source and mixed-quality datasets show improved stability and response quality. Notably, ReSURE enjoys positive Spearman correlations (0.21 ~ 1.0 across multiple benchmarks) between response scores and number of samples regardless of data quality, which potentially paves the way for utilizing large-scale data effectively. Code is publicly available at this https URL.\n精细调整多轮对话系统需要高质量的监督，但在面对低质量数据时性能往往会下降。早期回合中的监督错误可能会传播到后续的回合中，从而破坏连贯性和响应质量。现有的方法通常通过静态预过滤来解决数据质量问题，这种方式将质量控制与训练过程脱钩，并且无法缓解各轮次之间的错误传播问题。\n\n在这一背景下，我们提出了ReSURE（Regularizing Supervision UnREliability），这是一种自适应学习方法，能够在不进行显式过滤的情况下动态降低不可靠监督的权重。ReSURE使用Welford在线统计估算每一轮的损失分布，并根据实际情况重新调整样本损失权重。实验结果表明，在单源和混合质量数据集上，这种方法能够提高系统的稳定性和响应质量。\n\n值得注意的是，无论数据的质量如何，ReSURE在多个基准测试中均呈现出积极的斯皮尔曼相关性（0.21至1.0），这表明该方法有可能有效利用大规模数据。代码可在[此链接](https://this_https_URL.com)公开获取。\nhttps://arxiv.org/abs/2508.19996\nhttps://arxiv.org/pdf/2508.19996.pdf\nDeveloping adaptable, extensible, and accurate task bots with minimal or zero human intervention is a significant challenge in dialog research. This thesis examines the obstacles and potential solutions for creating such bots, focusing on innovative techniques that enable bots to learn and adapt autonomously in constantly changing environments.\n开发能够在最少或无需人工干预的情况下灵活、可扩展且准确执行任务的聊天机器人，在对话研究中是一个重大挑战。本论文探讨了创建此类机器人的障碍及潜在解决方案，重点关注能够使机器人在不断变化的环境中自主学习和适应的创新技术。\nhttps://arxiv.org/abs/2508.19689\nhttps://arxiv.org/pdf/2508.19689.pdf\nThis report presents VibeVoice, a novel model designed to synthesize long-form speech with multiple speakers by employing next-token diffusion, which is a unified method for modeling continuous data by autoregressively generating latent vectors via diffusion. To enable this, we introduce a novel continuous speech tokenizer that, when compared to the popular Encodec model, improves data compression by 80 times while maintaining comparable performance. The tokenizer effectively preserves audio fidelity while significantly boosting computational efficiency for processing long sequences. Thus, VibeVoice can synthesize long-form speech for up to 90 minutes (in a 64K context window length) with a maximum of 4 speakers, capturing the authentic conversational ``vibe'' and surpassing open-source and proprietary dialogue models.\n该报告介绍了VibeVoice，这是一种创新模型，旨在通过使用下一个标记扩散（next-token diffusion）技术合成包含多个说话者的长篇语音。这种统一方法能够通过对连续数据进行自回归地生成潜在向量来建模连续数据。为了实现这一点，我们引入了一种新的连续语音标记器，与流行的Encodec模型相比，它在保持相当性能的同时提高了80倍的数据压缩比。该标记器有效地保留了音频保真度，并且显著提升了处理长序列的计算效率。因此，VibeVoice能够合成长达90分钟（在一个64K上下文窗口长度内）包含最多4个说话者的长篇语音，捕捉到真实的对话氛围，并超越开源和专有的对话模型。\nhttps://arxiv.org/abs/2508.19205\nhttps://arxiv.org/pdf/2508.19205.pdf\nDespite the growing use of large language models (LLMs) in clinical dialogue systems, existing evaluations focus on task completion or fluency, offering little insight into the behavioral and risk management requirements essential for safety-critical systems. This paper presents MATRIX (Multi-Agent simulaTion fRamework for safe Interactions and conteXtual clinical conversational evaluation), a structured, extensible framework for safety-oriented evaluation of clinical dialogue agents. MATRIX integrates three components: (1) a safety-aligned taxonomy of clinical scenarios, expected system behaviors and failure modes derived through structured safety engineering methods; (2) BehvJudge, an LLM-based evaluator for detecting safety-relevant dialogue failures, validated against expert clinician annotations; and (3) PatBot, a simulated patient agent capable of producing diverse, scenario-conditioned responses, evaluated for realism and behavioral fidelity with human factors expertise, and a patient-preference study. Across three experiments, we show that MATRIX enables systematic, scalable safety evaluation. BehvJudge with Gemini 2.5-Pro achieves expert-level hazard detection (F1 0.96, sensitivity 0.999), outperforming clinicians in a blinded assessment of 240 dialogues. We also conducted one of the first realism analyses of LLM-based patient simulation, showing that PatBot reliably simulates realistic patient behavior in quantitative and qualitative evaluations. Using MATRIX, we demonstrate its effectiveness in benchmarking five LLM agents across 2,100 simulated dialogues spanning 14 hazard scenarios and 10 clinical domains. MATRIX is the first framework to unify structured safety engineering with scalable, validated conversational AI evaluation, enabling regulator-aligned safety auditing. We release all evaluation tools, prompts, structured scenarios, and datasets.\n尽管大型语言模型（LLM）在临床对话系统中的应用日益增多，但现有的评估方法主要关注任务完成和流畅度，却很少涉及保障安全关键系统所需的行文规范及风险管理要求。本文提出了MATRIX框架（用于安全互动和情境化临床会话评估的多智能体模拟框架），这是一个结构化且可扩展的安全导向型临床对话代理评估框架。MATRIX集成了三个组成部分：(1) 通过有组织的安全工程方法开发出的一套与安全性一致的临床场景分类、预期系统行为及失效模式；(2) BehvJudge，一种基于LLM的用于检测安全相关会话故障的评估器，并通过专家医生注释进行了验证；以及 (3) PatBot，一个能够生成多样化且情境适应性响应的模拟病人代理，其真实性和行为保真度经由人类因素专业知识和患者偏好研究进行评估。在三项实验中，我们展示了MATRIX如何实现系统化、可扩展的安全评估。BehvJudge结合了Gemini 2.5-Pro，在隐盲测试的240个对话中实现了专家级别的危险检测（F1值为0.96，灵敏度为0.999），超越了临床医生的表现。我们还进行了LLM基础患者模拟的第一个真实感分析之一，证明PatBot在定量和定性评估中能够可靠地模拟现实中的病人行为。利用MATRIX，我们在2,100个模拟对话（涵盖14种危险情景和10个临床领域）的基准测试中展示了五个LLM代理的效果。MATRIX是首个将结构化安全工程与可扩展且验证过的会话AI评估统一起来的框架，它能够支持监管合规的安全审计。我们发布了所有评估工具、提示、结构化场景以及数据集。\nhttps://arxiv.org/abs/2508.19163\nhttps://arxiv.org/pdf/2508.19163.pdf\nIn a doctor-patient dialogue, the primary objective of physicians is to diagnose patients and propose a treatment plan. Medical doctors guide these conversations through targeted questioning to efficiently gather the information required to provide the best possible outcomes for patients. To the best of our knowledge, this is the first work that studies physician intent trajectories in doctor-patient dialogues. We use the `Ambient Clinical Intelligence Benchmark' (Aci-bench) dataset for our study. We collaborate with medical professionals to develop a fine-grained taxonomy of physician intents based on the SOAP framework (Subjective, Objective, Assessment, and Plan). We then conduct a large-scale annotation effort to label over 5000 doctor-patient turns with the help of a large number of medical experts recruited using Prolific, a popular crowd-sourcing platform. This large labeled dataset is an important resource contribution that we use for benchmarking the state-of-the-art generative and encoder models for medical intent classification tasks. Our findings show that our models understand the general structure of medical dialogues with high accuracy, but often fail to identify transitions between SOAP categories. We also report for the first time common trajectories in medical dialogue structures that provide valuable insights for designing `differential diagnosis' systems. Finally, we extensively study the impact of intent filtering for medical dialogue summarization and observe a significant boost in performance. We make the codes and data, including annotation guidelines, publicly available at this https URL.\n在医生与患者之间的对话中，医师的主要目标是诊断病人并提出治疗方案。医学医生通过有针对性的提问来高效地收集提供最佳疗效所需的信息。据我们所知，这是首次研究医生在医患对话中的意图轨迹的工作。我们在研究中使用了`环境临床智能基准'（Aci-bench）数据集。\n\n为了进行这项研究，我们与医疗专业人员合作，基于SOAP框架（主观、客观、评估和计划）开发了一个细粒度的医师意图分类法。随后，我们开展了一次大规模的标注工作，在这个过程中招募了大量医学专家来帮助对超过5000个医患对话回合进行标记。这一大规模带标签的数据集是我们的重要资源贡献，用于为最先进的生成式和编码模型在医疗意图分类任务上的基准测试提供支持。\n\n我们的研究结果表明，这些模型能够以高精度理解医学对话的一般结构，但在识别SOAP类别之间的转换时往往表现不佳。此外，我们首次报告了医学对话结构中的常见轨迹，这为设计`鉴别诊断'系统提供了宝贵的见解。最后，我们详细探讨了意图过滤对医疗对话总结的影响，并观察到性能有了显著提升。\n\n我们将代码和数据（包括标注指南）公开分享在以下网址：[此URL]。\nhttps://arxiv.org/abs/2508.19077\nhttps://arxiv.org/pdf/2508.19077.pdf\nRecently, interactive digital human video generation has attracted widespread attention and achieved remarkable progress. However, building such a practical system that can interact with diverse input signals in real time remains challenging to existing methods, which often struggle with high latency, heavy computational cost, and limited controllability. In this work, we introduce an autoregressive video generation framework that enables interactive multimodal control and low-latency extrapolation in a streaming manner. With minimal modifications to a standard large language model (LLM), our framework accepts multimodal condition encodings including audio, pose, and text, and outputs spatially and semantically coherent representations to guide the denoising process of a diffusion head. To support this, we construct a large-scale dialogue dataset of approximately 20,000 hours from multiple sources, providing rich conversational scenarios for training. We further introduce a deep compression autoencoder with up to 64$\\times$ reduction ratio, which effectively alleviates the long-horizon inference burden of the autoregressive model. Extensive experiments on duplex conversation, multilingual human synthesis, and interactive world model highlight the advantages of our approach in low latency, high efficiency, and fine-grained multimodal controllability.\n最近，交互式数字人类视频生成吸引了广泛关注并取得了显著进展。然而，建立一个能够实时处理多种输入信号的实用系统仍然是现有方法面临的一大挑战，这些方法往往在延迟、计算成本和可控性方面存在局限性。为此，我们在本工作中引入了一种自回归视频生成框架，该框架能够在流式传输中实现交互式的多模态控制及低延迟外推。通过仅对标准大型语言模型（LLM）进行少量修改，我们的框架可以接受包括音频、姿态和文本在内的多种条件编码，并输出空间上和语义上连贯的表示来引导扩散头的去噪过程。为了支持这一点，我们构建了一个来自多个来源的大规模对话数据集，包含大约20,000小时的数据，为训练提供了丰富的会话场景。\n\n此外，我们引入了一种深度压缩自动编码器，其最大压缩比可达64倍，有效减轻了自回归模型的长时推断负担。在双工对话、多语言人类合成和交互式世界模型上的广泛实验展示了我们的方法在低延迟、高效率以及精细粒度的多模态控制方面的优势。\nhttps://arxiv.org/abs/2508.19320\nhttps://arxiv.org/pdf/2508.19320.pdf\nStandard single-turn, static benchmarks fall short in evaluating the nuanced capabilities of Large Language Models (LLMs) on complex tasks such as software engineering. In this work, we propose a novel interactive evaluation framework that assesses LLMs on multi-requirement programming tasks through structured, feedback-driven dialogue. Each task is modeled as a requirement dependency graph, and an ``interviewer'' LLM, aware of the ground-truth solution, provides minimal, targeted hints to an ``interviewee'' model to help correct errors and fulfill target constraints. This dynamic protocol enables fine-grained diagnostic insights into model behavior, uncovering strengths and systematic weaknesses that static benchmarks fail to measure. We build on DevAI, a benchmark of 55 curated programming tasks, by adding ground-truth solutions and evaluating the relevance and utility of interviewer hints through expert annotation. Our results highlight the importance of dynamic evaluation in advancing the development of collaborative code-generating agents.\n标准的单轮静态基准测试在评估大型语言模型（LLM）在软件工程等复杂任务上的细微能力方面存在不足。在这项工作中，我们提出了一种新颖的交互式评估框架，该框架通过结构化、反馈驱动的对话来评估LLM在多要求编程任务中的表现。每个任务被建模为一个需求依赖图，一个“面试官”模型（了解地面真实解决方案）提供最小且有针对性的提示给“应聘者”模型，以帮助纠正错误并满足目标约束。这种动态协议能够对模型行为进行精细诊断，揭示出静态基准测试无法测量的优势和系统性弱点。我们在DevAI上建立了一个包含55个精心策划的编程任务的基准，并添加了地面真实解决方案，通过专家注释评估面试官提示的相关性和实用性。我们的研究结果强调了动态评估在推进协作代码生成代理开发中的重要性。\nhttps://arxiv.org/abs/2508.18905\nhttps://arxiv.org/pdf/2508.18905.pdf\nConversational analytics has been on the forefront of transformation driven by the advances in Speech and Natural Language Processing techniques. Rapid adoption of Large Language Models (LLMs) in the analytics field has taken the problems that can be automated to a new level of complexity and scale. In this paper, we introduce Theme Detection as a critical task in conversational analytics, aimed at automatically identifying and categorizing topics within conversations. This process can significantly reduce the manual effort involved in analyzing expansive dialogs, particularly in domains like customer support or sales. Unlike traditional dialog intent detection, which often relies on a fixed set of intents for downstream system logic, themes are intended as a direct, user-facing summary of the conversation's core inquiry. This distinction allows for greater flexibility in theme surface forms and user-specific customizations. We pose Controllable Conversational Theme Detection problem as a public competition track at Dialog System Technology Challenge (DSTC) 12 -- it is framed as joint clustering and theme labeling of dialog utterances, with the distinctive aspect being controllability of the resulting theme clusters' granularity achieved via the provided user preference data. We give an overview of the problem, the associated dataset and the evaluation metrics, both automatic and human. Finally, we discuss the participant teams' submissions and provide insights from those. The track materials (data and code) are openly available in the GitHub repository.\n对话分析在由语音和自然语言处理技术进步推动的转型中一直处于前沿地位。大型语言模型（LLMs）在数据分析领域的迅速采用，将可以自动化的复杂性和规模问题提升到了新的高度。在这篇论文中，我们介绍了主题检测作为对话分析中的关键任务之一，旨在自动识别并分类对话中的话题。这一过程能够显著减少分析大规模对话所需的繁琐人工劳动，特别是在客户支持或销售等特定领域。\n\n与传统的对话意图识别不同，后者通常依赖于为下游系统逻辑设置的固定意图集，而主题则被设计为直接面向用户的对话核心查询摘要。这种区别允许主题表现在形式上更具灵活性，并且可以根据用户的具体需求进行定制化处理。我们以可控性对话主题检测问题的形式在第12届对话系统技术挑战赛（DSTC）中设立了一个公开竞赛赛道——这个问题框架要求对对话中的言语单元进行联合聚类和标签分类，其独特之处在于可以通过提供的用户偏好数据来控制最终生成的主题簇的粒度。\n\n我们概述了该问题、相关数据集以及自动与人工评判的标准。最后，我们讨论了参赛团队提交的作品，并提供了他们的见解。此赛道的相关材料（包括数据和代码）均已在GitHub存储库中公开提供。\nhttps://arxiv.org/abs/2508.18783\nhttps://arxiv.org/pdf/2508.18783.pdf\nEmotion Cause Triplet Extraction in Multimodal Conversations (MECTEC) has recently gained significant attention in social media analysis, aiming to extract emotion utterances, cause utterances, and emotion categories simultaneously. However, the scarcity of related datasets, with only one published dataset featuring highly uniform dialogue scenarios, hinders model development in this field. To address this, we introduce MECAD, the first multimodal, multi-scenario MECTEC dataset, comprising 989 conversations from 56 TV series spanning a wide range of dialogue contexts. In addition, existing MECTEC methods fail to explicitly model emotional and causal contexts and neglect the fusion of semantic information at different levels, leading to performance degradation. In this paper, we propose M3HG, a novel model that explicitly captures emotional and causal contexts and effectively fuses contextual information at both inter- and intra-utterance levels via a multimodal heterogeneous graph. Extensive experiments demonstrate the effectiveness of M3HG compared with existing state-of-the-art methods. The codes and dataset are available at this https URL.\n最近，在社交媒体分析领域中，情感原因三元组抽取（MECTEC）在多模态对话中的应用受到了广泛关注。其目的是同时提取情绪表达、原因表达和情绪类别。然而，由于缺乏相关数据集，特别是仅有的一个公开的数据集只包含高度统一的对话场景，这阻碍了该领域的模型开发进展。为了解决这个问题，我们引入了MECAD——第一个多模态、多情景的MECTEC数据集，其中包括来自56部电视剧的989个对话，涵盖了广泛不同的对话背景。\n\n此外，现有的MECTEC方法未能明确地建模情感和因果情境，并且忽视了不同层次语义信息融合的重要性，导致性能下降。为此，在这篇论文中，我们提出了一种新的模型——M3HG（Multimodal Multi-level Contextual Graph），该模型能够显式捕捉情绪和因果背景，并通过多模态异构图有效地在对话的内部和跨句子层面融合上下文信息。\n\n广泛的实验表明，与现有的最先进的方法相比，M3HG模型更为有效。代码和数据集可在提供的链接处获取。\nhttps://arxiv.org/abs/2508.18740\nhttps://arxiv.org/pdf/2508.18740.pdf\nThe rapid adoption of large language models (LLMs) in customer service introduces new risks, as malicious actors can exploit them to conduct large-scale user impersonation through machine-generated text (MGT). Current MGT detection methods often struggle in online conversational settings, reducing the reliability and interpretability essential for trustworthy AI deployment. In customer service scenarios where operators are typically non-expert users, explanation become crucial for trustworthy MGT detection. In this paper, we propose EMMM, an explanation-then-detection framework that balances latency, accuracy, and non-expert-oriented interpretability. Experimental results demonstrate that EMMM provides explanations accessible to non-expert users, with 70\\% of human evaluators preferring its outputs, while achieving competitive accuracy compared to state-of-the-art models and maintaining low latency, generating outputs within 1 second. Our code and dataset are open-sourced at this https URL.\n大型语言模型（LLMs）在客户服务中的快速采用引入了新的风险，因为恶意行为者可以通过生成机器生成的文本（MGT）来进行大规模用户模仿。当前的MGT检测方法在线对话环境中往往效果不佳，这降低了可靠性和可解释性对于值得信赖的人工智能部署的重要性。在客服场景中，操作员通常是非专家用户，在这种情况下，解释变得对可信的MGT检测至关重要。\n\n本文提出了一种名为EMMM的框架，这是一种解释然后检测框架，它在延迟、准确性以及针对非专业人士的可解释性之间取得了平衡。实验结果显示，EMMM提供的解释对于非专家用户来说是易于理解的，并且有70%的人类评估者更喜欢其输出结果；同时，在与最先进的模型相比时，该方法也达到了竞争性的准确度并保持了低延迟，生成输出的时间不超过1秒。\n\n我们的代码和数据集已在以下网址开源：[此链接]（请将\"this https URL\"替换为实际的URL）。\nhttps://arxiv.org/abs/2508.18715\nhttps://arxiv.org/pdf/2508.18715.pdf\nWith the development of speech large language models (speech LLMs), users can now interact directly with assistants via speech. However, most existing models simply convert the response content into speech without fully understanding the rich emotional and paralinguistic cues embedded in the user's query. In many cases, the same sentence can have different meanings depending on the emotional expression. Furthermore, emotional understanding is essential for improving user experience in human-machine interaction. Currently, most speech LLMs with empathetic capabilities are trained on massive datasets. This approach requires vast amounts of data and significant computational resources. Therefore, a key challenge lies in how to develop a speech LLM capable of generating empathetic responses with limited data and without the need for large-scale training. To address this challenge, we propose Emotion Omni, a novel model architecture designed to understand the emotional content of user speech input and generate empathetic speech responses. Additionally, we developed a data generation pipeline based on an open-source TTS framework to construct a 200k emotional dialogue dataset, which supports the construction of an empathetic speech assistant. The demos are available at this https URL\n随着语音大型语言模型（语音LLM）的发展，用户现在可以通过语音直接与助手交互。然而，大多数现有的模型只是将回答内容转换为语音，而未能充分理解用户查询中包含的丰富的情感和副语言线索。在许多情况下，同一个句子由于情感表达的不同可能会有不同的含义。此外，在人机互动中，情感理解对于改善用户体验至关重要。目前，大多数具有同理心能力的语音LLM都是通过大规模数据集进行训练的。这种方法需要大量的数据和计算资源。因此，一个关键挑战在于如何开发一种能够在有限数据下生成富有同情力的响应且无需大规模训练的语音LLM。\n\n为了解决这一挑战，我们提出了Emotion Omni，这是一种新的模型架构，旨在理解用户语音输入中的情感内容并生成具有同理心的语音回应。此外，我们还开发了一种基于开源TTS框架的数据生成管道，用于构建一个包含20万条情感对话的数据库，支持建设富有同情力的语音助手。演示可以在提供的链接中查看：[此URL]\nhttps://arxiv.org/abs/2508.18655\nhttps://arxiv.org/pdf/2508.18655.pdf\nOne of the enduring challenges in education is how to empower students to take ownership of their learning by setting meaningful goals, tracking their progress, and adapting their strategies when faced with setbacks. Research has shown that this form of leaner-centered learning is best cultivated through structured, supportive environments that promote guided practice, scaffolded inquiry, and collaborative dialogue. In response, educational efforts have increasingly embraced artificial-intelligence (AI)-powered digital learning environments, ranging from educational apps and virtual labs to serious games. Recent advances in large language models (LLMs) and neuro-symbolic systems, meanwhile, offer a transformative opportunity to reimagine how support is delivered in digital learning environments. LLMs are enabling socially interactive learning experiences and scalable, cross-domain learning support that can adapt instructional strategies across varied subjects and contexts. In parallel, neuro-symbolic AI provides new avenues for designing these agents that are not only adaptive but also scalable across domains. Based on these remarks, this paper presents a multi-agent, neuro-symbolic framework designed to resolve the aforementioned challenges. The framework assigns distinct pedagogical roles to specialized agents: an RL-based 'tutor' agent provides authoritative, non-verbal scaffolding, while a proactive, LLM-powered 'peer' agent facilitates the social dimensions of learning. While prior work has explored such agents in isolation, our framework's novelty lies in unifying them through a central educational ontology. Through case studies in both college-level and middle school settings, we demonstrate the framework's adaptability across domains. We conclude by outlining key insights and future directions for advancing AI-driven learning environments.\n教育领域长期面临的挑战之一是如何让学生通过设定有意义的目标、追踪进度并克服挫折时调整策略，来掌握学习的主动权。研究表明，这种以学生为中心的学习方式最好在结构化且支持性强的环境中培养，这些环境鼓励引导式实践、逐步探索和协作对话。为此，教育界越来越倾向于采用由人工智能（AI）驱动的数字化学习环境，包括教育应用程序、虚拟实验室以及严肃游戏等。\n\n与此同时，大型语言模型（LLMs）和神经符号系统方面的最新进展为重新构想数字学习环境中支持的交付方式提供了变革性的机会。LLM使得社会互动的学习体验成为可能，并且可以提供跨学科领域的可扩展性、跨领域适应的教学策略。平行地，神经符号AI开辟了新的途径来设计这些代理，使它们不仅具有适应能力，而且还能跨不同领域进行规模化应用。\n\n基于上述观点，本文提出了一种多代理的神经符号框架，旨在解决前述挑战。该框架为专业代理分配不同的教学角色：一个基于强化学习（RL）的“辅导”代理提供权威且非语言性的支架支持；而另一个主动、由LLM驱动的“同伴”代理则促进学习的社会维度。尽管之前的工作已经分别探讨了这些代理，但本框架的独特之处在于通过中央教育概念模型将它们统一起来。\n\n本文通过大学和中学的学习情境案例研究，展示了该框架在不同领域的适应性，并最终概述了推动AI驱动学习环境发展的关键洞见和未来方向。\nhttps://arxiv.org/abs/2508.18406\nhttps://arxiv.org/pdf/2508.18406.pdf\nSynthetic transcript generation is critical in contact center domains, where privacy and data scarcity limit model training and evaluation. Unlike prior synthetic dialogue generation work on open-domain or medical dialogues, contact center conversations are goal-oriented, role-asymmetric, and behaviorally complex, featuring disfluencies, ASR noise, and compliance-driven agent actions. In deployments where transcripts are unavailable, standard pipelines still yield derived call attributes such as Intent Summaries, Topic Flow, and QA Evaluation Forms. We leverage these as supervision signals to guide generation. To assess the quality of such outputs, we introduce a diagnostic framework of 18 linguistically and behaviorally grounded metrics for comparing real and synthetic transcripts. We benchmark four language-agnostic generation strategies, from simple prompting to characteristic-aware multi-stage approaches, alongside reference-free baselines. Results reveal persistent challenges: no method excels across all traits, with notable deficits in disfluency, sentiment, and behavioral realism. Our diagnostic tool exposes these gaps, enabling fine-grained evaluation and stress testing of synthetic dialogue across languages.\n合成对话生成在客服中心领域至关重要，因为隐私和数据稀缺限制了模型的训练与评估。不同于以往针对开放域或医疗对话的合成对话生成工作，客服对话具有目标导向性、角色不对称性和行为复杂性，包括不流畅、自动语音识别（ASR）噪音以及受合规驱动的操作。在缺乏真实对话记录的情况下部署时，标准流程仍能生成诸如意图摘要、话题流和问答评估表等衍生通话属性。我们利用这些作为监督信号来引导生成过程。\n\n为了评估此类输出的质量，我们引入了一个包含18项语言学及行为依据的诊断框架，用于比较真实的与合成的对话记录。我们对四种无语言限制的生成策略进行了基准测试，从简单的提示法到特征感知的多阶段方法，并将其与不依赖于参考的标准进行对比。结果揭示了持续存在的挑战：没有一种方法能够在所有特性上都表现出色，尤其在不流畅、情感和行为现实感方面存在明显的不足。\n\n我们的诊断工具揭露了这些差距，使我们能够对跨语言的合成对话进行细致入微的评估与压力测试。\nhttps://arxiv.org/abs/2508.18210\nhttps://arxiv.org/pdf/2508.18210.pdf\nRetrieval-augmented generation (RAG) has become a widely recognized paradigm to combine parametric memory with non-parametric memories. An RAG model consists of two serial connecting components (retriever and generator). A major challenge in end-to-end optimization of the RAG model is that marginalization over relevant passages (modeled as discrete latent variables) from a knowledge base is required. Traditional top-K marginalization and variational RAG (VRAG) suffer from biased or high-variance gradient estimates. In this paper, we propose and develop joint stochastic approximation (JSA) based end-to-end training of RAG, which is referred to as JSA-RAG. The JSA algorithm is a stochastic extension of the EM (expectation-maximization) algorithm and is particularly powerful in estimating discrete latent variable models. Extensive experiments are conducted on five datasets for two tasks (open-domain question answering, knowledge-grounded dialogs) and show that JSA-RAG significantly outperforms both vanilla RAG and VRAG. Further analysis shows the efficacy of JSA-RAG from the perspectives of generation, retrieval, and low-variance gradient estimate.\n基于检索的生成（RAG）已成为结合参数记忆与非参数记忆的一种广泛认可的方法论。一个RAG模型由两个串联组件组成，即检索器和生成器。在端到端优化RAG模型时的主要挑战在于需要对从知识库中相关段落进行边缘化处理（这些段落被建模为离散的潜在变量）。传统的top-K边际化以及变分RAG（VRAG）方法会遭受偏差或高方差梯度估计的问题。在这篇论文中，我们提出了基于联合随机逼近（JSA）的端到端训练RAG模型的方法，并将其称为JSA-RAG。该JSA算法是EM（期望-最大化）算法的一种随机扩展版本，在估计离散潜在变量模型时特别有效。我们在五个数据集上针对两个任务（开放领域问题回答、知识基础对话）进行了广泛的实验，结果表明JSA-RAG显著优于原始RAG和VRAG方法。进一步的分析从生成、检索以及低方差梯度估计的角度证明了JSA-RAG的有效性。\nhttps://arxiv.org/abs/2508.18168\nhttps://arxiv.org/pdf/2508.18168.pdf\nLarge language models (LLMs) are shown to be vulnerable to jailbreaking attacks where adversarial prompts are designed to elicit harmful responses. While existing defenses effectively mitigate single-turn attacks by detecting and filtering unsafe inputs, they fail against multi-turn jailbreaks that exploit contextual drift over multiple interactions, gradually leading LLMs away from safe behavior. To address this challenge, we propose a safety steering framework grounded in safe control theory, ensuring invariant safety in multi-turn dialogues. Our approach models the dialogue with LLMs using state-space representations and introduces a novel neural barrier function (NBF) to detect and filter harmful queries emerging from evolving contexts proactively. Our method achieves invariant safety at each turn of dialogue by learning a safety predictor that accounts for adversarial queries, preventing potential context drift toward jailbreaks. Extensive experiments under multiple LLMs show that our NBF-based safety steering outperforms safety alignment, prompt-based steering and lightweight LLM guardrails baselines, offering stronger defenses against multi-turn jailbreaks while maintaining a better trade-off among safety, helpfulness and over-refusal. Check out the website here this https URL . Our code is available on this https URL .\n大型语言模型（LLM）易受所谓的“越狱”攻击，即对手通过设计特定提示来诱使模型产生有害响应。尽管现有的防御措施能够有效识别并过滤出单轮次的不安全输入从而缓解单一回合的攻击，但对于多轮次“越狱”，这些方法却无能为力。这类多轮次“越狱”利用了在多次互动中逐渐累积的情境漂移（contextual drift），逐步引导LLM偏离正常行为。\n\n为了应对这一挑战，我们提出了一种基于安全控制理论的安全导航框架，确保在整个对话过程中持续不变的安全性。我们的方法通过状态空间表示来建模与LLM的对话，并引入一种新颖的神经屏障函数（Neural Barrier Function, NBF），用于主动检测和过滤在不断变化的情境中产生的有害查询。\n\n我们提出的方法通过学习一个能够考虑对抗性查询的安全预测器，在每一回合的对话过程中实现不变安全性，从而防止潜在的情境漂移导向“越狱”。多款大型语言模型上的广泛实验表明，基于NBF的安全导航方法优于安全对齐、提示引导和轻量级LLM防护等基线方法，能够在提供更强大防御的同时，保持更好的安全性、有用性和避免过度拒绝之间的平衡。\n\n欲了解更多详情，请访问该网站：[链接]。我们的代码可在以下网址获取：[链接]。\nhttps://arxiv.org/abs/2503.00187\nhttps://arxiv.org/pdf/2503.00187.pdf\nThe rapid evolution of e-commerce has exposed the limitations of traditional product retrieval systems in managing complex, multi-turn user interactions. Recent advances in multimodal generative retrieval -- particularly those leveraging multimodal large language models (MLLMs) as retrievers -- have shown promise. However, most existing methods are tailored to single-turn scenarios and struggle to model the evolving intent and iterative nature of multi-turn dialogues when applied naively. Concurrently, test-time scaling has emerged as a powerful paradigm for improving large language model (LLM) performance through iterative inference-time refinement. Yet, its effectiveness typically relies on two conditions: (1) a well-defined problem space (e.g., mathematical reasoning), and (2) the model's ability to self-correct -- conditions that are rarely met in conversational product search. In this setting, user queries are often ambiguous and evolving, and MLLMs alone have difficulty grounding responses in a fixed product corpus. Motivated by these challenges, we propose a novel framework that introduces test-time scaling into conversational multimodal product retrieval. Our approach builds on a generative retriever, further augmented with a test-time reranking (TTR) mechanism that improves retrieval accuracy and better aligns results with evolving user intent throughout the dialogue. Experiments across multiple benchmarks show consistent improvements, with average gains of 14.5 points in MRR and 10.6 points in nDCG@1.\n电子商务的快速发展已经揭示了传统产品检索系统在处理复杂、多轮次用户交互方面的局限性。最近，利用多模态大型语言模型（MLLMs）作为检索器的多模态生成式检索技术展现出了潜力。然而，大多数现有方法都是为单一轮次场景设计的，在没有适当调整的情况下很难模拟多轮对话中不断演变的意图和迭代性质。同时，测试时扩展作为一种强大的范例出现，通过在推理时间进行迭代改进来提升大型语言模型（LLM）的表现力。不过，这种方法的有效性通常依赖于两个条件：一是有明确定义的问题空间（例如数学推理），二是模型能够自我纠正的能力——而在对话式产品搜索中这两个条件很少能同时满足。在这种情况下，用户的查询通常是模棱两可且不断变化的，并且仅靠MLLMs很难将回应锚定在固定的产品库中。\n\n鉴于这些挑战，我们提出了一种新的框架，在多模式会话型产品检索中引入了测试时扩展机制。我们的方法建立在一个生成式检索器之上，并进一步通过加入测试时重新排序（TTR）机制来增强其功能，从而提高检索精度并使结果更好地与对话过程中不断变化的用户意图相匹配。在多个基准上的实验表明，该框架可以持续带来改进，在MRR指标上平均提高了14.5个点，在nDCG@1指标上平均提高了10.6个点。\nhttps://arxiv.org/abs/2508.18132\nhttps://arxiv.org/pdf/2508.18132.pdf\nGenerative models have advanced rapidly, enabling impressive talking head generation that brings AI to life. However, most existing methods focus solely on one-way portrait animation. Even the few that support bidirectional conversational interactions lack precise emotion-adaptive capabilities, significantly limiting their practical applicability. In this paper, we propose EAI-Avatar, a novel emotion-aware talking head generation framework for dyadic interactions. Leveraging the dialogue generation capability of large language models (LLMs, e.g., GPT-4), our method produces temporally consistent virtual avatars with rich emotional variations that seamlessly transition between speaking and listening states. Specifically, we design a Transformer-based head mask generator that learns temporally consistent motion features in a latent mask space, capable of generating arbitrary-length, temporally consistent mask sequences to constrain head motions. Furthermore, we introduce an interactive talking tree structure to represent dialogue state transitions, where each tree node contains information such as child/parent/sibling nodes and the current character's emotional state. By performing reverse-level traversal, we extract rich historical emotional cues from the current node to guide expression synthesis. Extensive experiments demonstrate the superior performance and effectiveness of our method.\n生成模型的进步非常迅速，使得具有高度逼真感的“说话头像”（talking head）生成成为可能，进而让人工智能技术以一种生动的方式展现出来。然而，大多数现有方法主要关注单向肖像动画制作，而那些支持双向对话互动的方法则缺乏精准的情感适应能力，这大大限制了它们的实际应用价值。在本文中，我们提出了EAI-Avatar——一个面向双人互动的情绪感知“说话头像”生成框架。通过利用大型语言模型（LLM，例如GPT-4）的对话生成功能，我们的方法可以创建具有丰富情感变化、并能够无缝转换于讲话和倾听状态之间的虚拟角色。\n\n具体来说，我们设计了一个基于Transformer的头部掩码生成器，在潜在掩码空间中学习时间一致性的运动特征，并且该生成器能够生成任意长度的时间一致性掩码序列来限制头部动作。此外，为了表示对话状态转移，我们引入了互动说话树结构，其中每个节点包含子节点、父节点和兄弟节点的信息以及当前角色的情绪状态。通过反向层级遍历，我们可以从当前节点提取丰富的历史情感线索以指导表情合成。\n\n广泛的实验展示了我们的方法在性能和有效性方面的优越性。\nhttps://arxiv.org/abs/2508.18337\nhttps://arxiv.org/pdf/2508.18337.pdf\nText-to-image (T2I) generation has greatly enhanced creative expression, yet achieving preference-aligned generation in a real-time and training-free manner remains challenging. Previous methods often rely on static, pre-collected preferences or fine-tuning, limiting adaptability to evolving and nuanced user intents. In this paper, we highlight the need for instant preference-aligned T2I generation and propose a training-free framework grounded in multimodal large language model (MLLM) priors. Our framework decouples the task into two components: preference understanding and preference-guided generation. For preference understanding, we leverage MLLMs to automatically extract global preference signals from a reference image and enrich a given prompt using structured instruction design. Our approach supports broader and more fine-grained coverage of user preferences than existing methods. For preference-guided generation, we integrate global keyword-based control and local region-aware cross-attention modulation to steer the diffusion model without additional training, enabling precise alignment across both global attributes and local elements. The entire framework supports multi-round interactive refinement, facilitating real-time and context-aware image generation. Extensive experiments on the Viper dataset and our collected benchmark demonstrate that our method outperforms prior approaches in both quantitative metrics and human evaluations, and opens up new possibilities for dialog-based generation and MLLM-diffusion integration.\n文本到图像（T2I）生成技术极大地促进了创意表达，然而，在不进行训练的情况下实现实时且符合用户偏好的图像生成仍然是一项挑战。以往的方法通常依赖于静态收集的偏好或微调模型参数，这限制了它们适应不断变化和细微差别的用户意图的能力。在本文中，我们强调了即时、与偏好一致的T2I生成的需求，并提出了一种基于多模态大型语言模型（MLLM）先验知识的无训练框架。\n\n我们的框架将任务分解为两个部分：偏好理解和偏好引导生成。对于偏好理解，我们利用MLLM从参考图像中自动提取全局偏好信号，并通过结构化指令设计来增强给定的提示语句，从而支持比现有方法更广泛和细致地覆盖用户偏好。在偏好引导生成方面，我们将基于全局关键词的控制与局部区域感知交叉注意力调制相结合，以指导扩散模型而无需额外训练，实现从整体属性到局部元素的精确对齐。\n\n整个框架还支持多轮交互式细化过程，从而促进实时且上下文相关的图像生成。在Viper数据集和我们收集的基准测试上的广泛实验表明，我们的方法在定量指标和人工评估中都优于先前的方法，并为对话驱动的生成及MLLM与扩散模型的融合开启了新的可能性。\nhttps://arxiv.org/abs/2508.17718\nhttps://arxiv.org/pdf/2508.17718.pdf\nTraditional dialogue retrieval aims to select the most appropriate utterance or image from recent dialogue history. However, they often fail to meet users' actual needs for revisiting semantically coherent content scattered across long-form conversations. To fill this gap, we define the Fine-grained Fragment Retrieval (FFR) task, requiring models to locate query-relevant fragments, comprising both utterances and images, from multimodal long-form dialogues. As a foundation for FFR, we construct MLDR, the longest-turn multimodal dialogue retrieval dataset to date, averaging 25.45 turns per dialogue, with each naturally spanning three distinct topics. To evaluate generalization in real-world scenarios, we curate and annotate a WeChat-based test set comprising real-world multimodal dialogues with an average of 75.38 turns. Building on these resources, we explore existing generation-based Vision-Language Models (VLMs) on FFR and observe that they often retrieve incoherent utterance-image fragments. While optimized for generating responses from visual-textual inputs, these models lack explicit supervision to ensure semantic coherence within retrieved fragments. To this end, we propose F2RVLM, a generative retrieval model trained in a two-stage paradigm: (1) supervised fine-tuning to inject fragment-level retrieval knowledge, and (2) GRPO-based reinforcement learning with multi-objective rewards promoting semantic precision, relevance, and contextual coherence. To handle varying intra-fragment complexity, from locally dense to sparsely distributed, we introduce difficulty-aware curriculum sampling that ranks training instances by model-predicted difficulty and gradually exposes the model to harder samples. This boosts reasoning ability in long, multi-turn contexts. F2RVLM outperforms popular VLMs in both in-domain and real-domain settings, demonstrating superior retrieval performance.\n传统的对话检索任务旨在从近期的对话历史中选择最合适的发言或图像。然而，这类方法往往无法满足用户在长时间对话中重新查找语义连贯内容的实际需求。为了解决这一问题，我们定义了一个细粒度片段检索（FFR）任务，该任务要求模型能够定位并提取多模态长篇对话中的查询相关片段，这些片段包含发言和图像。\n\n为了支持这个新任务，我们构建了MLDR，这是迄今为止最长的转轮多模态对话检索数据集，每段对话平均有25.45个回合，并且每个对话自然地涵盖了三个不同的主题。为评估实际场景中的泛化性能，我们在微信平台上编制并标注了一套测试集，这些真实的多模态对话平均每段包含75.38个回合。\n\n基于这些资源，我们探索了现有的以生成为基础的视觉语言模型（VLM）在FFR任务上的表现，并发现它们往往检索出语义不连贯的发言-图像片段。尽管这类模型优化了从视觉和文本输入中生成响应的能力，但它们缺乏明确的监督机制来确保检索片段内的语义连贯性。\n\n为此，我们提出了F2RVLM，这是一种通过两阶段范式进行训练的生成型检索模型：(1) 监督微调以注入片段级检索知识；(2) 基于GRPO的强化学习，并采用多目标奖励来促进语义精确度、相关性和上下文连贯性。为了处理片段内部复杂性的多样性，从局部密集到稀疏分布，我们引入了难度感知课程采样方法，该方法根据模型预测的难易程度对训练实例进行排序，并逐渐让模型接触更复杂的样本。这有助于增强在长回合对话中的推理能力。\n\n实验结果显示，在领域内和真实场景中，F2RVLM均超越了流行的VLMs模型，表现出卓越的检索性能。\nhttps://arxiv.org/abs/2508.17714\nhttps://arxiv.org/pdf/2508.17714.pdf\nLarge Language Models (LLMs) are increasingly used to generate dynamic dialogue for game NPCs. However, their integration raises new security concerns. In this study, we examine whether adversarial prompt injection can cause LLM-based NPCs to reveal hidden background secrets that are meant to remain undisclosed.\n大型语言模型（LLM）越来越多地被用于生成游戏NPC的动态对话。然而，它们的集成引发了新的安全问题。在这项研究中，我们考察了对抗性提示注入是否可以使基于LLM的NPC泄露原本不应披露的秘密背景信息。\nhttps://arxiv.org/abs/2508.19288\nhttps://arxiv.org/pdf/2508.19288.pdf\n\n            Recent Papers\n        \n\n            Contact me at:\n            \n\n\n\n\n\n\n京ICP备18036300号-1\n"
        },
        {
            "title": "A New Era in Medical Imaging Segmentation! MedSAM-2 Helps Machines Understand the Secrets of the Body",
            "content": "In the wave of artificial intelligence, <strong>medical</strong> <strong>imaging</strong> technology has welcomed a new game-changer—MedSAM-2. This technology, based on the Segment Anything Mode",
            "url": "https://www.aibase.com/news/10870",
            "source": "www.aibase.com",
            "published_date": "",
            "search_engine": "Brave",
            "query": "2025年 人工智能在医学影像识别 最新技术突破 medical imaging AI breakthrough 深度学习算法",
            "language": "en",
            "family_friendly": true,
            "category": "innovation_news",
            "search_source": "brave",
            "full_content": "Tracking Global AI Breakthroughs and Industry Transformation\nAI insights in 3 minutes daily\nCurated AI Open Source Solutions for Enterprise Intelligence\nAuthoritative AI tools ranking, one-stop selection\nSubmit AI products, build intelligent ecosystem together\nDiscover The Best AI Websites & Tools\nDeploy 100+ open-source software on a dedicated instance in <3 mins\nOpen Source Pre-trained Models for Faster AI Deployment\nComparison and ranking the performance of over 100 AI models\nConnect with Top LLM Providers Worldwide\nSubmitting your AI Model, monetize value quickly\nCompare LLM Capabilities, Choose Models Effortlessly\nCalculate LLM Costs Instantly, Stay Within Budget\nAI Performance Showdown: Battle-Tested, Best-in-Class\nBest mcp servers powering enterprise development and deployment\nMulti-model orchestration, complex business simplified\nStep-by-step guide to master core development and practical skills\nExplore the most popular MCP servers ranked\nSubmit MCP services, monetize value quickly\nConnect AI to Tools Instantly: Your Zero-Barrier MCP Playground\nOne-Click Integration: Seamlessly Bridge AI and Tools\nIn the wave of artificial intelligence, medical imaging technology has welcomed a new game-changer—MedSAM-2. This technology, based on the Segment Anything Model 2 (SAM2) framework, is opening new horizons for 2D and 3D medical image segmentation tasks.\nThe breakthrough of MedSAM-2 lies in its ability to treat medical images as video sequences, which not only enables it to handle three-dimensional images but also unlocks the innovative \"One-prompt Segmentation\" feature. This feature allows users to specify a target on a single image, and the model automatically identifies and segments the same type of object in all subsequent images, regardless of whether these images are continuous.\n\nThe innovations of MedSAM-2 include:\nIt adopts a video processing mindset, leveraging the intrinsic connections between image slices to enhance segmentation accuracy.\nIt possesses a one-click segmentation capability, simplifying the operation process, allowing users to achieve automatic segmentation with a single specification.\nAs a universal model, it can handle objects in any image, achieving zero-shot generalization and providing high flexibility in data processing.\nIn terms of performance, MedSAM-2 has demonstrated its superior capabilities in multiple benchmark tests. Compared to existing fully supervised segmentation models and SAM-based interactive models, MedSAM-2 has shown better performance in all tested methods, especially outstanding generalization ability in one-click segmentation settings.\nThe clinical application value of MedSAM-2 is significant. It not only improves the efficiency of medical image analysis but also ensures the accuracy of segmentation results, which is crucial for enhancing clinical diagnosis accuracy and guiding surgery.\nThe advent of MedSAM-2 marks a new milestone in medical image segmentation technology. With continuous development, MedSAM-2 is expected to demonstrate its powerful capabilities in more fields, bringing more possibilities to medical image analysis.\nPaper link: https://arxiv.org/pdf/2408.00874\nWelcome to the [AI Daily] column! This is your daily guide to exploring the world of artificial intelligence. Every day, we present you with hot topics in the AI field, focusing on developers, helping you understand technical trends, and learning about innovative AI product applications.\n3D AI firm Yingmo Tech secured millions in new funding, led by BlueRun Ventures, with follow-on investments from ByteDance and Sequoia China. The company plans to launch its hyper3d.ai platform with the Rodin Gen-2 model next month, already collaborating with global firms.....\n\nAlibaba's Tongyi Wanxiang introduces Wan 2.2-S2V, a new AI model that synchronizes video and audio generation, enhancing multimodal AI capabilities for creators.....\nMonetization strategy: Use AI-generated 'grandma' character on Xiaohongshu to post mystical/wellness shorts, like 'health benefits of jewelry', attracting likes for ad revenue. Ideal for creators avoiding real appearances, interested in wellness/mysticism, or using AI for bulk content. Difficulty: Medium (requires AI tools & topic planning). Steps: Define a relatable character (e.g., 'AI grandma') for trustworthiness.....\nAlibaba's Tongyi Wanxiang team announced Wan2.2-S2V, an AI model that generates videos with synchronized audio, enhancing multimodal content creation.....\n\nGoogle introduces a quick edit button in Drive for seamless video editing via AI tool Vids, enhancing Workspace users' experience.....\nDingTalk launched version 8.0, introducing DingTalk ONE, a next-gen AI office app, redefining work in the AI era with natural language interaction and agent-driven workflows.....\nOpenAI CEO Sam Altman proposed offering free ChatGPT Plus subscriptions to UK citizens, costing £2B, but the plan was not realized. Discussions aimed to explore broader OpenAI-UK collaborations.....\nRecently, Moonshot AI released the latest update about the Kimi K2 High-Speed Version AI model. After the efforts of engineers, the output speed of the Kimi K2 turbo-preview model has been significantly improved, with a stable output speed now reaching 60 Tokens per second and a maximum of up to 100 Tokens per second. This advancement marks a significant improvement in the model's efficiency when processing data, allowing users to experience smoother service when using it. Kimi K2 is Moonshot AI's..."
        },
        {
            "title": "Artificial intelligence in medical imaging: progress and perspectives",
            "content": "Artificial intelligence in <strong>medical</strong> <strong>imaging</strong> has shown huge applicable potentiallity in disease diagnosis and prognosis. However, it remains many challenges impeding its’ application in clinical practice. In this study, we investigated the progress and perspectives of artificial intelligence ...",
            "url": "https://www.ijmradiol.ac.cn/EN/10.19300/j.2023.S20494",
            "source": "www.ijmradiol.ac.cn",
            "published_date": "January 15, 2023",
            "search_engine": "Brave",
            "query": "2025年 人工智能在医学影像识别 最新技术突破 medical imaging AI breakthrough 深度学习算法",
            "language": "en",
            "family_friendly": true,
            "category": "innovation_news",
            "search_source": "brave",
            "full_content": "Artificial intelligence in medical imaging: progress and perspectives\n\r\n\t\t\t\t\t\tLIU Zaiyi, SHI Zhenwei\r\n\t\t\t\t\t\n\nINTERNATIONAL JOURNAL OF MEDICAL RADIOLOGY\n››\n2023, Vol. 46\n››\nIssue (1)\n: 1-4.\n\nArtificial intelligence in medical imaging: progress and perspectives\n\n\n\n\nEndNote\n\n\nRis (Procite)\n\n\nBibtex\n\n\nAccesses\n\nCitation\n\nAltmetric\n\n\n\nDetail\n\n/\n\n\n"
        },
        {
            "title": "AI-Powered Solutions for Healthcare & Life Sciences | NVIDIA",
            "content": "Cutting-Edge Technologies for Healthcare and Life Sciences.",
            "url": "https://www.nvidia.com/en-us/industries/healthcare-life-sciences/",
            "source": "www.nvidia.com",
            "published_date": "",
            "search_engine": "Brave",
            "query": "2025年 人工智能在医学影像识别 最新技术突破 medical imaging AI breakthrough 深度学习算法",
            "language": "en-us",
            "family_friendly": true,
            "category": "innovation_news",
            "search_source": "brave",
            "full_content": "Visit your regional NVIDIA website for local content, pricing, and where to buy partners specific to your country.\nAI-driven platform for life sciences research and discovery\nFully managed end-to-end AI platform on leading clouds\nExplore, test, and deploy AI models and agents\nIntegrate advanced simulation and AI into complex 3D workflows\nGuide for using NVIDIA NGC private registry with GPU cloud\nAccelerated, containerized AI models and SDKs\nModernizing data centers with AI and accelerated computing\nEnterprise AI factory for model development and deployment\nArchitecture for data centers that transform data into intelligence\nA supercomputer purpose-built for AI and HPC\nAdvanced functional safety and security for edge AI\nAccelerated computing with modular servers\nScalable data center infrastructure for high-performance AI\nLeading platform for autonomous machines and embedded applications\nPowerful in-vehicle computing for AI-driven autonomous vehicle systems\nAI-powered computing for innovative medical devices and imaging\nExplore graphics cards, gaming solutions, AI technology, and more\nRTX graphics cards bring game-changing AI capabilities\nThinnest and longest lasting RTX laptops, optimized by Max-Q\nSmooth, tear-free gaming with NVIDIA G-SYNC monitors\nNeural rendering tech boosts FPS and enhances image quality\nUltimate responsiveness for faster reactions and better aim\nAI PCs for gaming, creating, productivity and development\nHigh performance laptops and desktops, purpose-built for creators\nRTX-powered cloud gaming. Choose from 3 memberships\nOptimize gaming, streaming, and AI-powered creativity\nAI-enhanced voice and video for next-level streams, videos, and calls\nWorld-class streaming media performance\nThe engine of the new industrial revolution\nHigh performance, scalability, and security for every data center\nPerformance and energy efficiency for endless possibilities\nRTX graphics cards bring game-changing AI capabilities\nAccelerating professional AI, graphics, rendering and compute workloads\nVirtual solutions for scalable, high-performance computing\nGPU-powered laptops for gamers and creators\nHigh performance laptops purpose-built for creators\nAccelerate professional AI and visual computing from anywhere\nAccelerated networks for modern workloads\nSoftware-defined hardware accelerators for networking, storage, and security\nEthernet performance, availability, and ease of use across a wide range of applications\nHigh-performance networking for super computers, AI, and cloud data centers\nNetworking software for optimized performance and scalability\nIO subsystem for modern, GPU-accelerated data centers\nAccelerating professional AI, graphics, rendering, and compute workloads\nA Grace Blackwell AI Supercomputer on your desk\nThe ultimate desktop AI supercomputer powered by NVIDIA Grace Blackwell\nAccelerate innovation and productivity in AI workflows\nPowerful AI, graphics, rendering, and compute workloads\nAccelerate professional AI and visual computing from anywhere\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSimplify AI development with NVIDIA AI Workbench on GPUs\nExplore NVIDIA's AI models, blueprints, and tools for developers\nAI and HPC software solutions for data center acceleration\nMonitor and manage GPU performance in cluster environments\nExplore NVIDIA developer tools for AI, graphics, and HPC\nDiscover GPU-optimized AI, HPC, and data science software\nOptimize enterprise GPU management \nAccelerate AI and HPC workloads with NVIDIA GPU Cloud solutions\nEnhance multi-display productivity with NVIDIA RTX Desktop Manager\nCreative tools and AI-powered apps for artists and designers\nAI-powered audio and video enhancement\nAdd intelligence and efficiency to your business with AI and machine learning\nBuild AI agents designed to reason, plan, and act\nPowering a new class of enterprise infrastructure for AI\nEnables natural, personalized interactions with real-time speech AI\nAI-driven solutions to strengthen cybersecurity and AI infrastructure\nIterate on large datasets, deploy models more frequently, and lower total cost\nInstantly run and deploy Generative AI\nDrive breakthrough performance with AI-enabled applications and services\nPowering AI, HPC, and modern workloads with NVIDIA\nBringing enterprise storage into the era of agentic AI\nAccelerated computing uses specialized hardware to boost IT performance\nOn-demand IT resources and services, enabling scalability and intelligent insights\nAccelerate the scaling of AI across your organization\nAccelerate AI with MLOps\nHigh speed ethernet interconnect solutions and services\nSave energy and lower cost with AI and accelerated computing\nNVIDIA virtual GPU software delivers powerful GPU performance\nStreamline building, operating, and connecting metaverse apps\nDevelop real-time interactive design using AI-accelerated real-time digital twins\nHarness the power of large-scale, physically-based OpenUSD simulation\nBring state-of-the-art rendering to professional workflows\nInnovative solutions to take on your robotics, edge, and vision AI challenges\nEnablies researchers to visualize their large datasets at interactive speeds\nAI-defined vehicles are transforming the future of mobility\nTransform workflows with immersive, scalable interactions in virtual environments\nDiscover NVIDIA’s HPC solutions for AI, simulation, and accelerated computing\nBoost accuracy with GPU-accelerating HPC and AI\nEnables researchers to visualize large datasets at interactive speeds\nAccelerate simulation workloads\nFast-tracking the advancement of scientific innovations with QPUs\nInnovative solutions to take on robotics, edge, and vision AI challenges\nGPU-accelerated advances in AI perception, simulation, and software\nBring the power of NVIDIA AI to the edge for real-time decision-making solutions\nTransform data into valuable insights using vision AI\nAI-enhanced vehicles are transforming the future of mobility\nEssential data center tools for safe autonomous vehicle development\nExplore high-fidelity sensor simulation for safe autonomous vehicle development\nDevelop automated driving functions and immersive in-cabin experiences\nState-of-the-art system for AV safety, from the cloud to the car\nAI-driven platform for life sciences research and discovery\nFully managed end-to-end AI platform on leading clouds\nExplore, test, and deploy AI models and agents\nIntegrate advanced simulation and AI into complex 3D workflows\nGuide for using NVIDIA NGC private registry with GPU cloud\nAccelerated, containerized AI models and SDKs\nModernizing data centers with AI and accelerated computing\nEnterprise AI factory for model development and deployment\nArchitecture for data centers that transform data into intelligence\nA supercomputer purpose-built for AI and HPC\nAdvanced functional safety and security for edge AI\nAccelerated computing with modular servers\nScalable data center infrastructure for high-performance AI\nLeading platform for autonomous machines and embedded applications\nPowerful in-vehicle computing for AI-driven autonomous vehicle systems\nAI-powered computing for innovative medical devices and imaging\nExplore graphics cards, gaming solutions, AI technology, and more\nRTX graphics cards bring game-changing AI capabilities\nThinnest and longest lasting RTX laptops, optimized by Max-Q\nSmooth, tear-free gaming with NVIDIA G-SYNC monitors\nNeural rendering tech boosts FPS and enhances image quality\nUltimate responsiveness for faster reactions and better aim\nAI PCs for gaming, creating, productivity and development\nHigh performance laptops and desktops, purpose-built for creators\nRTX-powered cloud gaming. Choose from 3 memberships\nOptimize gaming, streaming, and AI-powered creativity\nAI-enhanced voice and video for next-level streams, videos, and calls\nWorld-class streaming media performance\nThe engine of the new industrial revolution\nHigh performance, scalability, and security for every data center\nPerformance and energy efficiency for endless possibilities\nRTX graphics cards bring game-changing AI capabilities\nAccelerating professional AI, graphics, rendering and compute workloads\nVirtual solutions for scalable, high-performance computing\nGPU-powered laptops for gamers and creators\nHigh performance laptops purpose-built for creators\nAccelerate professional AI and visual computing from anywhere\nAccelerated networks for modern workloads\nSoftware-defined hardware accelerators for networking, storage, and security\nEthernet performance, availability, and ease of use across a wide range of applications\nHigh-performance networking for super computers, AI, and cloud data centers\nNetworking software for optimized performance and scalability\nIO subsystem for modern, GPU-accelerated data centers\nAccelerating professional AI, graphics, rendering, and compute workloads\nA Grace Blackwell AI Supercomputer on your desk\nThe ultimate desktop AI supercomputer powered by NVIDIA Grace Blackwell\nAccelerate innovation and productivity in AI workflows\nPowerful AI, graphics, rendering, and compute workloads\nAccelerate professional AI and visual computing from anywhere\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSimplify AI development with NVIDIA AI Workbench on GPUs\nExplore NVIDIA's AI models, blueprints, and tools for developers\nAI and HPC software solutions for data center acceleration\nMonitor and manage GPU performance in cluster environments\nExplore NVIDIA developer tools for AI, graphics, and HPC\nDiscover GPU-optimized AI, HPC, and data science software\nOptimize enterprise GPU management \nAccelerate AI and HPC workloads with NVIDIA GPU Cloud solutions\nEnhance multi-display productivity with NVIDIA RTX Desktop Manager\nCreative tools and AI-powered apps for artists and designers\nAI-powered audio and video enhancement\nAdd intelligence and efficiency to your business with AI and machine learning\nBuild AI agents designed to reason, plan, and act\nPowering a new class of enterprise infrastructure for AI\nEnables natural, personalized interactions with real-time speech AI\nAI-driven solutions to strengthen cybersecurity and AI infrastructure\nIterate on large datasets, deploy models more frequently, and lower total cost\nInstantly run and deploy Generative AI\nDrive breakthrough performance with AI-enabled applications and services\nPowering AI, HPC, and modern workloads with NVIDIA\nBringing enterprise storage into the era of agentic AI\nAccelerated computing uses specialized hardware to boost IT performance\nOn-demand IT resources and services, enabling scalability and intelligent insights\nAccelerate the scaling of AI across your organization\nAccelerate AI with MLOps\nHigh speed ethernet interconnect solutions and services\nSave energy and lower cost with AI and accelerated computing\nNVIDIA virtual GPU software delivers powerful GPU performance\nStreamline building, operating, and connecting metaverse apps\nDevelop real-time interactive design using AI-accelerated real-time digital twins\nHarness the power of large-scale, physically-based OpenUSD simulation\nBring state-of-the-art rendering to professional workflows\nInnovative solutions to take on your robotics, edge, and vision AI challenges\nEnablies researchers to visualize their large datasets at interactive speeds\nAI-defined vehicles are transforming the future of mobility\nTransform workflows with immersive, scalable interactions in virtual environments\nDiscover NVIDIA’s HPC solutions for AI, simulation, and accelerated computing\nBoost accuracy with GPU-accelerating HPC and AI\nEnables researchers to visualize large datasets at interactive speeds\nAccelerate simulation workloads\nFast-tracking the advancement of scientific innovations with QPUs\nInnovative solutions to take on robotics, edge, and vision AI challenges\nGPU-accelerated advances in AI perception, simulation, and software\nBring the power of NVIDIA AI to the edge for real-time decision-making solutions\nTransform data into valuable insights using vision AI\nAI-enhanced vehicles are transforming the future of mobility\nEssential data center tools for safe autonomous vehicle development\nExplore high-fidelity sensor simulation for safe autonomous vehicle development\nDevelop automated driving functions and immersive in-cabin experiences\nState-of-the-art system for AV safety, from the cloud to the car\n\n\t\t    \t\n                    Survey Report\n\t\t\t\t\n\t    \t\nDiscover how AI is transforming healthcare with the latest trends, insights, and challenges in our new report.\nNVIDIA is advancing healthcare with AI across science, robotics, and intelligent agents. From lab research and genomics to diagnostic imaging and patient engagement, our ecosystem empowers partners to accelerate discovery, improve care, and drive innovation with scalable, high-performance solutions. \n\n\t\t    \t\n                    News and Blogs\n\t\t\t\t\n\t    \t\nLeading Danish enterprises, startups, and public health systems—including Novo Nordisk and the Danish Centre of AI Innovation (DCAI)—are partnering with NVIDIA to transform the future of Denmark’s healthcare with AI factories.\n\n\t\t    \t\n                    Use Cases\n\t\t\t\t\n\t    \t\n\n\t\t    \t\n                    Healthcare Solutions\n\t\t\t\t\n\t    \t\nNVIDIA’s full-stack AI platform supports every step of the healthcare journey—from biopharma research and genomic analysis to digital health, medical devices, and imaging—enabling innovation from data center to edge to cloud.\nBiopharma | Genomics | Digital Health | Medical Devices | Medical Imaging\n\n\t\t    \t\n                    Customer Stories\n\t\t\t\t\n\t    \t\nSee how healthcare leaders are overcoming big challenges — these stories reveal the strategies shaping the future of medicine.\n\n\t\t    \t\n                    Partners\n\t\t\t\t\n\t    \t\nOur solutions for the healthcare industry go beyond products. Our partners are here to assist your organization at every level to build and execute transformative AI strategies, products, and services.\n\n\t\t    \t\n                    Get Started\n\t\t\t\t\n\t    \t\nDo you have questions? Contact a member of our NVIDIA healthcare and life sciences team.\nNVIDIA Privacy Policy"
        },
        {
            "title": "Machine Learning and AI in Cancer Prognosis, Prediction, and Treatment Selection: A Critical Approach - PMC",
            "content": "<strong>Cancer</strong> is a leading cause of morbidity and mortality worldwide. While progress has been made in the diagnosis, prognosis, and treatment of <strong>cancer</strong> patients, individualized and data-driven care remains a challenge. Artificial intelligence (AI), which ...",
            "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC10312208/",
            "source": "pmc.ncbi.nlm.nih.gov",
            "published_date": "",
            "search_engine": "Brave",
            "query": "人工智能癌症早期诊断 重大研究进展 最近7天 breakthrough cancer detection machine learning",
            "language": "en",
            "family_friendly": true,
            "category": "innovation_news",
            "search_source": "brave",
            "full_content": "<strong>Cancer</strong> is a leading cause of morbidity and mortality worldwide. While progress has been made in the diagnosis, prognosis, and treatment of <strong>cancer</strong> patients, individualized and data-driven care remains a challenge. Artificial intelligence (AI), which ..."
        },
        {
            "title": "AI making cancer detection faster, more accurate",
            "content": "Conference discusses advances in precision oncology and predictive modelling",
            "url": "https://www.nature.com/articles/d44151-024-00107-6",
            "source": "www.nature.com",
            "published_date": "July 11, 2024",
            "search_engine": "Brave",
            "query": "人工智能癌症早期诊断 重大研究进展 最近7天 breakthrough cancer detection machine learning",
            "language": "en",
            "family_friendly": true,
            "category": "innovation_news",
            "search_source": "brave",
            "full_content": "Thank you for visiting nature.com. You are using a browser version with limited support for CSS. To obtain\n            the best experience, we recommend you use a more up to date browser (or turn off compatibility mode in\n            Internet Explorer). In the meantime, to ensure continued support, we are displaying the site without styles\n            and JavaScript.\nAdvertisement\n\nSearch author on:\nPubMed\n Google Scholar\n\nMicroscopic image of breast cancer cells resisting treatment. Credit: Callista Images/Image Source/Getty Images\nScientists are increasingly using Artificial Intelligence (AI) and machine learning (ML) algorithms to analyse genomics data and for predictive modelling and precision oncology.\nA recent conference on genomic analysis and technology at Delhi’s International Centre for Genetic Engineering and Biotechnology (ICGEB) discussed new advances in this field.\n“AI and ML are making it possible to massively improve the accuracy, efficiency and timeliness of cancer detection,” said Dinesh Gupta, group leader of translational bioinformatics at ICGEB. This directly leads to improved patient outcomes and reduced healthcare costs. Additionally, these technologies can take cancer care to underserved populations, he said.\nThe All India Institute of Medical Sciences (AIIMS) in Delhi recently launched an AI system trained on 500,000 radiological and histopathological images from 1,500 breast and ovarian cancer cases, the most prevalent forms of cancer in India.\nMelissa Fullwood, associate professor at Nanyang Technological University in Singapore, shared her team’s use of AI to predict chromatin interactions in cancer. Fullwood’s team developed the Chromatin Interaction Neural Network (ChINN), a convolutional neural network that predicts chromatin interactions using DNA sequences. This AI method aids drug target identification and discovery by analysing genomics and epigenomics data.\nNew AI-ML tools are now available for cancer pathology and histology analysis, imaging results, and circulating tumour nucleic acids analysis, advancing early cancer detection and precision medicine. Gupta emphasized the role of machine learning in multi-omics, combining data from genomes, transcriptomes, proteomes, epigenomes, and metabolomes for a comprehensive understanding of cancer biology.\nResearchers at the Indian Institute of Technology, Dharwad, are using multi-omics-based classification, which proved to be a more accurate prediction model, to identify five novel lung cancer cell clusters with different genetic and clinical features1.\nor\ndoi: https://doi.org/10.1038/d44151-024-00107-6\nKhadirnaikar, S. et al. Sci Rep 13, 4636 (2023).\nArticle \n    PubMed \n    \n                    Google Scholar \n                \nMhatre, A. et al. Front. Genet. 14 (2023).\nArticle \n    \n                    Google Scholar \n                \nDownload references\nReprints and permissions\nJob Title:    Associate Editor, Discover Journals   Location:   Pune/Beijing/Shanghai/Nanjing, Hybrid working model Closing Date: Sept 6, 2025   Ab...\nPune/Beijing/Shanghai/Nanjing, Hybrid working model\nSpringer Nature Ltd\nJob Title:    Associate Editor, Discover Journals   Location:   Pune/Beijing/Shanghai/Nanjing, Hybrid working model Closing Date: Sept 6, 2025   Ab...\nPune/Beijing/Shanghai/Nanjing, Hybrid working model\nSpringer Nature Ltd\nFaculty positions are open at four distinct ranks: Assistant Professor, Associate Professor, Full Professor, and Chair Professor.\nHangzhou, Zhejiang, China\nWestlake University\nMultiple positions are open at all ranks.\nHangzhou, Zhejiang, China\nWestlake University\nLeading Talent, Excellent Young Scholars (Overseas), Young Top Talents, Postdoctoral Fellow\nXi'an, Shaanxi (CN)\nHospital of Stomatology Xi’an Jiaotong University\nAn essential round-up of science news, opinion and analysis, delivered to your inbox every weekday.\nSign up for the Nature Briefing newsletter — what matters in science, free to your inbox daily.\n\n\n                        Nature India\n                    \n                    (Nat India)\n                \n\nISSN 1755-3180 (online)\n        \n© 2025 Springer Nature Limited"
        },
        {
            "title": "Artificial intelligence breakthroughs in pioneering early diagnosis and precision treatment of breast cancer: A multimethod study - ScienceDirect",
            "content": "This article delves into the potential of artificial intelligence (AI) to enhance early breast <strong>cancer</strong> (BC) <strong>detection</strong> for improved treatment outcomes a…",
            "url": "https://www.sciencedirect.com/science/article/abs/pii/S0959804924008839",
            "source": "www.sciencedirect.com",
            "published_date": "July 15, 2024",
            "search_engine": "Brave",
            "query": "人工智能癌症早期诊断 重大研究进展 最近7天 breakthrough cancer detection machine learning",
            "language": "en",
            "family_friendly": true,
            "category": "innovation_news",
            "search_source": "brave",
            "full_content": "This article delves into the potential of artificial intelligence (AI) to enhance early breast <strong>cancer</strong> (BC) <strong>detection</strong> for improved treatment outcomes a…"
        },
        {
            "title": "(PDF) Medical Diagnosis Coding Automation: Similarity Search vs. Generative AI",
            "content": "PDF | Objective: This study aims to predict ICD-10-CM codes for <strong>medical</strong> diagnoses from short <strong>diagnosis</strong> descriptions and compare two distinct approaches:... | Find, read and cite all the <strong>research</strong> you need on ResearchGate",
            "url": "https://www.researchgate.net/publication/380193793_Medical_Diagnosis_Coding_Automation_Similarity_Search_vs_Generative_AI",
            "source": "www.researchgate.net",
            "published_date": "April 29, 2024",
            "search_engine": "Brave",
            "query": "生成式AI在医疗诊断中的应用 最新研究 最近7天 generative AI medical diagnosis research paper",
            "language": "en",
            "family_friendly": true,
            "category": "innovation_news",
            "search_source": "brave",
            "full_content": "PDF | Objective: This study aims to predict ICD-10-CM codes for <strong>medical</strong> diagnoses from short <strong>diagnosis</strong> descriptions and compare two distinct approaches:... | Find, read and cite all the <strong>research</strong> you need on ResearchGate"
        },
        {
            "title": "Nature Medicine ：破解医学影像数据稀缺——生成式AI的全新突破 - 体外诊断专区 - 生物谷",
            "content": "在这一背景下，生成式人工智能（Generative AI）技术带来了前所未有的解决方案。12月11日Nature Medicine最新的研究报道<strong>Self-improving generative foundation model for synthetic medical image generation and clinical applications</strong>，一个名为MINIM的生成式...",
            "url": "https://news.bioon.com/article/23098552e593.html",
            "source": "news.bioon.com",
            "published_date": "",
            "search_engine": "Brave",
            "query": "生成式AI在医疗诊断中的应用 最新研究 最近7天 generative AI medical diagnosis research paper",
            "language": "zh",
            "family_friendly": true,
            "category": "innovation_news",
            "search_source": "brave",
            "full_content": "在这一背景下，生成式人工智能（Generative AI）技术带来了前所未有的解决方案。12月11日Nature Medicine最新的研究报道<strong>Self-improving generative foundation model for synthetic medical image generation and clinical applications</strong>，一个名为MINIM的生成式..."
        },
        {
            "title": "(PDF) Generative AI for medical imaging analysis and applications",
            "content": "PDF | <strong>Generative</strong> <strong>AI</strong> plays a pivotal role in <strong>medical</strong> imaging analysis, enabling precise <strong>diagnosis</strong>, treatment planning and disease monitoring. Techniques... | Find, read and cite all the <strong>research</strong> you need on ResearchGate",
            "url": "https://www.researchgate.net/publication/373725534_Generative_AI_for_medical_imaging_analysis_and_applications",
            "source": "www.researchgate.net",
            "published_date": "September 6, 2023",
            "search_engine": "Brave",
            "query": "生成式AI在医疗诊断中的应用 最新研究 最近7天 generative AI medical diagnosis research paper",
            "language": "en",
            "family_friendly": true,
            "category": "innovation_news",
            "search_source": "brave",
            "full_content": "PDF | <strong>Generative</strong> <strong>AI</strong> plays a pivotal role in <strong>medical</strong> imaging analysis, enabling precise <strong>diagnosis</strong>, treatment planning and disease monitoring. Techniques... | Find, read and cite all the <strong>research</strong> you need on ResearchGate"
        }
    ],
    "investment_news": [
        {
            "title": "Discover Logrium: A New Era in Blockchain Technology",
            "content": "5 days ago ... Our journey began with $3 million in seed funding from visionary investors, enabling us to develop our core AI model and foundational infrastructure.",
            "url": "https://www.farmerscoopelevator.com/markets/stocks.php?article=globeprwire-2025-8-23-discover-logrium-a-new-era-in-blockchain-technology",
            "source": "www.farmerscoopelevator.com",
            "published_date": "2025-08-28",
            "search_engine": "Google",
            "query": "医疗AI初创公司 最新融资 千万美元 最近7天 health tech startup funding million dollar",
            "description": "5 days ago ... Our journey began with $3 million in seed funding from visionary investors, enabling us to develop our core AI model and foundational infrastructure.",
            "image": "",
            "category": "investment_news",
            "search_source": "google",
            "full_content": "Farmers Cooperative Elevator Company Zero Tolerance Policy\nThe U.S. Food and Drug Administration and the U.S. Grain Standards Act allow ZERO TOLERANCE for treated seed occurring in grain.\nFCE wants to remind you to thoroughly clean all equipment that comes in contact with treated seed. Be sure to check and recheck to make sure it’s clean. One kernel of treated seed, whether it is corn, soybeans, or wheat, can contaminate an entire bin and can cost FCE millions of dollars. If you are checked and have treated kernels, by law, you may have to pay for the grain quantity that was tainted.\n© 2025 Barchart.com, Inc.  All market data is hosted and powered by Barchart.\nInformation presented is provided 'as-is' and solely for informational purposes, not for trading purposes or advice, and is delayed. No representations are made by Barchart as to its informational accuracy or completeness."
        },
        {
            "title": "Atlanticus Announces Approval of Quarterly Preferred Stock Dividend",
            "content": "7 days ago ... Atlanticus Holdings Corporation (NASDAQ: ATLC) (“Atlanticus,” the “Company,” “we” or “our”), a financial technology company that enables its bank, retail...",
            "url": "https://www.farmerscoopelevator.com/markets/stocks.php?article=gnwcq-2025-8-21-atlanticus-announces-approval-of-quarterly-preferred-stock-dividend",
            "source": "www.farmerscoopelevator.com",
            "published_date": "2025-08-28",
            "search_engine": "Google",
            "query": "医疗AI初创公司 最新融资 千万美元 最近7天 health tech startup funding million dollar",
            "description": "7 days ago ... Atlanticus Holdings Corporation (NASDAQ: ATLC) (“Atlanticus,” the “Company,” “we” or “our”), a financial technology company that enables its bank, retail...",
            "image": "",
            "category": "investment_news",
            "search_source": "google",
            "full_content": "Farmers Cooperative Elevator Company Zero Tolerance Policy\nThe U.S. Food and Drug Administration and the U.S. Grain Standards Act allow ZERO TOLERANCE for treated seed occurring in grain.\nFCE wants to remind you to thoroughly clean all equipment that comes in contact with treated seed. Be sure to check and recheck to make sure it’s clean. One kernel of treated seed, whether it is corn, soybeans, or wheat, can contaminate an entire bin and can cost FCE millions of dollars. If you are checked and have treated kernels, by law, you may have to pay for the grain quantity that was tainted.\n© 2025 Barchart.com, Inc.  All market data is hosted and powered by Barchart.\nInformation presented is provided 'as-is' and solely for informational purposes, not for trading purposes or advice, and is delayed. No representations are made by Barchart as to its informational accuracy or completeness."
        },
        {
            "title": "Bragar Eagel & Squire, P.C. Reminds Investors of KinderCare, CTO ...",
            "content": "7 days ago ... Additional information about each case can be found at the link provided. KinderCare Learning Companies, Inc. (NYSE:KLC). Lead Plaintiff Deadline: October 14, ...",
            "url": "https://www.farmerscoopelevator.com/markets/stocks.php?article=gnwcq-2025-8-21-bragar-eagel-and-squire-pc-reminds-investors-of-kindercare-cto-realty-and-charter-communications-that-lawsuits-have-been-filed-and-encourages-investors-to-contact-the-firm",
            "source": "www.farmerscoopelevator.com",
            "published_date": "2025-08-28",
            "search_engine": "Google",
            "query": "医疗AI初创公司 最新融资 千万美元 最近7天 health tech startup funding million dollar",
            "description": "7 days ago ... Additional information about each case can be found at the link provided. KinderCare Learning Companies, Inc. (NYSE:KLC). Lead Plaintiff Deadline: October 14, ...",
            "image": "",
            "category": "investment_news",
            "search_source": "google",
            "full_content": "Farmers Cooperative Elevator Company Zero Tolerance Policy\nThe U.S. Food and Drug Administration and the U.S. Grain Standards Act allow ZERO TOLERANCE for treated seed occurring in grain.\nFCE wants to remind you to thoroughly clean all equipment that comes in contact with treated seed. Be sure to check and recheck to make sure it’s clean. One kernel of treated seed, whether it is corn, soybeans, or wheat, can contaminate an entire bin and can cost FCE millions of dollars. If you are checked and have treated kernels, by law, you may have to pay for the grain quantity that was tainted.\n© 2025 Barchart.com, Inc.  All market data is hosted and powered by Barchart.\nInformation presented is provided 'as-is' and solely for informational purposes, not for trading purposes or advice, and is delayed. No representations are made by Barchart as to its informational accuracy or completeness."
        },
        {
            "title": "麻省理工科技评论-发现改变世界的新兴科技",
            "content": "作为世界上最权威的科技商业媒体之一，MIT Technology Review于1899年在美国麻省理工学院创刊，至今已经走过121年，为全世界超过300万专业人士及商业领袖提供前瞻性的资讯和独到深入的行业趋势分析。",
            "url": "https://www.mittrchina.com/news/detail/14229",
            "source": "www.mittrchina.com",
            "published_date": "",
            "search_engine": "Brave",
            "query": "AI病理诊断系统 最近7天 融资 投资 并购 funding series round AI pathology startup",
            "language": "en",
            "family_friendly": true,
            "category": "investment_news",
            "search_source": "brave",
            "full_content": "作为世界上最权威的科技商业媒体之一，MIT Technology Review于1899年在美国麻省理工学院创刊，至今已经走过121年，为全世界超过300万专业人士及商业领袖提供前瞻性的资讯和独到深入的行业趋势分析。"
        },
        {
            "title": "「最新融资」91360医学：成功完成新一轮融资，加速病理AI技术创新与市场版图拓展_摩熵医药(原药融云)",
            "content": "近日， 国内病理AI领域的头部公司91360宣布成功完成新一轮融资 。 本轮融资由 常春藤资本 领投，国资 杭州拱墅区国投 跟投。 此次融资将助力91360在宫颈细胞学AI辅助诊断产品整体方案的推广落地，以及数字化病理科解决...",
            "url": "https://www.pharnexcloud.com/zixun/yytrz_25109",
            "source": "www.pharnexcloud.com",
            "published_date": "",
            "search_engine": "Brave",
            "query": "AI病理诊断系统 最近7天 融资 投资 并购 funding series round AI pathology startup",
            "language": "zh",
            "family_friendly": true,
            "category": "investment_news",
            "search_source": "brave",
            "full_content": "客服电话\n\n近日，国内病理AI领域的头部公司91360宣布成功完成新一轮融资。本轮融资由常春藤资本领投，国资杭州拱墅区国投跟投。\n\n此次融资将助力91360在宫颈细胞学AI辅助诊断产品整体方案的推广落地，以及数字化病理科解决方案、远程会诊平台业务及创新业务的持续发展。\n\n自成立以来，91360始终坚持以用户为中心，致力于将先进的AI病理诊断技术与互联网技术深度融合，持续创新并自主研发了多项专业病理软件产品，其中“宫颈细胞学数字病理图像计算机辅助分析软件”更是取得了行业内的突破性成就，荣获中国病理AI领域首张NMPA三类注册证，并籍此荣获国家工信部、药监局于2024年8月揭榜的“人工智能医疗器械创新任务揭榜优胜单位”称号。\n\n伴随着本轮融资的成功，91360总部迁址至杭州拱墅区，成立了玖壹叁陆零医学科技（杭州）有限公司（以下简称“91360杭州”），作为公司未来发展的重要基地和上市主体。此次战略融资与迁址的达成，体现了政府和资本市场对91360未来发展的坚定信心，也标志着公司在战略布局、市场拓展及技术研发等方面的全面升级。\n\n声明：本稿件旨在传播医药行业资讯，欢迎转发至朋友圈，如有对所报道企业感兴趣，在本公众号回复留言并添加小编微信。\n\n扫描下方二维码关注公众号，获取行业和公司动态，在公众号右上角星号设置特别关注，防止错过重要消息哦！\n\n●细分行业研究集合：CGT、合成生物、mRNA、疫苗等热门领域\n●新锐公司分析合集：原启生物、至善唯新、宜联生物等新兴公司\n●融资事件汇总合集：创新药、生物技术、IVD、医疗服务、医疗器械五大领域\n●临床试验汇总合集：星耀坤泽、凌达生物、睿源生物等新进获批\n药圈时汇是由医药健康、券商基金、产业园区等领域从业者发起的自媒体交流平台，致力于为项目与资本方提供项目路演、资本交流、行业资讯等相关服务，如有合作意向或者项目的投融资需求，请联系Luna_jake\n\n喜欢文章，顺手“三连”吧 \n\n 收藏 \n登录后参与评论\n暂无评论\n400-9696-311 转1\n400-9696-311 转2\n400-9696-311 转3\n400-9696-311 转4\n\n\n\n\n\n\n 本网站未发布麻醉药品、精神药品、医疗用毒性药品、放射性药品、戒毒药品和医疗机构制剂的产品信息\n\n友情链接:\n摩熵医药\n摩熵医药企业版\n摩熵化学MolAid\n晓材Matmole\n摩熵医学MedXYZ\n载体质粒\n觅健肿瘤互助\n动脉网\n药物百科\n中国医疗人才网\n"
        },
        {
            "title": "今年14起融资，数字病理正在进入快车道，AI将重构产业 | 机器之心",
            "content": "A100 Data Intelligence · 专注未来出行及智能汽车科技",
            "url": "https://www.jiqizhixin.com/articles/2019-11-11-8",
            "source": "www.jiqizhixin.com",
            "published_date": "",
            "search_engine": "Brave",
            "query": "AI病理诊断系统 最近7天 融资 投资 并购 funding series round AI pathology startup",
            "language": "en",
            "family_friendly": true,
            "category": "investment_news",
            "search_source": "brave",
            "full_content": "登录"
        },
        {
            "title": "亿级以上融资21起，AI 医疗、具身智能成热门，早期融资继续主导 | 2025年1月人工智能投融资观察 · 极新月报 - 维科号",
            "content": "“ 谁在迭代？” 文｜小鱼&amp;云舒 编辑 | 小白 出品｜极新 1月重点关注： 1、本月人工智能领域投融资事件116起，披露金额69亿人民币。 2、亿级人民币以上金额的投资事件共21起 。 3、一...",
            "url": "https://mp.ofweek.com/medical/a156714742517",
            "source": "mp.ofweek.com",
            "published_date": "",
            "search_engine": "Brave",
            "query": "医疗AI初创公司 最新融资 千万美元 最近7天 health tech startup funding million dollar",
            "language": "en",
            "family_friendly": true,
            "category": "investment_news",
            "search_source": "brave",
            "full_content": "“ 谁在迭代？” 文｜小鱼&amp;云舒 编辑 | 小白 出品｜极新 1月重点关注： 1、本月人工智能领域投融资事件116起，披露金额69亿人民币。 2、亿级人民币以上金额的投资事件共21起 。 3、一..."
        },
        {
            "title": "两年融资2400万美元，这家公司用AI护理方案驱动价值医疗-36氪",
            "content": "2025年6月，人工智能（AI）护理解决方案提供商Guidehealth再获1000万美元战略融资，此轮融资由<strong>Memorial Hermann Health System牵头投资</strong>。 · 这笔资金将用于加速公司下一阶段发展，同时强化在转变患者护理模式、推进疾病预防及扩大...",
            "url": "https://36kr.com/p/3421931630431625",
            "source": "36kr.com",
            "published_date": "5 days ago",
            "search_engine": "Brave",
            "query": "医疗AI初创公司 最新融资 千万美元 最近7天 health tech startup funding million dollar",
            "language": "zh",
            "family_friendly": true,
            "category": "investment_news",
            "search_source": "brave",
            "full_content": "2025年6月，人工智能（AI）护理解决方案提供商Guidehealth再获1000万美元战略融资，此轮融资由Memorial Hermann Health System牵头投资。\n这笔资金将用于加速公司下一阶段发展，同时强化在转变患者护理模式、推进疾病预防及扩大高质量价值导向的医疗保健服务可及性等方面的实践。此次合作是Guidehealth与Memorial Hermann Health System携手开展，双方重点聚焦于改善乔治亚州数十万人口的初级保健服务与人口健康管理。\n这家创立于2023年的年轻公司，成长速度令人瞩目：2024年便完成1400万美元种子轮融资，同样由Memorial Hermann Health System牵头，医疗保健企业家Sidd Pagadipati等共同参与，所筹资金用于技术研发及优化此前收购的Arcadia托管服务组织，为业务扩张铺路。\n成立仅两年便斩获两轮融资，总融资达2400万美元，背后是资本市场对其商业模式与发展前景的高度认可。这家快速崛起的企业，究竟凭借什么赢得资本青睐？\n价值医疗（Value-Based care, VBC）是一种医疗模式，概念由美国哈佛大学商学院管理学教授迈克尔・波特（Michael Porter）在2006年出版的《Redefining Health Care Creating Value-Based Competition on Results》中提出，核心理念在于强调医疗质量和医疗服务的性价比，力求用合理成本取得尽可能大的医疗健康效益。\n当前，医疗领域存在诸多突出问题——医疗机构普遍面临成本高、人员紧张、管理负担重、医疗服务不连贯等难题，不仅影响治疗效果，还增加了不必要的医疗支出。\n据世界卫生组织（WHO）统计，全球约有10亿人无法获得基本的医疗服务，医疗资源分布不均导致许多患者难以享受优质医疗。与此同时，医疗成本持续攀升，仅在美国，根据美国医学会研究，美国医疗系统每年因低效操作和过度医疗导致的浪费高达6000亿至1.9万亿美元。\n这样的严峻形势下，传统医疗模式亟待变革，Guidehealth也因此应运而生。公司由护理初创公司Upstream的前首席执行官Sanjay Doddamani和Arcadia的前首席战略和创新官Michael Gleeson联合创立。其以解决医疗服务领域未被满足的临床需求为目标，开发了一个由人工智能提供支持的数字医疗保健平台，为医疗系统和临床网络提供护理支持。\n具体来看，Guidehealth构建了以“精准预测、高效连接、深度参与、持续优化”为核心的技术平台，通过多项前沿技术的融合应用，实现价值医疗的落地。\n\n用同理心+人工智能引导的全流程技术平台来自：Guidehealth官网\n传统医疗中，医护人员常陷入“被动应对”的困境：患者病情恶化到出现明显症状时才来就诊，此时干预难度已大幅增加，不仅治疗效果打折扣，医疗成本也居高不下。例如，高血压患者可能因连续几周血压波动未被察觉而突发脑卒中，慢阻肺患者可能因忽略“咳嗽频率增加” 的细微变化而发展为呼吸衰竭急诊入院。这些本可预防的健康事件，既让患者承受额外痛苦，也让医疗系统耗费大量资源在“补救性治疗”上。\nGuidehealth则通过个性化AI技术打破这种困局。具体来看，系统不仅整合患者的基础医疗数据，如过往病史、诊断记录、用药清单，还纳入了更细致的动态信息，如生活习惯、基因数据等，这些多维度数据构成了患者的健康全息画像，避免了单一数据导致的判断误差，以此精准识别高风险人群。\n此外，不同于一刀切的通用模型，Guidehealth的AI会针对特定疾病和特定人群进行专项数据训练，更精准捕捉疾病恶化的独特信号。这种精准性既能针对性评估慢性疾病患者的恶化风险并提前提醒干预，也能基于细分数据实现更科学的风险分层，最终帮助医疗机构合理分配资源、减少可避免的健康问题。\n传统医疗链条中，“脱节”是普遍存在的痛点：患者出院后易遗忘注意事项，慢性病患者因不知如何复诊而拖延，独居老人不适时难以及时咨询医生。这些断裂点常导致患者依从性下降、病情反复，甚至引发本可避免的急症，而这正是价值医疗的服务可及性与人文关怀需要填补的空白。\nGuidehealth通过虚拟嵌入的Healthguides技术，将被动等待转为主动触达。Healthguides充当初级保健医生（PCP）的延伸，会根据AI预测的风险信号、患者的诊疗节点主动出击，在患者需要干预前建立联系并融入其所在社区，通过线上沟通收集日常健康信息、建立信任关系，为后续医疗服务奠定基础。\n此外，Healthguides可实时反馈患者的健康动态，帮助医生制定更贴合个体需求的护理计划，解决传统医疗中服务与需求脱节的问题，这种持续连接最终转化为实实在在的健康价值。\n慢性病管理中，患者常因多机构护理计划混乱而陷入困境：可能出现看完心内科医生到手一张“低盐低脂、每日运动30分钟”的医嘱，转身在营养科又收到“需增加优质蛋白摄入”的建议，再到康复科可能被告知“近期不宜剧烈运动”。这些来自多科室的护理计划看似专业，却因表述差异、建议冲突让患者无所适从。\n更糟的是，医生往往因时间有限难以逐字解释，患者只能凭感觉执行，最终导致治疗效果打折。这种由信息混乱导致的低依从性，不仅让治疗效果大打折扣，更让价值医疗中患者主动参与的核心目标难以落地。\nGuidehealth正是通过大语言模型（LLMs）与生成式AI的协同，打破这种信息壁垒。LLMs会自动抓取多机构电子医嘱，识别冲突点后结合临床指南整合为无冲突的清单，并将复杂计划转化为易懂内容，方便患者及家属理解；生成式AI则根据患者年龄、职业、生活习惯生成个性化科普内容，助力患者认知自身疾病与护理要求。\nHealthguides借助这些技术，与患者家属紧密协作，共同监控护理情况和健康状态，从被动治疗转向主动预防，减少不良事件发生。当患者能看懂医嘱、认同计划、主动执行，医疗服务才能从医生单方面付出变成医患共同努力，而这正是提升质量、降低成本的关键。\n传统医疗中，效率低下如同隐形的枷锁，既困住了医护人员的精力，也拖慢了患者获得优质服务的速度。医生接诊时需在多个系统间切换查询数据，护士手动整理随访清单易漏记，多科室会诊因病历信息不同步而低效。这些琐碎的低效最终转化为患者的等待时间、重复检查与医疗系统的资源浪费。\nGuidehealth则通过相关技术优化流程并整合数据，让医护人员从机械劳动中解放，把时间还给患者，让数据从分散的孤岛变成可用的资源，让决策更精准、服务更顺畅。\n其中包括两大技术：\nAI原生工作流程技术：将Healthguides与AI驱动的工作流程结合，在关键节点自动汇总患者预测数据并处理海量信息，为医护人员提供决策支持。例如，患者就诊时，系统可基于AI分析自动推荐检查项目和治疗方案，减少人工操作，降低出错率，提升服务效率。\n电子健康记录（EHR）集成技术：实现与电子健康记录系统的无缝对接，让医疗团队快速获取患者的完整健康档案（包括病史、检查结果、治疗记录等），基于全面信息做出更精准的诊断和治疗决策，避免重复检查和错误用药，保障医疗安全。\n通过上述技术的协同应用，Guidehealth的平台不仅打通了“预测—连接—参与—优化”的全流程，更推动了患者、医护人员、医疗系统的高效联动，最终实现对患者健康、群体健康及医疗财务成果的正向影响，让价值医疗从理念转化为可落地的实践。\n在合作与业务拓展上，Guidehealth自成立开始便积极地开展业务布局，2023年便收购了医疗保健技术公司Arcadia的托管服务组织（Management Services Organization）和基于价值的护理服务部门。通过此次收购，Guidehealth获得了简化就诊访问和转诊、事先授权管理等关键工具，不仅在转诊和利用管理方面具备显著优势，还能帮助卫生系统增加患者黏性。\n同时，双方签署技术协议，Guidehealth得以借助Arcadia强大的数据分析平台为新收购业务提供支撑，这一举措大幅增强了其数据管理与分析能力，为服务客户奠定了更坚实的技术基础。\n次年，Guidehealth与宾夕法尼亚临床网络达成合作，在宾夕法尼亚州引入AI解决方案，通过技术赋能提升当地医疗服务效果与质量。\n2024年7月，公司进一步拓展业务边界。Guidehealth与虚拟专业护理提供商Story Health达成合作，为卫生系统和临床整合网络（CIN）启动心脏病护理计划，覆盖心力衰竭、高血压等心血管疾病患者。这一合作不仅标志着公司从慢性病管理向专科医疗的延伸，更验证了其技术平台的跨病种适配能力。\n从成立之初的战略收购，到与多方伙伴的协同布局，再到以AI为核心的技术平台落地。如今，这家年轻的企业已构建起覆盖数据管理、AI应用、专科护理的业务生态，服务超50万患者，既为医疗系统提供了降本增效的工具，也让患者在更精准、更贴心的服务中获得了对健康的掌控感。\n卫生经济学家将“价值医疗”定义为“最高性价比的医疗”。未来，随着AI技术的迭代与合作的深化，Guidehealth或将持续推动价值医疗从理念走向更广泛的实践。\n本文来自微信公众号“动脉网”（ID：vcbeat），作者：陈茂雨，36氪经授权发布。\n该文观点仅代表作者本人，36氪平台仅提供信息存储空间服务。\n好文章，需要你的鼓励\n服务未来医疗，公众号：vcbeat，关注可获更多最新好文~\n桥水二季度布局曝光\n2025-08-14\n推送和解读前沿、有料的科技创投资讯\n一级市场金融信息和系统服务提供商\n聚焦全球优秀创业者，项目融资率接近97%，领跑行业"
        },
        {
            "title": "2024年AI新贵：揭秘融资超1亿美元的初创公司 - BDOS 大数据操作系统",
            "content": "本文由智领云 LeetTools工具自动生成 · 如果您想试用，请点击链接：https://www.leettools.com/feedback/",
            "url": "https://www.linktimecloud.com/posts/8101",
            "source": "www.linktimecloud.com",
            "published_date": "September 30, 2024",
            "search_engine": "Brave",
            "query": "医疗AI初创公司 最新融资 千万美元 最近7天 health tech startup funding million dollar",
            "language": "zh",
            "family_friendly": true,
            "category": "investment_news",
            "search_source": "brave",
            "full_content": "本文由智领云 LeetTools工具自动生成 · 如果您想试用，请点击链接：https://www.leettools.com/feedback/"
        }
    ],
    "policy_news": [
        {
            "title": "UMED's World-First Automated Intermittent Bladder Irrigation System ...",
            "content": "7 days ago ... The study, conducted in collaboration with the medical team at the University of California San Diego (UCSD), demonstrated that UroRinse™ could be an effective ...",
            "url": "https://www.farmerscoopelevator.com/markets/stocks.php?article=getnews-2025-8-21-umeds-world-first-automated-intermittent-bladder-irrigation-system-urorinsetm-featured-in-significant-international-journal-following-successful-preclinical-study",
            "source": "www.farmerscoopelevator.com",
            "published_date": "2025-08-28",
            "search_engine": "Google",
            "query": "FDA 人工智能医疗设备 最新审批政策 2025年 regulatory update AI diagnostic device approval",
            "description": "7 days ago ... The study, conducted in collaboration with the medical team at the University of California San Diego (UCSD), demonstrated that UroRinse™ could be an effective ...",
            "image": "",
            "category": "policy_news",
            "search_source": "google",
            "full_content": "Farmers Cooperative Elevator Company Zero Tolerance Policy\nThe U.S. Food and Drug Administration and the U.S. Grain Standards Act allow ZERO TOLERANCE for treated seed occurring in grain.\nFCE wants to remind you to thoroughly clean all equipment that comes in contact with treated seed. Be sure to check and recheck to make sure it’s clean. One kernel of treated seed, whether it is corn, soybeans, or wheat, can contaminate an entire bin and can cost FCE millions of dollars. If you are checked and have treated kernels, by law, you may have to pay for the grain quantity that was tainted.\n© 2025 Barchart.com, Inc.  All market data is hosted and powered by Barchart.\nInformation presented is provided 'as-is' and solely for informational purposes, not for trading purposes or advice, and is delayed. No representations are made by Barchart as to its informational accuracy or completeness."
        },
        {
            "title": "Regulatory responses and approval status of artificial intelligence medical devices with a focus on China - PMC",
            "content": "This paper focuses on how <strong>regulatory</strong> bodies respond to artificial intelligence (<strong>AI</strong>)-enabled medical <strong>devices</strong>. To achieve this, we present a comparative overview of the United States (USA), European Union (EU), and China. Our search in the ...",
            "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC11410966/",
            "source": "pmc.ncbi.nlm.nih.gov",
            "published_date": "",
            "search_engine": "Brave",
            "query": "FDA 人工智能医疗设备 最新审批政策 2025年 regulatory update AI diagnostic device approval",
            "language": "en",
            "family_friendly": true,
            "category": "policy_news",
            "search_source": "brave",
            "full_content": "This paper focuses on how <strong>regulatory</strong> bodies respond to artificial intelligence (<strong>AI</strong>)-enabled medical <strong>devices</strong>. To achieve this, we present a comparative overview of the United States (USA), European Union (EU), and China. Our search in the ..."
        },
        {
            "title": "FDA近日发布《针对人工智能医疗器械开发商的综合指导草案》指南",
            "content": "检测到您的浏览器版本过低，可能导致某些功能无法正常使用，建议升级您的浏览器，或使用推荐浏览器 Google Chrome 、Edge、Firefox 。 X · <strong>AI</strong>（人工智能）赋能医疗器械产业近年来迎来发展爆发，据市场研究数据显示，2023年...",
            "url": "https://www.istis.sh.cn/cms/news/article/53/27328",
            "source": "www.istis.sh.cn",
            "published_date": "",
            "search_engine": "Brave",
            "query": "FDA 人工智能医疗设备 最新审批政策 2025年 regulatory update AI diagnostic device approval",
            "language": "zh",
            "family_friendly": true,
            "category": "policy_news",
            "search_source": "brave",
            "full_content": "检测到您的浏览器版本过低，可能导致某些功能无法正常使用，建议升级您的浏览器，或使用推荐浏览器 Google Chrome 、Edge、Firefox 。 X · <strong>AI</strong>（人工智能）赋能医疗器械产业近年来迎来发展爆发，据市场研究数据显示，2023年..."
        },
        {
            "title": "Artificial Intelligence Program: Research on AI/ML-Based Medical Devices | FDA",
            "content": "The <strong>AI</strong> Program conducts research to help ensure patient access to innovative medical <strong>devices</strong> made using <strong>AI</strong>.",
            "url": "https://www.fda.gov/medical-devices/medical-device-regulatory-science-research-programs-conducted-osel/artificial-intelligence-program-research-aiml-based-medical-devices",
            "source": "www.fda.gov",
            "published_date": "",
            "search_engine": "Brave",
            "query": "FDA 人工智能医疗设备 最新审批政策 2025年 regulatory update AI diagnostic device approval",
            "language": "en",
            "family_friendly": true,
            "category": "policy_news",
            "search_source": "brave",
            "full_content": "The <strong>AI</strong> Program conducts research to help ensure patient access to innovative medical <strong>devices</strong> made using <strong>AI</strong>."
        },
        {
            "title": "Clinical Evaluation of AI-assisted Diagnostic Medical Device Software in China – A new NMPA guidance",
            "content": "The <strong>new</strong> <strong>NMPA</strong> guidance CMDE 2023 No. 38 outlines the agency’s expectations on the <strong>clinical</strong> evaluation of <strong>AI</strong>-assisted diagnostic medical device software in China. It includes recommendations on <strong>clinical</strong> trial design, study subjects, evaluation metrics, <strong>clinical</strong> reference, sample size and statistics ...",
            "url": "https://www.qservegroup.com/eu/en/b1545/clinical-evaluation-of-ai-assisted-diagnostic-medical-device-software-in-china--a-new-nmpa-guidance",
            "source": "www.qservegroup.com",
            "published_date": "",
            "search_engine": "Brave",
            "query": "中国 人工智能医疗诊断 最新监管政策 2025年 NMPA new regulation AI clinical decision support",
            "language": "nl",
            "family_friendly": true,
            "category": "policy_news",
            "search_source": "brave",
            "full_content": "\nOur expertise on China market access will accelerate your time to market. \n\n \nAs an MD or IVD manufacturer, entering the Chinese medical device market requires navigating stringent NMPA regulations, including device classification (Class I–III), local product testing, potential clinical trials, and the mandatory appointment of a China Agent for registration, surveillance, and adverse event reporting. Imported devices face significant review timelines (1–2 years for Class II/III) and require renewal management every five years. Class I device approval is faster but complicated.\nOur team combines in-depth knowledge of NMPA regulatory pathways, local test houses, and agent requirements to streamline your China market entry. We manage everything while serving as your trusted China Agent to ensure fast, effective communication and reduce operational burden.\nWith our multilingual regulatory experts based in Nanjing and Bejing, our consultants have helped many venture startups and multiple national corporations in China and abroad with China Medical Device Regulations successfully obtain regulatory approval with the NMPA registration process since 2011.\nMultilingual\nRegulatory pathway classification, technical dossier preparation, local product testing, clinical evaluation reports, or clinical-exemption pathways.\nWe submit registrations, handle post-market surveillance and adverse event reporting, liaise with the NMPA, and maintain your legal credentials on official documents.\n•\tMedical device governing body\n•\tClassification\n•\tRegistration and filing\n•\tDomestic and imported devices\n•\tIn-country representation\n•\tCertificate validity and renewal\n•\tQMS requirements\nClarify the gaps between China's requirements and other countries. There are several fast-track routes within the NMPA program; we help you determine applicability.\nWhy is China an attractive market for medical devices and in vitro diagnostics? \nDue to the large base population and growing aging population, China is a compelling healthcare market and presents numerous opportunities for foreign companies. Although Europe and the US are the two biggest markets, China has become a new force with a rising growth rate. Compared with the development of pharmaceuticals and the market share of imported drugs, the medical device sectors still have enormous potential for development. In particular, the products with high technical barriers or innovative devices have no replaceable domestic products.\nWhat are the significant challenges for foreign applicants to enter the Chinese market?\nUnderstanding the practicalities of accessing the Chinese market is key. The regulatory environment is complex and dynamic because of the ongoing introduction of new/revised requirements. Devices with artificial intelligence are at the forefront of cybersecurity and data protection and are thoroughly monitored. Also, the language barrier and the cultural differences play a role and require knowledge and support. The overall business acumen requires you to have well-established relationships with local parties.  \nWhy is it essential to establish a global strategy early on for entering multiple markets?\nAlthough you’re not planning to enter different markets at once, if you consider them in your global strategy, this will be beneficial afterward. If you know the requirements from the start, you don’t have to start over with the design process at a later stage. And you made an efficient move by properly organizing comparable processes, which are similar in other countries.\nQserve helps build a customized global approach and creates an efficient manufacturer roadmap. The roadmap includes the global registration strategy, which considers country-specific regulations/guidelines/ standards in the early product R&D phase, and the expedited route, which leverages reference country technical documentation and approved certificates. It also addresses the type-testing requirements and clinical study plan, with a reasonable number of country-based subjects to justify the race or clinical practical differences.\nWhat is the role of the NMPA (National Medical Products Administration) in China?\nIn China, the National Medical Products Administration (NMPA) is crucial in overseeing and granting licenses for pharmaceuticals, medical devices, in vitro diagnostics, and cosmetics. Here's an overview of the key responsibilities undertaken by the NMPA:\nSafety Supervision: The NMPA ensures the safety of drugs, medical devices, and cosmetics. This involves devising regulatory policies, laws, and norms, and monitoring their implementation. Additionally, the NMPA researches to develop supportive policies that foster the advancement of new technologies and products in these sectors.\nStandards Management: The administration manages drug, medical device, and cosmetic standards. It oversees the development of the Chinese Pharmacopoeia and other relevant standards. The NMPA also establishes classification systems and ensures adherence to these standards.\nRegistration Regulation: The NMPA regulates the registration process for drugs, medical devices, and cosmetics. This includes designing registration management systems, conducting technical reviews for certification, streamlining approval processes, and overseeing implementation.\nQuality Management: The NMPA prioritizes quality assurance for drugs, medical devices, and cosmetics. It sets guidelines for Good Laboratory Practices (GLP), Good Clinical Practices (GCP), Good Manufacturing Practices (GMP), and Good Distribution Practices (GDP) and provides guidance for their implementation.\nPost-Market Risk Management: The NMPA manages risks associated with drugs, medical devices, and cosmetics after they enter the market. This involves monitoring, evaluating, and addressing adverse events. In cases of emergencies, the NMPA implements appropriate measures under the law. Supervision and Inspection: The administration organizes and guides the supervision and inspection of drugs, medical devices, and cosmetics. It establishes inspection protocols and investigates and acts against illegal activities during registration and manufacturing processes.\nInternational Collaboration: The NMPA engages in international exchange and cooperation regarding the regulation of drugs, medical devices, and cosmetics. It actively participates in developing international regulatory rules and standards, ensuring alignment with global best practices.\nYour Global MedTech Partner for Regulatory Affairs,\nQuality Assurance and Clinical Trials.\nAddress HQ\nArnhems Business Park Utrechtseweg 310 Bldg B42 6812 AR Arnhem\nGet in touch to discuss your needs and discover how we can help you achieve your goals efficiently and compliantly.\nService Areas\nSolutions\nExpertise\nGlobal Registrations\n© 2025 Qserve. All rights reserved. "
        },
        {
            "title": "国务院关于印发新一代人工智能发展规划的通知_科技_中国政府网",
            "content": "首页 · 登录 · 个人中心 · 退出 · 邮箱 · 无障碍 · https://www.gov.cn/ · 2017-07-20 16:53:00 · 首页 &gt; 信息公开 &gt; 国务院文件 &gt; 科技、教育 &gt; 科技 · 字号：默认 超大 | 打印",
            "url": "https://www.gov.cn/zhengce/content/2017-07/20/content_5211996.htm",
            "source": "www.gov.cn",
            "published_date": "",
            "search_engine": "Brave",
            "query": "中国 人工智能医疗诊断 最新监管政策 2025年 NMPA new regulation AI clinical decision support",
            "language": "zh",
            "family_friendly": true,
            "category": "policy_news",
            "search_source": "brave",
            "full_content": "000014349/2017-00142\n科技、教育\\科技\n国务院\n2017年07月08日\n国务院关于印发新一代人工智能发展规划的通知\n国发〔2017〕35号\n2017年07月20日\n国务院关于印发\n新一代人工智能发展规划的通知\n国发〔2017〕35号\n\n\n\n各省、自治区、直辖市人民政府，国务院各部委、各直属机构：\n现将《新一代人工智能发展规划》印发给你们，请认真贯彻执行。\n国务院　　　　　　　　\n\n2017年7月8日　　　　　　\n（此件公开发布）\n\n\n\n\n新一代人工智能发展规划\n\n\n人工智能的迅速发展将深刻改变人类社会生活、改变世界。为抢抓人工智能发展的重大战略机遇，构筑我国人工智能发展的先发优势，加快建设创新型国家和世界科技强国，按照党中央、国务院部署要求，制定本规划。\n一、战略态势\n人工智能发展进入新阶段。经过60多年的演进，特别是在移动互联网、大数据、超级计算、传感网、脑科学等新理论新技术以及经济社会发展强烈需求的共同驱动下，人工智能加速发展，呈现出深度学习、跨界融合、人机协同、群智开放、自主操控等新特征。大数据驱动知识学习、跨媒体协同处理、人机协同增强智能、群体集成智能、自主智能系统成为人工智能的发展重点，受脑科学研究成果启发的类脑智能蓄势待发，芯片化硬件化平台化趋势更加明显，人工智能发展进入新阶段。当前，新一代人工智能相关学科发展、理论建模、技术创新、软硬件升级等整体推进，正在引发链式突破，推动经济社会各领域从数字化、网络化向智能化加速跃升。\n人工智能成为国际竞争的新焦点。人工智能是引领未来的战略性技术，世界主要发达国家把发展人工智能作为提升国家竞争力、维护国家安全的重大战略，加紧出台规划和政策，围绕核心技术、顶尖人才、标准规范等强化部署，力图在新一轮国际科技竞争中掌握主导权。当前，我国国家安全和国际竞争形势更加复杂，必须放眼全球，把人工智能发展放在国家战略层面系统布局、主动谋划，牢牢把握人工智能发展新阶段国际竞争的战略主动，打造竞争新优势、开拓发展新空间，有效保障国家安全。\n人工智能成为经济发展的新引擎。人工智能作为新一轮产业变革的核心驱动力，将进一步释放历次科技革命和产业变革积蓄的巨大能量，并创造新的强大引擎，重构生产、分配、交换、消费等经济活动各环节，形成从宏观到微观各领域的智能化新需求，催生新技术、新产品、新产业、新业态、新模式，引发经济结构重大变革，深刻改变人类生产生活方式和思维模式，实现社会生产力的整体跃升。我国经济发展进入新常态，深化供给侧结构性改革任务非常艰巨，必须加快人工智能深度应用，培育壮大人工智能产业，为我国经济发展注入新动能。\n人工智能带来社会建设的新机遇。我国正处于全面建成小康社会的决胜阶段，人口老龄化、资源环境约束等挑战依然严峻，人工智能在教育、医疗、养老、环境保护、城市运行、司法服务等领域广泛应用，将极大提高公共服务精准化水平，全面提升人民生活品质。人工智能技术可准确感知、预测、预警基础设施和社会安全运行的重大态势，及时把握群体认知及心理变化，主动决策反应，将显著提高社会治理的能力和水平，对有效维护社会稳定具有不可替代的作用。\n人工智能发展的不确定性带来新挑战。人工智能是影响面广的颠覆性技术，可能带来改变就业结构、冲击法律与社会伦理、侵犯个人隐私、挑战国际关系准则等问题，将对政府管理、经济安全和社会稳定乃至全球治理产生深远影响。在大力发展人工智能的同时，必须高度重视可能带来的安全风险挑战，加强前瞻预防与约束引导，最大限度降低风险，确保人工智能安全、可靠、可控发展。\n我国发展人工智能具有良好基础。国家部署了智能制造等国家重点研发计划重点专项，印发实施了“互联网+”人工智能三年行动实施方案，从科技研发、应用推广和产业发展等方面提出了一系列措施。经过多年的持续积累，我国在人工智能领域取得重要进展，国际科技论文发表量和发明专利授权量已居世界第二，部分领域核心关键技术实现重要突破。语音识别、视觉识别技术世界领先，自适应自主学习、直觉感知、综合推理、混合智能和群体智能等初步具备跨越发展的能力，中文信息处理、智能监控、生物特征识别、工业机器人、服务机器人、无人驾驶逐步进入实际应用，人工智能创新创业日益活跃，一批龙头骨干企业加速成长，在国际上获得广泛关注和认可。加速积累的技术能力与海量的数据资源、巨大的应用需求、开放的市场环境有机结合，形成了我国人工智能发展的独特优势。\n同时，也要清醒地看到，我国人工智能整体发展水平与发达国家相比仍存在差距，缺少重大原创成果，在基础理论、核心算法以及关键设备、高端芯片、重大产品与系统、基础材料、元器件、软件与接口等方面差距较大；科研机构和企业尚未形成具有国际影响力的生态圈和产业链，缺乏系统的超前研发布局；人工智能尖端人才远远不能满足需求；适应人工智能发展的基础设施、政策法规、标准体系亟待完善。\n面对新形势新需求，必须主动求变应变，牢牢把握人工智能发展的重大历史机遇，紧扣发展、研判大势、主动谋划、把握方向、抢占先机，引领世界人工智能发展新潮流，服务经济社会发展和支撑国家安全，带动国家竞争力整体跃升和跨越式发展。\n二、总体要求\n（一）指导思想。\n全面贯彻党的十八大和十八届三中、四中、五中、六中全会精神，深入学习贯彻习近平总书记系列重要讲话精神和治国理政新理念新思想新战略，按照“五位一体”总体布局和“四个全面”战略布局，认真落实党中央、国务院决策部署，深入实施创新驱动发展战略，以加快人工智能与经济、社会、国防深度融合为主线，以提升新一代人工智能科技创新能力为主攻方向，发展智能经济，建设智能社会，维护国家安全，构筑知识群、技术群、产业群互动融合和人才、制度、文化相互支撑的生态系统，前瞻应对风险挑战，推动以人类可持续发展为中心的智能化，全面提升社会生产力、综合国力和国家竞争力，为加快建设创新型国家和世界科技强国、实现“两个一百年”奋斗目标和中华民族伟大复兴中国梦提供强大支撑。\n（二）基本原则。\n科技引领。把握世界人工智能发展趋势，突出研发部署前瞻性，在重点前沿领域探索布局、长期支持，力争在理论、方法、工具、系统等方面取得变革性、颠覆性突破，全面增强人工智能原始创新能力，加速构筑先发优势，实现高端引领发展。\n系统布局。根据基础研究、技术研发、产业发展和行业应用的不同特点，制定有针对性的系统发展策略。充分发挥社会主义制度集中力量办大事的优势，推进项目、基地、人才统筹布局，已部署的重大项目与新任务有机衔接，当前急需与长远发展梯次接续，创新能力建设、体制机制改革和政策环境营造协同发力。\n市场主导。遵循市场规律，坚持应用导向，突出企业在技术路线选择和行业产品标准制定中的主体作用，加快人工智能科技成果商业化应用，形成竞争优势。把握好政府和市场分工，更好发挥政府在规划引导、政策支持、安全防范、市场监管、环境营造、伦理法规制定等方面的重要作用。\n开源开放。倡导开源共享理念，促进产学研用各创新主体共创共享。遵循经济建设和国防建设协调发展规律，促进军民科技成果双向转化应用、军民创新资源共建共享，形成全要素、多领域、高效益的军民深度融合发展新格局。积极参与人工智能全球研发和治理，在全球范围内优化配置创新资源。\n（三）战略目标。\n分三步走：\n第一步，到2020年人工智能总体技术和应用与世界先进水平同步，人工智能产业成为新的重要经济增长点，人工智能技术应用成为改善民生的新途径，有力支撑进入创新型国家行列和实现全面建成小康社会的奋斗目标。\n——新一代人工智能理论和技术取得重要进展。大数据智能、跨媒体智能、群体智能、混合增强智能、自主智能系统等基础理论和核心技术实现重要进展，人工智能模型方法、核心器件、高端设备和基础软件等方面取得标志性成果。\n——人工智能产业竞争力进入国际第一方阵。初步建成人工智能技术标准、服务体系和产业生态链，培育若干全球领先的人工智能骨干企业，人工智能核心产业规模超过1500亿元，带动相关产业规模超过1万亿元。\n——人工智能发展环境进一步优化，在重点领域全面展开创新应用，聚集起一批高水平的人才队伍和创新团队，部分领域的人工智能伦理规范和政策法规初步建立。\n第二步，到2025年人工智能基础理论实现重大突破，部分技术与应用达到世界领先水平，人工智能成为带动我国产业升级和经济转型的主要动力，智能社会建设取得积极进展。\n——新一代人工智能理论与技术体系初步建立，具有自主学习能力的人工智能取得突破，在多领域取得引领性研究成果。\n——人工智能产业进入全球价值链高端。新一代人工智能在智能制造、智能医疗、智慧城市、智能农业、国防建设等领域得到广泛应用，人工智能核心产业规模超过4000亿元，带动相关产业规模超过5万亿元。\n——初步建立人工智能法律法规、伦理规范和政策体系，形成人工智能安全评估和管控能力。\n第三步，到2030年人工智能理论、技术与应用总体达到世界领先水平，成为世界主要人工智能创新中心，智能经济、智能社会取得明显成效，为跻身创新型国家前列和经济强国奠定重要基础。\n——形成较为成熟的新一代人工智能理论与技术体系。在类脑智能、自主智能、混合智能和群体智能等领域取得重大突破，在国际人工智能研究领域具有重要影响，占据人工智能科技制高点。\n——人工智能产业竞争力达到国际领先水平。人工智能在生产生活、社会治理、国防建设各方面应用的广度深度极大拓展，形成涵盖核心技术、关键系统、支撑平台和智能应用的完备产业链和高端产业群，人工智能核心产业规模超过1万亿元，带动相关产业规模超过10万亿元。\n——形成一批全球领先的人工智能科技创新和人才培养基地，建成更加完善的人工智能法律法规、伦理规范和政策体系。\n（四）总体部署。\n发展人工智能是一项事关全局的复杂系统工程，要按照“构建一个体系、把握双重属性、坚持三位一体、强化四大支撑”进行布局，形成人工智能健康持续发展的战略路径。\n构建开放协同的人工智能科技创新体系。针对原创性理论基础薄弱、重大产品和系统缺失等重点难点问题，建立新一代人工智能基础理论和关键共性技术体系，布局建设重大科技创新基地，壮大人工智能高端人才队伍，促进创新主体协同互动，形成人工智能持续创新能力。\n把握人工智能技术属性和社会属性高度融合的特征。既要加大人工智能研发和应用力度，最大程度发挥人工智能潜力；又要预判人工智能的挑战，协调产业政策、创新政策与社会政策，实现激励发展与合理规制的协调，最大限度防范风险。\n坚持人工智能研发攻关、产品应用和产业培育“三位一体”推进。适应人工智能发展特点和趋势，强化创新链和产业链深度融合、技术供给和市场需求互动演进，以技术突破推动领域应用和产业升级，以应用示范推动技术和系统优化。在当前大规模推动技术应用和产业发展的同时，加强面向中长期的研发布局和攻关，实现滚动发展和持续提升，确保理论上走在前面、技术上占领制高点、应用上安全可控。\n全面支撑科技、经济、社会发展和国家安全。以人工智能技术突破带动国家创新能力全面提升，引领建设世界科技强国进程；通过壮大智能产业、培育智能经济，为我国未来十几年乃至几十年经济繁荣创造一个新的增长周期；以建设智能社会促进民生福祉改善，落实以人民为中心的发展思想；以人工智能提升国防实力，保障和维护国家安全。\n三、重点任务\n立足国家发展全局，准确把握全球人工智能发展态势，找准突破口和主攻方向，全面增强科技创新基础能力，全面拓展重点领域应用深度广度，全面提升经济社会发展和国防应用智能化水平。\n（一）构建开放协同的人工智能科技创新体系。\n围绕增加人工智能创新的源头供给，从前沿基础理论、关键共性技术、基础平台、人才队伍等方面强化部署，促进开源共享，系统提升持续创新能力，确保我国人工智能科技水平跻身世界前列，为世界人工智能发展作出更多贡献。\n1.建立新一代人工智能基础理论体系。\n聚焦人工智能重大科学前沿问题，兼顾当前需求与长远发展，以突破人工智能应用基础理论瓶颈为重点，超前布局可能引发人工智能范式变革的基础研究，促进学科交叉融合，为人工智能持续发展与深度应用提供强大科学储备。\n突破应用基础理论瓶颈。瞄准应用目标明确、有望引领人工智能技术升级的基础理论方向，加强大数据智能、跨媒体感知计算、人机混合智能、群体智能、自主协同与决策等基础理论研究。大数据智能理论重点突破无监督学习、综合深度推理等难点问题，建立数据驱动、以自然语言理解为核心的认知计算模型，形成从大数据到知识、从知识到决策的能力。跨媒体感知计算理论重点突破低成本低能耗智能感知、复杂场景主动感知、自然环境听觉与言语感知、多媒体自主学习等理论方法，实现超人感知和高动态、高维度、多模式分布式大场景感知。混合增强智能理论重点突破人机协同共融的情境理解与决策学习、直觉推理与因果模型、记忆与知识演化等理论，实现学习与思考接近或超过人类智能水平的混合增强智能。群体智能理论重点突破群体智能的组织、涌现、学习的理论与方法，建立可表达、可计算的群智激励算法和模型，形成基于互联网的群体智能理论体系。自主协同控制与优化决策理论重点突破面向自主无人系统的协同感知与交互、自主协同控制与优化决策、知识驱动的人机物三元协同与互操作等理论，形成自主智能无人系统创新性理论体系架构。\n布局前沿基础理论研究。针对可能引发人工智能范式变革的方向，前瞻布局高级机器学习、类脑智能计算、量子智能计算等跨领域基础理论研究。高级机器学习理论重点突破自适应学习、自主学习等理论方法，实现具备高可解释性、强泛化能力的人工智能。类脑智能计算理论重点突破类脑的信息编码、处理、记忆、学习与推理理论，形成类脑复杂系统及类脑控制等理论与方法，建立大规模类脑智能计算的新模型和脑启发的认知计算模型。量子智能计算理论重点突破量子加速的机器学习方法，建立高性能计算与量子算法混合模型，形成高效精确自主的量子人工智能系统架构。\n开展跨学科探索性研究。推动人工智能与神经科学、认知科学、量子科学、心理学、数学、经济学、社会学等相关基础学科的交叉融合，加强引领人工智能算法、模型发展的数学基础理论研究，重视人工智能法律伦理的基础理论问题研究，支持原创性强、非共识的探索性研究，鼓励科学家自由探索，勇于攻克人工智能前沿科学难题，提出更多原创理论，作出更多原创发现。\n\n\n\n\n专栏1　基础理论\n1.大数据智能理论。研究数据驱动与知识引导相结合的人工智能新方法、以自然语言理解和图像图形为核心的认知计算理论和方法、综合深度推理与创意人工智能理论与方法、非完全信息下智能决策基础理论与框架、数据驱动的通用人工智能数学模型与理论等。\n2.跨媒体感知计算理论。研究超越人类视觉能力的感知获取、面向真实世界的主动视觉感知及计算、自然声学场景的听知觉感知及计算、自然交互环境的言语感知及计算、面向异步序列的类人感知及计算、面向媒体智能感知的自主学习、城市全维度智能感知推理引擎。\n3.混合增强智能理论。研究“人在回路”的混合增强智能、人机智能共生的行为增强与脑机协同、机器直觉推理与因果模型、联想记忆模型与知识演化方法、复杂数据和任务的混合增强智能学习方法、云机器人协同计算方法、真实世界环境下的情境理解及人机群组协同。\n4.群体智能理论。研究群体智能结构理论与组织方法、群体智能激励机制与涌现机理、群体智能学习理论与方法、群体智能通用计算范式与模型。\n5.自主协同控制与优化决策理论。研究面向自主无人系统的协同感知与交互，面向自主无人系统的协同控制与优化决策，知识驱动的人机物三元协同与互操作等理论。\n6.高级机器学习理论。研究统计学习基础理论、不确定性推理与决策、分布式学习与交互、隐私保护学习、小样本学习、深度强化学习、无监督学习、半监督学习、主动学习等学习理论和高效模型。\n7.类脑智能计算理论。研究类脑感知、类脑学习、类脑记忆机制与计算融合、类脑复杂系统、类脑控制等理论与方法。\n8.量子智能计算理论。探索脑认知的量子模式与内在机制，研究高效的量子智能模型和算法、高性能高比特的量子人工智能处理器、可与外界环境交互信息的实时量子人工智能系统等。\n\n\n\n2.建立新一代人工智能关键共性技术体系。\n围绕提升我国人工智能国际竞争力的迫切需求，新一代人工智能关键共性技术的研发部署要以算法为核心，以数据和硬件为基础，以提升感知识别、知识计算、认知推理、运动执行、人机交互能力为重点，形成开放兼容、稳定成熟的技术体系。\n知识计算引擎与知识服务技术。重点突破知识加工、深度搜索和可视交互核心技术，实现对知识持续增量的自动获取，具备概念识别、实体发现、属性预测、知识演化建模和关系挖掘能力，形成涵盖数十亿实体规模的多源、多学科和多数据类型的跨媒体知识图谱。\n跨媒体分析推理技术。重点突破跨媒体统一表征、关联理解与知识挖掘、知识图谱构建与学习、知识演化与推理、智能描述与生成等技术，实现跨媒体知识表征、分析、挖掘、推理、演化和利用，构建分析推理引擎。\n群体智能关键技术。重点突破基于互联网的大众化协同、大规模协作的知识资源管理与开放式共享等技术，建立群智知识表示框架，实现基于群智感知的知识获取和开放动态环境下的群智融合与增强，支撑覆盖全国的千万级规模群体感知、协同与演化。\n混合增强智能新架构与新技术。重点突破人机协同的感知与执行一体化模型、智能计算前移的新型传感器件、通用混合计算架构等核心技术，构建自主适应环境的混合增强智能系统、人机群组混合增强智能系统及支撑环境。\n自主无人系统的智能技术。重点突破自主无人系统计算架构、复杂动态场景感知与理解、实时精准定位、面向复杂环境的适应性智能导航等共性技术，无人机自主控制以及汽车、船舶和轨道交通自动驾驶等智能技术，服务机器人、特种机器人等核心技术，支撑无人系统应用和产业发展。\n虚拟现实智能建模技术。重点突破虚拟对象智能行为建模技术，提升虚拟现实中智能对象行为的社会性、多样性和交互逼真性，实现虚拟现实、增强现实等技术与人工智能的有机结合和高效互动。\n智能计算芯片与系统。重点突破高能效、可重构类脑计算芯片和具有计算成像功能的类脑视觉传感器技术，研发具有自主学习能力的高效能类脑神经网络架构和硬件系统，实现具有多媒体感知信息理解和智能增长、常识推理能力的类脑智能系统。\n自然语言处理技术。重点突破自然语言的语法逻辑、字符概念表征和深度语义分析的核心技术，推进人类与机器的有效沟通和自由交互，实现多风格多语言多领域的自然语言智能理解和自动生成。\n\n\n\n\n专栏2　关键共性技术\n1.知识计算引擎与知识服务技术。研究知识计算和可视交互引擎，研究创新设计、数字创意和以可视媒体为核心的商业智能等知识服务技术，开展大规模生物数据的知识发现。\n2.跨媒体分析推理技术。研究跨媒体统一表征、关联理解与知识挖掘、知识图谱构建与学习、知识演化与推理、智能描述与生成等技术，开发跨媒体分析推理引擎与验证系统。\n3.群体智能关键技术。开展群体智能的主动感知与发现、知识获取与生成、协同与共享、评估与演化、人机整合与增强、自我维持与安全交互等关键技术研究，构建群智空间的服务体系结构，研究移动群体智能的协同决策与控制技术。\n4.混合增强智能新架构和新技术。研究混合增强智能核心技术、认知计算框架，新型混合计算架构，人机共驾、在线智能学习技术，平行管理与控制的混合增强智能框架。\n5.自主无人系统的智能技术。研究无人机自主控制和汽车、船舶、轨道交通自动驾驶等智能技术，服务机器人、空间机器人、海洋机器人、极地机器人技术，无人车间/智能工厂智能技术，高端智能控制技术和自主无人操作系统。研究复杂环境下基于计算机视觉的定位、导航、识别等机器人及机械手臂自主控制技术。\n6.虚拟现实智能建模技术。研究虚拟对象智能行为的数学表达与建模方法，虚拟对象与虚拟环境和用户之间进行自然、持续、深入交互等问题，智能对象建模的技术与方法体系。\n7.智能计算芯片与系统。研发神经网络处理器以及高能效、可重构类脑计算芯片等，新型感知芯片与系统、智能计算体系结构与系统，人工智能操作系统。研究适合人工智能的混合计算架构等。\n8.自然语言处理技术。研究短文本的计算与分析技术，跨语言文本挖掘技术和面向机器认知智能的语义理解技术，多媒体信息理解的人机对话系统。\n\n\n\n3.统筹布局人工智能创新平台。\n建设布局人工智能创新平台，强化对人工智能研发应用的基础支撑。人工智能开源软硬件基础平台重点建设支持知识推理、概率统计、深度学习等人工智能范式的统一计算框架平台，形成促进人工智能软件、硬件和智能云之间相互协同的生态链。群体智能服务平台重点建设基于互联网大规模协作的知识资源管理与开放式共享工具，形成面向产学研用创新环节的群智众创平台和服务环境。混合增强智能支撑平台重点建设支持大规模训练的异构实时计算引擎和新型计算集群，为复杂智能计算提供服务化、系统化平台和解决方案。自主无人系统支撑平台重点建设面向自主无人系统复杂环境下环境感知、自主协同控制、智能决策等人工智能共性核心技术的支撑系统，形成开放式、模块化、可重构的自主无人系统开发与试验环境。人工智能基础数据与安全检测平台重点建设面向人工智能的公共数据资源库、标准测试数据集、云服务平台等，形成人工智能算法与平台安全性测试评估的方法、技术、规范和工具集。促进各类通用软件和技术平台的开源开放。各类平台要按照军民深度融合的要求和相关规定，推进军民共享共用。\n\n\n\n\n专栏3　基础支撑平台\n1.人工智能开源软硬件基础平台。建立大数据人工智能开源软件基础平台、终端与云端协同的人工智能云服务平台、新型多元智能传感器件与集成平台、基于人工智能硬件的新产品设计平台、未来网络中的大数据智能化服务平台等。\n2.群体智能服务平台。建立群智众创计算支撑平台、科技众创服务系统、群智软件开发与验证自动化系统、群智软件学习与创新系统、开放环境的群智决策系统、群智共享经济服务系统。\n3.混合增强智能支撑平台。建立人工智能超级计算中心、大规模超级智能计算支撑环境、在线智能教育平台、“人在回路”驾驶脑、产业发展复杂性分析与风险评估的智能平台、支撑核电安全运营的智能保障平台、人机共驾技术研发与测试平台等。\n4.自主无人系统支撑平台。建立自主无人系统共性核心技术支撑平台，无人机自主控制以及汽车、船舶和轨道交通自动驾驶支撑平台，服务机器人、空间机器人、海洋机器人、极地机器人支撑平台，智能工厂与智能控制装备技术支撑平台等。\n5.人工智能基础数据与安全检测平台。建设面向人工智能的公共数据资源库、标准测试数据集、云服务平台，建立人工智能算法与平台安全性测试模型及评估模型，研发人工智能算法与平台安全性测评工具集。\n\n\n\n4.加快培养聚集人工智能高端人才。\n把高端人才队伍建设作为人工智能发展的重中之重，坚持培养和引进相结合，完善人工智能教育体系，加强人才储备和梯队建设，特别是加快引进全球顶尖人才和青年人才，形成我国人工智能人才高地。\n培育高水平人工智能创新人才和团队。支持和培养具有发展潜力的人工智能领军人才，加强人工智能基础研究、应用研究、运行维护等方面专业技术人才培养。重视复合型人才培养，重点培养贯通人工智能理论、方法、技术、产品与应用等的纵向复合型人才，以及掌握“人工智能+”经济、社会、管理、标准、法律等的横向复合型人才。通过重大研发任务和基地平台建设，汇聚人工智能高端人才，在若干人工智能重点领域形成一批高水平创新团队。鼓励和引导国内创新人才、团队加强与全球顶尖人工智能研究机构合作互动。\n加大高端人工智能人才引进力度。开辟专门渠道，实行特殊政策，实现人工智能高端人才精准引进。重点引进神经认知、机器学习、自动驾驶、智能机器人等国际顶尖科学家和高水平创新团队。鼓励采取项目合作、技术咨询等方式柔性引进人工智能人才。统筹利用“千人计划”等现有人才计划，加强人工智能领域优秀人才特别是优秀青年人才引进工作。完善企业人力资本成本核算相关政策，激励企业、科研机构引进人工智能人才。\n建设人工智能学科。完善人工智能领域学科布局，设立人工智能专业，推动人工智能领域一级学科建设，尽快在试点院校建立人工智能学院，增加人工智能相关学科方向的博士、硕士招生名额。鼓励高校在原有基础上拓宽人工智能专业教育内容，形成“人工智能+X”复合专业培养新模式，重视人工智能与数学、计算机科学、物理学、生物学、心理学、社会学、法学等学科专业教育的交叉融合。加强产学研合作，鼓励高校、科研院所与企业等机构合作开展人工智能学科建设。\n（二）培育高端高效的智能经济。\n加快培育具有重大引领带动作用的人工智能产业，促进人工智能与各产业领域深度融合，形成数据驱动、人机协同、跨界融合、共创分享的智能经济形态。数据和知识成为经济增长的第一要素，人机协同成为主流生产和服务方式，跨界融合成为重要经济模式，共创分享成为经济生态基本特征，个性化需求与定制成为消费新潮流，生产率大幅提升，引领产业向价值链高端迈进，有力支撑实体经济发展，全面提升经济发展质量和效益。\n1.大力发展人工智能新兴产业。\n加快人工智能关键技术转化应用，促进技术集成与商业模式创新，推动重点领域智能产品创新，积极培育人工智能新兴业态，布局产业链高端，打造具有国际竞争力的人工智能产业集群。\n智能软硬件。开发面向人工智能的操作系统、数据库、中间件、开发工具等关键基础软件，突破图形处理器等核心硬件，研究图像识别、语音识别、机器翻译、智能交互、知识处理、控制决策等智能系统解决方案，培育壮大面向人工智能应用的基础软硬件产业。\n智能机器人。攻克智能机器人核心零部件、专用传感器，完善智能机器人硬件接口标准、软件接口协议标准以及安全使用标准。研制智能工业机器人、智能服务机器人，实现大规模应用并进入国际市场。研制和推广空间机器人、海洋机器人、极地机器人等特种智能机器人。建立智能机器人标准体系和安全规则。\n智能运载工具。发展自动驾驶汽车和轨道交通系统，加强车载感知、自动驾驶、车联网、物联网等技术集成和配套，开发交通智能感知系统，形成我国自主的自动驾驶平台技术体系和产品总成能力，探索自动驾驶汽车共享模式。发展消费类和商用类无人机、无人船，建立试验鉴定、测试、竞技等专业化服务体系，完善空域、水域管理措施。\n虚拟现实与增强现实。突破高性能软件建模、内容拍摄生成、增强现实与人机交互、集成环境与工具等关键技术，研制虚拟显示器件、光学器件、高性能真三维显示器、开发引擎等产品，建立虚拟现实与增强现实的技术、产品、服务标准和评价体系，推动重点行业融合应用。\n智能终端。加快智能终端核心技术和产品研发，发展新一代智能手机、车载智能终端等移动智能终端产品和设备，鼓励开发智能手表、智能耳机、智能眼镜等可穿戴终端产品，拓展产品形态和应用服务。\n物联网基础器件。发展支撑新一代物联网的高灵敏度、高可靠性智能传感器件和芯片，攻克射频识别、近距离机器通信等物联网核心技术和低功耗处理器等关键器件。\n2.加快推进产业智能化升级。\n推动人工智能与各行业融合创新，在制造、农业、物流、金融、商务、家居等重点行业和领域开展人工智能应用试点示范，推动人工智能规模化应用，全面提升产业发展智能化水平。\n智能制造。围绕制造强国重大需求，推进智能制造关键技术装备、核心支撑软件、工业互联网等系统集成应用，研发智能产品及智能互联产品、智能制造使能工具与系统、智能制造云服务平台，推广流程智能制造、离散智能制造、网络化协同制造、远程诊断与运维服务等新型制造模式，建立智能制造标准体系，推进制造全生命周期活动智能化。\n智能农业。研制农业智能传感与控制系统、智能化农业装备、农机田间作业自主系统等。建立完善天空地一体化的智能农业信息遥感监测网络。建立典型农业大数据智能决策分析系统，开展智能农场、智能化植物工厂、智能牧场、智能渔场、智能果园、农产品加工智能车间、农产品绿色智能供应链等集成应用示范。\n智能物流。加强智能化装卸搬运、分拣包装、加工配送等智能物流装备研发和推广应用，建设深度感知智能仓储系统，提升仓储运营管理水平和效率。完善智能物流公共信息平台和指挥系统、产品质量认证及追溯系统、智能配货调度体系等。\n智能金融。建立金融大数据系统，提升金融多媒体数据处理与理解能力。创新智能金融产品和服务，发展金融新业态。鼓励金融行业应用智能客服、智能监控等技术和装备。建立金融风险智能预警与防控系统。\n智能商务。鼓励跨媒体分析与推理、知识计算引擎与知识服务等新技术在商务领域应用，推广基于人工智能的新型商务服务与决策系统。建设涵盖地理位置、网络媒体和城市基础数据等跨媒体大数据平台，支撑企业开展智能商务。鼓励围绕个人需求、企业管理提供定制化商务智能决策服务。\n智能家居。加强人工智能技术与家居建筑系统的融合应用，提升建筑设备及家居产品的智能化水平。研发适应不同应用场景的家庭互联互通协议、接口标准，提升家电、耐用品等家居产品感知和联通能力。支持智能家居企业创新服务模式，提供互联共享解决方案。\n3.大力发展智能企业。\n大规模推动企业智能化升级。支持和引导企业在设计、生产、管理、物流和营销等核心业务环节应用人工智能新技术，构建新型企业组织结构和运营方式，形成制造与服务、金融智能化融合的业态模式，发展个性化定制，扩大智能产品供给。鼓励大型互联网企业建设云制造平台和服务平台，面向制造企业在线提供关键工业软件和模型库，开展制造能力外包服务，推动中小企业智能化发展。\n推广应用智能工厂。加强智能工厂关键技术和体系方法的应用示范，重点推广生产线重构与动态智能调度、生产装备智能物联与云化数据采集、多维人机物协同与互操作等技术，鼓励和引导企业建设工厂大数据系统、网络化分布式生产设施等，实现生产设备网络化、生产数据可视化、生产过程透明化、生产现场无人化，提升工厂运营管理智能化水平。\n加快培育人工智能产业领军企业。在无人机、语音识别、图像识别等优势领域加快打造人工智能全球领军企业和品牌。在智能机器人、智能汽车、可穿戴设备、虚拟现实等新兴领域加快培育一批龙头企业。支持人工智能企业加强专利布局，牵头或参与国际标准制定。推动国内优势企业、行业组织、科研机构、高校等联合组建中国人工智能产业技术创新联盟。支持龙头骨干企业构建开源硬件工厂、开源软件平台，形成集聚各类资源的创新生态，促进人工智能中小微企业发展和各领域应用。支持各类机构和平台面向人工智能企业提供专业化服务。\n4.打造人工智能创新高地。\n结合各地区基础和优势，按人工智能应用领域分门别类进行相关产业布局。鼓励地方围绕人工智能产业链和创新链，集聚高端要素、高端企业、高端人才，打造人工智能产业集群和创新高地。\n开展人工智能创新应用试点示范。在人工智能基础较好、发展潜力较大的地区，组织开展国家人工智能创新试验，探索体制机制、政策法规、人才培育等方面的重大改革，推动人工智能成果转化、重大产品集成创新和示范应用，形成可复制、可推广的经验，引领带动智能经济和智能社会发展。\n建设国家人工智能产业园。依托国家自主创新示范区和国家高新技术产业开发区等创新载体，加强科技、人才、金融、政策等要素的优化配置和组合，加快培育建设人工智能产业创新集群。\n建设国家人工智能众创基地。依托从事人工智能研究的高校、科研院所集中地区，搭建人工智能领域专业化创新平台等新型创业服务机构，建设一批低成本、便利化、全要素、开放式的人工智能众创空间，完善孵化服务体系，推进人工智能科技成果转移转化，支持人工智能创新创业。\n（三）建设安全便捷的智能社会。\n围绕提高人民生活水平和质量的目标，加快人工智能深度应用，形成无时不有、无处不在的智能化环境，全社会的智能化水平大幅提升。越来越多的简单性、重复性、危险性任务由人工智能完成，个体创造力得到极大发挥，形成更多高质量和高舒适度的就业岗位；精准化智能服务更加丰富多样，人们能够最大限度享受高质量服务和便捷生活；社会治理智能化水平大幅提升，社会运行更加安全高效。\n1.发展便捷高效的智能服务。\n围绕教育、医疗、养老等迫切民生需求，加快人工智能创新应用，为公众提供个性化、多元化、高品质服务。\n智能教育。利用智能技术加快推动人才培养模式、教学方法改革，构建包含智能学习、交互式学习的新型教育体系。开展智能校园建设，推动人工智能在教学、管理、资源建设等全流程应用。开发立体综合教学场、基于大数据智能的在线学习教育平台。开发智能教育助理，建立智能、快速、全面的教育分析系统。建立以学习者为中心的教育环境，提供精准推送的教育服务，实现日常教育和终身教育定制化。\n智能医疗。推广应用人工智能治疗新模式新手段，建立快速精准的智能医疗体系。探索智慧医院建设，开发人机协同的手术机器人、智能诊疗助手，研发柔性可穿戴、生物兼容的生理监测系统，研发人机协同临床智能诊疗方案，实现智能影像识别、病理分型和智能多学科会诊。基于人工智能开展大规模基因组识别、蛋白组学、代谢组学等研究和新药研发，推进医药监管智能化。加强流行病智能监测和防控。\n智能健康和养老。加强群体智能健康管理，突破健康大数据分析、物联网等关键技术，研发健康管理可穿戴设备和家庭智能健康检测监测设备，推动健康管理实现从点状监测向连续监测、从短流程管理向长流程管理转变。建设智能养老社区和机构，构建安全便捷的智能化养老基础设施体系。加强老年人产品智能化和智能产品适老化，开发视听辅助设备、物理辅助设备等智能家居养老设备，拓展老年人活动空间。开发面向老年人的移动社交和服务平台、情感陪护助手，提升老年人生活质量。\n2.推进社会治理智能化。\n围绕行政管理、司法管理、城市管理、环境保护等社会治理的热点难点问题，促进人工智能技术应用，推动社会治理现代化。\n智能政务。开发适于政府服务与决策的人工智能平台，研制面向开放环境的决策引擎，在复杂社会问题研判、政策评估、风险预警、应急处置等重大战略决策方面推广应用。加强政务信息资源整合和公共需求精准预测，畅通政府与公众的交互渠道。\n智慧法庭。建设集审判、人员、数据应用、司法公开和动态监控于一体的智慧法庭数据平台，促进人工智能在证据收集、案例分析、法律文件阅读与分析中的应用，实现法院审判体系和审判能力智能化。\n智慧城市。构建城市智能化基础设施，发展智能建筑，推动地下管廊等市政基础设施智能化改造升级；建设城市大数据平台，构建多元异构数据融合的城市运行管理体系，实现对城市基础设施和城市绿地、湿地等重要生态要素的全面感知以及对城市复杂系统运行的深度认知；研发构建社区公共服务信息系统，促进社区服务系统与居民智能家庭系统协同；推进城市规划、建设、管理、运营全生命周期智能化。\n智能交通。研究建立营运车辆自动驾驶与车路协同的技术体系。研发复杂场景下的多维交通信息综合大数据应用平台，实现智能化交通疏导和综合运行协调指挥，建成覆盖地面、轨道、低空和海上的智能交通监控、管理和服务系统。\n智能环保。建立涵盖大气、水、土壤等环境领域的智能监控大数据平台体系，建成陆海统筹、天地一体、上下协同、信息共享的智能环境监测网络和服务平台。研发资源能源消耗、环境污染物排放智能预测模型方法和预警方案。加强京津冀、长江经济带等国家重大战略区域环境保护和突发环境事件智能防控体系建设。\n3.利用人工智能提升公共安全保障能力。\n促进人工智能在公共安全领域的深度应用，推动构建公共安全智能化监测预警与控制体系。围绕社会综合治理、新型犯罪侦查、反恐等迫切需求，研发集成多种探测传感技术、视频图像信息分析识别技术、生物特征识别技术的智能安防与警用产品，建立智能化监测平台。加强对重点公共区域安防设备的智能化改造升级，支持有条件的社区或城市开展基于人工智能的公共安防区域示范。强化人工智能对食品安全的保障，围绕食品分类、预警等级、食品安全隐患及评估等，建立智能化食品安全预警系统。加强人工智能对自然灾害的有效监测，围绕地震灾害、地质灾害、气象灾害、水旱灾害和海洋灾害等重大自然灾害，构建智能化监测预警与综合应对平台。\n4.促进社会交往共享互信。\n充分发挥人工智能技术在增强社会互动、促进可信交流中的作用。加强下一代社交网络研发，加快增强现实、虚拟现实等技术推广应用，促进虚拟环境和实体环境协同融合，满足个人感知、分析、判断与决策等实时信息需求，实现在工作、学习、生活、娱乐等不同场景下的流畅切换。针对改善人际沟通障碍的需求，开发具有情感交互功能、能准确理解人的需求的智能助理产品，实现情感交流和需求满足的良性循环。促进区块链技术与人工智能的融合，建立新型社会信用体系，最大限度降低人际交往成本和风险。\n（四）加强人工智能领域军民融合。\n深入贯彻落实军民融合发展战略，推动形成全要素、多领域、高效益的人工智能军民融合格局。以军民共享共用为导向部署新一代人工智能基础理论和关键共性技术研发，建立科研院所、高校、企业和军工单位的常态化沟通协调机制。促进人工智能技术军民双向转化，强化新一代人工智能技术对指挥决策、军事推演、国防装备等的有力支撑，引导国防领域人工智能科技成果向民用领域转化应用。鼓励优势民口科研力量参与国防领域人工智能重大科技创新任务，推动各类人工智能技术快速嵌入国防创新领域。加强军民人工智能技术通用标准体系建设，推进科技创新平台基地的统筹布局和开放共享。\n（五）构建泛在安全高效的智能化基础设施体系。\n大力推动智能化信息基础设施建设，提升传统基础设施的智能化水平，形成适应智能经济、智能社会和国防建设需要的基础设施体系。加快推动以信息传输为核心的数字化、网络化信息基础设施，向集融合感知、传输、存储、计算、处理于一体的智能化信息基础设施转变。优化升级网络基础设施，研发布局第五代移动通信（5G）系统，完善物联网基础设施，加快天地一体化信息网络建设，提高低时延、高通量的传输能力。统筹利用大数据基础设施，强化数据安全与隐私保护，为人工智能研发和广泛应用提供海量数据支撑。建设高效能计算基础设施，提升超级计算中心对人工智能应用的服务支撑能力。建设分布式高效能源互联网，形成支撑多能源协调互补、及时有效接入的新型能源网络，推广智能储能设施、智能用电设施，实现能源供需信息的实时匹配和智能化响应。\n\n\n\n\n专栏4　智能化基础设施\n1.网络基础设施。加快布局实时协同人工智能的5G增强技术研发及应用，建设面向空间协同人工智能的高精度导航定位网络，加强智能感知物联网核心技术攻关和关键设施建设，发展支撑智能化的工业互联网、面向无人驾驶的车联网等，研究智能化网络安全架构。加快建设天地一体化信息网络，推进天基信息网、未来互联网、移动通信网的全面融合。\n2.大数据基础设施。依托国家数据共享交换平台、数据开放平台等公共基础设施，建设政府治理、公共服务、产业发展、技术研发等领域大数据基础信息数据库，支撑开展国家治理大数据应用。整合社会各类数据平台和数据中心资源，形成覆盖全国、布局合理、链接畅通的一体化服务能力。\n3.高效能计算基础设施。继续加强超级计算基础设施、分布式计算基础设施和云计算中心建设，构建可持续发展的高性能计算应用生态环境。推进下一代超级计算机研发应用。\n\n\n\n（六）前瞻布局新一代人工智能重大科技项目。\n针对我国人工智能发展的迫切需求和薄弱环节，设立新一代人工智能重大科技项目。加强整体统筹，明确任务边界和研发重点，形成以新一代人工智能重大科技项目为核心、现有研发布局为支撑的“1+N”人工智能项目群。\n“1”是指新一代人工智能重大科技项目，聚焦基础理论和关键共性技术的前瞻布局，包括研究大数据智能、跨媒体感知计算、混合增强智能、群体智能、自主协同控制与决策等理论，研究知识计算引擎与知识服务技术、跨媒体分析推理技术、群体智能关键技术、混合增强智能新架构与新技术、自主无人控制技术等，开源共享人工智能基础理论和共性技术。持续开展人工智能发展的预测和研判，加强人工智能对经济社会综合影响及对策研究。\n“N”是指国家相关规划计划中部署的人工智能研发项目，重点是加强与新一代人工智能重大科技项目的衔接，协同推进人工智能的理论研究、技术突破和产品研发应用。加强与国家科技重大专项的衔接，在“核高基”（核心电子器件、高端通用芯片、基础软件）、集成电路装备等国家科技重大专项中支持人工智能软硬件发展。加强与其他“科技创新2030—重大项目”的相互支撑，加快脑科学与类脑计算、量子信息与量子计算、智能制造与机器人、大数据等研究，为人工智能重大技术突破提供支撑。国家重点研发计划继续推进高性能计算等重点专项实施，加大对人工智能相关技术研发和应用的支持；国家自然科学基金加强对人工智能前沿领域交叉学科研究和自由探索的支持。在深海空间站、健康保障等重大项目，以及智慧城市、智能农机装备等国家重点研发计划重点专项部署中，加强人工智能技术的应用示范。其他各类科技计划支持的人工智能相关基础理论和共性技术研究成果应开放共享。\n创新新一代人工智能重大科技项目组织实施模式，坚持集中力量办大事、重点突破的原则，充分发挥市场机制作用，调动部门、地方、企业和社会各方面力量共同推进实施。明确管理责任，定期开展评估，加强动态调整，提高管理效率。\n四、资源配置\n充分利用已有资金、基地等存量资源，统筹配置国际国内创新资源，发挥好财政投入、政策激励的引导作用和市场配置资源的主导作用，撬动企业、社会加大投入，形成财政资金、金融资本、社会资本多方支持的新格局。\n（一）建立财政引导、市场主导的资金支持机制。\n统筹政府和市场多渠道资金投入，加大财政资金支持力度，盘活现有资源，对人工智能基础前沿研究、关键共性技术攻关、成果转移转化、基地平台建设、创新应用示范等提供支持。利用现有政府投资基金支持符合条件的人工智能项目，鼓励龙头骨干企业、产业创新联盟牵头成立市场化的人工智能发展基金。利用天使投资、风险投资、创业投资基金及资本市场融资等多种渠道，引导社会资本支持人工智能发展。积极运用政府和社会资本合作等模式，引导社会资本参与人工智能重大项目实施和科技成果转化应用。\n（二）优化布局建设人工智能创新基地。\n按照国家级科技创新基地布局和框架，统筹推进人工智能领域建设若干国际领先的创新基地。引导现有与人工智能相关的国家重点实验室、企业国家重点实验室、国家工程实验室等基地，聚焦新一代人工智能的前沿方向开展研究。按规定程序，以企业为主体、产学研合作组建人工智能领域的相关技术和产业创新基地，发挥龙头骨干企业技术创新示范带动作用。发展人工智能领域的专业化众创空间，促进最新技术成果和资源、服务的精准对接。充分发挥各类创新基地聚集人才、资金等创新资源的作用，突破人工智能基础前沿理论和关键共性技术，开展应用示范。\n（三）统筹国际国内创新资源。\n支持国内人工智能企业与国际人工智能领先高校、科研院所、团队合作。鼓励国内人工智能企业“走出去”，为有实力的人工智能企业开展海外并购、股权投资、创业投资和建立海外研发中心等提供便利和服务。鼓励国外人工智能企业、科研机构在华设立研发中心。依托“一带一路”战略，推动建设人工智能国际科技合作基地、联合研究中心等，加快人工智能技术在“一带一路”沿线国家推广应用。推动成立人工智能国际组织，共同制定相关国际标准。支持相关行业协会、联盟及服务机构搭建面向人工智能企业的全球化服务平台。\n五、保障措施\n围绕推动我国人工智能健康快速发展的现实要求，妥善应对人工智能可能带来的挑战，形成适应人工智能发展的制度安排，构建开放包容的国际化环境，夯实人工智能发展的社会基础。\n（一）制定促进人工智能发展的法律法规和伦理规范。\n加强人工智能相关法律、伦理和社会问题研究，建立保障人工智能健康发展的法律法规和伦理道德框架。开展与人工智能应用相关的民事与刑事责任确认、隐私和产权保护、信息安全利用等法律问题研究，建立追溯和问责制度，明确人工智能法律主体以及相关权利、义务和责任等。重点围绕自动驾驶、服务机器人等应用基础较好的细分领域，加快研究制定相关安全管理法规，为新技术的快速应用奠定法律基础。开展人工智能行为科学和伦理等问题研究，建立伦理道德多层次判断结构及人机协作的伦理框架。制定人工智能产品研发设计人员的道德规范和行为守则，加强对人工智能潜在危害与收益的评估，构建人工智能复杂场景下突发事件的解决方案。积极参与人工智能全球治理，加强机器人异化和安全监管等人工智能重大国际共性问题研究，深化在人工智能法律法规、国际规则等方面的国际合作，共同应对全球性挑战。\n（二）完善支持人工智能发展的重点政策。\n落实对人工智能中小企业和初创企业的财税优惠政策，通过高新技术企业税收优惠和研发费用加计扣除等政策支持人工智能企业发展。完善落实数据开放与保护相关政策，开展公共数据开放利用改革试点，支持公众和企业充分挖掘公共数据的商业价值，促进人工智能应用创新。研究完善适应人工智能的教育、医疗、保险、社会救助等政策体系，有效应对人工智能带来的社会问题。\n（三）建立人工智能技术标准和知识产权体系。\n加强人工智能标准框架体系研究。坚持安全性、可用性、互操作性、可追溯性原则，逐步建立并完善人工智能基础共性、互联互通、行业应用、网络安全、隐私保护等技术标准。加快推动无人驾驶、服务机器人等细分应用领域的行业协会和联盟制定相关标准。鼓励人工智能企业参与或主导制定国际标准，以技术标准“走出去”带动人工智能产品和服务在海外推广应用。加强人工智能领域的知识产权保护，健全人工智能领域技术创新、专利保护与标准化互动支撑机制，促进人工智能创新成果的知识产权化。建立人工智能公共专利池，促进人工智能新技术的利用与扩散。\n（四）建立人工智能安全监管和评估体系。\n加强人工智能对国家安全和保密领域影响的研究与评估，完善人、技、物、管配套的安全防护体系，构建人工智能安全监测预警机制。加强对人工智能技术发展的预测、研判和跟踪研究，坚持问题导向，准确把握技术和产业发展趋势。增强风险意识，重视风险评估和防控，强化前瞻预防和约束引导，近期重点关注对就业的影响，远期重点考虑对社会伦理的影响，确保把人工智能发展规制在安全可控范围内。建立健全公开透明的人工智能监管体系，实行设计问责和应用监督并重的双层监管结构，实现对人工智能算法设计、产品开发和成果应用等的全流程监管。促进人工智能行业和企业自律，切实加强管理，加大对数据滥用、侵犯个人隐私、违背道德伦理等行为的惩戒力度。加强人工智能网络安全技术研发，强化人工智能产品和系统网络安全防护。构建动态的人工智能研发应用评估评价机制，围绕人工智能设计、产品和系统的复杂性、风险性、不确定性、可解释性、潜在经济影响等问题，开发系统性的测试方法和指标体系，建设跨领域的人工智能测试平台，推动人工智能安全认证，评估人工智能产品和系统的关键性能。\n（五）大力加强人工智能劳动力培训。\n加快研究人工智能带来的就业结构、就业方式转变以及新型职业和工作岗位的技能需求，建立适应智能经济和智能社会需要的终身学习和就业培训体系，支持高等院校、职业学校和社会化培训机构等开展人工智能技能培训，大幅提升就业人员专业技能，满足我国人工智能发展带来的高技能高质量就业岗位需要。鼓励企业和各类机构为员工提供人工智能技能培训。加强职工再就业培训和指导，确保从事简单重复性工作的劳动力和因人工智能失业的人员顺利转岗。\n（六）广泛开展人工智能科普活动。\n支持开展形式多样的人工智能科普活动，鼓励广大科技工作者投身人工智能的科普与推广，全面提高全社会对人工智能的整体认知和应用水平。实施全民智能教育项目，在中小学阶段设置人工智能相关课程，逐步推广编程教育，鼓励社会力量参与寓教于乐的编程教学软件、游戏的开发和推广。建设和完善人工智能科普基础设施，充分发挥各类人工智能创新基地平台等的科普作用，鼓励人工智能企业、科研机构搭建开源平台，面向公众开放人工智能研发平台、生产设施或展馆等。支持开展人工智能竞赛，鼓励进行形式多样的人工智能科普创作。鼓励科学家参与人工智能科普。\n六、组织实施\n新一代人工智能发展规划是关系全局和长远的前瞻谋划。必须加强组织领导，健全机制，瞄准目标，紧盯任务，以钉钉子的精神切实抓好落实，一张蓝图干到底。\n（一）组织领导。\n按照党中央、国务院统一部署，由国家科技体制改革和创新体系建设领导小组牵头统筹协调，审议重大任务、重大政策、重大问题和重点工作安排，推动人工智能相关法律法规建设，指导、协调和督促有关部门做好规划任务的部署实施。依托国家科技计划（专项、基金等）管理部际联席会议，科技部会同有关部门负责推进新一代人工智能重大科技项目实施，加强与其他计划任务的衔接协调。成立人工智能规划推进办公室，办公室设在科技部，具体负责推进规划实施。成立人工智能战略咨询委员会，研究人工智能前瞻性、战略性重大问题，对人工智能重大决策提供咨询评估。推进人工智能智库建设，支持各类智库开展人工智能重大问题研究，为人工智能发展提供强大智力支持。\n（二）保障落实。\n加强规划任务分解，明确责任单位和进度安排，制定年度和阶段性实施计划。建立年度评估、中期评估等规划实施情况的监测评估机制。适应人工智能快速发展的特点，根据任务进展情况、阶段目标完成情况、技术发展新动向等，加强对规划和项目的动态调整。\n（三）试点示范。\n对人工智能重大任务和重点政策措施，要制定具体方案，开展试点示范。加强对各部门、各地方试点示范的统筹指导，及时总结推广可复制的经验和做法。通过试点先行、示范引领，推进人工智能健康有序发展。\n（四）舆论引导。\n充分利用各种传统媒体和新兴媒体，及时宣传人工智能新进展、新成效，让人工智能健康发展成为全社会共识，调动全社会参与支持人工智能发展的积极性。及时做好舆论引导，更好应对人工智能发展可能带来的社会、伦理和法律等挑战。\n主办单位：国务院办公厅　运行维护单位：中国政府网运行中心 \n版权所有：中国政府网　中文域名：中国政府网.政务\n\n网站标识码bm01000001　京ICP备05070218号　京公网安备11010202000001号\n\n中国政府网微博、微信\n主办单位：国务院办公厅　运行维护单位：中国政府网运行中心\n版权所有：中国政府网　中文域名：中国政府网.政务\n\n网站标识码bm01000001\n京ICP备05070218号　京公网安备11010202000001号"
        },
        {
            "title": "人工智能辅助检测医疗器械（软件）临床评价注册审查指导原则（2023） – ReguVerse",
            "content": "本指导原则旨在指导注册申请人开展人工智能（artificial intelligence，<strong>AI</strong>）辅助检测类医疗器械临床评价的资料准备，同时为技术审评部门审评人工智能辅助检测类产品临床评价资料提供参考。 · 本指导原则进一步明确人工智...",
            "url": "https://reguverse.com/documentation/nmpa-regulations-index/pre-market-submission/guidance-document/21-sw/cmde-2023-38/",
            "source": "reguverse.com",
            "published_date": "",
            "search_engine": "Brave",
            "query": "中国 人工智能医疗诊断 最新监管政策 2025年 NMPA new regulation AI clinical decision support",
            "language": "en",
            "family_friendly": true,
            "category": "policy_news",
            "search_source": "brave",
            "full_content": "本指导原则旨在指导注册申请人开展人工智能（artificial intelligence，AI）辅助检测类医疗器械临床评价的资料准备，同时为技术审评部门审评人工智能辅助检测类产品临床评价资料提供参考。\n本指导原则进一步明确人工智能辅助检测类医疗器械临床评价的要求和适用情形。申请人可依据产品的具体特征确定其中内容是否适用，若不适用，需阐述理由并提供相应的科学依据，并依据产品的具体特征对临床评价资料的内容进行充实和细化。\n本指导原则是供注册申请人和技术审评人员使用的指导性文件，但不包括审评审批所涉及的行政事项，亦不作为法规强制执行，应在遵循相关法规的前提下使用本指导原则。如果有能够满足相关法规要求的其他方法，也可以采用，但是需要提供详细的合理性论述和验证确认资料。\n本指导原则是在现行法规和标准体系以及当前认知水平下制定，随着法规和标准的不断完善，以及科学技术的不断发展，相关内容也将适时进行调整。\n人工智能医疗器械从与预期用途角度可分为辅助决策类和非辅助决策类。其中，辅助决策是指通过提供诊疗活动建议辅助医务人员进行临床决策，如通过异常识别、自动制定手术计划进行辅助分诊、辅助检测、辅助诊断、辅助治疗等。\n人工智能辅助检测产品，是指基于计算机人工智能算法，可包含模式识别和数据分析等功能，通过识别、标记、突出等方式提示医师关注可能的异常/病变区域，从而辅助临床医师做出相应诊疗决策的产品，可为独立软件或嵌入式软件；分类编码为21-04-02，管理类别为第III类；产品还可同时包含非辅助决策功能，如结构化报告生成、前后图像对比、正常解剖组织的分割（如肺叶、肋骨等）、尺寸测量、CT值测量等临床功能和数据储存、传输等非临床功能。人工智能辅助检测产品常见的有针对肺结节、乳腺结节、骨折、血管狭窄、结肠息肉等病变/异常的检出产品，本指导原则给出此类产品的通用要求，并以肺结节辅助检测和结肠息肉辅助检测产品为例（详见附件），阐述对人工智能辅助检测产品的临床试验中具体要素的考虑建议，同类型产品可参考相关适用部分。\n本指导原则不适用于如下情形（但下述产品可参照本指导原则中适用部分的要求）：1.可鉴别病变的性质（如良恶性）或疾病分期分型的人工智能辅助诊断类产品；2.预测疾病发生概率的产品；3.可同时辅助检测、鉴别诊断多种病变的多分类人工智能辅助检测产品（例如同时辅助检测并分类肺结节、条索、胸膜增厚、胸腔积液、肋骨骨折等的产品）；4.人工智能辅助分诊、转诊产品，此类产品通过初步评估患者是否疑似患有目标疾病，从而对患者的分诊转诊提供辅助决策建议，该类产品不给出具体病变情况，且无论辅助分诊结果为阴性、阳性，均需专业医师再一次对患者影像进行评阅，常见的有糖尿病视网膜病变辅助分诊、肺炎辅助分诊、脑出血辅助分诊等；5. 配合体外诊断试剂产品使用的人工智能辅助分析软件。\n临床试验目的一般是评价申报产品在预期适用范围下使用时辅助检测的诊断学性能，亦可一并观察产品的可用性与安全性。\n人工智能辅助检测类产品的临床意义通常在于提升医师的病变检测准确度，为充分评估产品的临床受益风险可接受性，此类产品一般需考虑开展对照试验，根据产品特征及临床诊疗实际，可以为随机平行对照、交叉自身对照或多阅片者多数据样本（multiple reader multiple case，MRMC）试验设计。\n试验组一般为医师在软件的辅助下完成异常/病变的检测，对照组一般为临床医师独立的异常/病变的检测，比较二者的检测准确度。\n1．适用人群的影像学样本\n预期人群的影像学样本是人工智能辅助检测产品临床试验的典型研究对象，影像学样本需基于定义明确的入选和排除标准收集，可为临床已有数据（如临床诊疗中产生的真实世界数据）。考虑到AI与医师观察、操作的协同交互等因素，基于实时影像的辅助检测产品临床试验，推荐考虑前瞻性采集影像检查，作为临床试验研究对象。\n为了保证临床试验质量以及结果的可靠性，选取研究对象时，申请人需考虑如下措施：一是纳入数据样本独立于申报产品或前代产品开发所用数据集，如申报产品或前代产品的训练集、测试集。二是采用临床已有数据进行研究时，需基于明确且严格的入排标准和临床试验计划，连续收集过往某段时间内、特定医疗机构内患者影像学数据，避免主观挑选病例。三是考虑阳性样本中，目标疾病的疾病谱分布（如分型、分期）合理性，某些对辅助检测具有挑战性的分期、分型，必要时在临床试验中富集相关具有代表性的亚组。四是通常情况下，需避免在一项临床试验中同时入组同一患者同一目标部位的多组样本数据。五是临床已有数据收集时，需尽可能全面的收集与疾病相关的信息（适用的），具体包括但不限于：\n（1）人口统计学信息(如年龄、性别);\n（2）与辅助检测目标疾病相关的信息，如病史、疾病状态、分期、分型、病变大小、病变位置、器官特征(如乳腺腺体分型)、伴随疾病等。\n（3）确定为阳性/阴性病例的依据，如既往诊断结论，以及确定疾病状态、部位和程度的方法。\n2. 阅片者\n由于阅片者表现的变异度及其与患者样本变异度和诊断方法（即AI辅助器械）之间的交互效应，一般情况下宜将阅片者列入研究对象。基于非实时影像的辅助检测产品（如肺结节/骨折/乳腺结节辅助检测等），采用MRMC设计可较好的控制阅片者偏倚，同等情况下所需的样本量一般较少，申请人可优先考虑选择。采用MRMC设计时，根据预期的使用者情况，选取不同年资的多位医师作为阅片者，申请人需论述阅片者数量的合理性。\n主要评价指标应结合产品设计特征进行综合选择，一般认为灵敏度、特异度、ROC或其衍生曲线等诊断准确性指标受样本患病率差异的影响较小，因此，宜优先考虑此类指标作为主要评价指标。\n无论选择哪些指标作为主要评价指标，该类产品临床试验应当考虑整体的优效性设计，例如ROC或其衍生曲线下面积（Area Under Curve，AUC）的优效设计，或者目标疾病辅助检测特异度非劣效前提下的灵敏度优效性，或者息肉/腺瘤初检检出率的优效性等。\n申请人应详述临床参考标准的选择、构建方法及理由。可供选择的临床参考标准构建方法包括：一是以临床已确认结果为临床参考标准，即临床上结合患者影像学检查、病史、实验室检查（如病理检查）、长期随访结果等方法综合判定的临床诊断结果；二是通过专家组对研究对象（影像样本）的阅片判定作为临床参考标准。\n对于人工智能辅助检测产品，若根据产品设计判定可采用专家组意见作为临床参考标准，通常可选择高年资医师组成的阅片专家组综合意见为临床参考标准，阅片专家组的成员需独立于“试验和对照组的阅片研究者”，并需要明确：1.专家数量；2.专家经验及专业水平；3.决策机制（如遵循多数意见、背靠背第三人仲裁等）；4.专家决策时所依据的信息（如图像上是否有标记，是否还提供了病史或其他检查结果等）；5.判定所依据的临床准则（如临床指南、诊疗规范、专家共识等）。\n对于试验中对病灶的检出是否与临床参考标准专家组意见一致，一般可考虑1.试验组/对照组勾画病灶的中心在专家组勾画的病灶轮廓边界范围内；2.试验组/对照组勾画病灶与专家组勾画病灶的像素重合度高于一定比例（需提供比例设定的支持依据）；若采用其他判定方法，则需论述合理性。\n若采用临床已确认结果作为临床参考标准，则需明确1.已确认结果所依据的临床信息，包括检查类型及结果；2.各类影像学检查的设备信息，包括影像检查的扫描条件等；3. 已确认结果的临床诊断依据；4.得出已确认结果的医师情况，包括专家会诊，需明确医师资质；5.若还依据了临床随访数据，则还需明确随访的时间以及随访所做的检查类型及结果。\n样本量估算需综合考虑临床试验设计、主要评价指标和统计学要求。申请人需明确计算公式、相应参数及确定理由，以及所用的统计软件。\n临床试验资料中可以提供样本患病率以及目标疾病的流行病学研究的患病率情况进行合理性论述，并确保临床研究设计中样本数据随机分配给阅片者进行评阅。\n平行对照试验样本量计算可参考《医疗器械临床试验设计指导原则》中的相关内容。\n若采用MRMC的试验设计，样本量计算需首先明确具体的分析方法，如Obuchowski-Rockette Analysis（OR分析方法）、Dorfman-Berbaum-Metz-Hillis Analysis（DBMH分析法），并进一步明确受试医师数量，检验水准α、检验效能1-β、预计效应值，优效/非劣效界值，其中预计效应值可通过预试验或调研并汇总分析目标病灶检出的诊断学研究文献获得，优效/非劣效界值应通过同品种产品临床试验结果或权威文献研究等确定。不同疾病的检测效应不同，在试验设计中应明确优效/非劣效界值的设定依据。以DBMH分析法为例，样本量估算与交互随机效应值，检验的检验效能、检验水准，受试医师数量，优效界值，非劣效界值（如有）等要素有关。用于样本量估算的效应值（effect size）选择时，可通过预试验得到的误差和混合效应方差来估计，并考虑预试验样本量等情况，在预试验结果的基础上采用适度保守的估计。\n所有应用了试验产品的受试医师和患者都将被纳入分析，对于主要指标，除给出主要评价指标（灵敏度、特异度、AUC）的点估计外，还将分别对其对应的95%置信区间进行估计，通过试验组与对照组的优效/非劣效比较判断本试验产品是否满足临床应用的需要。\n1. 临床试验培训\n在试验前对阅片者开展必要的培训，可以有效降低试验的偏倚。除基本情况培训（试验流程、术语定义、数据样本阅片的评价标准等）以外，还需考虑案例培训和典型数据样本讲解等，且所用案例独立于试验研究数据样本；培训中宜设置阅片者培训结果测试及合格接受标准；建议临床试验中，对阅片者的培训与临床应用时的培训，在方法、时间、接受标准上尽量保持一致性；并考虑对临床试验阅片者资质、能力等要求与临床应用时使用者情况的匹配性。对专家组的培训时间和接受标准宜显著高于试验组/对照组的阅片者的培训时间和接受标准。\n2. 影像样本评阅质量控制\n一是由阅片者在独立盲法条件下对患者关于试验的影像检测结果进行解释。二是在选取试验阅片者时基于其专业能力和参与研究的可能性，充分保证阅片者对预期使用者的代表性。可考虑不同医疗机构来源、不同年资/专业水平的医师。三是将数据样本的临床诊断结果、临床参考标准判定结果、其他诊断信息（如生化检测结果、后续治疗等）或基本信息（如年龄、病史等）对试验的阅片者设盲。四是对照试验可考虑采用交叉阅片设计，交叉设计中可根据相关领域记忆曲线的研究设置合理的洗脱期。若采用多阅片者，最好每位阅片者按照不同的数据样本顺序进行评阅，有文献报道典型的洗脱期一般为4~6周。是否采用交叉设计可根据申报产品的临床应用方法、场景及适用范围等综合判定。\n人工智能辅助检测医疗器械产品还可能包括结构化报告生成、前后图像对比、正常解剖组织的分割（如肺叶、肋骨等）、流程优化、尺寸测量（包括大小、体积等）、CT值测量等非辅助决策类软件功能，可在临床试验中设置次要评价指标用于评价这些功能的安全有效性，亦可根据《医疗器械临床评价技术指导原则》提交相应临床评价资料。若提交临床评价，上述功能的验证确认可以考虑测试集测试、压力测试、对抗测试、质量良好的数据库测试结果中的一种或几种作为临床评价的支持证据；若在临床试验中设置次要指标，指标一般采用临床上对功能准确性评价的临床参考标准或学术上常用的方法，如分割的准确性考虑，与医师的分割结果的分割一致性Dice相似系数（Dice similarity coefficient）等；配准功能可考虑标志点配准误差(Fiducial Registration Error)、目标配准误差（target registration error）、标志点定位误差(Fiducial Localization Error)等。\n说明书中一般需结合临床与非临床资料给出下列适用的信息：1）临床试验总结；2）适用范围；3）数据采集设备和数据采集过程相关要求等。临床试验总结通常考虑临床数据基本信息、评价指标与结果（必要时含亚组结果）。人工智能辅助检测类产品的适用范围需明确辅助检测适应症（如肺结节、骨折），所基于的影像类型（如胸部CT或结肠镜检查影像），产品其他主要功能（如影像的显示、处理、测量和分析），以及产品临床定位（不能单独用作临床诊疗决策依据）等。\n建议申请人根据产品实际情况在说明书中体现如下方面的警告注意事项。1)软件仅辅助医师进行病灶检测，存在假阴/阳性可能，应由专业医师结合患者的病史、症状、体征、其他检查结果情况综合给出最终的病灶检出结论，核实是否需要进一步诊疗的决策，并对临床诊断结果负责。2) 产品依据YYYY年版指南（例如《胸部CT肺结节数据标注与质量控制专家共识（2018）》）设计，如诊疗指南有所更新，使用者应充分评估指南差异可能带来的风险。3）明确产品临床试验中未对病灶边界分割的准确性进行评价，如依据软件的检测结果决定手术干预或穿刺活检，医师应当充分评估其风险。\n[1]中华医学会呼吸病学分会肺癌学组， 中国肺癌防治联盟专家组，肺结节诊治中国专家共识（2018版）[J]. 中华结核和呼吸杂志， 2018,41(10): 763-771.\n[2]原国家食品药品监督管理总局.医疗器械临床试验设计指导原则:食品药品监管总局通告2018年第6号[Z].\n[3]国家药品监督管理局医疗器械技术审评中心.深度学习辅助决策医疗器械软件审评要点：国家食品药品监督管理局医疗器械技术审评中心通告2019年第7号[Z].\n[4]Guidance for Industry and Food and Drug Administration Staff Computer-Assisted Detection Devices Applied to Radiology Images and Radiology Device Data – Premarket Notification [510(k)] Submissions（FDA，2012.07.03）\n[5]Lung Cancer Screening CT ProtocolsVersion5.1（AAPM，2019.9.13）\n[6]FDA. Clinical Performance Assessment: Considerations for Computer-Assisted Detection Devices Applied to Radiology Images and Radiology Device Data – Premarket Approval (PMA) and Premarket Notification [510(k)] Submissions – Guidance for Industry and FDA Staff，(2012.11)[2021-4-15]. https://www.fda.gov/regulatory-information/search-fda-guidance-documents/clinical-performance-assessment-considerations-computer-assisted-detection-devices-applied-radiology\n[6]Sica, G.T., 2006. Bias in research studies. Radiology, 238(3), pp.780-789.\n[7]Xiao-Hua Zhou, Nacy A. Obuchowski, Donna K. McClish. Statistical Methods in Diagnostic Medicine [M]. Hoboken: JohnWiley&Sons, 2011.\n[8]国家卫生健康委员会. 人工智能辅助诊断技术临床应用质量控制指标（2017年版）:国家卫生计生委通知2017第7号[Z].\n[9]Brandon D. Gallas.et al. Evaluating Imaging and Computer-aided Detection and Diagnosis Devices at the FDA [J]. Acad Radiol. 2012 Apr; 19(4): 463–477. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5557046/\n[10]萧毅, 刘士远. 肺结节影像人工智能技术现状与思考[J]. 肿瘤影像学, 2018, 27: 249-252.\n[11]Rees, Colin J., et al. “UK key performance indicators and quality assurance standards for colonoscopy.” Gut 65.12 (2016): 1923-1929.\n[12]Misawa, Masashi, et al. “Development of a computer-aided detection system for colonoscopy and a publicly accessible large colonoscopy video database (with video).” Gastrointestinal endoscopy 93.4 (2021): 960-967.\n[13]国家消化系统疾病临床医学研究中心（上海）， 中华医学会消化内镜学分会，中华医学会健康管理学分会等. 中国早期结直肠癌筛查流程专家共识意见（2019）[J]. 中华医学会杂志，2019,99（38）：2961-2970.\n[14]Rompianesi, Gianluca, et al. “Artificial intelligence in the diagnosis and management of colorectal cancer liver metastases.” World Journal of Gastroenterology 28.1 (2022): 108.\n  [15]国家药品监督管理局.医疗器械临床评价技术指导原则:国家药品监督管理局通告2021年第73号[Z].\n肺结节CT影像辅助检测产品采用MRMC设计、交叉自身对照设计或平行对照设计。平行对照设计的具体过程及方法参考《医疗器械临床试验设计指导原则》相关内容。\n若采用MRMC的设计，试验过程具体为，将参与试验的临床医师随机划分为A、B两组。A组：医师第一阶段在使用AI辅助的条件下完成所有数据样本CT影像的肺结节检测；经一定的洗脱期后，第二阶段在不使用AI辅助的条件下完成所有数据样本CT影像的肺结节检测。B组：医师第一阶段在不使用AI辅助的条件下完成所有数据样本CT影像的肺结节检测；经一定的洗脱期后，第二阶段在使用AI辅助的条件下完成所有数据样本CT影像的肺结节检测。该试验中医师使用AI辅助的作为试验组，不使用AI辅助的作为对照组，对比试验组与对照组肺结节检测的诊断准确度指标，阅片过程中每名医师需分别在使用AI和不使用AI的条件下都完成所有入组影像数据样本的阅片。\n此类产品临床试验通常选择检出的灵敏度（以病灶为单位）和特异度(以患者为单位)等组成的复合指标，亦可在此基础上添加受试者工作特征（receiver operating Characteristic, ROC）曲线或其衍生曲线组成复合指标，如因变量自由的受试者工作特征（free-response receiver operating Characteristic, FROC）曲线、定位受试者工作特征（location-specific receiver operating characteristic, LROC）曲线等。使用ROC曲线或其衍生曲线时，可考虑在试验组医师阅片阶段，由阅片医师标记对目标疾病判定的检验效能（可考虑百分数评分），从而构建ROC曲线，并需考虑在培训阶段明确医师百分数评分的标准和方法。\n次要指标可包括AI辅助医师检出目标疾病的阳性预测值/阴性预测值，似然比，Kappa系数，软件独立检出目标疾病的灵敏度（以病灶为单位），软件独立检出目标疾病的灵敏度和特异度（以患者为单位），阅片时间，医生诊断信心评价，软件功能易用性和稳定性（可采用主观感受评价，如李克特量表等）。若同时观察量化分析等非辅助决策临床功能的性能表现，亦可设立相应次要评价指标。\n以MRMC设计为例，说明样本量估算考虑要素，若主要评价指标为AUC、结节水平灵敏度、患者水平特异度。试验中，检验的检验效能取80%。对于主要评价指标，优效性与非劣效统计分析将在单侧0.025的检验水准下进行。下述内容仅为示意举例，可根据产品具体情况合理调整相关参数：\n1.基于AUC的样本量计算，预试验或文献资料确认效应值取0.05，研究医师设定为12名，AUC优效界值取0，则总样本量不得低于87例，阴性、阳性患者比为1:1。\n2.基于结节水平灵敏度，效应值取0.069，研究医师设定为12名，优效界值取0，需阳性样本至少116个结节，假定平均每个阳性患者存在2个结节，则至少需要58例阳性数据样本。\n3.基于患者水平的特异度，效应值取-0.055，非劣效值取0.1（参考Riverain ClearRead CT 的特异度95%区间下限），在当受试医师为12名时，非劣效验证（非劣效界值取0.1）需要阴性样本达到至少157例。\n综合以上各指标样本量估算值，本试验样本量最低为215例，阳性数据样本58例，阴性数据样本157例。考虑到样本存在剔除和脱落的可能，当预期脱落率为5%，剔除率为5%时，阳性样本需要达到至少62例，阴性样本需要达到至少166例，共计228例。\n结肠息肉电子内窥镜图像辅助检测产品可考虑平行对照设计，此处以序贯的平行对照为例进行说明，申请人亦可采用其他科学的平行对照设计开展研究，序贯的平行对照具体设计及试验程序如下：\n入组的受试者随机分配到A组和B组，A组：常规结直肠镜检查先进行组：先接受无 AI 辅助的常规结直肠镜检查，对检查过程中发现的所有息肉都冻结影像留存图片。第一次退镜后在AI 辅助下再做一次结直肠镜检查，对检查过程中发现的所有息肉都冻结影像留存图片。B 组：AI 辅助检查先进行组：先接受 AI 辅助的结直肠镜检查，对检查过程中发现的所有息肉都冻结影像留存图片。第一次退镜后再进行一次常规结直肠镜检查，对检查过程中发现的所有息肉都冻结影像留存图片。分别以各组两次检出的肠息肉作为基准，比较第一次检查时，有AI辅助检测和无AI辅助检测的病变检出能力。原则上，结肠息肉人工智能辅助检测产品临床试验需将前瞻性肠镜检查样本作为临床试验研究对象。\n此类产品临床试验因不完美临床参考标准等问题，可考虑选取息肉/腺瘤的初检检出率/漏诊率或息肉检出的灵敏度与特异性等作为主要评价指标。相关指标需以息肉/腺瘤水平而非受试者水平进行计算，例如初检息肉检出率=初检发现的息肉数量÷两次检测到的息肉总数，腺瘤漏诊率=第二次检测到的腺瘤数÷两次检测到的腺瘤总数。\n次要评价指标可考虑息肉/腺瘤检出率（受试者水平），平均息肉/腺瘤检出数，软件独立评估性能（即软件独立的息肉/腺瘤检出率、息肉检测准确性等）：结直肠镜操作时间（包括各次进镜时间、退镜时间）、软件易用性评价、稳定性评价、不良事件情况等。若同时观察图像传输、储存等非辅助决策临床功能的性能表现，亦可设立相应次要评价指标。\n1. 以序贯的平行对照试验为例，若采用腺瘤漏诊率作为评价指标，样本量的计算基于腺瘤漏检率的主要结果（每个病灶），假设如下： AI 先进行组别的腺瘤漏诊率为 10%，标准检查先进行组别的腺瘤漏诊率为 30%，优效界值设定为0，在 80％统计功效的条件下， 双侧α值设为 0.05。主要评价指标为初检息肉检出率，其样本量估算公式为：\n假设每位患者的平均腺瘤数为 0.6，考虑10%的脱落率，需入组220名受试者，AI 先进行组110名，标准检查先进行组110名。\n2. 若采用初检息肉检出率作为主要评价指标，A 组： 常规结直肠镜检查先进行组； B 组： AI 辅助检查先进行组。PC、 PT 分别为 A 组和 B 组预期初检息肉检出率，其中 PC=70%、 PT=80%；|D|为两组预期率差的绝对值， |D| = |PC – PT|； Δ 为优效性界值，取 0。主要评价指标为初检息肉检出率，其样本量估算公式为：\n假设每位患者的平均息肉数为 2.5， 需约 234 例，脱落率为 20%，最终样本量为 296 例， A组和 B 组各 148 例。\n\n\n\r\n                                    如有合规相关问题，欢迎您咨询                                \n\n\r\n                                此文档有帮助吗？                                \n\r\n                                        Yes                                    \n\r\n                                        No                                    \n\n\n 或复制链接 \n京公网安备11011202101028号 | 京ICP备19053168号-2 | © 2024-2025 All Rights Reserved by ReguVerse.com"
        }
    ],
    "trend_news": [
        {
            "title": "In-depth analysis report on the development trend of China's coffee ...",
            "content": "3 days ago ... 8.1 Market Size and Growth Forecast. Based on existing data and industry ... compound annual growth rate (CAGR) of 11.7%. Market size : China's coffee ...",
            "url": "https://www.farmerscoopelevator.com/markets/stocks.php?article=abnewswire-2025-8-25-in-depth-analysis-report-on-the-development-trend-of-chinas-coffee-industry-2024-2028",
            "source": "www.farmerscoopelevator.com",
            "published_date": "2025-08-28",
            "search_engine": "Google",
            "query": "全球AI医疗诊断市场 2025年预测 增长趋势 market size forecast compound annual growth rate",
            "description": "3 days ago ... 8.1 Market Size and Growth Forecast. Based on existing data and industry ... compound annual growth rate (CAGR) of 11.7%. Market size : China's coffee ...",
            "image": "",
            "category": "trend_news",
            "search_source": "google",
            "full_content": "Farmers Cooperative Elevator Company Zero Tolerance Policy\nThe U.S. Food and Drug Administration and the U.S. Grain Standards Act allow ZERO TOLERANCE for treated seed occurring in grain.\nFCE wants to remind you to thoroughly clean all equipment that comes in contact with treated seed. Be sure to check and recheck to make sure it’s clean. One kernel of treated seed, whether it is corn, soybeans, or wheat, can contaminate an entire bin and can cost FCE millions of dollars. If you are checked and have treated kernels, by law, you may have to pay for the grain quantity that was tainted.\n© 2025 Barchart.com, Inc.  All market data is hosted and powered by Barchart.\nInformation presented is provided 'as-is' and solely for informational purposes, not for trading purposes or advice, and is delayed. No representations are made by Barchart as to its informational accuracy or completeness."
        },
        {
            "title": "5E Advanced Materials Announces Proposed Public Offering",
            "content": "7 days ago ... HESPERIA, CA / ACCESS Newswire / August 21, 2025 / 5E Advanced Materials, Inc. (\"5E\" or the \"Company\") (Nasdaq:FEAM)(ASX:5EA), a development stage company.",
            "url": "https://www.farmerscoopelevator.com/markets/stocks.php?article=accwirecq-2025-8-21-5e-advanced-materials-announces-proposed-public-offering",
            "source": "www.farmerscoopelevator.com",
            "published_date": "2025-08-28",
            "search_engine": "Google",
            "query": "全球AI医疗诊断市场 2025年预测 增长趋势 market size forecast compound annual growth rate",
            "description": "7 days ago ... HESPERIA, CA / ACCESS Newswire / August 21, 2025 / 5E Advanced Materials, Inc. (\"5E\" or the \"Company\") (Nasdaq:FEAM)(ASX:5EA), a development stage company.",
            "image": "",
            "category": "trend_news",
            "search_source": "google",
            "full_content": "Farmers Cooperative Elevator Company Zero Tolerance Policy\nThe U.S. Food and Drug Administration and the U.S. Grain Standards Act allow ZERO TOLERANCE for treated seed occurring in grain.\nFCE wants to remind you to thoroughly clean all equipment that comes in contact with treated seed. Be sure to check and recheck to make sure it’s clean. One kernel of treated seed, whether it is corn, soybeans, or wheat, can contaminate an entire bin and can cost FCE millions of dollars. If you are checked and have treated kernels, by law, you may have to pay for the grain quantity that was tainted.\n© 2025 Barchart.com, Inc.  All market data is hosted and powered by Barchart.\nInformation presented is provided 'as-is' and solely for informational purposes, not for trading purposes or advice, and is delayed. No representations are made by Barchart as to its informational accuracy or completeness."
        },
        {
            "title": "The API management market size is expected to grow from USD 4.5 billion in 2022 to USD 13.7 billion by 2027, at a Compound Annual Growth Rate (CAGR) of 25.1%",
            "content": "during the <strong>forecast</strong> period. The increase in SaaS and hybrid deployments is boosting the <strong>growth</strong> of API management <strong>market</strong>. ...",
            "url": "https://www.globenewswire.com/fr/news-release/2022/07/19/2481943/0/en/The-API-management-market-size-is-expected-to-grow-from-USD-4-5-billion-in-2022-to-USD-13-7-billion-by-2027-at-a-Compound-Annual-Growth-Rate-CAGR-of-25-1.html",
            "source": "www.globenewswire.com",
            "published_date": "",
            "search_engine": "Brave",
            "query": "全球AI医疗诊断市场 2025年预测 增长趋势 market size forecast compound annual growth rate",
            "language": "fr",
            "family_friendly": true,
            "category": "trend_news",
            "search_source": "brave",
            "full_content": "\n\n\n19 juil. 2022 09h16 HE\n\n\n | Source:\r\n            \nReportLinker\n\n\n\n\n\nReportLinker\n\nNew York, July  19, 2022  (GLOBE NEWSWIRE) -- Reportlinker.com announces the release of the report \"API Management Market by Component, Deployment Type, Organization Size, Vertical and Region - Global Forecast to 2027\" - https://www.reportlinker.com/p04796894/?utm_source=GNW API security is a way of protecting APIs from external or internal cyberattacks.As APIs are very frequently used and enable access to sensitive software functions and data, they are becoming the main target for hackers and attackers.API security is a core component of modern web application security. Thus, API security solutions support regulatory compliance, and enhanced API security is necessary to develop, deploy, manage, and operate APIs.Retail & consumer goods industry vertical to hold second largest market share in 2022APIs are driving every aspect of retail transformation, from improving customer experiences to store operations.API is a critical technology that enables retailers to transform their IT infrastructure and systems.It has provided retailers access to their remote customers with services, such as online orders, pick-up, order deliveries through delivery partners, and personalized recommendations while shopping online.Europe to hold a significant market share of API management market in 2022Europe is expected to hold the second-largest market share of the API management market.The UK, Germany, France, and the rest of Europe are considered to study the API management market.The increasing competition among organizations and the growing adoption of digital initiatives are compelling organizations to adopt the API technology.In the process of determining and verifying the market size for several segments and subsegments gathered through secondary research, extensive primary interviews were conducted with the key people.The breakup of the profiles of the primary participants as follows:• By Company Type: Tier I: 33%, Tier II: 42%, and Tier III: 25%• By Designation: C-Level: 36%, D-Level: 45%, and Others: 19%• By Region: North America: 42%, Europe: 35%, APAC: 15%, Row: 8%The report profiles the following key vendors:1. Google (US)2. IBM(US)3. Amazon Web Services (US)4. Microsoft (US)5. MuleSoft (US)6. Akamai Technologies (US)7.  WSO2(US)8. Broadcom(US)9. Axway (France)10. TIBCO Software (US)11. Oracle (US)12. Software AG (Germany)13. Red Hat (US)14. SAP (Germany)15. Torry Harris Business Solutions (US)16. Sensedia(Brazil)17. Postman (US)18. Workato (US)19. Boomi (US)20. Kong, Inc. (US)21. Tray.io (US)Research CoverageThe report segments the API management market by component(solutions and services), deployment type(on-premises, cloud), organization size (SMEs, large enterprises), vertical, and regions.The component segment includes solutions and services.Based on solutions, the market is segmented into API platform, API analytics, and API security.Based on services, the market is categorized into integration & implementation, consulting, support & maintenance, and training & education.Based on deployment type, the market is segmented into on-premises and cloud.The market is segmented based on organization size as SMEs and large enterprises.Different Vertical using API management solutions include BFSI, IT & telecom, retail & consumer goods, travel & transportation, government, media & entertainment, healthcare & life sciences, manufacturing and other industry verticals.The geographic analysis of the API management market is spread across five major regions: North America, Europe, Asia Pacific, Middle East & Africa, and Latin America.Key Benefits of Buying the ReportThe report will help the market leaders/new entrants in the API management market with information on the closest approximations of the revenue numbers for the overall API management market and the subsegments.The report will help stakeholders understand the competitive landscape and gain more insights to better position their businesses and to plan suitable go-to-market strategies.The report also helps stakeholders understand the pulse of the market and provides them with information on key market drivers, restraints, challenges, and opportunities.Read the full report: https://www.reportlinker.com/p04796894/?utm_source=GNWAbout ReportlinkerReportLinker is an award-winning market research solution. Reportlinker finds and organizes the latest industry data so you get all the market research you need - instantly, in one place.__________________________"
        },
        {
            "title": "In Vitro Diagnostic Chip Research: expanding at a compound annual growth rate (CAGR) of 8.3%",
            "content": "QY Research Inc Global <strong>Market</strong> Report Research Publisher announces the release of 2025 latest report In Vitro Diagnostic Chip Global <strong>Market</strong> Share and Ranking Overall Sales and Demand <strong>Forecast</strong> 2025 2031 Based on current situation and impact historical analysis 2020 ...",
            "url": "https://www.openpr.com/news/4157716/in-vitro-diagnostic-chip-research-expanding-at-a-compound",
            "source": "www.openpr.com",
            "published_date": "3 days ago",
            "search_engine": "Brave",
            "query": "全球AI医疗诊断市场 2025年预测 增长趋势 market size forecast compound annual growth rate",
            "language": "en",
            "family_friendly": true,
            "category": "trend_news",
            "search_source": "brave",
            "full_content": "\n Permanent link to this press release: \n\n\n\n\n\n\n Delete press release\n\n\n\n\n\n\n Edit press release\n\n\n All 5 Releases"
        },
        {
            "title": "到2030年，医疗保健市场的人工智能（AI）价值将达到1106.1亿美元，复合年增长率为38.6% MarketsandMarkets™-动脉网",
            "content": "透皮技术创新联盟TTIA · 脉网APP 扫码下载",
            "url": "https://www.vbdata.cn/intelDetail/718815",
            "source": "www.vbdata.cn",
            "published_date": "",
            "search_engine": "Brave",
            "query": "全球AI医疗诊断市场 2025年预测 增长趋势 market size forecast compound annual growth rate",
            "language": "en",
            "family_friendly": true,
            "category": "trend_news",
            "search_source": "brave",
            "full_content": "扫码下载\n动脉网\n动脉新医药\n动脉橙果局\n产业链接\nVBEF发布\n动脉生物制造\n透皮技术创新联盟TTIA\n动脉网APP扫码下载\n可切换为仅中文\n微信扫码分享\nDELRAY BEACH, Fla.\n        佛罗里达州德尔雷海滩\n,\n，\nMay 9, 2025\n2025年5月9日\n/PRNewswire/ -- The global\n/PRNewswire/ -- 全球\nArtificial Intelligence (AI) in Healthcare Market\n        医疗保健市场中的人工智能（AI）        \n, valued at\n，价值\nUS$14.92 billion\n149.2亿美元\nin 2024, is forecasted to grow at a robust CAGR of 38.6%, reaching\n在2024年，预计将以38.6%的强劲复合年增长率增长，达到\nUS$21.66 billion\n216.6亿美元\nin 2025 and an impressive\n2025年，而且令人印象深刻的是\nUS$110.61billion\n1106.1亿美元\nby 2030. The growing incidence of chronic diseases, linked with an increasing geriatric population, puts substantial financial pressure on healthcare providers. There is a rising need for the early detection of conditions such as dementia and cardiovascular disorders. This can be done by analysing imaging data to recognize patterns, which helps create personalized treatment plans. .\n到2030年，慢性病的发病率不断上升，加之老年人口的增加，给医疗服务提供者带来了巨大的财务压力。对于痴呆症和心血管疾病等疾病的早期检测需求日益增长。这可以通过分析影像数据来识别模式，从而有助于制定个性化的治疗方案。\nDownload PDF Brochure:\n下载PDF手册：\nhttps://www.marketsandmarkets.com/pdfdownloadNew.asp?id=54679303\nhttps://www.marketsandmarkets.com/pdfdownloadNew.asp?id=54679303\nBrowse in-depth TOC on '\n浏览详细目录 '\nArtificial Intelligence (AI) in Healthcare Market\n        医疗保健市场中的人工智能（AI）        \n'\n'\n882 - Tables\n882 - 表格\n61 - Figures\n61 - 数字\n738 - Pages\n738页\nBy tools,\n通过工具，\nthe Artificial Intelligence (AI) in healthcare market for machine learning has been bifurcated into deep learning, supervised learning, reinforcement learning, unsupervised learning, and other machine learning technologies. The deep learning segment accounted for the largest share of the Artificial Intelligence (AI) in healthcare market in 2024.\n医疗保健市场中用于机器学习的人工智能（AI）已分为深度学习、监督学习、强化学习、无监督学习和其他机器学习技术。深度学习部分在2024年占据了医疗保健市场中人工智能（AI）的最大份额。\nThe capability to process vast amounts of unstructured medical data, such as electronic health records (HER), imaging, and genomics, allows accurate disease diagnosis and prediction. The integration of deep learning into healthcare is significantly boosting the AI in healthcare market, leading to substantial investments in diagnostic tools and predictive analytics.\n处理大量非结构化医疗数据（如电子健康记录 (HER)、影像和基因组学）的能力可以实现准确的疾病诊断和预测。深度学习在医疗保健领域的整合正在显著推动医疗保健市场的人工智能发展，导致诊断工具和预测分析方面的大量投资。\nAs computational power and data availability continue to increase, deep learning is set to unlock further advancements, solidifying its position as a key enabler of next-generation healthcare technologies..\n随着计算能力的提升和数据可用性的增加，深度学习将迎来更多进展，进一步巩固其作为下一代医疗技术关键推动者的重要地位。\nBy end user,\n按最终用户，\nthe AI in healthcare market is segmented into healthcare providers, healthcare payers, patients, and other end users. In 2024, healthcare providers accounted for the largest share of the AI in healthcare market. The large share of this end-user segment can be attributed to the increasing budgets of hospitals to improve the quality of care provided and reduce the cost of care..\n医疗保健市场中的人工智能按医疗服务提供者、医疗支付者、患者和其他最终用户进行细分。2024年，医疗服务提供者占据了医疗保健市场中人工智能的最大份额。这一最终用户细分市场的较大份额可归因于医院不断增加预算以提高护理质量并降低护理成本。\nBy geography,\n按地理，\nthe Artificial Intelligence (AI) in healthcare market is segmented into five main regions:\n医疗保健市场中的人工智能（AI）分为五个主要地区：\nNorth America\n北美\n,\n，\nEurope\n欧洲\n,\n，\nAsia Pacific\n亚太地区\n,\n，\nLatin America\n拉丁美洲\n, and the\n，以及\nMiddle East\n中东地区\n&\n&\nAfrica\n非洲\n. The\n。这个\nAsia Pacific\n亚太地区\nregion is projected to see a substantial growth rate during the forecast period. The\n该地区在预测期内预计将出现显著的增长率。\nAsia Pacific\n亚太地区\n(APAC) region is experiencing substantial growth in adopting AI technologies within the healthcare sector, driven by a combination of demographic shifts, technological advancements, and increased investments in innovation. The rising elderly population in the region is a key factor, with the proportion of individuals aged 65 years and above increasing significantly.\n亚太地区（APAC）在医疗保健领域采用人工智能技术方面正经历显著增长，这一趋势受到人口结构变化、技术进步以及对创新投资增加的共同推动。该地区老年人口的增加是一个关键因素，65岁及以上人口的比例显著上升。\nThe demand for advanced healthcare solutions has surged as the aging population faces chronic and age-related conditions, necessitating efficient diagnostic, monitoring, and treatment tools. AI technologies are being integrated into various healthcare applications, including predictive analytics, telemedicine, medical imaging, and patient management systems.\n随着老龄化人口面临慢性病和与年龄相关的疾病，对先进医疗解决方案的需求激增，这使得高效的诊断、监测和治疗工具成为必要。人工智能技术正被整合到各种医疗应用中，包括预测分析、远程医疗、医学影像和患者管理系统。\nThese innovations aim to address gaps in healthcare access, improve diagnostic accuracy, and streamline operations across the region..\n这些创新旨在解决医疗保健获取方面的差距，提高诊断准确性，并简化整个地区的运营。\nRequest Sample Pages :\n请求样本页面：\nhttps://www.marketsandmarkets.com/requestsampleNew.asp?id=54679303\nhttps://www.marketsandmarkets.com/requestsampleNew.asp?id=54679303\nThe prominent players operating in the Artificial Intelligence (AI) in healthcare market include Koninklijke Philips N.V. (\n在医疗保健市场的人工智能（AI）领域中，主要的参与者包括皇家飞利浦电子公司（Koninklijke Philips N.V.）。\nNetherlands\n荷兰\n), Microsoft Corporation  (US), Siemens Healthineers AG (\n），微软公司（美国），西门子医疗集团（\nGermany\n德国\n), NVIDIA Corporation (US), Epic Systems Corporation (US), GE Healthcare (US), Medtronic (US), Oracle (US), Veradigm LLC  (US), Merative (IBM) (US), Google (US), Cognizant (US), Johnson & Johnson (US), Amazon Web Services, Inc. (US), among others. These companies adopted strategies such as product launches, product updates, expansions, partnerships, collaborations, mergers, and acquisitions to strengthen their market presence in the Artificial Intelligence (AI) in healthcare market..\n），英伟达公司（美国）、Epic Systems公司（美国）、GE医疗（美国）、美敦力（美国）、甲骨文（美国）、Veradigm LLC（美国）、Merative（IBM）（美国）、谷歌（美国）、高知特（美国）、强生（美国）、亚马逊网络服务公司（美国）等。这些公司采用了产品发布、产品更新、扩展、合作伙伴关系、协作、并购等策略，以加强其在医疗保健领域的人工智能（AI）市场中的地位。\nKoninklijke Philips N.V. (\n皇家飞利浦公司 (\nNetherlands\n荷兰\n)\n        )     \nKoninklijke Philips N.V. is a leading player in the AI in the healthcare market. The company utilizes AI to deliver innovative tools across various areas, including diagnostic imaging, patient monitoring, and precision medicine. Its advanced AI-driven platforms, such as the Philips HealthSuite, facilitate the integration and analysis of extensive clinical data, which supports personalized treatment plans and improves patient outcomes.\n皇家飞利浦公司是医疗保健市场中人工智能领域的领先者。该公司利用人工智能在诊断成像、患者监测和精准医疗等多个领域提供创新工具。其先进的AI驱动平台（如飞利浦HealthSuite）有助于整合和分析大量临床数据，支持个性化治疗方案并改善患者预后。\nPhilips focuses on organic and inorganic growth strategies to expand its market presence..\n飞利浦专注于有机和无机增长战略以扩大其市场影响力。\nStrategic partnerships in high-potential markets and collaborations have been the key growth strategies of the company over the years. For example, in\n多年来，公司在高潜力市场的战略合作伙伴关系和合作一直是公司增长战略的关键。例如，在\nFebruary 2025\n2025年2月\n, Philips partnered with Medtronic to educate and train cardiologists and radiologists in\n，飞利浦与美敦力合作，教育和培训心脏病学家和放射科医生在\nIndia\n印度\non advanced imaging techniques for structural heart diseases. This partnership aims to upskill 300+ clinicians in multi-modality imaging such as echocardiography (echo) and Magnetic Resonance Imaging (MRI), especially for End-Stage Renal Disease (ESRD) patients. In\n关于结构性心脏病的先进成像技术。该合作旨在提升300多名临床医生在多模式成像技术方面的能力，例如超声心动图（echo）和磁共振成像（MRI），特别是针对终末期肾病（ESRD）患者。在\nNovember 2023\n2023年11月\n, Philips and NYU Langone Health partnered to focus on patient safety and outcomes. This partnership integrated innovative health technologies, including digital pathology, clinical informatics, and AI-enabled diagnostics, enabling real-time collaboration among clinicians. The company also focuses on winning contracts across several companies in the healthcare space.\nPhilips与纽约大学朗格尼健康中心合作，重点关注患者安全和治疗效果。这项合作融合了创新的健康技术，包括数字病理学、临床信息学和人工智能诊断，使临床医生能够实时协作。该公司还专注于赢得医疗领域多家公司的合同。\nThis helps the company expand its footprint. For instance, in .\n这有助于公司扩大其业务范围。例如，在 。\nSeptember 2022\n        2022年9月        \n, Philips and Mandaya Royal Hospital Puri (MRHP) in\n，飞利浦和曼达亚皇家医院普里（MRHP）在\nJakarta\n雅加达\nunderwent a digital transformation in a strategic partnership, enhancing patient-centered care and healthcare services.\n在战略合作中经历了数字化转型，提升了以患者为中心的护理和医疗服务。\nMicrosoft\n微软\nCorporation (US):\n公司（美国）：\nMicrosoft Corporation is one of the leading providers of software & tools that include advanced AI capabilities in healthcare to improve patient outcomes, streamline operations, and drive innovation. Its Azure-based AI solutions support distinct applications such as medical imaging, genomics, and precision medicine.\n微软公司是提供软件和工具的领先供应商之一，这些软件和工具在医疗保健领域包含先进的人工智能功能，用于改善患者预后、简化操作并推动创新。其基于 Azure 的人工智能解决方案支持多种应用，例如医学影像、基因组学和精准医学。\nThe company also provides healthcare-specific AI models through its Azure AI Model Catalog, which is constructed to support hospitals and research institutions in building and deploying tailored AI solutions proficiently. Moreover, the integration of Nuance's AI-powered clinical and diagnostic tools encourages its capacity to support healthcare providers in decision-making and care delivery.\n该公司还通过其 Azure AI 模型目录提供针对医疗保健的 AI 模型，该目录旨在帮助医院和研究机构高效构建和部署定制的 AI 解决方案。此外，Nuance 的 AI 驱动临床和诊断工具的集成进一步增强了其支持医疗机构进行决策和护理交付的能力。\nThe company continuously brings AI capabilities to the platforms in large-scale customer models. For instance, in .\n公司不断在大规模客户模型的平台上引入人工智能能力。例如，在 。\nMarch 2025\n2025年3月\n, the company launched Microsoft Dragon Copilot, the first unified voice AI assistant in the healthcare industry that enables clinicians to streamline clinical documentation, surface information, and automate tasks.\n，该公司推出了微软龙副驾，这是医疗保健行业的第一个统一语音人工智能助手，使临床医生能够简化临床文档、呈现信息并自动执行任务。\nMicrosoft Corporation has invested significantly in R&D, which has improved its product portfolio and position in the AI market. Machine Learning (ML), deep learning, Natural Language Processing (NLP), and speech processing are the key focus areas of the company in the AI in healthcare market. The company continuously invests in a series of services and computational biology projects, including research support tools for next-generation precision healthcare, genomics, immunomics, CRISPR, and cellular and molecular biologics.\n微软公司在研发方面投入了大量资金，这使其产品组合和在人工智能市场的地位得到了提升。机器学习（ML）、深度学习、自然语言处理（NLP）和语音处理是该公司在医疗保健领域人工智能市场的重点聚焦领域。公司持续投资于一系列服务和计算生物学项目，包括支持下一代精准医疗、基因组学、免疫组学、CRISPR 以及细胞和分子生物学的研究工具。\nIt has a strong global presence, with key operations supported through its Azure cloud infrastructure across regions like .\n它在全球范围内拥有强大的影响力，其关键业务通过其Azure云基础设施在各个地区得到支持，如 。\nNorth America\n北美\n,\n        ，\nEurope\n欧洲\n,\n        ，\nAsia-Pacific\n亚太地区\n, and the\n，以及\nMiddle East\n中东地区\n.\n。\nFor more information,\n更多信息，请参阅以下内容。\nInquire Now!\n立即咨询！\nRelated Reports:\n相关报告：\nHealthcare IT Market\n        医疗保健IT市场\nBiomarkers Market\n生物标志物市场\nLife Science Instrumentation Market\n生命科学仪器市场\nBioinformatics Market\n        生物信息学市场\nHealthcare Analytics Market\n        医疗保健分析市场\nGet access to the latest updates on\n获取最新更新的访问权限\nArtificial Intelligence (AI) in Healthcare Companies\n        医疗保健公司中的人工智能 (AI)\nand\n和\nArtificial Intelligence (AI) in Healthcare Market Size\n        医疗保健市场中的人工智能（AI）规模\nAbout MarketsandMarkets™:\n        关于MarketsandMarkets™：\nMarketsandMarkets™ has been recognized as one of America's Best Management Consulting Firms by Forbes, as per their recent report.\n根据福布斯最近的报告，MarketsandMarkets™ 被评为美国最佳管理咨询公司之一。\nMarketsandMarkets™ is a blue ocean alternative in growth consulting and program management, leveraging a man-machine offering to drive supernormal growth for progressive organizations in the B2B space. With the widest lens on emerging technologies, we are proficient in co-creating supernormal growth for clients across the globe..\nMarketsandMarkets™ 是一家在增长咨询和项目管理领域的蓝海替代方案，利用人机协作的服务模式，为B2B领域的前沿企业推动超常规增长。凭借对新兴技术的最广泛洞察，我们擅长与全球客户共同创造超常规的增长。\nToday,\n今天，\n80% of Fortune 2000 companies rely on MarketsandMarkets\n80% 的财富2000强企业依赖 MarketsandMarkets\n, and\n，以及\n90 of the top 100 companies in each sector trust us to accelerate their revenue growth\n每个行业的前100家公司中有90家信任我们，以加速他们的收入增长。\n. With a\n。随着\nglobal clientele of over 13,000 organizations\n全球超过13,000家组织的客户群\n, we help businesses thrive in a disruptive ecosystem.\n，我们帮助企业在一个颠覆性的生态系统中茁壮成长。\nThe B2B economy is witnessing the emergence of\n        B2B经济正在见证新兴的\n$25 trillion\n25万亿美元\nin new revenue streams that are replacing existing ones within this decade. We work with clients on growth programs, helping them monetize this\n在这一十年内，新的收入来源正在取代现有的收入来源。我们与客户合作开展增长计划，帮助他们将此变现。\n$25 trillion\n25万亿美元\nopportunity through our service lines – TAM Expansion, Go-to-Market (GTM) Strategy to Execution, Market Share Gain, Account Enablement, and Thought Leadership Marketing.\n通过我们的服务线创造机会——TAM扩展、市场进入（GTM）战略到执行、市场份额增长、账户支持以及思想领导力营销。\nBuilt on the 'GIVE Growth' principle, we collaborate with several Forbes Global 2000 B2B companies to keep them future-ready. Our insights and strategies are powered by industry experts, cutting-edge AI, and our\n基于“GIVE Growth”原则，我们与多家《福布斯》全球2000强B2B公司合作，助他们保持未来竞争力。我们的洞察和策略由行业专家、尖端人工智能以及我们的资源驱动。\nMarket Intelligence Cloud, KnowledgeStore™\n市场情报云，知识库™\n, which integrates research and provides ecosystem-wide visibility into revenue shifts.\n，它整合了研究并提供了生态系统范围内的收入变化可见性。\nTo find out more, visit\n要了解更多信息，请访问\nwww.MarketsandMarkets™.com\nwww.MarketsandMarkets™.com\nor follow us on\n或关注我们\nTwitter\nTwitter\n,\n，\nLinkedIn\n领英\nand\n和\nFacebook\n脸书\n.\n。\nContact:\n联系人：\nMr.\n先生\nRohan Salgarkar\n罗汉·萨尔加卡尔\nMarketsandMarkets™ INC.\n        MarketsandMarkets™ 公司\n1615 South Congress Ave.\n        南国会大道1615号        \nSuite 103,\n        103室，        \nDelray Beach, FL\n        佛罗里达州德尔雷海滩\n33445\n33445\nUSA\n美国\n: +1-888-600-6441\n: +1-888-600-6441\nEmail:\n电子邮件：\nsales@marketsandmarkets.com\nsales@marketsandmarkets.com\nVisit Our Website:\n访问我们的网站：\nhttps://www.marketsandmarkets.com/\nhttps://www.marketsandmarkets.com/\nLogo:\n标志：\nhttps://mma.prnewswire.com/media/1868219/MarketsandMarkets_Logo.jpg\nhttps://mma.prnewswire.com/media/1868219/MarketsandMarkets_Logo.jpg\nSOURCE MarketsandMarkets\n        来源 MarketsandMarkets        \nWANT YOUR COMPANY'S NEWS\n        想要你公司的新闻吗\nFEATURED ON PRNEWSWIRE.COM?\n刊登在PRNEWSWIRE.COM上？\n440k+\n44万+\nNewsrooms &\n新闻编辑室 &\nInfluencers\n影响者\n9k+\n9千+\nDigital Media\n数字媒体\nOutlets\n插座\n270k+\n27万+\nJournalists\n记者\nOpted In\n已选择加入\nGET STARTED\n开始使用\n全球产业链接平台\n重庆市渝北区金星科技大厦A区5楼512室\n联系电话：023-67685030（重庆）\n商务合作\n动脉网APP\n友情链接"
        },
        {
            "title": "How AI Is Improving Diagnostics, Decision-Making and Care | AHA",
            "content": "The next five years will be critical for <strong>hospitals</strong> and health <strong>systems</strong> to build the infrastructure needed to support <strong>AI</strong> technology, according to the recently released Futurescan 2023.",
            "url": "https://www.aha.org/aha-center-health-innovation-market-scan/2023-05-09-how-ai-improving-diagnostics-decision-making-and-care",
            "source": "www.aha.org",
            "published_date": "",
            "search_engine": "Brave",
            "query": "AI辅助诊断系统 医院应用率 2025年预测 adoption rate hospital AI diagnosis system",
            "language": "en",
            "family_friendly": true,
            "category": "trend_news",
            "search_source": "brave",
            "full_content": "\nArtificial intelligence (AI) holds great promise in helping health care providers gain insights and improve health outcomes. Although many questions remain regarding its safety, regulation and impact, the use of AI in clinical care is no longer in its infancy and is expected to experience exponential growth in the coming years.\nAI is improving data processing, identifying patterns and generating insights that otherwise might elude discovery from a physician’s manual effort. The next five years will be critical for hospitals and health systems to build the infrastructure needed to support AI technology, according to the recently released Futurescan 2023.\n\nDeveloped by the AHA’s Society for Health Care Strategy & Market Development in collaboration with the American College of Healthcare Executives, Futurescan 2023 provides insights from thought leaders across eight topic areas, including workforce trends, health equity, the competitive environment and more. Each report is supported with data from a 2022 survey of health care leaders.\nOne of AI’s most promising roles is in clinical decision support at the point of patient care. AI algorithms analyze a vast amount of patient data to assist medical professionals in making more informed decisions about care — outperforming traditional tools like the Modified Early Warning Score (MEWS), commonly used by hospitals to calculate the risk for clinical deterioration in a patient over the next several hours.\n“While MEWS has served its purpose for a long time, and certainly did move the needle further in trying to be proactive with clinical deterioration, I think it’s pretty clear now that most of the tools that are developed using AI methods are more accurate than those bedside calculations,” said Juan Rojas, M.D., a pulmonary and critical care specialist at the University of Chicago and an expert in the application of machine learning to electronic health record data.\nRojas noted, however, that the usefulness of AI tools soon will be judged by how well they are integrated into health care systems. These complex tools require experts to monitor their use and safety, an information technology infrastructure sophisticated enough to support them and a willingness by front-line users to engage with these models.\nThe greatest application of AI in diagnostics so far has been in imaging. “The one that’s nearest and dearest to me is AI-based assistance for lung nodule detection on CT scans, but there are similar stories around breast imaging and other areas, where AI is being used by radiologists to augment their clinical decisions,” said Rojas.\nAI’s ability to recognize and process a great amount of both structured and unstructured data has led to nearly 400 Food and Drug Administration approvals of AI algorithms for the radiology field.\nHospitals today perform 3.6 billion imaging procedures annually, generating a massive amount of data. Approximately 97% of these data go unused. Machine learning allows health care professionals to structure, index and leverage this information for more accurate diagnostics.\nThe use of AI has advanced patient safety by evaluating data to produce insights, improve decision-making and optimize health outcomes. Systems that incorporate AI can improve error detection, stratify patients and manage drug delivery.\n“One of the main aspirational visions is improving patient safety across the board: identifying diagnoses, problems or risks for an event earlier so that you might change the trajectory of the final outcome for the better,” Rojas said. “If you’re a radiologist using a lung nodule model and are missing early cancers, using AI will improve patient safety. The same thing goes for six-month mortality scores or other early warning signs. I would say that all the models being used in 2022 and in the next five years will be about trying to improve patient safety.”\nDeployment of medical AI systems in routine clinical care presents an important, yet largely unfulfilled, opportunity as the medical AI community navigates the complex ethical issues that can arise from using AI in health care. According to the Futurescan survey, more than 48% of hospital CEOs and strategy leaders are confident that by 2028, health systems will have the infrastructure in place to make use of AI in augmenting clinical decision-making.\nAI is designed to enhance — not replace — traditional care delivery. Thoughtful implementation of AI offers boundless opportunities for clinical care improvements. The greatest potential for AI in the next five years is in human-centered AI design, according to Rojas.\n\nNoncommercial use of original content on www.aha.org is granted to AHA Institutional Members, their employees and State, Regional and Metro Hospital Associations unless otherwise indicated. AHA does not claim ownership of any content, including content incorporated by permission into AHA produced materials, created by any third party and cannot grant permission to use, distribute or otherwise reproduce such third party content. Request permission to reproduce AHA content."
        },
        {
            "title": "How are US hospitals adopting artificial intelligence? Early evidence from 2022 | Health Affairs Scholar | Oxford Academic",
            "content": "Abstract. US <strong>hospitals</strong> are rapidly <strong>adopting</strong> artificial intelligence (<strong>AI</strong>), but there is a lack of knowledge about <strong>AI</strong>-<strong>adopting</strong> <strong>hospitals</strong>&#x27; characteristics, tr",
            "url": "https://academic.oup.com/healthaffairsscholar/article/2/10/qxae123/7775605",
            "source": "academic.oup.com",
            "published_date": "October 4, 2024",
            "search_engine": "Brave",
            "query": "AI辅助诊断系统 医院应用率 2025年预测 adoption rate hospital AI diagnosis system",
            "language": "en",
            "family_friendly": true,
            "category": "trend_news",
            "search_source": "brave",
            "full_content": "Abstract. US <strong>hospitals</strong> are rapidly <strong>adopting</strong> artificial intelligence (<strong>AI</strong>), but there is a lack of knowledge about <strong>AI</strong>-<strong>adopting</strong> <strong>hospitals</strong>&#x27; characteristics, tr"
        },
        {
            "title": "The Role of AI in Hospitals and Clinics: Transforming Healthcare in the 21st Century - PMC",
            "content": "As healthcare <strong>systems</strong> around the world face challenges such as escalating costs, limited access, and growing demand for personalized care, artificial intelligence (<strong>AI</strong>) is emerging as a key force for transformation. This review is motivated by the ...",
            "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC11047988/",
            "source": "pmc.ncbi.nlm.nih.gov",
            "published_date": "",
            "search_engine": "Brave",
            "query": "AI辅助诊断系统 医院应用率 2025年预测 adoption rate hospital AI diagnosis system",
            "language": "en",
            "family_friendly": true,
            "category": "trend_news",
            "search_source": "brave",
            "full_content": "As healthcare <strong>systems</strong> around the world face challenges such as escalating costs, limited access, and growing demand for personalized care, artificial intelligence (<strong>AI</strong>) is emerging as a key force for transformation. This review is motivated by the ..."
        }
    ],
    "company_news": [],
    "total_count": 44
}