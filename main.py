
from mcp.server.fastmcp import FastMCP
import sys
import os
import json
from pathlib import Path
from typing import List, Dict, Optional, Union, Any
from dataclasses import dataclass
from datetime import datetime

# åŠ è½½ç¯å¢ƒå˜é‡
try:
    from dotenv import load_dotenv
    load_dotenv()
    print("âœ… ç¯å¢ƒå˜é‡åŠ è½½æˆåŠŸ")
except ImportError:
    print("âš ï¸ python-dotenvæœªå®‰è£…ï¼Œè·³è¿‡.envæ–‡ä»¶åŠ è½½")
except Exception as e:
    print(f"âš ï¸ åŠ è½½.envæ–‡ä»¶å¤±è´¥: {e}")

# æ·»åŠ search_mcpè·¯å¾„ - å¿…é¡»åœ¨collectorsè·¯å¾„ä¹‹å‰ï¼Œé¿å…åç§°å†²çª
search_mcp_path = Path(__file__).parent / "search_mcp" / "src"
if str(search_mcp_path) not in sys.path:
    sys.path.insert(0, str(search_mcp_path))

# ç¡®ä¿search_mcpæ¨¡å—å¯ä»¥è¢«æ‰¾åˆ°
search_mcp_module_path = search_mcp_path / "search_mcp"
if str(search_mcp_module_path.parent) not in sys.path:
    sys.path.insert(0, str(search_mcp_module_path.parent))

# æ·»åŠ collectorsè·¯å¾„ - æ”¾åœ¨search_mcpä¹‹åï¼Œé¿å…åç§°å†²çª
collectors_path = Path(__file__).parent / "collectors"
if str(collectors_path) not in sys.path:
    sys.path.insert(0, str(collectors_path))

# å°è¯•å¯¼å…¥æœç´¢ç»„ä»¶
try:
    # è°ƒè¯•ä¿¡æ¯
    print(f"ğŸ” å°è¯•ä»è·¯å¾„å¯¼å…¥: {search_mcp_path}")
    print(f"ğŸ” search_mcpç›®å½•å­˜åœ¨: {search_mcp_path.exists()}")
    print(f"ğŸ” config.pyæ–‡ä»¶å­˜åœ¨: {(search_mcp_path / 'search_mcp' / 'config.py').exists()}")
    
    # ç¡®ä¿æ­£ç¡®çš„å¯¼å…¥è·¯å¾„
    from search_mcp.config import SearchConfig
    from search_mcp.generators import SearchOrchestrator
    
    # åˆå§‹åŒ–æœç´¢ç»„ä»¶
    config = SearchConfig()
    print(f"ğŸ” é…ç½®åˆ›å»ºæˆåŠŸï¼ŒAPIå¯†é’¥çŠ¶æ€: {config.get_api_keys()}")
    
    orchestrator = SearchOrchestrator(config)
    search_available = True
    print(f"âœ… æœç´¢ç»„ä»¶åˆå§‹åŒ–æˆåŠŸï¼Œå¯ç”¨æ•°æ®æº: {config.get_enabled_sources()}")
except Exception as e:
    print(f"âš ï¸ æœç´¢ç»„ä»¶åˆå§‹åŒ–å¤±è´¥: {e}")
    print(f"   é”™è¯¯ç±»å‹: {type(e).__name__}")
    print(f"   æœç´¢ç»„ä»¶è·¯å¾„: {search_mcp_path}")
    print(f"   å½“å‰sys.pathåŒ…å«: {[p for p in sys.path if 'search_mcp' in p]}")
    
    # å°è¯•ç›´æ¥å¯¼å…¥æµ‹è¯•
    try:
        import search_mcp
        print(f"   search_mcpæ¨¡å—è·¯å¾„: {search_mcp.__file__}")
    except:
        print("   æ— æ³•å¯¼å…¥search_mcpæ¨¡å—")
    
    orchestrator = None
    search_available = False

# å°è¯•å¯¼å…¥LLMå¤„ç†å™¨
try:
    from collectors.llm_processor import LLMProcessor
    llm_processor = LLMProcessor()
    llm_available = True
except Exception as e:
    llm_processor = None
    llm_available = False

# Create an MCP server
mcp = FastMCP("Search Server")

@mcp.tool()
def search(query: str, max_results: int = 5) -> str:
    """Search for information using multiple sources
    
    Args:
        query: The search query string
        max_results: Maximum number of results to return (default: 5)
    """
    # å£°æ˜å…¨å±€å˜é‡
    global search_available, orchestrator
    
    # å°è¯•é‡æ–°åˆå§‹åŒ–æœç´¢æœåŠ¡
    if not search_available:
        try:
            # é‡æ–°å¯¼å…¥æœç´¢ç»„ä»¶
            from search_mcp.config import SearchConfig
            from search_mcp.generators import SearchOrchestrator
            
            config = SearchConfig()
            orchestrator = SearchOrchestrator(config)
            search_available = True
            print(f"ğŸ”„ æœç´¢æœåŠ¡é‡æ–°åˆå§‹åŒ–æˆåŠŸï¼Œå¯ç”¨æ•°æ®æº: {config.get_enabled_sources()}")
            
        except Exception as e:
            return f"æœç´¢æœåŠ¡åˆå§‹åŒ–å¤±è´¥: {str(e)}\n\nè¯·æ£€æŸ¥APIå¯†é’¥é…ç½®å’Œä¾èµ–é¡¹å®‰è£…ã€‚\nè¯¦ç»†é”™è¯¯: {type(e).__name__}"
    
    try:
        # ç›´æ¥ä½¿ç”¨SearchOrchestrator
        documents = orchestrator.search_by_category([query], "web", max_results)
        
        if documents:
            result_text = f"æ‰¾åˆ° {len(documents)} æ¡æœç´¢ç»“æœï¼š\n\n"
            for i, doc in enumerate(documents[:max_results], 1):
                result_text += f"{i}. **{doc.title}**\n"
                result_text += f"   {doc.content[:200]}...\n"
                result_text += f"   æ¥æº: {doc.url}\n\n"
            return result_text
        else:
            return "æœªæ‰¾åˆ°ç›¸å…³æœç´¢ç»“æœ"
            
    except Exception as e:
        return f"æœç´¢è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}\n\nè¯·æ£€æŸ¥APIå¯†é’¥é…ç½®å’Œç½‘ç»œè¿æ¥ã€‚"

@mcp.tool()
def parallel_search(queries: list, max_results: int = 3) -> str:
    """Search multiple queries in parallel
    
    Args:
        queries: List of search query strings
        max_results: Maximum number of results per query (default: 3)
    """
    if not search_available:
        return "æœç´¢æœåŠ¡æš‚æ—¶ä¸å¯ç”¨ï¼Œè¯·æ£€æŸ¥é…ç½®"
    
    try:
        all_results = []
        for query in queries:
            documents = orchestrator.search_by_category([query], "web", max_results)
            all_results.extend(documents[:max_results])
        
        if all_results:
            result_text = f"å¹¶è¡Œæœç´¢å®Œæˆï¼Œå…±æ‰¾åˆ° {len(all_results)} æ¡ç»“æœï¼š\n\n"
            for i, doc in enumerate(all_results, 1):
                result_text += f"{i}. **{doc.title}**\n"
                result_text += f"   {doc.content[:150]}...\n"
                result_text += f"   æ¥æº: {doc.url}\n\n"
            return result_text
        else:
            return "å¹¶è¡Œæœç´¢æœªæ‰¾åˆ°ç»“æœ"
            
    except Exception as e:
        return f"å¹¶è¡Œæœç´¢è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}"

@mcp.tool()
def analysis_mcp(analysis_type: str, data: Union[List[Dict], Dict, str], topic: str = "", **kwargs) -> str:
    """Comprehensive analysis tool that provides quality assessment, relevance analysis, intent analysis, etc.
    
    Args:
        analysis_type: Type of analysis ('quality', 'relevance', 'intent', 'structure_parsing', 'gap_analysis')
        data: Data to analyze (list of documents/dicts for quality/gap analysis, single dict for relevance, string for intent/structure)
        topic: Topic for analysis context
        **kwargs: Additional parameters specific to analysis type
        
    Returns:
        str: Analysis results in JSON format
    """
    if not llm_available:
        return json.dumps({
            "error": "LLMå¤„ç†å™¨ä¸å¯ç”¨",
            "analysis_type": analysis_type,
            "fallback_result": True
        }, ensure_ascii=False)
    
    try:
        if analysis_type == "quality":
            return _analyze_quality(data, topic, kwargs.get("analysis_aspects"))
        elif analysis_type == "relevance":
            return _analyze_relevance(data, topic)
        elif analysis_type == "intent":
            return _analyze_intent(data, kwargs.get("context", ""))
        elif analysis_type == "structure_parsing":
            return _parse_structure(data, kwargs.get("parsing_goal", ""), kwargs.get("output_schema"))
        elif analysis_type == "gap_analysis":
            return _analyze_gaps(topic, data, kwargs.get("expected_aspects"))
        else:
            return json.dumps({
                "error": f"ä¸æ”¯æŒçš„åˆ†æç±»å‹: {analysis_type}",
                "supported_types": ["quality", "relevance", "intent", "structure_parsing", "gap_analysis"]
            }, ensure_ascii=False)
            
    except Exception as e:
        return json.dumps({
            "error": f"{analysis_type}åˆ†æå¤±è´¥: {str(e)}",
            "analysis_type": analysis_type
        }, ensure_ascii=False)

def _analyze_quality(data: List[Dict], topic: str, analysis_aspects: List[str] = None) -> str:
    """Internal quality analysis function"""
    # å‡†å¤‡åˆ†ææ•°æ®
    content_data = ""
    for i, item in enumerate(data[:5]):
        if isinstance(item, dict):
            title = item.get("title", "æœªçŸ¥æ ‡é¢˜")
            content = item.get("content", item.get("summary", ""))[:200]
            source = item.get("source", "æœªçŸ¥æ¥æº")
            content_data += f"[{i+1}] æ ‡é¢˜: {title}\næ¥æº: {source}\nå†…å®¹: {content}...\n\n"
    
    template = """
è¯·å¯¹ä»¥ä¸‹æœç´¢ç»“æœè¿›è¡Œ5ç»´åº¦è´¨é‡è¯„ä¼°ã€‚

è¯„ä¼°ç»´åº¦ï¼š
1. ç›¸å…³æ€§ (Relevance): å†…å®¹ä¸ä¸»é¢˜"{topic}"çš„åŒ¹é…ç¨‹åº¦
2. å¯ä¿¡åº¦ (Credibility): æ¥æºçš„æƒå¨æ€§å’Œå†…å®¹çš„å‡†ç¡®æ€§  
3. å®Œæ•´æ€§ (Completeness): ä¿¡æ¯çš„å…¨é¢æ€§å’Œæ·±åº¦
4. æ—¶æ•ˆæ€§ (Timeliness): ä¿¡æ¯çš„æ–°é²œåº¦å’Œæ—¶é—´ç›¸å…³æ€§
5. æ€»ä½“è´¨é‡ (Overall): ç»¼åˆè¯„ä¼°

æœç´¢ç»“æœï¼š
{content_data}

è¾“å‡ºæ ¼å¼ï¼š
```json
{{
    "analysis_type": "quality_assessment",
    "score": 0.82,
    "details": {{
        "relevance": 0.85,
        "credibility": 0.75,
        "completeness": 0.80,
        "timeliness": 0.90,
        "overall": 0.82,
        "analysis_details": {{
            "relevance_reasons": "å…·ä½“åˆ†æç›¸å…³æ€§çš„ç†ç”±",
            "credibility_factors": "å½±å“å¯ä¿¡åº¦çš„å› ç´ ",
            "completeness_gaps": "ä¿¡æ¯å®Œæ•´æ€§çš„ç¼ºé™·æˆ–ä¼˜åŠ¿",
            "timeliness_assessment": "æ—¶æ•ˆæ€§è¯„ä¼°",
            "improvement_suggestions": ["æ”¹è¿›å»ºè®®1", "å»ºè®®2"]
        }}
    }},
    "reasoning": "5ç»´åº¦è´¨é‡è¯„ä¼°ï¼šç›¸å…³æ€§0.85, å¯ä¿¡åº¦0.75, å®Œæ•´æ€§0.80, æ—¶æ•ˆæ€§0.90"
}}
```
"""
    
    prompt = template.format(topic=topic, content_data=content_data)
    response = llm_processor.call_llm_api_json(
        prompt,
        "ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„ä¿¡æ¯è´¨é‡è¯„ä¼°ä¸“å®¶ï¼Œæ“…é•¿ä»å¤šä¸ªç»´åº¦è¯„ä¼°ä¿¡æ¯çš„è´¨é‡å’Œä»·å€¼ã€‚"
    )
    
    if isinstance(response, dict):
        return json.dumps(response, ensure_ascii=False, indent=2)
    else:
        return json.dumps({"error": "è´¨é‡åˆ†æç»“æœæ ¼å¼ä¸æ­£ç¡®"}, ensure_ascii=False)

def _analyze_relevance(content: Dict, topic: str) -> str:
    """Internal relevance analysis function"""
    title = content.get("title", "æœªçŸ¥æ ‡é¢˜")
    abstract = content.get("content", content.get("abstract", ""))
    if len(abstract) > 500:
        abstract = abstract[:500] + "..."
    authors = ", ".join(content.get("authors", [])) if content.get("authors") else "æœªçŸ¥"
    publish_date = content.get("publish_date", "æœªçŸ¥")
    
    template = """
è¯·åˆ†æä»¥ä¸‹å†…å®¹ä¸ä¸»é¢˜"{topic}"çš„ç›¸å…³æ€§ã€‚

åˆ†æå†…å®¹ï¼š
æ ‡é¢˜ï¼š{title}
æ‘˜è¦ï¼š{abstract}
ä½œè€…ï¼š{authors}
å‘è¡¨æ—¶é—´ï¼š{publish_date}

è¾“å‡ºæ ¼å¼ï¼š
```json
{{
    "analysis_type": "relevance_analysis",
    "score": 0.85,
    "details": {{
        "relevance_score": 0.85,
        "matching_keywords": ["å…³é”®è¯1", "å…³é”®è¯2"],
        "topic_alignment": "é«˜åº¦ç›¸å…³",
        "content_quality": "ä¼˜ç§€"
    }},
    "reasoning": "ç›¸å…³æ€§è¯„åˆ†: 0.85, ä¸»é¢˜åŒ¹é…: é«˜åº¦ç›¸å…³",
    "metadata": {{
        "matching_keywords": ["å…³é”®è¯1", "å…³é”®è¯2"],
        "content_quality": "ä¼˜ç§€"
    }}
}}
```
"""
    
    prompt = template.format(topic=topic, title=title, abstract=abstract, authors=authors, publish_date=publish_date)
    response = llm_processor.call_llm_api_json(
        prompt,
        "ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„å†…å®¹ç›¸å…³æ€§è¯„ä¼°ä¸“å®¶ï¼Œæ“…é•¿åˆ¤æ–­å†…å®¹ä¸ä¸»é¢˜çš„åŒ¹é…ç¨‹åº¦ã€‚"
    )
    
    if isinstance(response, dict):
        return json.dumps(response, ensure_ascii=False, indent=2)
    else:
        return json.dumps({"error": "ç›¸å…³æ€§åˆ†æè¿”å›æ ¼å¼ä¸æ­£ç¡®"}, ensure_ascii=False)

def _analyze_intent(user_query: str, context: str = "") -> str:
    """Internal intent analysis function"""
    template = """
è¯·åˆ†æç”¨æˆ·æŸ¥è¯¢çš„æ·±å±‚æ„å›¾å’Œéœ€æ±‚ã€‚

ç”¨æˆ·æŸ¥è¯¢ï¼š"{user_query}"
æŸ¥è¯¢ä¸Šä¸‹æ–‡ï¼š{context}

è¾“å‡ºæ ¼å¼ï¼š
```json
{{
    "analysis_type": "intent_analysis",
    "score": 0.85,
    "details": {{
        "primary_intent": "ä¸»è¦æ„å›¾æè¿°",
        "secondary_intents": ["æ¬¡è¦æ„å›¾1", "æ¬¡è¦æ„å›¾2"],
        "information_needs": {{
            "factual_info": "æ˜¯å¦éœ€è¦äº‹å®ä¿¡æ¯",
            "analysis_info": "æ˜¯å¦éœ€è¦åˆ†ææ€§ä¿¡æ¯"
        }},
        "urgency_level": "ä¸­"
    }},
    "reasoning": "ä¸»è¦æ„å›¾: ä¿¡æ¯æŸ¥è¯¢, ç½®ä¿¡åº¦: 0.85",
    "metadata": {{
        "search_queries": ["æ¨èæŸ¥è¯¢1", "æ¨èæŸ¥è¯¢2"],
        "recommended_sources": ["æ¨èä¿¡æ¯æº1", "æ¨èä¿¡æ¯æº2"]
    }}
}}
```
"""
    
    prompt = template.format(user_query=user_query, context=context)
    response = llm_processor.call_llm_api_json(
        prompt,
        "ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„ç”¨æˆ·æ„å›¾åˆ†æä¸“å®¶ï¼Œæ“…é•¿ç†è§£ç”¨æˆ·æŸ¥è¯¢èƒŒåçš„çœŸå®éœ€æ±‚ã€‚"
    )
    
    if isinstance(response, dict):
        return json.dumps(response, ensure_ascii=False, indent=2)
    else:
        return json.dumps({"error": "æ„å›¾åˆ†æè¿”å›æ ¼å¼ä¸æ­£ç¡®"}, ensure_ascii=False)

def _parse_structure(input_text: str, parsing_goal: str, output_schema: Dict = None) -> str:
    """Internal structure parsing function"""
    template = """
è¯·å°†ä»¥ä¸‹éç»“æ„åŒ–æ–‡æœ¬è§£æä¸ºç»“æ„åŒ–çš„JSONæ ¼å¼ã€‚

è¾“å…¥æ–‡æœ¬ï¼š{input_text}
è§£æç›®æ ‡ï¼š{parsing_goal}
è¾“å‡ºæ¨¡å¼ï¼š{output_schema}

è¾“å‡ºæ ¼å¼ï¼š
```json
{{
    "analysis_type": "structure_parsing",
    "score": 1.0,
    "details": {{
        "parsed_data": "è§£æåçš„ç»“æ„åŒ–æ•°æ®"
    }},
    "reasoning": "æˆåŠŸè§£ææ–‡æœ¬ç»“æ„ï¼Œç›®æ ‡: {parsing_goal}"
}}
```
"""
    
    schema_text = json.dumps(output_schema, ensure_ascii=False, indent=2) if output_schema else "é€šç”¨JSONç»“æ„"
    prompt = template.format(input_text=input_text, parsing_goal=parsing_goal, output_schema=schema_text)
    
    response = llm_processor.call_llm_api_json(
        prompt,
        "ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„æ–‡æœ¬ç»“æ„åˆ†æä¸“å®¶ï¼Œæ“…é•¿å°†éç»“æ„åŒ–æ–‡æœ¬è½¬æ¢ä¸ºç»“æ„åŒ–æ•°æ®ã€‚"
    )
    
    if isinstance(response, dict):
        return json.dumps(response, ensure_ascii=False, indent=2)
    else:
        return json.dumps({"error": "ç»“æ„è§£æè¿”å›æ ¼å¼ä¸æ­£ç¡®"}, ensure_ascii=False)

def _analyze_gaps(topic: str, existing_data: List[Dict], expected_aspects: List[str] = None) -> str:
    """Internal gap analysis function"""
    if expected_aspects is None:
        expected_aspects = ["æŠ€æœ¯åŸç†", "å‘å±•å†å²", "åº”ç”¨åœºæ™¯", "å¸‚åœºæƒ…å†µ", "æŒ‘æˆ˜é—®é¢˜", "æœªæ¥è¶‹åŠ¿"]
    
    # ç®€åŒ–çš„æ•°æ®æ‘˜è¦
    data_summary = f"å…±{len(existing_data)}æ¡æ•°æ®" if existing_data else "æ— æ•°æ®"
    
    template = """
è¯·åˆ†æå·²æœ‰ä¿¡æ¯çš„è¦†ç›–æƒ…å†µï¼Œè¯†åˆ«ä¿¡æ¯ç¼ºå£ã€‚

ä¸»é¢˜ï¼š{topic}
å·²æœ‰ä¿¡æ¯ï¼š{data_summary}
æœŸæœ›è¦†ç›–çš„æ–¹é¢ï¼š{expected_aspects}

è¾“å‡ºæ ¼å¼ï¼š
```json
{{
    "analysis_type": "gap_analysis",
    "score": 0.6,
    "details": {{
        "coverage_analysis": {{
            "well_covered": ["å·²å……åˆ†è¦†ç›–çš„æ–¹é¢1"],
            "partially_covered": ["éƒ¨åˆ†è¦†ç›–çš„æ–¹é¢1"],
            "not_covered": ["æœªè¦†ç›–çš„æ–¹é¢1"]
        }},
        "information_gaps": [
            {{
                "gap_type": "ç¼ºå£ç±»å‹",
                "description": "ç¼ºå£æè¿°",
                "priority": "é«˜",
                "suggested_queries": ["å»ºè®®æŸ¥è¯¢1"]
            }}
        ]
    }},
    "reasoning": "ä¿¡æ¯è¦†ç›–ç‡: 0.6, å‘ç°2ä¸ªä¸»è¦ç¼ºå£"
}}
```
"""
    
    prompt = template.format(
        topic=topic,
        data_summary=data_summary,
        expected_aspects=", ".join(expected_aspects)
    )
    
    response = llm_processor.call_llm_api_json(
        prompt,
        "ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„ä¿¡æ¯åˆ†æä¸“å®¶ï¼Œæ“…é•¿è¯†åˆ«ä¿¡æ¯è¦†ç›–çš„ç¼ºå£å’Œä¸è¶³ã€‚"
    )
    
    if isinstance(response, dict):
        return json.dumps(response, ensure_ascii=False, indent=2)
    else:
        return json.dumps({"error": "ç¼ºå£åˆ†æè¿”å›æ ¼å¼ä¸æ­£ç¡®"}, ensure_ascii=False)

@mcp.tool()
def query_generation_mcp(topic: str, strategy: str = "initial", context: str = "", **kwargs) -> str:
    """Generate search queries using different strategies
    
    Args:
        topic: The topic to generate queries for
        strategy: Query strategy ('initial', 'iterative', 'targeted', 'academic', 'news')
        context: Context information for query generation
        **kwargs: Additional parameters (report_type, target_audience, existing_data, etc.)
        
    Returns:
        str: Generated queries in JSON format
    """
    if not llm_available:
        # å¤‡ç”¨æŸ¥è¯¢ç”Ÿæˆ
        base_queries = [
            f"{topic} æœ€æ–°å‘å±•",
            f"{topic} æŠ€æœ¯åŸç†", 
            f"{topic} åº”ç”¨æ¡ˆä¾‹",
            f"{topic} å¸‚åœºè¶‹åŠ¿",
            f"{topic} æŒ‘æˆ˜é—®é¢˜"
        ]
        return json.dumps({
            "queries": base_queries,
            "strategy": strategy,
            "method": "fallback"
        }, ensure_ascii=False)
    
    try:
        if strategy == "initial":
            return _generate_initial_queries(topic, kwargs)
        elif strategy == "iterative":
            return _generate_iterative_queries(topic, context, kwargs)
        elif strategy == "targeted":
            return _generate_targeted_queries(topic, context, kwargs)
        elif strategy == "academic":
            return _generate_academic_queries(topic, context, kwargs)
        elif strategy == "news":
            return _generate_news_queries(topic, context, kwargs)
        else:
            return json.dumps({
                "error": f"ä¸æ”¯æŒçš„æŸ¥è¯¢ç­–ç•¥: {strategy}",
                "supported_strategies": ["initial", "iterative", "targeted", "academic", "news"]
            }, ensure_ascii=False)
            
    except Exception as e:
        return json.dumps({
            "error": f"æŸ¥è¯¢ç”Ÿæˆå¤±è´¥: {str(e)}",
            "strategy": strategy
        }, ensure_ascii=False)

def _generate_initial_queries(topic: str, kwargs: Dict) -> str:
    """Generate initial search queries"""
    report_type = kwargs.get("report_type", "ç»¼åˆæŠ¥å‘Š")
    target_audience = kwargs.get("target_audience", "é€šç”¨")
    
    template = """
ä½œä¸ºä¸€ä¸ªä¸“ä¸šçš„ä¿¡æ¯ç ”ç©¶å‘˜ï¼Œè¯·ä¸ºä¸»é¢˜"{topic}"ç”Ÿæˆåˆå§‹æœç´¢æŸ¥è¯¢ã€‚

æŠ¥å‘Šç±»å‹ï¼š{report_type}
ç›®æ ‡å—ä¼—ï¼š{target_audience}

ä»»åŠ¡è¦æ±‚ï¼š
1. ç”Ÿæˆ5-8ä¸ªå¤šæ ·åŒ–çš„æœç´¢æŸ¥è¯¢
2. è¦†ç›–ä¸»é¢˜çš„æ ¸å¿ƒæ¦‚å¿µã€æœ€æ–°å‘å±•ã€åº”ç”¨åœºæ™¯ã€æŒ‘æˆ˜é—®é¢˜
3. æŸ¥è¯¢åº”è¯¥å…·ä½“ä¸”å¯æœç´¢ï¼Œé¿å…è¿‡äºå®½æ³›
4. åŒ…å«ä¸­è‹±æ–‡å…³é”®è¯ç»„åˆï¼Œæé«˜æœç´¢è¦†ç›–é¢

è¯·æŒ‰ä»¥ä¸‹æ ¼å¼è¾“å‡ºJSONï¼š
```json
{{
    "queries": [
        "æŸ¥è¯¢1",
        "æŸ¥è¯¢2",
        "æŸ¥è¯¢3",
        "æŸ¥è¯¢4",
        "æŸ¥è¯¢5"
    ],
    "reasoning": "ç”Ÿæˆè¿™äº›æŸ¥è¯¢çš„ç†ç”±å’Œç­–ç•¥è¯´æ˜"
}}
```
"""
    
    prompt = template.format(topic=topic, report_type=report_type, target_audience=target_audience)
    response = llm_processor.call_llm_api_json(
        prompt,
        "ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„æœç´¢æŸ¥è¯¢ä¸“å®¶ï¼Œæ“…é•¿ä¸ºä¸åŒä¸»é¢˜ç”Ÿæˆé«˜æ•ˆçš„æœç´¢æŸ¥è¯¢ã€‚"
    )
    
    if isinstance(response, dict) and "queries" in response:
        return json.dumps(response, ensure_ascii=False, indent=2)
    else:
        return json.dumps({"error": "åˆå§‹æŸ¥è¯¢ç”Ÿæˆæ ¼å¼ä¸æ­£ç¡®"}, ensure_ascii=False)

def _generate_iterative_queries(topic: str, context: str, kwargs: Dict) -> str:
    """Generate iterative/supplementary queries"""
    existing_data_summary = kwargs.get("existing_data_summary", "æ— å·²æœ‰æ•°æ®")
    
    template = """
åŸºäºä»¥ä¸‹å·²çŸ¥ä¿¡æ¯å’Œæœç´¢ç»“æœï¼Œä¸ºä¸»é¢˜"{topic}"ç”Ÿæˆè¡¥å……æ€§æŸ¥è¯¢ã€‚

å·²çŸ¥ä¿¡æ¯æ‘˜è¦ï¼š{context}
ç°æœ‰æ•°æ®ç±»å‹ï¼š{existing_data_summary}

ä»»åŠ¡è¦æ±‚ï¼š
1. åˆ†æå·²æœ‰ä¿¡æ¯çš„è¦†ç›–é¢å’Œç¼ºå£
2. ç”Ÿæˆ3-5ä¸ªå…¨æ–°çš„ã€è¡¥å……æ€§çš„æŸ¥è¯¢
3. é‡ç‚¹å…³æ³¨å°šæœªå……åˆ†æ¢ç´¢çš„æ–¹é¢
4. é¿å…ä¸å·²æœ‰æŸ¥è¯¢é‡å¤

è¾“å‡ºæ ¼å¼ï¼š
```json
{{
    "gaps_identified": ["å‘ç°çš„ä¿¡æ¯ç¼ºå£1", "ç¼ºå£2"],
    "queries": [
        "è¡¥å……æŸ¥è¯¢1",
        "è¡¥å……æŸ¥è¯¢2",
        "è¡¥å……æŸ¥è¯¢3"
    ],
    "reasoning": "åŸºäºç¼ºå£åˆ†æç”ŸæˆæŸ¥è¯¢çš„ç†ç”±"
}}
```
"""
    
    prompt = template.format(topic=topic, context=context, existing_data_summary=existing_data_summary)
    response = llm_processor.call_llm_api_json(
        prompt,
        "ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„ä¿¡æ¯åˆ†æä¸“å®¶ï¼Œæ“…é•¿è¯†åˆ«ä¿¡æ¯ç¼ºå£å¹¶ç”Ÿæˆè¡¥å……æ€§æŸ¥è¯¢ã€‚"
    )
    
    if isinstance(response, dict):
        return json.dumps(response, ensure_ascii=False, indent=2)
    else:
        return json.dumps({"error": "è¿­ä»£æŸ¥è¯¢ç”Ÿæˆæ ¼å¼ä¸æ­£ç¡®"}, ensure_ascii=False)

def _generate_targeted_queries(topic: str, context: str, kwargs: Dict) -> str:
    """Generate targeted queries for specific sections"""
    section_title = context.split("|")[0] if "|" in context else "æœªæŒ‡å®šç« èŠ‚"
    section_context = context.split("|")[1] if "|" in context else context
    
    template = """
ä¸ºæŠ¥å‘Šç« èŠ‚"{section_title}"ç”Ÿæˆé«˜åº¦é’ˆå¯¹æ€§çš„æœç´¢æŸ¥è¯¢ã€‚

ä¸»é¢˜ï¼š{topic}
ç« èŠ‚ä¿¡æ¯ï¼š{section_context}

ä»»åŠ¡è¦æ±‚ï¼š
1. ç”Ÿæˆ3-4ä¸ªä¸“é—¨é’ˆå¯¹è¯¥ç« èŠ‚çš„ç²¾å‡†æŸ¥è¯¢
2. æŸ¥è¯¢åº”è¯¥èƒ½è·å–è¯¥ç« èŠ‚æ‰€éœ€çš„å…·ä½“ä¿¡æ¯
3. è€ƒè™‘ç« èŠ‚åœ¨æ•´ä¸ªæŠ¥å‘Šä¸­çš„ä½œç”¨å’Œä½ç½®
4. ç¡®ä¿æŸ¥è¯¢çš„ä¸“ä¸šæ€§å’Œé’ˆå¯¹æ€§

è¾“å‡ºæ ¼å¼ï¼š
```json
{{
    "section_focus": "è¯¥ç« èŠ‚çš„æ ¸å¿ƒå…³æ³¨ç‚¹",
    "queries": [
        "é’ˆå¯¹æ€§æŸ¥è¯¢1",
        "é’ˆå¯¹æ€§æŸ¥è¯¢2",
        "é’ˆå¯¹æ€§æŸ¥è¯¢3"
    ],
    "expected_content": "æœŸæœ›é€šè¿‡è¿™äº›æŸ¥è¯¢è·å¾—çš„ä¿¡æ¯ç±»å‹"
}}
```
"""
    
    prompt = template.format(topic=topic, section_title=section_title, section_context=section_context)
    response = llm_processor.call_llm_api_json(
        prompt,
        "ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„å†…å®¹ç­–åˆ’ä¸“å®¶ï¼Œæ“…é•¿ä¸ºç‰¹å®šç« èŠ‚ç”Ÿæˆç²¾å‡†çš„æœç´¢æŸ¥è¯¢ã€‚"
    )
    
    if isinstance(response, dict):
        return json.dumps(response, ensure_ascii=False, indent=2)
    else:
        return json.dumps({"error": "å®šå‘æŸ¥è¯¢ç”Ÿæˆæ ¼å¼ä¸æ­£ç¡®"}, ensure_ascii=False)

def _generate_academic_queries(topic: str, context: str, kwargs: Dict) -> str:
    """Generate academic-oriented queries"""
    academic_level = kwargs.get("academic_level", "ç ”ç©¶ç”Ÿ/ä¸“ä¸šç ”ç©¶äººå‘˜çº§åˆ«")
    
    template = """
ä¸ºå­¦æœ¯ç ”ç©¶ä¸»é¢˜"{topic}"ç”Ÿæˆå­¦æœ¯å¯¼å‘çš„æœç´¢æŸ¥è¯¢ã€‚

ç ”ç©¶èƒŒæ™¯ï¼š{context}
ç ”ç©¶æ·±åº¦è¦æ±‚ï¼š{academic_level}

ä»»åŠ¡è¦æ±‚ï¼š
1. ç”Ÿæˆ4-6ä¸ªå­¦æœ¯æ€§æœç´¢æŸ¥è¯¢
2. åŒ…å«ä¸“ä¸šæœ¯è¯­å’Œæ¦‚å¿µ
3. è¦†ç›–ç†è®ºåŸºç¡€ã€ç ”ç©¶æ–¹æ³•ã€æœ€æ–°è¿›å±•
4. é€‚åˆåœ¨å­¦æœ¯æ•°æ®åº“ä¸­æœç´¢

è¾“å‡ºæ ¼å¼ï¼š
```json
{{
    "academic_areas": ["ç†è®ºåŸºç¡€", "ç ”ç©¶æ–¹æ³•", "åº”ç”¨å®ä¾‹"],
    "queries": [
        "å­¦æœ¯æŸ¥è¯¢1",
        "å­¦æœ¯æŸ¥è¯¢2",
        "å­¦æœ¯æŸ¥è¯¢3"
    ],
    "keywords": ["å…³é”®å­¦æœ¯æœ¯è¯­1", "æœ¯è¯­2"]
}}
```
"""
    
    prompt = template.format(topic=topic, context=context, academic_level=academic_level)
    response = llm_processor.call_llm_api_json(
        prompt,
        "ä½ æ˜¯ä¸€ä½èµ„æ·±çš„å­¦æœ¯ç ”ç©¶ä¸“å®¶ï¼Œæ“…é•¿ä¸ºå­¦æœ¯ç ”ç©¶ç”Ÿæˆä¸“ä¸šçš„æœç´¢æŸ¥è¯¢ã€‚"
    )
    
    if isinstance(response, dict):
        return json.dumps(response, ensure_ascii=False, indent=2)
    else:
        return json.dumps({"error": "å­¦æœ¯æŸ¥è¯¢ç”Ÿæˆæ ¼å¼ä¸æ­£ç¡®"}, ensure_ascii=False)

def _generate_news_queries(topic: str, context: str, kwargs: Dict) -> str:
    """Generate news-oriented queries"""
    time_range = kwargs.get("time_range", "æœ€è¿‘30å¤©")
    news_focus = kwargs.get("news_focus", "è¡Œä¸šåŠ¨æ€å’Œæ”¿ç­–å˜åŒ–")
    
    template = """
ä¸ºæ–°é—»è¯é¢˜"{topic}"ç”Ÿæˆæ—¶æ•ˆæ€§æœç´¢æŸ¥è¯¢ã€‚

æ—¶é—´èŒƒå›´ï¼š{time_range}
å…³æ³¨ç„¦ç‚¹ï¼š{news_focus}
èƒŒæ™¯ä¿¡æ¯ï¼š{context}

ä»»åŠ¡è¦æ±‚ï¼š
1. ç”Ÿæˆ4-5ä¸ªæ–°é—»å¯¼å‘çš„æœç´¢æŸ¥è¯¢
2. å…³æ³¨æœ€æ–°åŠ¨æ€ã€çªå‘äº‹ä»¶ã€è¶‹åŠ¿å˜åŒ–
3. åŒ…å«æ—¶é—´æ•æ„Ÿçš„å…³é”®è¯
4. é€‚åˆåœ¨æ–°é—»å¹³å°æœç´¢

è¾“å‡ºæ ¼å¼ï¼š
```json
{{
    "news_angles": ["çªå‘äº‹ä»¶", "æ”¿ç­–å˜åŒ–", "å¸‚åœºåŠ¨æ€"],
    "queries": [
        "æ–°é—»æŸ¥è¯¢1",
        "æ–°é—»æŸ¥è¯¢2",
        "æ–°é—»æŸ¥è¯¢3"
    ],
    "urgency_level": "ä¿¡æ¯æ—¶æ•ˆæ€§è¯„ä¼°"
}}
```
"""
    
    prompt = template.format(topic=topic, context=context, time_range=time_range, news_focus=news_focus)
    response = llm_processor.call_llm_api_json(
        prompt,
        "ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„æ–°é—»åˆ†æå¸ˆï¼Œæ“…é•¿ä¸ºæ—¶äº‹è¯é¢˜ç”Ÿæˆé«˜æ—¶æ•ˆæ€§çš„æœç´¢æŸ¥è¯¢ã€‚"
    )
    
    if isinstance(response, dict):
        return json.dumps(response, ensure_ascii=False, indent=2)
    else:
        return json.dumps({"error": "æ–°é—»æŸ¥è¯¢ç”Ÿæˆæ ¼å¼ä¸æ­£ç¡®"}, ensure_ascii=False)

@mcp.tool()
def outline_writer_mcp(topic: str, report_type: str = "comprehensive", user_requirements: str = "", **kwargs) -> str:
    """Create structured outlines for reports
    
    Args:
        topic: The report topic
        report_type: Type of report ('academic', 'business', 'technical', 'industry', 'comprehensive')
        user_requirements: User's specific requirements
        **kwargs: Additional parameters (reference_data, etc.)
        
    Returns:
        str: Structured outline in JSON format
    """
    if not llm_available:
        # å¤‡ç”¨å¤§çº²ç”Ÿæˆ
        return _generate_fallback_outline(topic, report_type)
    
    try:
        template = _get_outline_template(report_type)
        
        template_params = {
            "topic": topic,
            "report_type": report_type,
            "user_requirements": user_requirements or "æ— ç‰¹æ®Šè¦æ±‚"
        }
        
        # å¦‚æœæœ‰å‚è€ƒæ•°æ®ï¼Œæ·»åŠ ç›¸å…³ä¿¡æ¯
        reference_data = kwargs.get("reference_data", [])
        if reference_data:
            data_summary = _summarize_reference_data(reference_data)
            template_params["reference_info"] = f"\nå‚è€ƒæ•°æ®æ‘˜è¦ï¼š\n{data_summary}"
        else:
            template_params["reference_info"] = ""
        
        prompt = template.format(**template_params)
        
        response = llm_processor.call_llm_api_json(
            prompt,
            f"ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„{report_type}ä¸“å®¶ï¼Œæ“…é•¿åˆ›å»ºé€»è¾‘æ¸…æ™°ã€ç»“æ„åˆç†çš„æŠ¥å‘Šå¤§çº²ã€‚"
        )
        
        if isinstance(response, dict):
            return json.dumps(response, ensure_ascii=False, indent=2)
        else:
            return json.dumps({"error": "å¤§çº²ç”Ÿæˆæ ¼å¼ä¸æ­£ç¡®"}, ensure_ascii=False)
            
    except Exception as e:
        return json.dumps({
            "error": f"å¤§çº²ç”Ÿæˆå¤±è´¥: {str(e)}",
            "fallback": _generate_fallback_outline(topic, report_type)
        }, ensure_ascii=False)

def _get_outline_template(report_type: str) -> str:
    """Get outline template based on report type"""
    templates = {
        "academic": """
è¯·ä¸ºå­¦æœ¯ç ”ç©¶ä¸»é¢˜"{topic}"åˆ›å»ºä¸€ä¸ªæ ‡å‡†çš„å­¦æœ¯æŠ¥å‘Šå¤§çº²ã€‚

ç”¨æˆ·ç‰¹æ®Šè¦æ±‚ï¼š{user_requirements}
{reference_info}

è¯·æŒ‰ç…§å­¦æœ¯æŠ¥å‘Šçš„æ ‡å‡†ç»“æ„ï¼Œåˆ›å»ºè¯¦ç»†çš„å±‚çº§åŒ–å¤§çº²ï¼š

1. **ç ”ç©¶èƒŒæ™¯ä¸æ„ä¹‰**
2. **æ–‡çŒ®ç»¼è¿°**
3. **ç ”ç©¶ç›®æ ‡ä¸å†…å®¹**
4. **ç ”ç©¶æ–¹æ³•ä¸æŠ€æœ¯è·¯çº¿**
5. **é¢„æœŸç»“æœä¸åˆ›æ–°ç‚¹**
6. **å‚è€ƒæ–‡çŒ®**

è¾“å‡ºJSONæ ¼å¼çš„ç»“æ„åŒ–å¤§çº²ã€‚
""",
        "business": """
è¯·ä¸ºå•†ä¸šä¸»é¢˜"{topic}"åˆ›å»ºä¸€ä¸ªæ ‡å‡†çš„å•†ä¸šæŠ¥å‘Šå¤§çº²ã€‚

ç”¨æˆ·ç‰¹æ®Šè¦æ±‚ï¼š{user_requirements}
{reference_info}

è¯·æŒ‰ç…§å•†ä¸šæŠ¥å‘Šçš„æ ‡å‡†ç»“æ„ï¼š

1. **æ‰§è¡Œæ‘˜è¦**
2. **å¸‚åœºåˆ†æ**
3. **äº§å“/æœåŠ¡åˆ†æ**
4. **å•†ä¸šæ¨¡å¼**
5. **é£é™©è¯„ä¼°**
6. **å»ºè®®ä¸ç»“è®º**

è¾“å‡ºJSONæ ¼å¼çš„ç»“æ„åŒ–å¤§çº²ã€‚
""",
        "comprehensive": """
è¯·ä¸ºä¸»é¢˜"{topic}"åˆ›å»ºä¸€ä¸ªç»¼åˆæ€§æŠ¥å‘Šå¤§çº²ã€‚

æŠ¥å‘Šç±»å‹ï¼š{report_type}
ç”¨æˆ·ç‰¹æ®Šè¦æ±‚ï¼š{user_requirements}
{reference_info}

è¯·æ ¹æ®ä¸»é¢˜ç‰¹ç‚¹å’Œç”¨æˆ·è¦æ±‚ï¼Œåˆ›å»ºä¸€ä¸ªé€»è¾‘æ¸…æ™°ã€ç»“æ„åˆç†çš„å¤§çº²ã€‚

åŸºæœ¬ç»“æ„æ¡†æ¶ï¼š
1. **å¼•è¨€/æ¦‚è¿°** - èƒŒæ™¯ä»‹ç»å’Œç›®æ ‡è®¾å®š
2. **æ ¸å¿ƒå†…å®¹** - æ ¹æ®ä¸»é¢˜ç‰¹ç‚¹ç»„ç»‡2-4ä¸ªä¸»è¦ç« èŠ‚
3. **åˆ†æè®¨è®º** - æ·±å…¥åˆ†æå’Œè®¨è®º
4. **ç»“è®ºå»ºè®®** - æ€»ç»“å’Œå»ºè®®

è¾“å‡ºJSONæ ¼å¼çš„ç»“æ„åŒ–å¤§çº²ã€‚
"""
    }
    
    return templates.get(report_type.lower(), templates["comprehensive"])

def _generate_fallback_outline(topic: str, report_type: str) -> str:
    """Generate fallback outline when LLM is not available"""
    if "academic" in report_type.lower():
        sections = [
            {"title": "ç ”ç©¶èƒŒæ™¯", "description": "ä»‹ç»ç ”ç©¶èƒŒæ™¯å’Œæ„ä¹‰"},
            {"title": "æ–‡çŒ®ç»¼è¿°", "description": "å›é¡¾ç›¸å…³ç ”ç©¶å’Œç†è®ºåŸºç¡€"},
            {"title": "ç ”ç©¶æ–¹æ³•", "description": "è¯´æ˜ç ”ç©¶æ–¹æ³•å’ŒæŠ€æœ¯è·¯çº¿"},
            {"title": "é¢„æœŸç»“æœ", "description": "æè¿°é¢„æœŸæˆæœå’Œåˆ›æ–°ç‚¹"}
        ]
    elif "business" in report_type.lower():
        sections = [
            {"title": "å¸‚åœºåˆ†æ", "description": "åˆ†æå¸‚åœºç°çŠ¶å’Œå‘å±•è¶‹åŠ¿"},
            {"title": "äº§å“æœåŠ¡", "description": "æè¿°äº§å“æˆ–æœåŠ¡ç‰¹ç‚¹"},
            {"title": "å•†ä¸šæ¨¡å¼", "description": "è¯´æ˜å•†ä¸šæ¨¡å¼å’Œç›ˆåˆ©æ¨¡å¼"},
            {"title": "é£é™©è¯„ä¼°", "description": "è¯†åˆ«å’Œè¯„ä¼°ä¸»è¦é£é™©"}
        ]
    else:
        sections = [
            {"title": "æ¦‚è¿°", "description": f"ä»‹ç»{topic}çš„åŸºæœ¬æƒ…å†µ"},
            {"title": "ç°çŠ¶åˆ†æ", "description": f"åˆ†æ{topic}çš„ç°çŠ¶"},
            {"title": "å‘å±•è¶‹åŠ¿", "description": f"æ¢è®¨{topic}çš„å‘å±•è¶‹åŠ¿"},
            {"title": "æ€»ç»“å»ºè®®", "description": "æ€»ç»“å’Œå»ºè®®"}
        ]
    
    outline = {
        "title": f"{topic}æŠ¥å‘Š",
        "level": 0,
        "order": 0,
        "description": f"å…³äº{topic}çš„{report_type}æŠ¥å‘Š",
        "subsections": [
            {
                "title": section["title"],
                "level": 1,
                "order": i + 1,
                "description": section["description"],
                "estimated_length": "800-1200å­—",
                "subsections": []
            }
            for i, section in enumerate(sections)
        ]
    }
    
    return json.dumps(outline, ensure_ascii=False, indent=2)

def _summarize_reference_data(reference_data: List[Dict]) -> str:
    """Summarize reference data for outline generation"""
    if not reference_data:
        return "æ— å‚è€ƒæ•°æ®"
    
    summaries = []
    for i, item in enumerate(reference_data[:5]):
        if isinstance(item, dict):
            title = item.get("title", f"æ–‡æ¡£{i+1}")
            content = item.get("content", "")[:100]
            summaries.append(f"[{i+1}] {title} - {content}...")
    
    return "\n".join(summaries)

@mcp.tool()
def summary_writer_mcp(content_data: Union[List[Dict], str], length_constraint: str = "200-300å­—", format: str = "paragraph", **kwargs) -> str:
    """Generate summaries from content data
    
    Args:
        content_data: Content to summarize (list of documents/dicts or string)
        length_constraint: Length constraint (e.g., "200-300å­—")
        format: Output format ('paragraph', 'bullet_points', 'structured', 'executive', 'academic')
        **kwargs: Additional parameters (focus_areas, tone, target_audience, etc.)
        
    Returns:
        str: Generated summary
    """
    if not llm_available:
        return _generate_fallback_summary(content_data, length_constraint, format)
    
    try:
        # å‡†å¤‡å†…å®¹æ•°æ®
        prepared_content = _prepare_content_for_summary(content_data)
        
        # è·å–é…ç½®å‚æ•°
        focus_areas = kwargs.get("focus_areas", [])
        tone = kwargs.get("tone", "professional")
        target_audience = kwargs.get("target_audience", "é€šç”¨")
        
        # é€‰æ‹©åˆé€‚çš„æ¨¡æ¿
        template = _get_summary_template(format)
        
        # å‡†å¤‡æ¨¡æ¿å‚æ•°
        tone_descriptions = {
            "professional": "ä¸“ä¸šã€æ­£å¼çš„å•†åŠ¡è¯­è¨€",
            "academic": "å­¦æœ¯ã€ä¸¥è°¨çš„ç ”ç©¶è¯­è¨€",
            "casual": "è½»æ¾ã€æ˜“æ‡‚çš„é€šä¿—è¯­è¨€",
            "technical": "æŠ€æœ¯ã€ç²¾ç¡®çš„ä¸“ä¸šè¯­è¨€"
        }
        
        template_params = {
            "content_data": prepared_content,
            "length_constraint": length_constraint,
            "target_audience": target_audience,
            "tone_description": tone_descriptions.get(tone, tone_descriptions["professional"]),
            "focus_areas": ", ".join(focus_areas) if focus_areas else "å…¨é¢è¦†ç›–ä¸»è¦å†…å®¹"
        }
        
        # æ ¼å¼åŒ–prompt
        prompt = template.format(**template_params)
        
        # è®¡ç®—tokené™åˆ¶
        length_parts = length_constraint.replace("å­—", "").replace("è¯", "").replace(" ", "")
        if "-" in length_parts:
            try:
                max_length = int(length_parts.split("-")[1])
                max_tokens = min(max_length * 2, 2000)
            except:
                max_tokens = 1000
        else:
            try:
                max_tokens = min(int(length_parts) * 2, 2000)
            except:
                max_tokens = 1000
        
        # è°ƒç”¨LLM
        summary = llm_processor.call_llm_api(
            prompt,
            f"ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„å†…å®¹æ‘˜è¦ä¸“å®¶ï¼Œæ“…é•¿å°†å¤æ‚ä¿¡æ¯æµ“ç¼©ä¸ºç®€æ´ã€å‡†ç¡®çš„æ‘˜è¦ã€‚ä½ çš„è¯­è¨€é£æ ¼æ˜¯{tone}ï¼Œç›®æ ‡å—ä¼—æ˜¯{target_audience}ã€‚",
            temperature=0.3,
            max_tokens=max_tokens
        )
        
        # åå¤„ç†æ‘˜è¦
        return _post_process_summary(summary)
        
    except Exception as e:
        return f"æ‘˜è¦ç”Ÿæˆå¤±è´¥: {str(e)}"

def _get_summary_template(format: str) -> str:
    """Get summary template based on format"""
    templates = {
        "paragraph": """
è¯·ä¸ºä»¥ä¸‹å†…å®¹æ’°å†™ä¸€ä¸ªç®€æ´ã€å‡†ç¡®çš„æ®µè½å¼æ‘˜è¦ã€‚

åŸå§‹å†…å®¹ï¼š
{content_data}

æ‘˜è¦è¦æ±‚ï¼š
- é•¿åº¦é™åˆ¶ï¼š{length_constraint}
- ç›®æ ‡å—ä¼—ï¼š{target_audience}
- è¯­è¨€é£æ ¼ï¼š{tone_description}
- é‡ç‚¹å…³æ³¨ï¼š{focus_areas}

æ’°å†™åŸåˆ™ï¼š
1. **æµ“ç¼©ç²¾ç‚¼**ï¼šå»é™¤å†—ä½™ä¿¡æ¯ï¼Œä¿ç•™æ ¸å¿ƒè¦ç‚¹
2. **å¿ äºäº‹å®**ï¼šä¸æ·»åŠ åŸæ–‡ä¸­æ²¡æœ‰çš„ä¿¡æ¯
3. **é€»è¾‘æ¸…æ™°**ï¼šæŒ‰ç…§é‡è¦æ€§å’Œé€»è¾‘é¡ºåºç»„ç»‡å†…å®¹
4. **è¯­è¨€æµç•…**ï¼šä½¿ç”¨è¿è´¯çš„æ®µè½å½¢å¼è¡¨è¾¾

è¯·ç›´æ¥è¾“å‡ºæ‘˜è¦å†…å®¹ï¼Œä¸è¦åŒ…å«å…¶ä»–è§£é‡Šæ€§æ–‡å­—ã€‚
""",
        "bullet_points": """
è¯·ä¸ºä»¥ä¸‹å†…å®¹æ’°å†™ä¸€ä¸ªè¦ç‚¹å¼æ‘˜è¦ã€‚

åŸå§‹å†…å®¹ï¼š
{content_data}

æ‘˜è¦è¦æ±‚ï¼š
- é•¿åº¦é™åˆ¶ï¼š{length_constraint}
- ç›®æ ‡å—ä¼—ï¼š{target_audience}
- æ ¼å¼ï¼šé¡¹ç›®ç¬¦å·åˆ—è¡¨
- é‡ç‚¹å…³æ³¨ï¼š{focus_areas}

è¾“å‡ºæ ¼å¼ï¼š
- æ ¸å¿ƒè¦ç‚¹1
- æ ¸å¿ƒè¦ç‚¹2
- æ ¸å¿ƒè¦ç‚¹3
- ...

è¯·ç›´æ¥è¾“å‡ºè¦ç‚¹åˆ—è¡¨ï¼Œä¸è¦åŒ…å«å…¶ä»–è§£é‡Šæ€§æ–‡å­—ã€‚
""",
        "executive": """
è¯·ä¸ºä»¥ä¸‹å†…å®¹æ’°å†™ä¸€ä¸ªæ‰§è¡Œæ‘˜è¦ã€‚

åŸå§‹å†…å®¹ï¼š
{content_data}

æ‘˜è¦è¦æ±‚ï¼š
- é•¿åº¦é™åˆ¶ï¼š{length_constraint}
- ç›®æ ‡å—ä¼—ï¼š{target_audience}ï¼ˆå†³ç­–è€…å’Œç®¡ç†å±‚ï¼‰
- æ ¼å¼ï¼šæ‰§è¡Œæ‘˜è¦
- é‡ç‚¹å…³æ³¨ï¼š{focus_areas}

è¾“å‡ºæ ¼å¼ï¼š
**æ¦‚è¿°**
[ç®€è¦æ¦‚è¿°æ ¸å¿ƒå†…å®¹]

**å…³é”®å‘ç°**
[æœ€é‡è¦çš„å‘ç°å’Œæ´å¯Ÿ]

**å½±å“åˆ†æ**
[å¯¹ä¸šåŠ¡/è¡Œä¸šçš„å½±å“]

**å»ºè®®è¡ŒåŠ¨**
[æ¨èçš„å…·ä½“è¡ŒåŠ¨]

è¯·æŒ‰ç…§æ‰§è¡Œæ‘˜è¦çš„æ ‡å‡†æ ¼å¼è¾“å‡ºï¼Œè¯­è¨€è¦ä¸“ä¸šä¸”å…·æœ‰è¯´æœåŠ›ã€‚
"""
    }
    
    return templates.get(format, templates["paragraph"])

def _prepare_content_for_summary(content_data: Union[List[Dict], str]) -> str:
    """Prepare content data for summary generation"""
    if isinstance(content_data, str):
        return content_data
    
    if not content_data:
        return "æ— å†…å®¹æ•°æ®"
    
    content_parts = []
    for i, item in enumerate(content_data):
        if isinstance(item, dict):
            title = item.get("title", f"æ–‡æ¡£{i+1}")
            content = item.get("content", item.get("summary", item.get("abstract", "")))
            content_parts.append(f"[{title}]\n{content}")
    
    return "\n\n".join(content_parts)

def _post_process_summary(summary: str) -> str:
    """Post-process the generated summary"""
    if not summary:
        return "æ‘˜è¦ç”Ÿæˆå¤±è´¥"
    
    # æ¸…ç†æ ¼å¼
    summary = summary.strip()
    
    # ç§»é™¤å¯èƒ½çš„æ ‡é¢˜æˆ–å‰è¨€
    unwanted_prefixes = [
        "æ‘˜è¦ï¼š", "æ€»ç»“ï¼š", "æ¦‚è¿°ï¼š", "Summary:", "ä»¥ä¸‹æ˜¯æ‘˜è¦ï¼š", 
        "æ ¹æ®æä¾›çš„å†…å®¹", "åŸºäºä»¥ä¸Šä¿¡æ¯", "æ‘˜è¦å¦‚ä¸‹ï¼š"
    ]
    
    for prefix in unwanted_prefixes:
        if summary.startswith(prefix):
            summary = summary[len(prefix):].strip()
    
    return summary

def _generate_fallback_summary(content_data, length_constraint, format) -> str:
    """Generate fallback summary when LLM is not available"""
    try:
        # å‡†å¤‡å†…å®¹
        if isinstance(content_data, str):
            text = content_data
        elif isinstance(content_data, list) and content_data:
            if isinstance(content_data[0], dict):
                text = " ".join([item.get("content", "")[:200] for item in content_data[:3]])
            else:
                text = str(content_data)
        else:
            return "æ— å¯ç”¨å†…å®¹è¿›è¡Œæ‘˜è¦"
        
        # ç®€å•çš„å¥å­æå–
        sentences = [s.strip() for s in text.split('ã€‚') if len(s.strip()) > 10]
        
        # æ ¹æ®æ ¼å¼è°ƒæ•´è¾“å‡º
        if format == "bullet_points":
            return "\n".join([f"- {s}" for s in sentences[:5]])
        else:
            summary_text = "ã€‚".join(sentences[:3]) + "ã€‚" if sentences else "æ— å¯ç”¨å†…å®¹"
            return summary_text[:300] + "..." if len(summary_text) > 300 else summary_text
            
    except Exception as e:
        return f"æ‘˜è¦ç”Ÿæˆå¤±è´¥: {str(e)}"

@mcp.tool()
def content_writer_mcp(section_title: str, content_data: List[Dict], overall_report_context: str, **kwargs) -> str:
    """Write detailed content for report sections
    
    Args:
        section_title: Title of the section to write
        content_data: Reference content data (list of documents/dicts)
        overall_report_context: Overall report context
        **kwargs: Writing configuration (writing_style, target_audience, tone, etc.)
        
    Returns:
        str: Generated section content
    """
    print(f"ğŸ” [content_writer_mcp] å¼€å§‹ç”Ÿæˆå†…å®¹ï¼Œæ ‡é¢˜: {section_title}")
    print(f"ğŸ” [content_writer_mcp] llm_available: {llm_available}")
    print(f"ğŸ” [content_writer_mcp] content_dataé•¿åº¦: {len(content_data) if content_data else 0}")
    
    if not llm_available:
        print("âš ï¸ [content_writer_mcp] LLMä¸å¯ç”¨ï¼Œä½¿ç”¨fallbackå†…å®¹")
        return _generate_fallback_content(section_title, content_data)
    
    try:
        # è·å–é…ç½®å‚æ•°
        writing_style = kwargs.get("writing_style", "professional")
        target_audience = kwargs.get("target_audience", "é€šç”¨")
        tone = kwargs.get("tone", "objective")
        depth_level = kwargs.get("depth_level", "detailed")
        include_examples = kwargs.get("include_examples", True)
        include_citations = kwargs.get("include_citations", True)
        word_count_requirement = kwargs.get("word_count_requirement", "800-1200å­—")
        
        # å‡†å¤‡å‚è€ƒå†…å®¹
        reference_content = _prepare_reference_content_for_writing(content_data)
        
        # ç¡®å®šå†™ä½œè§’è‰²
        role = _determine_writing_role(section_title, overall_report_context)
        
        # é€‰æ‹©åˆé€‚çš„æ¨¡æ¿
        template = _get_content_writing_template(writing_style, role)
        
        # å‡†å¤‡æ¨¡æ¿å‚æ•°
        template_params = _prepare_content_template_params(
            section_title, overall_report_context, reference_content, 
            writing_style, target_audience, tone, depth_level, 
            include_examples, word_count_requirement, role
        )
        
        # æ ¼å¼åŒ–prompt
        prompt = template.format(**template_params)
        
        # è®¡ç®—tokené™åˆ¶ï¼ˆæ”¾å®½ä¸Šé™ï¼Œå…è®¸é€šè¿‡kwargsè¦†ç›–ï¼‰
        try:
            upper = int(word_count_requirement.split("-")[1].replace("å­—", "")) if "-" in word_count_requirement else int(word_count_requirement.replace("å­—", ""))
        except Exception:
            upper = 2000
        requested = int(kwargs.get("max_tokens", upper * 2))
        try:
            import config as _cfg
            cap = getattr(_cfg, "LLM_MAX_TOKENS", 8000)
        except Exception:
            cap = 8000
        max_tokens = min(requested, cap)
        
        # è°ƒç”¨LLMç”Ÿæˆå†…å®¹
        print(f"ğŸ” [content_writer_mcp] å‡†å¤‡è°ƒç”¨LLM APIï¼Œmax_tokens: {max_tokens}")
        print(f"ğŸ” [content_writer_mcp] prompté•¿åº¦: {len(prompt)}")
        content = llm_processor.call_llm_api(
            prompt,
            f"ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„å†…å®¹åˆ›ä½œä¸“å®¶ï¼Œä¸“é—¨è´Ÿè´£æ’°å†™é«˜è´¨é‡çš„{writing_style}é£æ ¼å†…å®¹ã€‚",
            temperature=0.3,
            max_tokens=max_tokens
        )
        print(f"ğŸ” [content_writer_mcp] LLM APIè°ƒç”¨å®Œæˆï¼Œç”Ÿæˆäº†{len(content)}å­—ç¬¦çš„å†…å®¹")
        
        # åå¤„ç†å†…å®¹
        print(f"ğŸ” [content_writer_mcp] å¼€å§‹åå¤„ç†å†…å®¹ï¼Œinclude_citations: {include_citations}")
        processed_content = _post_process_content(content, include_citations)
        print(f"ğŸ” [content_writer_mcp] åå¤„ç†å®Œæˆï¼Œæœ€ç»ˆå†…å®¹é•¿åº¦: {len(processed_content)}")
        
        # è·å–usageä¿¡æ¯
        usage_info = llm_processor.last_usage if hasattr(llm_processor, 'last_usage') else None
        print(f"ğŸ” [content_writer_mcp] è·å–åˆ°usageä¿¡æ¯: {usage_info}")
        
        print(f"ğŸ” [content_writer_mcp] å‡†å¤‡è¿”å›ç»“æœ...")
        
        # è¿”å›å†…å®¹å’Œusageä¿¡æ¯çš„å­—å…¸
        result = {
            "content": processed_content,
            "usage": usage_info
        }
        return json.dumps(result, ensure_ascii=False)
        
    except Exception as e:
        print(f"âŒ [content_writer_mcp] ç« èŠ‚'{section_title}'æ’°å†™å¤±è´¥: {str(e)}")
        import traceback
        print(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯:")
        traceback.print_exc()
        return f"ç« èŠ‚'{section_title}'æ’°å†™å¤±è´¥: {str(e)}"

def _prepare_reference_content_for_writing(content_data: List[Dict]) -> str:
    """Prepare reference content for writing"""
    if not content_data:
        return "æ— å‚è€ƒèµ„æ–™"
    
    reference_parts = []
    for i, item in enumerate(content_data[:8]):  # é™åˆ¶å‰8ä¸ªå‚è€ƒèµ„æ–™
        if isinstance(item, dict):
            title = item.get("title", f"å‚è€ƒèµ„æ–™{i+1}")
            content = item.get("content", item.get("summary", ""))[:300]
            source = item.get("source", "æœªçŸ¥æ¥æº")
            ref_text = f"[{i+1}] {title}\næ¥æº: {source}\nå†…å®¹: {content}..."
            reference_parts.append(ref_text)
    
    return "\n\n".join(reference_parts)

def _determine_writing_role(section_title: str, context: str) -> str:
    """Determine writing role based on section title and context"""
    title_lower = section_title.lower()
    context_lower = context.lower()
    
    # å­¦æœ¯ç›¸å…³å…³é”®è¯
    if any(keyword in title_lower for keyword in ["ç ”ç©¶", "ç†è®º", "æ–¹æ³•", "æ–‡çŒ®", "å­¦æœ¯"]):
        return "academic"
    # å•†ä¸šç›¸å…³å…³é”®è¯  
    if any(keyword in title_lower for keyword in ["å¸‚åœº", "å•†ä¸š", "æŠ•èµ„", "æ”¶ç›Š", "ç­–ç•¥", "ç«äº‰"]):
        return "business"
    # æŠ€æœ¯ç›¸å…³å…³é”®è¯
    if any(keyword in title_lower for keyword in ["æŠ€æœ¯", "ç®—æ³•", "æ¶æ„", "å®ç°", "ç³»ç»Ÿ", "å¼€å‘"]):
        return "technical"
    
    return "general"

def _get_content_writing_template(writing_style: str, role: str) -> str:
    """Get content writing template"""
    return """
ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„{role}ï¼Œæ­£åœ¨æ’°å†™å…³äº"{overall_topic}"çš„{section_title}ç« èŠ‚ã€‚

ç« èŠ‚è¦æ±‚ï¼š
- ç›®æ ‡å—ä¼—ï¼š{target_audience}
- å†™ä½œé£æ ¼ï¼š{writing_style}
- å†…å®¹æ·±åº¦ï¼š{depth_level}
- å­—æ•°è¦æ±‚ï¼š{word_count_requirement}

å‚è€ƒèµ„æ–™ï¼š
{reference_content}

å…¨å±€æŠ¥å‘Šä¸Šä¸‹æ–‡ï¼š
{overall_report_context}

æ’°å†™è¦æ±‚ï¼š
1. **ä¸“ä¸šæ·±åº¦**ï¼šåŸºäºå‚è€ƒèµ„æ–™æä¾›æ·±å…¥ã€ä¸“ä¸šçš„åˆ†æ
2. **é€»è¾‘ç»“æ„**ï¼šå†…å®¹åº”æœ‰æ¸…æ™°çš„é€»è¾‘å±‚æ¬¡å’Œæ®µè½ç»“æ„
3. **å®ç”¨ä»·å€¼**ï¼šçªå‡ºå®é™…åº”ç”¨ä»·å€¼å’Œç°å®æ„ä¹‰
4. **å¼•ç”¨è§„èŒƒ**ï¼šåœ¨å¼•ç”¨å‚è€ƒèµ„æ–™æ—¶ä½¿ç”¨[1]ã€[2]ç­‰æ ‡è®°
5. **è¯­è¨€è¡¨è¾¾**ï¼šä½¿ç”¨{tone}çš„è¯­è°ƒï¼Œé€‚åˆ{target_audience}é˜…è¯»

ç« èŠ‚ç»“æ„å»ºè®®ï¼š
- å¼€ç¯‡ï¼šç®€è¦ä»‹ç»æœ¬ç« èŠ‚çš„æ ¸å¿ƒè®®é¢˜
- ä¸»ä½“ï¼šå›´ç»•å…³é”®è¦ç‚¹å±•å¼€è¯¦ç»†åˆ†æ
- å®ä¾‹ï¼š{example_instruction}
- æ€»ç»“ï¼šæ¦‚æ‹¬æœ¬ç« èŠ‚çš„ä¸»è¦è§‚ç‚¹

è¯·æ’°å†™å®Œæ•´çš„ç« èŠ‚å†…å®¹ï¼Œç¡®ä¿ä¿¡æ¯å‡†ç¡®ã€é€»è¾‘æ¸…æ™°ã€è¡¨è¾¾æµç•…ã€‚
"""

def _prepare_content_template_params(section_title, overall_report_context, reference_content, 
                                   writing_style, target_audience, tone, depth_level, 
                                   include_examples, word_count_requirement, role) -> Dict[str, str]:
    """Prepare template parameters for content writing"""
    # æå–æ•´ä½“ä¸»é¢˜
    overall_topic = overall_report_context.split('\n')[0] if overall_report_context else "ç›¸å…³ä¸»é¢˜"
    
    # è§’è‰²æè¿°
    role_descriptions = {
        "academic": "å­¦æœ¯ç ”ç©¶ä¸“å®¶å’Œæ•™æˆ",
        "business": "å•†ä¸šåˆ†æå¸ˆå’Œæˆ˜ç•¥é¡¾é—®", 
        "technical": "æŠ€æœ¯ä¸“å®¶å’Œæ¶æ„å¸ˆ",
        "general": "ä¸“ä¸šå†…å®¹åˆ†æå¸ˆ"
    }
    
    # ç¤ºä¾‹æŒ‡å¯¼
    example_instruction = "ç»“åˆå…·ä½“æ¡ˆä¾‹å’Œå®ä¾‹è¿›è¡Œè¯´æ˜" if include_examples else "é‡ç‚¹è¿›è¡Œç†è®ºåˆ†æ"
    
    # è¯­è°ƒæè¿°
    tone_descriptions = {
        "objective": "å®¢è§‚ã€ä¸­æ€§",
        "analytical": "åˆ†ææ€§ã€æ·±å…¥",
        "professional": "ä¸“ä¸šã€æ­£å¼",
        "engaging": "ç”ŸåŠ¨ã€å¼•äººå…¥èƒœ"
    }
    
    return {
        "role": role_descriptions.get(role, role_descriptions["general"]),
        "overall_topic": overall_topic,
        "section_title": section_title,
        "target_audience": target_audience,
        "writing_style": writing_style,
        "depth_level": depth_level,
        "word_count_requirement": word_count_requirement,
        "reference_content": reference_content,
        "overall_report_context": overall_report_context,
        "tone": tone_descriptions.get(tone, "å®¢è§‚ã€ä¸“ä¸š"),
        "example_instruction": example_instruction
    }

def _post_process_content(content: str, include_citations: bool) -> str:
    """Post-process generated content"""
    if not content:
        return "å†…å®¹ç”Ÿæˆå¤±è´¥"
    
    # æ¸…ç†æ ¼å¼
    content = content.strip()
    
    # ç§»é™¤å¯èƒ½çš„æ ‡é¢˜é‡å¤
    lines = content.split('\n')
    if lines and lines[0].strip().startswith('#'):
        content = '\n'.join(lines[1:]).strip()
    
    # ç¡®ä¿å¼•ç”¨æ ¼å¼æ­£ç¡®
    if include_citations:
        import re
        # ç»Ÿä¸€å¼•ç”¨æ ¼å¼ä¸º [æ•°å­—]
        content = re.sub(r'\[(\d+)\]', r'[\1]', content)
        content = re.sub(r'ï¼ˆ(\d+)ï¼‰', r'[\1]', content)
        content = re.sub(r'\((\d+)\)', r'[\1]', content)
    
    return content

def _generate_fallback_content(section_title: str, content_data: List[Dict]) -> str:
    """Generate fallback content when LLM is not available"""
    try:
        content_parts = [f"## {section_title}\n"]
        
        if content_data:
            content_parts.append("åŸºäºç°æœ‰èµ„æ–™åˆ†æï¼Œæœ¬ç« èŠ‚ä¸»è¦å†…å®¹åŒ…æ‹¬ï¼š\n")
            
            for i, item in enumerate(content_data[:3]):
                if isinstance(item, dict):
                    title = item.get("title", f"è¦ç‚¹{i+1}")
                    summary = item.get("content", "")[:200]
                    content_parts.append(f"### {title}\n{summary}\n")
        else:
            content_parts.append(f"æœ¬ç« èŠ‚å°†è¯¦ç»†ä»‹ç»{section_title}çš„ç›¸å…³å†…å®¹ï¼ŒåŒ…æ‹¬åŸºæœ¬æ¦‚å¿µã€å‘å±•ç°çŠ¶ã€åº”ç”¨åœºæ™¯ç­‰æ–¹é¢ã€‚\n")
        
        content_parts.append("æ›´å¤šè¯¦ç»†å†…å®¹æœ‰å¾…è¿›ä¸€æ­¥ç ”ç©¶å’Œåˆ†æã€‚")
        
        return '\n'.join(content_parts)
        
    except Exception as e:
        return f"## {section_title}\n\nå†…å®¹ç”Ÿæˆå¤±è´¥: {str(e)}\n\næœ¬ç« èŠ‚éœ€è¦è¿›ä¸€æ­¥å®Œå–„ã€‚"

@mcp.tool()
def orchestrator_mcp_simple(task: str, **kwargs) -> str:
    """
    ç®€åŒ–ç‰ˆMCPè°ƒåº¦å™¨ - ç”¨äºæµ‹è¯•å’Œæ¼”ç¤º
    """
    try:
        print(f"\nğŸ¯ [ç®€åŒ–è°ƒåº¦å™¨] å¼€å§‹æ‰§è¡Œä»»åŠ¡: {task}")
        
        # 1. æ„å›¾è¯†åˆ«
        intent_result = analysis_mcp("intent", task, "")
        intent_data = json.loads(intent_result)
        primary_intent = intent_data.get('details', {}).get('primary_intent', 'æœªè¯†åˆ«')
        print(f"âœ… æ„å›¾è¯†åˆ«: {primary_intent}")
        
        # 2. ç”Ÿæˆå¤§çº²
        topic = _extract_topic_from_task(task)
        outline_result = outline_writer_mcp(topic, "comprehensive", task)
        outline_data = json.loads(outline_result)
        
        # 3. å¼ºåˆ¶åˆ›å»ºç« èŠ‚å†…å®¹ï¼ˆç»•è¿‡æœç´¢é—®é¢˜ï¼‰
        default_sections = [
            {"name": "è¡Œä¸šæ¦‚è¿°", "content": f"{topic}çš„åŸºæœ¬æƒ…å†µå’Œå‘å±•èƒŒæ™¯"},
            {"name": "é‡å¤§äº‹ä»¶", "content": "æœ€è¿‘çš„é‡è¦äº‹ä»¶å’Œè¡Œä¸šæ–°é—»"},
            {"name": "æŠ€æœ¯å‘å±•", "content": "æŠ€æœ¯åˆ›æ–°å’Œçªç ´æ€§è¿›å±•"},
            {"name": "å¸‚åœºåŠ¨æ€", "content": "å¸‚åœºå˜åŒ–ã€æŠ•èµ„å’Œç«äº‰æ€åŠ¿"},
            {"name": "æœªæ¥å±•æœ›", "content": "å‘å±•å‰æ™¯ã€è¶‹åŠ¿é¢„æµ‹å’Œå»ºè®®"}
        ]
        
        print(f"ğŸ“ å¼€å§‹ç”Ÿæˆ {len(default_sections)} ä¸ªç« èŠ‚...")
        
        section_contents = {}
        for section in default_sections:
            section_name = section['name']
            print(f"  ğŸ“„ ç”Ÿæˆç« èŠ‚: {section_name}")
            
            content = content_writer_mcp(
                section_title=section_name,
                content_data=[{
                    "title": f"{topic}ç›¸å…³ä¿¡æ¯",
                    "content": f"å…³äº{topic}çš„{section['content']}ï¼ŒåŒ…æ‹¬ç›¸å…³åˆ†æå’Œè§è§£ã€‚",
                    "source": "æ™ºèƒ½ç”Ÿæˆ"
                }],
                overall_report_context=f"{topic}è¡Œä¸šåˆ†ææŠ¥å‘Š",
                writing_style="professional",
                target_audience="è¡Œä¸šåˆ†æå¸ˆ"
            )
            section_contents[section_name] = content
        
        # 4. ç”Ÿæˆæ‘˜è¦
        executive_summary = summary_writer_mcp(
            content_data=list(section_contents.values()),
            length_constraint="300-400å­—",
            format="executive"
        )
        
        # 5. ç»„è£…æŠ¥å‘Š
        final_report = f"""# {topic} - è¡Œä¸šåˆ†ææŠ¥å‘Š

**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
**è¯†åˆ«æ„å›¾**: {primary_intent}

## ğŸ“‹ æ‰§è¡Œæ‘˜è¦

{executive_summary}

## ğŸ“Š è¯¦ç»†åˆ†æ

"""
        
        for section_name, content in section_contents.items():
            final_report += f"### {section_name}\n\n{content}\n\n"
        
        final_report += "\n---\n*æœ¬æŠ¥å‘Šç”±MCPç®€åŒ–è°ƒåº¦å™¨ç”Ÿæˆ*"
        
        result = {
            "status": "completed",
            "task": task,
            "report_content": final_report,
            "metadata": {
                "topic": topic,
                "sections_count": len(section_contents),
                "intent": primary_intent
            }
        }
        
        print(f"ğŸ‰ ç®€åŒ–è°ƒåº¦å™¨å®Œæˆï¼ç”Ÿæˆäº† {len(section_contents)} ä¸ªç« èŠ‚")
        return json.dumps(result, ensure_ascii=False, indent=2)
        
    except Exception as e:
        return json.dumps({
            "status": "failed",
            "error": str(e)
        }, ensure_ascii=False)

@mcp.tool()
def orchestrator_mcp(task: str, task_type: str = "auto", **kwargs) -> str:
    """
    MCPå·¥å…·è°ƒåº¦å™¨ - ä¸²è”è°ƒç”¨å„ä¸ªMCPå·¥å…·å®Œæˆå¤æ‚ä»»åŠ¡
    
    Args:
        task: ä»»åŠ¡æè¿° (å¦‚ "ç”ŸæˆAI Agentè¡Œä¸šåŠ¨æ€æŠ¥å‘Š")
        task_type: ä»»åŠ¡ç±»å‹ ('news_report', 'research_report', 'industry_analysis', 'auto')
        **kwargs: å…¶ä»–å‚æ•° (days, companies, quality_thresholdç­‰)
        
    Returns:
        str: å®Œæ•´çš„æ‰§è¡Œç»“æœå’Œç”Ÿæˆçš„æŠ¥å‘Š
    """
    try:
        print(f"\nğŸ¯ [MCPè°ƒåº¦å™¨] å¼€å§‹æ‰§è¡Œä»»åŠ¡: {task}")
        print(f"ğŸ“‹ ä»»åŠ¡ç±»å‹: {task_type}")
        print("=" * 60)
        
        # æ­¥éª¤1: æ„å›¾è¯†åˆ«å’Œä»»åŠ¡è§„åˆ’
        print("\nğŸ§  [æ­¥éª¤1] æ„å›¾è¯†åˆ«å’Œä»»åŠ¡è§„åˆ’...")
        intent_result = analysis_mcp(
            analysis_type="intent",
            data=task,
            context=f"ä»»åŠ¡ç±»å‹: {task_type}",
            task_planning="true",
            detailed_analysis="true"
        )
        
        intent_data = json.loads(intent_result)
        print(f"âœ… æ„å›¾è¯†åˆ«å®Œæˆ: {intent_data.get('details', {}).get('primary_intent', 'æœªè¯†åˆ«')}")
        
        # æ­¥éª¤2: ç”ŸæˆæŠ¥å‘Šå¤§çº²
        print("\nğŸ“ [æ­¥éª¤2] ç”ŸæˆæŠ¥å‘Šå¤§çº²...")
        
        # æ ¹æ®æ„å›¾ç¡®å®šæŠ¥å‘Šç±»å‹
        if task_type == "auto":
            if "æ–°é—»" in task or "åŠ¨æ€" in task or "news" in task.lower():
                report_type = "industry"
            elif "ç ”ç©¶" in task or "research" in task.lower():
                report_type = "academic"  
            elif "åˆ†æ" in task or "analysis" in task.lower():
                report_type = "business"
            else:
                report_type = "comprehensive"
        else:
            type_mapping = {
                "news_report": "industry",
                "research_report": "academic", 
                "industry_analysis": "business"
            }
            report_type = type_mapping.get(task_type, "comprehensive")
        
        # æå–ä¸»é¢˜
        topic = _extract_topic_from_task(task)
        
        outline_result = outline_writer_mcp(
            topic=topic,
            report_type=report_type,
            user_requirements=task
        )
        
        outline_data = json.loads(outline_result)
        print(f"ğŸ” [è°ƒè¯•] å¤§çº²æ•°æ®keys: {list(outline_data.keys())}")
        
        # é€‚é…ä¸åŒçš„å¤§çº²æ•°æ®ç»“æ„
        sections = []
        
        if 'subsections' in outline_data:
            sections = outline_data['subsections']
            print(f"ğŸ” [è°ƒè¯•] ä½¿ç”¨subsectionsï¼Œæ•°é‡: {len(sections)}")
        elif 'sections' in outline_data:
            sections = outline_data['sections']
            print(f"ğŸ” [è°ƒè¯•] ä½¿ç”¨sectionsï¼Œæ•°é‡: {len(sections)}")
        elif 'structure' in outline_data:
            print(f"ğŸ” [è°ƒè¯•] ä½¿ç”¨structureè§£æ")
            # è§£æåµŒå¥—ç»“æ„
            structure = outline_data['structure']
            print(f"ğŸ” [è°ƒè¯•] structure keys: {list(structure.keys())}")
            
            for main_key, main_value in structure.items():
                print(f"ğŸ” [è°ƒè¯•] å¤„ç†: {main_key}, ç±»å‹: {type(main_value)}")
                if isinstance(main_value, dict):
                    # å¦‚æœæ˜¯åµŒå¥—ç»“æ„ï¼Œæå–å­ç« èŠ‚
                    for sub_key, sub_value in main_value.items():
                        sections.append({
                            "name": sub_key,
                            "title": sub_key,
                            "content": sub_value if isinstance(sub_value, str) else str(sub_value)
                        })
                        print(f"ğŸ” [è°ƒè¯•] æ·»åŠ å­ç« èŠ‚: {sub_key}")
                else:
                    # å¦‚æœæ˜¯ç®€å•ç»“æ„
                    sections.append({
                        "name": main_key,
                        "title": main_key,
                        "content": main_value
                    })
                    print(f"ğŸ” [è°ƒè¯•] æ·»åŠ ä¸»ç« èŠ‚: {main_key}")
        
        print(f"âœ… å¤§çº²ç”Ÿæˆå®Œæˆï¼ŒåŒ…å« {len(sections)} ä¸ªä¸»è¦ç« èŠ‚")
        if sections:
            print("ğŸ“‹ ç« èŠ‚åˆ—è¡¨:")
            for i, section in enumerate(sections[:5], 1):
                print(f"  {i}. {section.get('name', section.get('title', 'æœªçŸ¥'))}")
        else:
            print("âš ï¸ æœªè§£æåˆ°ä»»ä½•ç« èŠ‚")
        
        # æ­¥éª¤3: ç”¨æˆ·äº¤äº’ç¡®è®¤å¤§çº²
        print("\nğŸ‘¤ [æ­¥éª¤3] ç”¨æˆ·äº¤äº’ç¡®è®¤...")
        interaction_result = user_interaction_mcp(
            interaction_type="confirmation",
            prompt=f"å·²ä¸º'{topic}'ç”ŸæˆæŠ¥å‘Šå¤§çº²ï¼Œæ˜¯å¦ç¡®è®¤ç»§ç»­ï¼Ÿå¤§çº²åŒ…å«ä»¥ä¸‹ç« èŠ‚ï¼š\n" + 
                  "\n".join([f"- {section.get('name', section.get('title', 'æœªçŸ¥ç« èŠ‚'))}" for section in sections]),
            default=True,
            auto_confirm=kwargs.get('auto_confirm', True)
        )
        
        interaction_data = json.loads(interaction_result)
        print(f"âœ… ç”¨æˆ·ç¡®è®¤: {interaction_data.get('status', 'confirmed')}")
        
        # æ­¥éª¤4: å¹¶è¡Œå†…å®¹æ£€ç´¢
        print("\nğŸ” [æ­¥éª¤4] å¹¶è¡Œå†…å®¹æ£€ç´¢...")
        
        # ä¸ºæ¯ä¸ªç« èŠ‚ç”Ÿæˆä¸“é—¨çš„æŸ¥è¯¢
        days = kwargs.get('days', 7)
        all_search_results = []
        
        # ç”Ÿæˆå¤šè§’åº¦æŸ¥è¯¢
        query_result = query_generation_mcp(
            topic=topic,
            strategy="news" if "æ–°é—»" in task or "åŠ¨æ€" in task else "initial",
            context=f"ä¸º{report_type}æŠ¥å‘Šç”ŸæˆæŸ¥è¯¢",
            time_range=f"past_{days}_days",
            focus_areas=["é‡å¤§äº‹ä»¶", "æŠ€æœ¯å‘å±•", "å¸‚åœºåŠ¨æ€", "æ”¿ç­–å˜åŒ–"]
        )
        
        query_data = json.loads(query_result)
        queries = query_data.get('queries', [])
        print(f"âœ… ç”Ÿæˆ {len(queries)} ä¸ªæœç´¢æŸ¥è¯¢")
        
        # å¹¶è¡Œæœç´¢
        try:
            search_result = parallel_search(
                queries=queries[:5],  # é™åˆ¶æŸ¥è¯¢æ•°é‡
                max_results=3
            )
            print(f"ğŸ” æœç´¢ç»“æœé•¿åº¦: {len(search_result) if search_result else 0}")
            
            # æ£€æŸ¥æœç´¢ç»“æœæ˜¯å¦ä¸ºé”™è¯¯ä¿¡æ¯
            if isinstance(search_result, str) and "æœç´¢æœåŠ¡æš‚æ—¶ä¸å¯ç”¨" in search_result:
                raise Exception("æœç´¢æœåŠ¡ä¸å¯ç”¨")
                
        except Exception as e:
            print(f"âš ï¸ æœç´¢å¤±è´¥: {str(e)}")
            # ç”Ÿæˆæ›´ä¸°å¯Œçš„æ¨¡æ‹Ÿæ•°æ®
            search_result = f"""
            åŸºäº{topic}çš„æœ€æ–°è¡Œä¸šåŠ¨æ€ï¼š
            
            1. æŠ€æœ¯çªç ´ï¼šAI Agentåœ¨å¤šæ¨¡æ€äº¤äº’æ–¹é¢å–å¾—é‡è¦è¿›å±•ï¼Œæ”¯æŒè¯­éŸ³ã€æ–‡æœ¬ã€å›¾åƒçš„ç»¼åˆå¤„ç†èƒ½åŠ›
            
            2. äº§å“å‘å¸ƒï¼šå¤šå®¶ç§‘æŠ€å…¬å¸å‘å¸ƒäº†æ–°ä¸€ä»£AI Agentå¹³å°ï¼ŒåŒ…æ‹¬å¢å¼ºçš„è‡ªç„¶è¯­è¨€ç†è§£å’Œä»»åŠ¡æ‰§è¡Œèƒ½åŠ›
            
            3. å¸‚åœºåŠ¨æ€ï¼šAI Agentå¸‚åœºé¢„è®¡å°†åœ¨æœªæ¥å‡ å¹´å†…å®ç°å¿«é€Ÿå¢é•¿ï¼Œä¼ä¸šçº§åº”ç”¨éœ€æ±‚æ—ºç››
            
            4. æ”¿ç­–æ³•è§„ï¼šç›¸å…³ç›‘ç®¡éƒ¨é—¨æ­£åœ¨åˆ¶å®šAI Agentçš„ä½¿ç”¨è§„èŒƒå’Œå®‰å…¨æ ‡å‡†
            
            5. æŠ•èµ„è¶‹åŠ¿ï¼šé£é™©æŠ•èµ„å¯¹AI Agenté¢†åŸŸçš„æŠ•èµ„çƒ­æƒ…æŒç»­é«˜æ¶¨ï¼Œå¤šä¸ªåˆåˆ›å…¬å¸è·å¾—å¤§é¢èèµ„
            
            6. åº”ç”¨åœºæ™¯ï¼šAI Agentåœ¨å®¢æˆ·æœåŠ¡ã€å†…å®¹åˆ›ä½œã€æ•°æ®åˆ†æç­‰é¢†åŸŸçš„åº”ç”¨æ¡ˆä¾‹ä¸æ–­æ¶Œç°
            """
        
        print(f"âœ… æœç´¢å®Œæˆï¼Œè·å¾—åˆå§‹æ•°æ®")
        
        # æ­¥éª¤5: é€’å½’è´¨é‡è¯„ä¼°å’Œè¡¥å……æœç´¢
        print("\nğŸ“Š [æ­¥éª¤5] è´¨é‡è¯„ä¼°å’Œè¿­ä»£ä¼˜åŒ–...")
        
        max_iterations = kwargs.get('max_iterations', 3)
        quality_threshold = kwargs.get('quality_threshold', 7.0)
        
        for iteration in range(max_iterations):
            print(f"\nğŸ”„ ç¬¬ {iteration + 1} è½®è´¨é‡è¯„ä¼°...")
            
            # è¯„ä¼°å½“å‰æ•°æ®è´¨é‡
            quality_result = analysis_mcp(
                analysis_type="quality",
                data=[{"title": "æœç´¢ç»“æœæ±‡æ€»", "content": search_result[:500], "source": "search"}],
                topic=topic,
                analysis_aspects=["relevance", "completeness", "timeliness"]
            )
            
            quality_data = json.loads(quality_result)
            current_score = quality_data.get('score', 0)
            print(f"ğŸ“ˆ å½“å‰è´¨é‡è¯„åˆ†: {current_score:.2f}/10")
            
            if current_score >= quality_threshold:
                print(f"âœ… è´¨é‡è¾¾æ ‡ (â‰¥{quality_threshold})ï¼Œåœæ­¢è¿­ä»£")
                break
            
            if iteration < max_iterations - 1:
                print(f"âš ï¸ è´¨é‡ä¸è¶³ï¼Œè¿›è¡Œè¡¥å……æœç´¢...")
                
                # ç¼ºå£åˆ†æ
                gap_result = analysis_mcp(
                    analysis_type="gap_analysis",
                    topic=topic,
                    data=[{"content": search_result}],
                    expected_aspects=["æŠ€æœ¯å‘å±•", "å¸‚åœºåŠ¨æ€", "æ”¿ç­–å˜åŒ–", "è¡Œä¸šè¶‹åŠ¿"]
                )
                
                gap_data = json.loads(gap_result)
                gaps = gap_data.get('details', {}).get('information_gaps', [])
                
                # æ ¹æ®ç¼ºå£ç”Ÿæˆè¡¥å……æŸ¥è¯¢
                if gaps:
                    gap_queries = []
                    for gap in gaps[:3]:  # æœ€å¤š3ä¸ªè¡¥å……æŸ¥è¯¢
                        gap_desc = gap.get('description', '')
                        suggested_queries = gap.get('suggested_queries', [])
                        gap_queries.extend(suggested_queries[:2])
                    
                    if gap_queries:
                        additional_result = parallel_search(
                            queries=gap_queries[:3],
                            max_results=2
                        )
                        search_result += f"\n\nè¡¥å……æœç´¢ç»“æœ:\n{additional_result}"
                        print(f"âœ… è¡¥å……æœç´¢å®Œæˆ")
            else:
                print(f"âš ï¸ å·²è¾¾æœ€å¤§è¿­ä»£æ¬¡æ•° ({max_iterations})ï¼Œä½¿ç”¨å½“å‰æ•°æ®")
        
        # æ­¥éª¤6: å¹¶è¡ŒæŠ¥å‘Šç”Ÿæˆ
        print("\nğŸ“ [æ­¥éª¤6] å¹¶è¡ŒæŠ¥å‘Šç”Ÿæˆ...")
        
        # ä¸ºæ¯ä¸ªç« èŠ‚ç”Ÿæˆå†…å®¹
        section_contents = {}
        # ä½¿ç”¨ä¹‹å‰è§£æçš„sectionså˜é‡
        
        # å¦‚æœå¤§çº²ä¸ºç©ºï¼Œåˆ›å»ºé»˜è®¤ç« èŠ‚
        if not sections:
            print("âš ï¸ å¤§çº²ä¸ºç©ºï¼Œåˆ›å»ºé»˜è®¤ç« èŠ‚ç»“æ„...")
            sections = [
                {"name": "è¡Œä¸šæ¦‚è¿°", "content": f"{topic}è¡Œä¸šåŸºæœ¬æƒ…å†µ"},
                {"name": "é‡å¤§äº‹ä»¶", "content": "æœ€è¿‘çš„é‡è¦äº‹ä»¶å’Œæ–°é—»"},
                {"name": "æŠ€æœ¯å‘å±•", "content": "æŠ€æœ¯åˆ›æ–°å’Œçªç ´"},
                {"name": "å¸‚åœºåŠ¨æ€", "content": "å¸‚åœºå˜åŒ–å’Œè¶‹åŠ¿"},
                {"name": "æœªæ¥å±•æœ›", "content": "å‘å±•å‰æ™¯å’Œé¢„æµ‹"}
            ]
        
        print(f"ğŸ”„ å¼€å§‹ç”Ÿæˆ {len(sections)} ä¸ªç« èŠ‚å†…å®¹...")
        
        for i, section in enumerate(sections[:5]):  # é™åˆ¶ç« èŠ‚æ•°é‡é¿å…è¿‡é•¿
            section_title = section.get('name', section.get('title', f'ç« èŠ‚{i+1}'))
            print(f"  ğŸ“„ ç”Ÿæˆç« èŠ‚: {section_title}")
            
            content = content_writer_mcp(
                section_title=section_title,
                content_data=[{
                    "title": f"{topic}ç›¸å…³ä¿¡æ¯",
                    "content": search_result[:800] if isinstance(search_result, str) else str(search_result)[:800],  # é™åˆ¶é•¿åº¦
                    "source": "ç»¼åˆæœç´¢"
                }],
                overall_report_context=f"{topic}è¡Œä¸š{report_type}æŠ¥å‘Š",
                writing_style="professional",
                target_audience="è¡Œä¸šåˆ†æå¸ˆ",
                word_count_requirement="600-800å­—"
            )
            
            section_contents[section_title] = content
        
        # ç”Ÿæˆæ‰§è¡Œæ‘˜è¦
        print("ğŸ“„ ç”Ÿæˆæ‰§è¡Œæ‘˜è¦...")
        executive_summary = summary_writer_mcp(
            content_data=list(section_contents.values()),
            length_constraint="400-500å­—",
            format="executive",
            focus_areas=["å…³é”®å‘ç°", "é‡è¦è¶‹åŠ¿", "æˆ˜ç•¥å»ºè®®"],
            tone="professional"
        )
        
        # æ­¥éª¤7: ç»„è£…æœ€ç»ˆæŠ¥å‘Š
        print("\nğŸ“‹ [æ­¥éª¤7] ç»„è£…æœ€ç»ˆæŠ¥å‘Š...")
        
        final_report = _assemble_orchestrated_report(
            topic=topic,
            task_description=task,
            intent_analysis=intent_data,
            outline=outline_data,
            executive_summary=executive_summary,
            section_contents=section_contents,
            search_summary=f"å…±æ£€ç´¢ {len(queries)} ä¸ªæŸ¥è¯¢ï¼Œç»è¿‡ {iteration + 1} è½®ä¼˜åŒ–",
            quality_score=current_score if 'current_score' in locals() else 0.0
        )
        
        print("âœ… æŠ¥å‘Šç»„è£…å®Œæˆ")
        
        # è¿”å›ç»“æœ
        result = {
            "status": "completed",
            "task": task,
            "report_content": final_report,
            "metadata": {
                "topic": topic,
                "report_type": report_type,
                "sections_count": len(section_contents),
                "final_quality_score": current_score,
                "iterations_used": iteration + 1,
                "queries_executed": len(queries)
            }
        }
        
        print(f"\nğŸ‰ [MCPè°ƒåº¦å™¨] ä»»åŠ¡å®Œæˆ!")
        print(f"ğŸ“Š æœ€ç»ˆè´¨é‡è¯„åˆ†: {current_score:.2f}/10")
        print(f"ğŸ“ æŠ¥å‘ŠåŒ…å« {len(section_contents)} ä¸ªç« èŠ‚")
        
        return json.dumps(result, ensure_ascii=False, indent=2)
        
    except Exception as e:
        error_result = {
            "status": "failed",
            "task": task,
            "error": str(e),
            "timestamp": datetime.now().isoformat()
        }
        return json.dumps(error_result, ensure_ascii=False, indent=2)

def _extract_topic_from_task(task: str) -> str:
    """ä»ä»»åŠ¡æè¿°ä¸­æå–ä¸»é¢˜"""
    # ç§»é™¤å¸¸è§çš„ä»»åŠ¡è¯æ±‡
    stop_words = ["ç”Ÿæˆ", "åˆ†æ", "æŠ¥å‘Š", "ç ”ç©¶", "å†™", "åˆ›å»º", "åˆ¶ä½œ", "è¡Œä¸š", "åŠ¨æ€", "æ–°é—»"]
    words = task.split()
    topic_words = []
    
    for word in words:
        if word not in stop_words and len(word) > 1:
            topic_words.append(word)
    
    if topic_words:
        return " ".join(topic_words[:3])  # å–å‰3ä¸ªè¯ä½œä¸ºä¸»é¢˜
    else:
        return "æœªæŒ‡å®šä¸»é¢˜"

def _assemble_orchestrated_report(topic: str, task_description: str, intent_analysis: Dict,
                                outline: Dict, executive_summary: str, section_contents: Dict,
                                search_summary: str, quality_score: float) -> str:
    """ç»„è£…è°ƒåº¦ç”Ÿæˆçš„æŠ¥å‘Š"""
    from datetime import datetime
    
    report_parts = []
    
    # æŠ¥å‘Šæ ‡é¢˜å’Œå…ƒä¿¡æ¯
    report_parts.append(f"# {topic} - æ™ºèƒ½è°ƒåº¦ç”ŸæˆæŠ¥å‘Š\n")
    report_parts.append(f"**åŸå§‹ä»»åŠ¡**: {task_description}")
    report_parts.append(f"**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    report_parts.append(f"**è´¨é‡è¯„åˆ†**: {quality_score:.2f}/10")
    report_parts.append(f"**æœç´¢æ‘˜è¦**: {search_summary}\n")
    
    # æ„å›¾åˆ†ææ‘˜è¦
    primary_intent = intent_analysis.get('details', {}).get('primary_intent', 'æœªè¯†åˆ«')
    report_parts.append(f"**è¯†åˆ«æ„å›¾**: {primary_intent}\n")
    
    # æ‰§è¡Œæ‘˜è¦
    report_parts.append("## ğŸ” æ‰§è¡Œæ‘˜è¦\n")
    report_parts.append(executive_summary)
    report_parts.append("\n")
    
    # è¯¦ç»†å†…å®¹ç« èŠ‚
    report_parts.append("## ğŸ“Š è¯¦ç»†åˆ†æ\n")
    
    for section_title, content in section_contents.items():
        report_parts.append(f"### {section_title}\n")
        report_parts.append(content)
        report_parts.append("\n")
    
    # æŠ¥å‘Šè¯´æ˜
    report_parts.append("---\n")
    report_parts.append("*æœ¬æŠ¥å‘Šç”±MCPæ™ºèƒ½è°ƒåº¦å™¨ç”Ÿæˆï¼Œæ•´åˆäº†æ„å›¾è¯†åˆ«ã€å¤§çº²ç”Ÿæˆã€å†…å®¹æ£€ç´¢ã€è´¨é‡è¯„ä¼°ã€ç”¨æˆ·äº¤äº’ç­‰å¤šä¸ªMCPå·¥å…·*")
    
    return '\n'.join(report_parts)

@mcp.tool()
def generate_insight_report(request: Dict[str, Any]) -> str:
    """Generate insight report based on request parameters
    
    Args:
        request: Request parameters containing topic, report_type, depth_level, etc.
        
    Returns:
        str: Generated insight report content
    """
    try:
        topic = request.get("topic", "æœªæŒ‡å®šä¸»é¢˜")
        report_type = request.get("report_type", "insight")
        depth_level = request.get("depth_level", "detailed")
        target_audience = request.get("target_audience", "è¡Œä¸šä¸“å®¶")
        include_citations = request.get("include_citations", True)
        max_sections = request.get("max_sections", 8)
        
        print(f"ğŸ¯ ç”Ÿæˆæ´å¯ŸæŠ¥å‘Š: {topic}")
        print(f"ğŸ“‹ æŠ¥å‘Šç±»å‹: {report_type}, æ·±åº¦: {depth_level}")
        
        # ä½¿ç”¨orchestrator_mcp_simpleç”ŸæˆæŠ¥å‘Š
        task_description = f"ç”Ÿæˆ{topic}çš„{report_type}æŠ¥å‘Šï¼Œæ·±åº¦çº§åˆ«ï¼š{depth_level}ï¼Œç›®æ ‡å—ä¼—ï¼š{target_audience}"
        
        result = orchestrator_mcp_simple(task_description)
        result_data = json.loads(result)
        
        if result_data.get("status") == "completed":
            return result_data.get("report_content", "æŠ¥å‘Šç”Ÿæˆå¤±è´¥")
        else:
            return f"æ´å¯ŸæŠ¥å‘Šç”Ÿæˆå¤±è´¥: {result_data.get('error', 'æœªçŸ¥é”™è¯¯')}"
            
    except Exception as e:
        return f"æ´å¯ŸæŠ¥å‘Šç”Ÿæˆè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}"

@mcp.tool()
def generate_industry_dynamic_report(industry: str, time_range: str = "recent", focus_areas: List[str] = None, include_analysis: bool = True, data_sources: List[str] = None, **kwargs) -> str:
    """Generate industry dynamic report based on parameters
    
    Args:
        industry: Target industry for the report
        time_range: Time range for analysis (recent, 1month, 3months, etc.)
        focus_areas: List of focus areas for the report
        include_analysis: Whether to include detailed analysis
        data_sources: List of data sources to use
        **kwargs: Additional parameters
        
    Returns:
        str: Generated industry dynamic report content
    """
    try:
        if focus_areas is None:
            focus_areas = ["å¸‚åœºè¶‹åŠ¿", "æŠ€æœ¯åˆ›æ–°", "æ”¿ç­–å½±å“", "ç«äº‰æ ¼å±€"]
        if data_sources is None:
            data_sources = ["news", "research", "market_data"]
            
        print(f"ğŸ­ ç”Ÿæˆè¡Œä¸šåŠ¨æ€æŠ¥å‘Š: {industry}")
        print(f"ğŸ“… æ—¶é—´èŒƒå›´: {time_range}, å…³æ³¨é¢†åŸŸ: {focus_areas}")
        
        # ä½¿ç”¨orchestrator_mcpç”Ÿæˆè¡Œä¸šåŠ¨æ€æŠ¥å‘Š
        task_description = f"ç”Ÿæˆ{industry}è¡Œä¸šåŠ¨æ€æŠ¥å‘Šï¼Œæ—¶é—´èŒƒå›´ï¼š{time_range}ï¼Œå…³æ³¨é¢†åŸŸï¼š{focus_areas}ï¼ŒåŒ…å«åˆ†æï¼š{include_analysis}ï¼Œæ•°æ®æºï¼š{data_sources}"
        
        result = orchestrator_mcp(task_description, task_type="industry_report")
        result_data = json.loads(result)
        
        if result_data.get("status") == "completed":
            return result_data.get("report_content", "æŠ¥å‘Šç”Ÿæˆå¤±è´¥")
        else:
            return f"è¡Œä¸šåŠ¨æ€æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {result_data.get('error', 'æœªçŸ¥é”™è¯¯')}"
            
    except Exception as e:
        print(f"âŒ è¡Œä¸šåŠ¨æ€æŠ¥å‘Šç”Ÿæˆè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
        return f"è¡Œä¸šåŠ¨æ€æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {str(e)}"

@mcp.tool()
def generate_academic_research_report(research_topic: str, academic_level: str = "advanced", research_methodology: str = "comprehensive", include_literature_review: bool = True, citation_style: str = "academic", max_pages: int = 20, **kwargs) -> str:
    """Generate academic research report based on parameters
    
    Args:
        research_topic: Research topic for the report
        academic_level: Academic level (basic, intermediate, advanced)
        research_methodology: Research methodology approach
        include_literature_review: Whether to include literature review
        citation_style: Citation style to use
        max_pages: Maximum number of pages
        **kwargs: Additional parameters
        
    Returns:
        str: Generated academic research report content
    """
    try:
        print(f"ğŸ“ ç”Ÿæˆå­¦æœ¯ç ”ç©¶æŠ¥å‘Š: {research_topic}")
        print(f"ğŸ“š å­¦æœ¯çº§åˆ«: {academic_level}, ç ”ç©¶æ–¹æ³•: {research_methodology}")
        
        # ä½¿ç”¨orchestrator_mcpç”ŸæˆæŠ¥å‘Š
        task_description = f"ç”Ÿæˆ{research_topic}çš„å­¦æœ¯ç ”ç©¶æŠ¥å‘Šï¼Œå­¦æœ¯çº§åˆ«ï¼š{academic_level}ï¼Œç ”ç©¶æ–¹æ³•ï¼š{research_methodology}"
        
        result = orchestrator_mcp(task_description, task_type="research_report", days=30)
        result_data = json.loads(result)
        
        if result_data.get("status") == "completed":
            return result_data.get("report_content", "æŠ¥å‘Šç”Ÿæˆå¤±è´¥")
        else:
            return f"å­¦æœ¯ç ”ç©¶æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {result_data.get('error', 'æœªçŸ¥é”™è¯¯')}"
            
    except Exception as e:
        return f"å­¦æœ¯ç ”ç©¶æŠ¥å‘Šç”Ÿæˆè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}"

@mcp.tool()
def comprehensive_search(topic: str, search_type: str = "comprehensive", max_results: int = 10, days: int = 30, sources: List[str] = None) -> str:
    """Comprehensive search across multiple sources
    
    Args:
        topic: Search topic
        search_type: Type of search (comprehensive, academic, news, etc.)
        max_results: Maximum number of results to return
        days: Number of days to search back
        sources: List of sources to search (web, academic, news)
        
    Returns:
        str: Search results
    """
    try:
        if sources is None:
            sources = ["web", "academic", "news"]
            
        print(f"ğŸ” ç»¼åˆæœç´¢: {topic}")
        print(f"ğŸ“Š æœç´¢ç±»å‹: {search_type}, æœ€å¤§ç»“æœ: {max_results}, æ—¶é—´èŒƒå›´: {days}å¤©")
        
        # ç”Ÿæˆæœç´¢æŸ¥è¯¢
        query_result = query_generation_mcp(
            topic=topic,
            strategy="academic" if "academic" in search_type else "news" if "news" in search_type else "initial",
            context=f"æœç´¢ç±»å‹: {search_type}",
            time_range=f"past_{days}_days"
        )
        
        query_data = json.loads(query_result)
        queries = query_data.get('queries', [f"{topic} æœ€æ–°å‘å±•", f"{topic} ç ”ç©¶è¿›å±•"])
        
        # æ‰§è¡Œå¹¶è¡Œæœç´¢
        search_result = parallel_search(queries[:5], max_results)
        
        return search_result if search_result else f"æœªæ‰¾åˆ°å…³äº'{topic}'çš„ç›¸å…³ä¿¡æ¯"
        
    except Exception as e:
        return f"ç»¼åˆæœç´¢è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}"

@mcp.tool()
def user_interaction_mcp(interaction_type: str, prompt: str, options: List[str] = None, **kwargs) -> str:
    """Handle user interactions and get user input
    
    Args:
        interaction_type: Type of interaction ('choice', 'input', 'confirmation', 'rating', 'multi_choice')
        prompt: Prompt message for the user
        options: List of options for choice-based interactions
        **kwargs: Additional parameters (allow_custom, default, validation_rules, etc.)
        
    Returns:
        str: User response or interaction result in JSON format
    """
    try:
        if interaction_type == "choice":
            return _handle_choice_interaction(prompt, options or [], kwargs)
        elif interaction_type == "input":
            return _handle_input_interaction(prompt, kwargs)
        elif interaction_type == "confirmation":
            return _handle_confirmation_interaction(prompt, kwargs)
        elif interaction_type == "rating":
            return _handle_rating_interaction(prompt, kwargs)
        elif interaction_type == "multi_choice":
            return _handle_multi_choice_interaction(prompt, options or [], kwargs)
        else:
            return json.dumps({
                "error": f"ä¸æ”¯æŒçš„äº¤äº’ç±»å‹: {interaction_type}",
                "supported_types": ["choice", "input", "confirmation", "rating", "multi_choice"]
            }, ensure_ascii=False)
            
    except Exception as e:
        return json.dumps({
            "error": f"ç”¨æˆ·äº¤äº’å¤„ç†å¤±è´¥: {str(e)}",
            "interaction_type": interaction_type
        }, ensure_ascii=False)

def _handle_choice_interaction(prompt: str, options: List[str], kwargs: Dict) -> str:
    """Handle choice interaction"""
    allow_custom = kwargs.get("allow_custom", False)
    default = kwargs.get("default", "")
    
    interaction_data = {
        "type": "choice",
        "prompt": prompt,
        "options": [{"value": str(i), "label": option} for i, option in enumerate(options, 1)],
        "allow_custom": allow_custom,
        "default": default,
        "status": "pending_user_response"
    }
    
    return json.dumps(interaction_data, ensure_ascii=False, indent=2)

def _handle_input_interaction(prompt: str, kwargs: Dict) -> str:
    """Handle input interaction"""
    max_length = kwargs.get("max_length", 1000)
    required = kwargs.get("required", True)
    validation_pattern = kwargs.get("validation_pattern")
    
    interaction_data = {
        "type": "input",
        "prompt": prompt,
        "validation_rules": {
            "max_length": max_length,
            "required": required,
            "pattern": validation_pattern
        },
        "status": "pending_user_response"
    }
    
    return json.dumps(interaction_data, ensure_ascii=False, indent=2)

def _handle_confirmation_interaction(prompt: str, kwargs: Dict) -> str:
    """Handle confirmation interaction"""
    default = kwargs.get("default")
    
    interaction_data = {
        "type": "confirmation",
        "prompt": prompt,
        "options": [
            {"value": "yes", "label": "æ˜¯"},
            {"value": "no", "label": "å¦"}
        ],
        "default": "yes" if default else "no" if default is not None else "",
        "status": "pending_user_response"
    }
    
    return json.dumps(interaction_data, ensure_ascii=False, indent=2)

def _handle_rating_interaction(prompt: str, kwargs: Dict) -> str:
    """Handle rating interaction"""
    min_score = kwargs.get("min_score", 1)
    max_score = kwargs.get("max_score", 5)
    labels = kwargs.get("labels", {1: "å¾ˆå·®", 2: "è¾ƒå·®", 3: "ä¸€èˆ¬", 4: "è¾ƒå¥½", 5: "å¾ˆå¥½"})
    
    options = []
    for score in range(min_score, max_score + 1):
        label = labels.get(score, str(score))
        options.append({"value": str(score), "label": f"{score}åˆ†", "description": label})
    
    interaction_data = {
        "type": "rating",
        "prompt": prompt,
        "options": options,
        "validation_rules": {"min": min_score, "max": max_score},
        "status": "pending_user_response"
    }
    
    return json.dumps(interaction_data, ensure_ascii=False, indent=2)

def _handle_multi_choice_interaction(prompt: str, options: List[str], kwargs: Dict) -> str:
    """Handle multi-choice interaction"""
    min_selections = kwargs.get("min_selections", 1)
    max_selections = kwargs.get("max_selections", len(options))
    
    interaction_data = {
        "type": "multi_choice",
        "prompt": f"{prompt}\n(è¯·é€‰æ‹©{min_selections}-{max_selections}é¡¹ï¼Œç”¨é€—å·åˆ†éš”)",
        "options": [{"value": str(i), "label": option} for i, option in enumerate(options, 1)],
        "validation_rules": {
            "min_selections": min_selections,
            "max_selections": max_selections
        },
        "status": "pending_user_response"
    }
    
    return json.dumps(interaction_data, ensure_ascii=False, indent=2)

# Add a dynamic search resource
@mcp.resource("search://{query}")
def get_search_info(query: str) -> str:
    """Get information about a search query"""
    return f"æœç´¢æŸ¥è¯¢: '{query}'\nè¿™ä¸ªèµ„æºå¯ä»¥æä¾›å…³äºè¯¥æŸ¥è¯¢çš„è¯¦ç»†æœç´¢ä¿¡æ¯ã€‚"


# # Add a prompt
# @mcp.prompt()
# def greet_user(name: str, style: str = "friendly") -> str:
#     """Generate a greeting prompt"""
#     styles = {
#         "friendly": "Please write a warm, friendly greeting",
#         "formal": "Please write a formal, professional greeting",
#         "casual": "Please write a casual, relaxed greeting",
#     }

#     return f"{styles.get(style, styles['friendly'])} for someone named {name}."

# æ·»åŠ è‡ªå®šä¹‰HTTPç«¯ç‚¹æ¥æ”¯æŒæµ‹è¯•æ–‡ä»¶çš„è¯·æ±‚æ ¼å¼
from fastapi import FastAPI, HTTPException
from fastapi.responses import StreamingResponse
import uvicorn
import asyncio
import json

# åˆ›å»ºFastAPIåº”ç”¨æ¥å¤„ç†è‡ªå®šä¹‰è¯·æ±‚æ ¼å¼
http_app = FastAPI(title="MCP HTTP API", version="1.0.0")

@http_app.post("/mcp/tools/call")
async def tools_call(request: dict):
    """å¤„ç†tools/callè¯·æ±‚æ ¼å¼"""
    try:
        # è§£æè¯·æ±‚
        method = request.get("method")
        params = request.get("params", {})
        request_id = request.get("id", 1)
        
        if method != "tools/call":
            raise HTTPException(status_code=400, detail="Invalid method")
        
        tool_name = params.get("name")
        arguments = params.get("arguments", {})
        
        print(f"ğŸ”§ è°ƒç”¨å·¥å…·: {tool_name}")
        print(f"ğŸ“‹ å‚æ•°: {arguments}")
        
        # ç§»é™¤ç‰¹æ®Šå¤„ç†ï¼Œç»Ÿä¸€ä½¿ç”¨æ ‡å‡†MCPå·¥å…·è°ƒç”¨
        
        # è¿”å›SSEæ ¼å¼çš„å“åº”
        async def generate_sse_response():
            # å‘é€å¼€å§‹æ‰§è¡Œæ¶ˆæ¯
            start_message = {
                "method": "notifications/message",
                "params": {
                    "level": "info",
                    "data": {
                        "msg": {
                            "status": "started",
                            "message": f"å¼€å§‹æ‰§è¡Œå·¥å…· {tool_name}",
                            "details": {
                                "id": request_id,
                                "name": tool_name,
                                "content": f"æ­£åœ¨å¤„ç† {tool_name} è¯·æ±‚..."
                            }
                        },
                        "extra": None
                    }
                },
                "jsonrpc": "2.0"
            }
            try:
                yield f"data: {json.dumps(start_message, ensure_ascii=False)}\n\n"
            except (GeneratorExit, asyncio.CancelledError, BrokenPipeError, ConnectionResetError) as e:
                print(f"ğŸ”Œ [SSE] è¿æ¥åœ¨å‘é€å¼€å§‹æ¶ˆæ¯æ—¶å…³é—­/å–æ¶ˆ: {e} (id={request_id}, tool={tool_name})")
                return
            
            # å‘é€è¿›åº¦æ›´æ–°æ¶ˆæ¯
            progress_message = {
                "method": "notifications/message",
                "params": {
                    "level": "info",
                    "data": {
                        "msg": {
                            "status": "processing",
                            "message": f"å·¥å…· {tool_name} æ­£åœ¨æ‰§è¡Œä¸­",
                            "details": {
                                "id": request_id,
                                "name": tool_name,
                                "content": f"æ­£åœ¨ç”ŸæˆæŠ¥å‘Šå†…å®¹ï¼Œè¯·ç¨å€™..."
                            }
                        },
                        "extra": None
                    }
                },
                "jsonrpc": "2.0"
            }
            try:
                yield f"data: {json.dumps(progress_message, ensure_ascii=False)}\n\n"
            except (GeneratorExit, asyncio.CancelledError, BrokenPipeError, ConnectionResetError) as e:
                print(f"ğŸ”Œ [SSE] è¿æ¥åœ¨å‘é€è¿›åº¦æ¶ˆæ¯æ—¶å…³é—­/å–æ¶ˆ: {e} (id={request_id}, tool={tool_name})")
                return
            
            # æ ¹æ®å·¥å…·åç§°è°ƒç”¨ç›¸åº”çš„å‡½æ•°
            result = None
            try:
                if tool_name == "generate_insight_report":
                    result = generate_insight_report(arguments.get("request", {}))
                elif tool_name == "generate_academic_research_report":
                    result = generate_academic_research_report(arguments.get("request", {}))
                elif tool_name == "comprehensive_search":
                    result = comprehensive_search(
                        arguments.get("topic", ""),
                        arguments.get("search_type", "comprehensive"),
                        arguments.get("max_results", 10),
                        arguments.get("days", 30),
                        arguments.get("sources", ["web", "academic", "news"])
                    )
                elif tool_name == "search":
                    result = search(
                        arguments.get("query", ""),
                        arguments.get("max_results", 5)
                    )
                elif tool_name == "parallel_search":
                    result = parallel_search(
                        arguments.get("queries", []),
                        arguments.get("max_results", 3)
                    )
                elif tool_name == "generate_industry_dynamic_report":
                    # ä½¿ç”¨æµå¼orchestratorè€Œä¸æ˜¯ç›´æ¥è°ƒç”¨å·¥å…·å‡½æ•°
                    from streaming_orchestrator import StreamingOrchestrator
                    orchestrator = StreamingOrchestrator()
                    
                    # æ„å»ºè¯·æ±‚å‚æ•°
                    request_params = {
                        "industry": arguments.get("industry", ""),
                        "time_range": arguments.get("time_range", "recent"),
                        "focus_areas": arguments.get("focus_areas", ["å¸‚åœºè¶‹åŠ¿", "æŠ€æœ¯åˆ›æ–°", "æ”¿ç­–å½±å“", "ç«äº‰æ ¼å±€"]),
                        "days": arguments.get("days", 30),
                        "use_local_data": arguments.get("use_local_data", False)
                    }
                    
                    # ä½¿ç”¨æµå¼æ–¹æ³•ç”ŸæˆæŠ¥å‘Š
                    try:
                        async for message in orchestrator.stream_industry_dynamic_report(request_params):
                            yield message
                    except (GeneratorExit, asyncio.CancelledError, BrokenPipeError, ConnectionResetError) as e:
                        print(f"ğŸ”Œ [SSE] è¿æ¥åœ¨æµå¼ç”Ÿæˆè¿‡ç¨‹ä¸­å…³é—­/å–æ¶ˆ: {e} (id={request_id}, tool={tool_name})")
                    except Exception as e:
                        print(f"âŒ æµå¼æŠ¥å‘Šç”Ÿæˆå¼‚å¸¸: {e}")
                        yield f"data: {json.dumps({'jsonrpc': '2.0', 'id': request_id, 'error': {'code': -32000, 'message': 'Tool execution failed', 'data': {'type': 'unknown', 'message': str(e)}}}, ensure_ascii=False)}\n\n"
                    
                    # æµå¼å¤„ç†å®Œæˆï¼Œç›´æ¥ç»“æŸ
                    return
                elif tool_name == "analysis_mcp":
                    result = analysis_mcp(
                        arguments.get("analysis_type", "quality"),
                        arguments.get("data", []),
                        arguments.get("topic", ""),
                        **arguments.get("kwargs", {})
                    )
                elif tool_name == "query_generation_mcp":
                    result = query_generation_mcp(
                        arguments.get("topic", ""),
                        arguments.get("strategy", "initial"),
                        arguments.get("context", ""),
                        **arguments.get("kwargs", {})
                    )
                elif tool_name == "outline_writer_mcp":
                    result = outline_writer_mcp(
                        arguments.get("topic", ""),
                        arguments.get("report_type", "comprehensive"),
                        arguments.get("user_requirements", ""),
                        **arguments.get("kwargs", {})
                    )
                elif tool_name == "summary_writer_mcp":
                    result = summary_writer_mcp(
                        arguments.get("content_data", ""),
                        arguments.get("length_constraint", "200-300å­—"),
                        arguments.get("format", "paragraph"),
                        **arguments.get("kwargs", {})
                    )
                elif tool_name == "content_writer_mcp":
                    result = content_writer_mcp(
                        arguments.get("section_title", ""),
                        arguments.get("content_data", []),
                        arguments.get("overall_report_context", ""),
                        **arguments.get("kwargs", {})
                    )
                elif tool_name == "orchestrator_mcp_simple":
                    result = orchestrator_mcp_simple(
                        arguments.get("task", ""),
                        **arguments.get("kwargs", {})
                    )
                elif tool_name == "orchestrator_mcp":
                    result = orchestrator_mcp(
                        arguments.get("task", ""),
                        arguments.get("task_type", "auto"),
                        **arguments.get("kwargs", {})
                    )
                elif tool_name == "user_interaction_mcp":
                    result = user_interaction_mcp(
                        arguments.get("interaction_type", "confirmation"),
                        arguments.get("prompt", ""),
                        arguments.get("options", []),
                        **arguments.get("kwargs", {})
                    )
                else:
                    raise HTTPException(status_code=404, detail=f"Tool '{tool_name}' not found")
            except Exception as e:
                print(f"âŒ å·¥å…·æ‰§è¡Œå¼‚å¸¸: {str(e)}")
                error_message = {
                    "jsonrpc": "2.0",
                    "id": request_id,
                    "error": {
                        "code": -32000,
                        "message": "Tool execution failed",
                        "data": {
                            "tool": tool_name,
                            "error": str(e)
                        }
                    }
                }
                try:
                    yield f"data: {json.dumps(error_message, ensure_ascii=False)}\n\n"
                except (GeneratorExit, asyncio.CancelledError, BrokenPipeError, ConnectionResetError) as e2:
                    print(f"ğŸ”Œ [SSE] è¿æ¥åœ¨å‘é€é”™è¯¯æ¶ˆæ¯æ—¶å…³é—­/å–æ¶ˆ: {e2} (id={request_id}, tool={tool_name})")
                return

            # å‘é€æœ€ç»ˆç»“æœæ¶ˆæ¯ï¼ˆJSON-RPCé£æ ¼ï¼Œä¿æŒä¸StreamingOrchestratorä¸€è‡´ï¼‰
            final_message = {
                "jsonrpc": "2.0",
                "id": request_id,
                "result": {
                    "content": result,
                    "tool": tool_name
                }
            }
            try:
                yield f"data: {json.dumps(final_message, ensure_ascii=False)}\n\n"
            except (GeneratorExit, asyncio.CancelledError, BrokenPipeError, ConnectionResetError) as e:
                print(f"ğŸ”Œ [SSE] è¿æ¥åœ¨å‘é€æœ€ç»ˆç»“æœæ—¶å…³é—­/å–æ¶ˆ: {e} (id={request_id}, tool={tool_name})")
                return
        
        
        return StreamingResponse(
            generate_sse_response(),
            media_type="text/event-stream",
            headers={
                "Cache-Control": "no-cache",
                "Connection": "keep-alive"
            }
        )
        
    except Exception as e:
        print(f"âŒ å·¥å…·è°ƒç”¨é”™è¯¯: {str(e)}")
        error_response = {
            "jsonrpc": "2.0",
            "id": request.get("id", 1),
            "error": {
                "code": -32603,
                "message": "Internal error",
                "data": {
                    "type": "tool_execution_failed",
                    "message": str(e)
                }
            }
        }
        return error_response

@http_app.get("/")
async def root():
    """æ ¹ç«¯ç‚¹"""
    return {"message": "MCP HTTP API Server", "version": "1.0.0"}

@http_app.get("/health")
async def health():
    """å¥åº·æ£€æŸ¥ç«¯ç‚¹"""
    return {"status": "healthy", "timestamp": datetime.now().isoformat()}

if __name__ == "__main__":
    print("ğŸš€ å¯åŠ¨MCPæœåŠ¡å™¨...")
    print("ğŸ“¡ æ”¯æŒç«¯ç‚¹: /mcp/tools/call (HTTP API)")
    print("ğŸ”§ æ”¯æŒçš„å·¥å…·: generate_insight_report, generate_industry_dynamic_report, generate_academic_research_report, comprehensive_search")
    
    # å¯åŠ¨HTTPæœåŠ¡å™¨ï¼ˆç”¨äºæµ‹è¯•æ–‡ä»¶çš„è¯·æ±‚æ ¼å¼ï¼‰
    import threading
    import time
    
    def start_http_server():
        print("ğŸš€ å¯åŠ¨HTTP APIæœåŠ¡å™¨...")
        uvicorn.run(http_app, host="0.0.0.0", port=8001)
    
    # åœ¨åå°çº¿ç¨‹å¯åŠ¨HTTPæœåŠ¡å™¨
    http_thread = threading.Thread(target=start_http_server, daemon=True)
    http_thread.start()
    
    # ç­‰å¾…HTTPæœåŠ¡å™¨å¯åŠ¨
    time.sleep(2)
    
    # å¯åŠ¨FastMCPæœåŠ¡å™¨ï¼ˆç”¨äºIDEé›†æˆï¼‰
    print("ğŸš€ å¯åŠ¨FastMCPæœåŠ¡å™¨...")
    mcp.run(transport="streamable-http")
