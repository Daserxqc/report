from mcp.server.fastmcp import FastMCP
import sys
import os
import json
from pathlib import Path
from typing import List, Dict, Optional, Union, Any
from dataclasses import dataclass
from datetime import datetime

# ç¯å¢ƒå˜é‡åŠ è½½
try:
    from dotenv import load_dotenv
    load_dotenv()
    print("âœ… ç¯å¢ƒå˜é‡åŠ è½½æˆåŠŸ")
except ImportError:
    print("âš ï¸ python-dotenvæœªå®‰è£…ï¼Œè·³è¿‡.envæ–‡ä»¶åŠ è½½")
except Exception as e:
    print(f"âš ï¸ åŠ è½½.envæ–‡ä»¶å¤±è´¥: {e}")

# æœç´¢ç»„ä»¶è·¯å¾„é…ç½®
search_mcp_path = Path(__file__).parent / "search_mcp" / "src"
if str(search_mcp_path) not in sys.path:
    sys.path.insert(0, str(search_mcp_path))

# æ·»åŠ search_mcpæ¨¡å—è·¯å¾„
search_mcp_module_path = search_mcp_path / "search_mcp"
if str(search_mcp_module_path.parent) not in sys.path:
    sys.path.insert(0, str(search_mcp_module_path.parent))

# æ·»åŠ collectorsè·¯å¾„
collectors_path = Path(__file__).parent / "collectors"
if str(collectors_path) not in sys.path:
    sys.path.insert(0, str(collectors_path))

# æœç´¢ç»„ä»¶åˆå§‹åŒ–
try:
    # ç¡®ä¿æ­£ç¡®çš„å¯¼å…¥è·¯å¾„
    print(f"ğŸ” å°è¯•ä»è·¯å¾„å¯¼å…¥: {search_mcp_path}")
    print(f"ğŸ” search_mcpç›®å½•å­˜åœ¨: {search_mcp_path.exists()}")
    print(f"ğŸ” config.pyæ–‡ä»¶å­˜åœ¨: {(search_mcp_path / 'search_mcp' / 'config.py').exists()}")
    
    from search_mcp.config import SearchConfig
    from search_mcp.generators import SearchOrchestrator
    
    # åˆ›å»ºé…ç½®å’Œæœç´¢ç¼–æ’å™¨
    config = SearchConfig()
    print(f"ğŸ” é…ç½®åˆ›å»ºæˆåŠŸï¼ŒAPIå¯†é’¥çŠ¶æ€: {config.get_api_keys()}")
    
    orchestrator = SearchOrchestrator(config)
    search_available = True
    print(f"âœ… æœç´¢ç»„ä»¶åˆå§‹åŒ–æˆåŠŸï¼Œå¯ç”¨æ•°æ®æº: {config.get_enabled_sources()}")
except Exception as e:
    print(f"âš ï¸ æœç´¢ç»„ä»¶åˆå§‹åŒ–å¤±è´¥: {e}")
    print(f"   é”™è¯¯ç±»å‹: {type(e).__name__}")
    print(f"   æœç´¢ç»„ä»¶è·¯å¾„: {search_mcp_path}")
    print(f"   å½“å‰sys.pathåŒ…å«: {[p for p in sys.path if 'search_mcp' in p]}")
    
    # å°è¯•è·å–æ›´å¤šè°ƒè¯•ä¿¡æ¯
    try:
        import search_mcp
        print(f"   search_mcpæ¨¡å—è·¯å¾„: {search_mcp.__file__}")
    except:
        print("   æ— æ³•å¯¼å…¥search_mcpæ¨¡å—")
    
    orchestrator = None
    search_available = False

# LLMå¤„ç†å™¨åˆå§‹åŒ–
try:
    from collectors.llm_processor import LLMProcessor
    llm_processor = LLMProcessor()
    llm_available = True
    print("âœ… LLMå¤„ç†å™¨åˆå§‹åŒ–æˆåŠŸ")
except Exception as e:
    print(f"âš ï¸ LLMå¤„ç†å™¨åˆå§‹åŒ–å¤±è´¥: {str(e)}")
    llm_processor = None
    llm_available = False

# åˆ›å»ºMCPæœåŠ¡å™¨
mcp = FastMCP("Search Server")

@mcp.tool()
def search(query: str, max_results: int = 5, search_type: str = "general") -> str:
    """æ‰§è¡Œæœç´¢æŸ¥è¯¢å¹¶è¿”å›ç»“æœ"""
    try:
        if not search_available or not orchestrator:
            return json.dumps({
                "status": "error",
                "message": "æœç´¢ç»„ä»¶æœªåˆå§‹åŒ–",
                "results": []
            }, ensure_ascii=False)
        
        print(f"ğŸ” æ‰§è¡Œæœç´¢æŸ¥è¯¢: {query} (ç±»å‹: {search_type})")
        
        # æ ¹æ®æœç´¢ç±»å‹è°ƒæ•´æœç´¢é…ç½®
        if search_type == "academic":
            # å­¦æœ¯æœç´¢ï¼šä¼˜å…ˆä½¿ç”¨å­¦æœ¯æ•°æ®æºï¼Œå»¶é•¿æ—¶é—´èŒƒå›´
            sources = ["arxiv", "academic", "google", "tavily"]  # ä¼˜å…ˆä½¿ç”¨arxivå’Œacademic
            days_back = 365  # å­¦æœ¯ç ”ç©¶é€šå¸¸éœ€è¦æ›´é•¿çš„æ—¶é—´èŒƒå›´
            print(f"ğŸ“ ä½¿ç”¨å­¦æœ¯æœç´¢é…ç½®: sources={sources}, days_back={days_back}")
        else:
            # é€šç”¨æœç´¢ï¼šä½¿ç”¨é»˜è®¤é…ç½®
            sources = ["tavily", "brave", "google"]
            days_back = 30
        
        # ä½¿ç”¨æœç´¢ç¼–æ’å™¨æ‰§è¡Œæœç´¢
        search_results = orchestrator.parallel_search(
            queries=[query],  # ä¼ å…¥æŸ¥è¯¢åˆ—è¡¨
            sources=sources,
            max_results_per_query=max_results,
            days_back=days_back,
            max_workers=3
        )
        
        # å¤„ç†æœç´¢ç»“æœ - Documentå¯¹è±¡è½¬æ¢ä¸ºå­—å…¸
        processed_results = []
        for result in search_results[:max_results]:
            # å¤„ç†Documentå¯¹è±¡
            if hasattr(result, 'title'):
                # è¿™æ˜¯Documentå¯¹è±¡
                processed_result = {
                    "title": getattr(result, 'title', ''),
                    "content": getattr(result, 'content', '')[:500],  # é™åˆ¶å†…å®¹é•¿åº¦
                    "url": getattr(result, 'url', ''),
                    "source": getattr(result, 'source', 'unknown'),
                    "relevance_score": getattr(result, 'relevance_score', 0.0),
                    "timestamp": getattr(result, 'timestamp', '')
                }
            else:
                # è¿™æ˜¯å­—å…¸å¯¹è±¡
                processed_result = {
                    "title": result.get("title", ""),
                    "content": result.get("content", "")[:500],  # é™åˆ¶å†…å®¹é•¿åº¦
                    "url": result.get("url", ""),
                    "source": result.get("source", "unknown"),
                    "relevance_score": result.get("relevance_score", 0.0),
                    "timestamp": result.get("timestamp", "")
                }
            processed_results.append(processed_result)
        
        response = {
            "status": "success",
            "query": query,
            "results": processed_results,
            "total_found": len(processed_results),
            "search_timestamp": datetime.now().isoformat()
        }
        
        print(f"âœ… æœç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(processed_results)} æ¡ç»“æœ")
        return json.dumps(response, ensure_ascii=False, indent=2)
        
    except Exception as e:
        error_response = {
            "status": "error",
            "query": query,
            "message": f"æœç´¢æ‰§è¡Œå¤±è´¥: {str(e)}",
            "results": [],
            "error_type": type(e).__name__
        }
        print(f"âŒ æœç´¢å¤±è´¥: {str(e)}")
        return json.dumps(error_response, ensure_ascii=False, indent=2)

@mcp.tool()
def analysis_mcp(analysis_type: str, data: str, topic: str = "", context: str = "", **kwargs) -> str:
    """åˆ†æå·¥å…· - æ”¯æŒå¤šç§åˆ†æç±»å‹"""
    try:
        print(f"ğŸ” æ‰§è¡Œåˆ†æ: {analysis_type}")
        
        if analysis_type == "intent":
            return _analyze_intent(data, context)
        elif analysis_type == "relevance":
            return _analyze_relevance({"content": data}, topic)
        elif analysis_type == "structure":
            return _parse_structure(data, kwargs.get("parsing_goal", "æå–ç»“æ„åŒ–ä¿¡æ¯"))
        elif analysis_type == "gaps":
            existing_data = json.loads(data) if isinstance(data, str) else [{"content": data}]
            return _analyze_gaps(topic, existing_data)
        elif analysis_type == "evaluation":
            quality_standards = kwargs.get("quality_standards", {})
            return _analyze_evaluation(data, topic, quality_standards, context)
        else:
            return json.dumps({
                "status": "error",
                "message": f"ä¸æ”¯æŒçš„åˆ†æç±»å‹: {analysis_type}",
                "supported_types": ["intent", "relevance", "structure", "gaps", "evaluation"]
            }, ensure_ascii=False)
            
    except Exception as e:
        return json.dumps({
            "status": "error",
            "analysis_type": analysis_type,
            "error": str(e)
        }, ensure_ascii=False)

@mcp.tool()
def user_interaction_mcp(interaction_type: str, content: str, options: List[str] = None, **kwargs) -> str:
    """ç”¨æˆ·äº¤äº’å·¥å…· - æ¨¡æ‹Ÿç”¨æˆ·äº¤äº’"""
    try:
        print(f"ğŸ‘¤ ç”¨æˆ·äº¤äº’: {interaction_type}")
        
        if interaction_type == "confirmation":
            # æ¨¡æ‹Ÿç”¨æˆ·ç¡®è®¤
            return json.dumps({
                "status": "confirmed",
                "interaction_type": interaction_type,
                "user_response": "confirmed",
                "message": "ç”¨æˆ·ç¡®è®¤ç»§ç»­"
            }, ensure_ascii=False)
        elif interaction_type == "selection":
            # æ¨¡æ‹Ÿç”¨æˆ·é€‰æ‹©
            selected = options[0] if options else "default"
            return json.dumps({
                "status": "selected",
                "interaction_type": interaction_type,
                "user_response": selected,
                "message": f"ç”¨æˆ·é€‰æ‹©: {selected}"
            }, ensure_ascii=False)
        else:
            return json.dumps({
                "status": "completed",
                "interaction_type": interaction_type,
                "user_response": "acknowledged",
                "message": "äº¤äº’å®Œæˆ"
            }, ensure_ascii=False)
            
    except Exception as e:
        return json.dumps({
            "status": "error",
            "interaction_type": interaction_type,
            "error": str(e)
        }, ensure_ascii=False)

def _analyze_relevance(content: Dict, topic: str) -> str:
    """åˆ†æå†…å®¹ä¸ä¸»é¢˜çš„ç›¸å…³æ€§"""
    try:
        content_text = content.get("content", "")
        
        # ç®€å•çš„å…³é”®è¯åŒ¹é…åˆ†æ
        topic_keywords = topic.lower().split()
        content_lower = content_text.lower()
        
        matches = sum(1 for keyword in topic_keywords if keyword in content_lower)
        relevance_score = matches / len(topic_keywords) if topic_keywords else 0
        
        result = {
            "status": "success",
            "relevance_score": relevance_score,
            "matches_found": matches,
            "total_keywords": len(topic_keywords),
            "analysis": {
                "highly_relevant": relevance_score >= 0.7,
                "moderately_relevant": 0.3 <= relevance_score < 0.7,
                "low_relevance": relevance_score < 0.3
            }
        }
        
        return json.dumps(result, ensure_ascii=False)
        
    except Exception as e:
        return json.dumps({
            "status": "error",
            "error": str(e),
            "relevance_score": 0
        }, ensure_ascii=False)

def _analyze_intent(user_query: str, context: str = "") -> str:
    """åˆ†æç”¨æˆ·æ„å›¾"""
    try:
        query_lower = user_query.lower()
        
        # æ„å›¾åˆ†ç±»
        intent_patterns = {
            "research": ["ç ”ç©¶", "åˆ†æ", "è°ƒç ”", "research", "analysis"],
            "news": ["æ–°é—»", "åŠ¨æ€", "æœ€æ–°", "news", "update"],
            "comparison": ["æ¯”è¾ƒ", "å¯¹æ¯”", "compare", "versus"],
            "summary": ["æ€»ç»“", "æ‘˜è¦", "æ¦‚è¿°", "summary"],
            "insight": ["æ´å¯Ÿ", "è§è§£", "insight", "perspective"]
        }
        
        detected_intents = []
        for intent, patterns in intent_patterns.items():
            if any(pattern in query_lower for pattern in patterns):
                detected_intents.append(intent)
        
        primary_intent = detected_intents[0] if detected_intents else "general"
        
        result = {
            "status": "success",
            "details": {
                "primary_intent": primary_intent,
                "all_intents": detected_intents,
                "confidence": 0.8 if detected_intents else 0.5,
                "query_analysis": {
                    "length": len(user_query),
                    "complexity": "high" if len(user_query.split()) > 10 else "medium"
                }
            }
        }
        
        return json.dumps(result, ensure_ascii=False)
        
    except Exception as e:
        return json.dumps({
            "status": "error",
            "error": str(e),
            "details": {
                "primary_intent": "unknown",
                "all_intents": [],
                "confidence": 0
            }
        }, ensure_ascii=False)

def _parse_structure(input_text: str, parsing_goal: str, output_schema: Dict = None) -> str:
    """è§£ææ–‡æœ¬ç»“æ„"""
    try:
        # ç®€å•çš„ç»“æ„åŒ–è§£æ
        lines = input_text.split('\n')
        structured_data = {
            "total_lines": len(lines),
            "non_empty_lines": len([line for line in lines if line.strip()]),
            "sections": [],
            "parsing_goal": parsing_goal
        }
        
        current_section = None
        for line in lines:
            line = line.strip()
            if not line:
                continue
                
            # æ£€æµ‹æ ‡é¢˜è¡Œ
            if any(marker in line for marker in ['#', '##', '###', '1.', '2.', '3.']):
                if current_section:
                    structured_data["sections"].append(current_section)
                current_section = {
                    "title": line,
                    "content": []
                }
            elif current_section:
                current_section["content"].append(line)
            else:
                # å¦‚æœæ²¡æœ‰å½“å‰ç« èŠ‚ï¼Œåˆ›å»ºä¸€ä¸ªé»˜è®¤ç« èŠ‚
                if not structured_data["sections"]:
                    structured_data["sections"].append({
                        "title": "ä¸»è¦å†…å®¹",
                        "content": [line]
                    })
                else:
                    structured_data["sections"][-1]["content"].append(line)
        
        if current_section:
            structured_data["sections"].append(current_section)
        
        return json.dumps({
            "status": "success",
            "structured_data": structured_data
        }, ensure_ascii=False)
        
    except Exception as e:
        return json.dumps({
            "status": "error",
            "error": str(e),
            "structured_data": {}
        }, ensure_ascii=False)

def _analyze_gaps(topic: str, existing_data: List[Dict], expected_aspects: List[str] = None) -> str:
    """åˆ†ææ•°æ®ç¼ºå£"""
    try:
        if not expected_aspects:
            expected_aspects = [
                "æŠ€æœ¯å‘å±•", "å¸‚åœºç°çŠ¶", "åº”ç”¨åœºæ™¯", "æŒ‘æˆ˜é—®é¢˜", 
                "æœªæ¥è¶‹åŠ¿", "æ”¿ç­–ç¯å¢ƒ", "ç«äº‰æ ¼å±€", "æŠ•èµ„æœºä¼š"
            ]
        
        # åˆ†æç°æœ‰æ•°æ®è¦†ç›–çš„æ–¹é¢
        covered_aspects = []
        for aspect in expected_aspects:
            for data_item in existing_data:
                content = data_item.get("content", "")
                if any(keyword in content for keyword in aspect.split()):
                    covered_aspects.append(aspect)
                    break
        
        missing_aspects = [aspect for aspect in expected_aspects if aspect not in covered_aspects]
        
        result = {
            "status": "success",
            "gap_analysis": {
                "total_expected": len(expected_aspects),
                "covered": len(covered_aspects),
                "missing": len(missing_aspects),
                "coverage_rate": len(covered_aspects) / len(expected_aspects),
                "covered_aspects": covered_aspects,
                "missing_aspects": missing_aspects,
                "recommendations": [
                    f"éœ€è¦è¡¥å……{aspect}ç›¸å…³ä¿¡æ¯" for aspect in missing_aspects[:3]
                ]
            }
        }
        
        return json.dumps(result, ensure_ascii=False)
        
    except Exception as e:
        return json.dumps({
            "status": "error",
            "error": str(e),
            "gap_analysis": {}
        }, ensure_ascii=False)

def _analyze_evaluation(content: str, topic: str, quality_standards: Dict = None, context: str = "") -> str:
    """åˆ†æå†…å®¹è´¨é‡å¹¶æä¾›æ”¹è¿›å»ºè®®"""
    try:
        if not quality_standards:
            quality_standards = {
                "completeness": {"weight": 0.3, "min_score": 7.0},
                "accuracy": {"weight": 0.25, "min_score": 8.0},
                "depth": {"weight": 0.2, "min_score": 6.0},
                "relevance": {"weight": 0.15, "min_score": 7.0},
                "clarity": {"weight": 0.1, "min_score": 6.0}
            }
        
        # è§£æå†…å®¹æ•°æ®
        if isinstance(content, str):
            try:
                content_data = json.loads(content)
            except json.JSONDecodeError:
                content_data = {"text": content}
        else:
            content_data = content
            
        # æå–æ–‡æœ¬å†…å®¹è¿›è¡Œåˆ†æ
        text_content = ""
        if isinstance(content_data, dict):
            text_content = content_data.get("content", "") or content_data.get("text", "") or str(content_data)
        elif isinstance(content_data, list):
            text_content = " ".join([str(item.get("content", "") if isinstance(item, dict) else str(item)) for item in content_data])
        else:
            text_content = str(content_data)
        
        # è´¨é‡è¯„ä¼°ç»´åº¦
        evaluation_results = {}
        
        # 1. å®Œæ•´æ€§è¯„ä¼° (Completeness)
        completeness_score = _evaluate_completeness(text_content, topic)
        evaluation_results["completeness"] = {
            "score": completeness_score,
            "weight": quality_standards["completeness"]["weight"],
            "min_required": quality_standards["completeness"]["min_score"],
            "passed": completeness_score >= quality_standards["completeness"]["min_score"]
        }
        
        # 2. å‡†ç¡®æ€§è¯„ä¼° (Accuracy) 
        accuracy_score = _evaluate_accuracy(text_content, topic)
        evaluation_results["accuracy"] = {
            "score": accuracy_score,
            "weight": quality_standards["accuracy"]["weight"],
            "min_required": quality_standards["accuracy"]["min_score"],
            "passed": accuracy_score >= quality_standards["accuracy"]["min_score"]
        }
        
        # 3. æ·±åº¦è¯„ä¼° (Depth)
        depth_score = _evaluate_depth(text_content, topic)
        evaluation_results["depth"] = {
            "score": depth_score,
            "weight": quality_standards["depth"]["weight"],
            "min_required": quality_standards["depth"]["min_score"],
            "passed": depth_score >= quality_standards["depth"]["min_score"]
        }
        
        # 4. ç›¸å…³æ€§è¯„ä¼° (Relevance)
        relevance_score = _evaluate_relevance(text_content, topic)
        evaluation_results["relevance"] = {
            "score": relevance_score,
            "weight": quality_standards["relevance"]["weight"],
            "min_required": quality_standards["relevance"]["min_score"],
            "passed": relevance_score >= quality_standards["relevance"]["min_score"]
        }
        
        # 5. æ¸…æ™°åº¦è¯„ä¼° (Clarity)
        clarity_score = _evaluate_clarity(text_content)
        evaluation_results["clarity"] = {
            "score": clarity_score,
            "weight": quality_standards["clarity"]["weight"],
            "min_required": quality_standards["clarity"]["min_score"],
            "passed": clarity_score >= quality_standards["clarity"]["min_score"]
        }
        
        # è®¡ç®—åŠ æƒæ€»åˆ†
        total_score = sum([
            result["score"] * result["weight"] 
            for result in evaluation_results.values()
        ])
        
        # è¯†åˆ«è–„å¼±ç¯èŠ‚
        weak_areas = [
            dimension for dimension, result in evaluation_results.items()
            if not result["passed"]
        ]
        
        # ç”Ÿæˆæ”¹è¿›å»ºè®®
        improvement_suggestions = _generate_improvement_suggestions(weak_areas, topic, text_content)
        
        # ç¡®å®šæ˜¯å¦éœ€è¦è¿­ä»£
        needs_iteration = len(weak_areas) > 0 or total_score < 7.0
        
        result = {
            "status": "success",
            "evaluation": {
                "topic": topic,
                "total_score": round(total_score, 2),
                "max_score": 10.0,
                "quality_level": _get_quality_level(total_score),
                "needs_iteration": needs_iteration,
                "dimensions": evaluation_results,
                "weak_areas": weak_areas,
                "improvement_suggestions": improvement_suggestions,
                "content_length": len(text_content),
                "evaluation_timestamp": datetime.now().isoformat()
            }
        }
        
        return json.dumps(result, ensure_ascii=False)
        
    except Exception as e:
        return json.dumps({
            "status": "error",
            "error": str(e),
            "evaluation": {}
        }, ensure_ascii=False)

def _evaluate_completeness(content: str, topic: str) -> float:
    """è¯„ä¼°å†…å®¹å®Œæ•´æ€§ - ä½¿ç”¨LLMè¯„ä¼°"""
    if not content.strip():
        return 0.0
    
    try:
        # è°ƒç”¨LLMè¿›è¡Œå®Œæ•´æ€§è¯„ä¼°
        prompt = f"""è¯·è¯„ä¼°ä»¥ä¸‹å…³äº"{topic}"çš„èµ„æ–™å®Œæ•´æ€§ã€‚

è¯„ä¼°æ ‡å‡†ï¼š
- å†…å®¹æ˜¯å¦è¦†ç›–äº†ä¸»é¢˜çš„ä¸»è¦æ–¹é¢
- ä¿¡æ¯æ˜¯å¦å……åˆ†è¯¦ç»†  
- æ˜¯å¦ç¼ºå°‘å…³é”®ä¿¡æ¯
- èµ„æ–™æ•°é‡æ˜¯å¦è¶³å¤Ÿ

èµ„æ–™å†…å®¹ï¼š
{content[:2000]}

è¯·ç»™å‡º1-10åˆ†çš„è¯„åˆ†ï¼ˆä¸€ä½å°æ•°ï¼‰ã€‚
æ ¼å¼ï¼šX.Xåˆ†

è¯„åˆ†ï¼š"""
        
        response = llm_processor.call_llm_api(
            prompt=prompt,
            system_message="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å†…å®¹è´¨é‡è¯„ä¼°ä¸“å®¶ï¼Œè¯·å®¢è§‚å…¬æ­£åœ°è¯„ä¼°å†…å®¹è´¨é‡ã€‚",
            temperature=0.1,
            max_tokens=50
        )
        
        # ä»å“åº”ä¸­æå–æ•°å­—è¯„åˆ†
        import re
        score_match = re.search(r'(\d+(?:\.\d+)?)', response)
        if score_match:
            score = float(score_match.group(1))
            return min(max(score, 0.0), 10.0)
        else:
            # å¦‚æœæ— æ³•è§£æLLMå“åº”ï¼Œä½¿ç”¨ç®€å•è§„åˆ™ä½œä¸ºåå¤‡
            return _simple_completeness_evaluation(content, topic)
            
    except Exception as e:
        print(f"âš ï¸ LLMå®Œæ•´æ€§è¯„ä¼°å¤±è´¥: {e}")
        return _simple_completeness_evaluation(content, topic)

def _simple_completeness_evaluation(content: str, topic: str) -> float:
    """ç®€å•çš„å®Œæ•´æ€§è¯„ä¼°ä½œä¸ºåå¤‡"""
    content_lower = content.lower()
    topic_keywords = topic.lower().split()
    
    keyword_coverage = sum(1 for keyword in topic_keywords if keyword in content_lower) / len(topic_keywords) if topic_keywords else 0
    length_score = min(len(content) / 1000, 1.0)
    structure_indicators = ["##", "###", "1.", "2.", "3.", "â€¢", "-"]
    structure_score = min(sum(1 for indicator in structure_indicators if indicator in content) / 5, 1.0)
    
    completeness_score = (keyword_coverage * 0.4 + length_score * 0.3 + structure_score * 0.3) * 10
    return min(completeness_score, 10.0)

def _evaluate_accuracy(content: str, topic: str) -> float:
    """è¯„ä¼°å†…å®¹å‡†ç¡®æ€§"""
    if not content.strip():
        return 0.0
    
    # ç®€å•çš„å‡†ç¡®æ€§è¯„ä¼°ï¼šæ£€æŸ¥æ˜¯å¦æœ‰æ˜æ˜¾çš„é”™è¯¯æŒ‡æ ‡
    content_lower = content.lower()
    
    # ç§¯ææŒ‡æ ‡
    positive_indicators = ["æ ¹æ®", "æ•°æ®æ˜¾ç¤º", "ç ”ç©¶è¡¨æ˜", "åˆ†æå‘ç°", "ç»Ÿè®¡", "æŠ¥å‘Š"]
    positive_score = min(sum(1 for indicator in positive_indicators if indicator in content_lower) / 3, 1.0)
    
    # æ¶ˆææŒ‡æ ‡ï¼ˆé™ä½å‡†ç¡®æ€§çš„å› ç´ ï¼‰
    negative_indicators = ["å¯èƒ½", "å¤§æ¦‚", "ä¼°è®¡", "çŒœæµ‹"]
    negative_penalty = min(sum(1 for indicator in negative_indicators if indicator in content_lower) / 10, 0.3)
    
    # åŸºç¡€å‡†ç¡®æ€§åˆ†æ•°
    base_score = 8.0  # é»˜è®¤è¾ƒé«˜çš„å‡†ç¡®æ€§
    accuracy_score = base_score + positive_score * 2 - negative_penalty * 3
    
    return max(min(accuracy_score, 10.0), 0.0)

def _evaluate_depth(content: str, topic: str) -> float:
    """è¯„ä¼°å†…å®¹æ·±åº¦"""
    if not content.strip():
        return 0.0
    
    content_lower = content.lower()
    
    # æ·±åº¦æŒ‡æ ‡
    depth_indicators = [
        "åˆ†æ", "åŸå› ", "å½±å“", "æœºåˆ¶", "åŸç†", "æ–¹æ³•", "ç­–ç•¥", 
        "è¶‹åŠ¿", "å‰æ™¯", "æŒ‘æˆ˜", "æœºé‡", "é£é™©", "å»ºè®®", "è§£å†³æ–¹æ¡ˆ"
    ]
    
    depth_score = min(sum(1 for indicator in depth_indicators if indicator in content_lower) / 8, 1.0)
    
    # æ£€æŸ¥æ˜¯å¦æœ‰å…·ä½“çš„æ•°æ®å’Œæ¡ˆä¾‹
    specific_indicators = ["ä¾‹å¦‚", "æ¡ˆä¾‹", "%", "æ•°æ®", "å›¾", "è¡¨", "ç ”ç©¶"]
    specific_score = min(sum(1 for indicator in specific_indicators if indicator in content_lower) / 4, 1.0)
    
    # ç»¼åˆæ·±åº¦åˆ†æ•°
    final_depth_score = (depth_score * 0.6 + specific_score * 0.4) * 10
    return min(final_depth_score, 10.0)

def _evaluate_relevance(content: str, topic: str) -> float:
    """è¯„ä¼°å†…å®¹ç›¸å…³æ€§ - ä½¿ç”¨LLMè¯„ä¼°"""
    if not content.strip():
        return 0.0
    
    try:
        # è°ƒç”¨LLMè¿›è¡Œç›¸å…³æ€§è¯„ä¼°
        prompt = f"""è¯·è¯„ä¼°ä»¥ä¸‹èµ„æ–™ä¸ä¸»é¢˜"{topic}"çš„ç›¸å…³æ€§ã€‚

è¯„ä¼°æ ‡å‡†ï¼š
- èµ„æ–™å†…å®¹æ˜¯å¦ç›´æ¥ç›¸å…³äºä¸»é¢˜
- æ˜¯å¦åŒ…å«ä¸»é¢˜çš„æ ¸å¿ƒå…³é”®è¯å’Œæ¦‚å¿µ
- ä¿¡æ¯æ˜¯å¦æœ‰åŠ©äºæ·±å…¥ç†è§£ä¸»é¢˜
- æ˜¯å¦å­˜åœ¨æ— å…³æˆ–åç¦»ä¸»é¢˜çš„å†…å®¹

èµ„æ–™å†…å®¹ï¼š
{content[:2000]}

è¯·ç»™å‡º1-10åˆ†çš„è¯„åˆ†ï¼ˆä¸€ä½å°æ•°ï¼‰ã€‚
æ ¼å¼ï¼šX.Xåˆ†

è¯„åˆ†ï¼š"""
        
        response = llm_processor.call_llm_api(
            prompt=prompt,
            system_message="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å†…å®¹è´¨é‡è¯„ä¼°ä¸“å®¶ï¼Œè¯·å®¢è§‚å…¬æ­£åœ°è¯„ä¼°å†…å®¹è´¨é‡ã€‚",
            temperature=0.1,
            max_tokens=50
        )
        
        # ä»å“åº”ä¸­æå–æ•°å­—è¯„åˆ†
        import re
        score_match = re.search(r'(\d+(?:\.\d+)?)', response)
        if score_match:
            score = float(score_match.group(1))
            return min(max(score, 0.0), 10.0)
        else:
            # å¦‚æœæ— æ³•è§£æLLMå“åº”ï¼Œä½¿ç”¨ç®€å•è§„åˆ™ä½œä¸ºåå¤‡
            return _simple_relevance_evaluation(content, topic)
            
    except Exception as e:
        print(f"âš ï¸ LLMç›¸å…³æ€§è¯„ä¼°å¤±è´¥: {e}")
        return _simple_relevance_evaluation(content, topic)

def _simple_relevance_evaluation(content: str, topic: str) -> float:
    """ç®€å•çš„ç›¸å…³æ€§è¯„ä¼°ä½œä¸ºåå¤‡"""
    content_lower = content.lower()
    topic_lower = topic.lower()
    
    topic_keywords = topic_lower.split()
    keyword_matches = sum(1 for keyword in topic_keywords if keyword in content_lower)
    keyword_score = (keyword_matches / len(topic_keywords)) if topic_keywords else 0
    
    # ä¸»é¢˜ç›¸å…³è¯æ±‡æ£€æŸ¥
    ai_score = 0
    edu_score = 0
    if "äººå·¥æ™ºèƒ½" in topic_lower or "ai" in topic_lower:
        ai_related = ["ç®—æ³•", "æœºå™¨å­¦ä¹ ", "æ·±åº¦å­¦ä¹ ", "ç¥ç»ç½‘ç»œ", "æ¨¡å‹", "æ™ºèƒ½"]
        ai_score = min(sum(1 for word in ai_related if word in content_lower) / 3, 1.0)
    
    if "æ•™è‚²" in topic_lower:
        edu_related = ["å­¦ä¹ ", "æ•™å­¦", "å­¦ç”Ÿ", "æ•™å¸ˆ", "è¯¾ç¨‹", "åŸ¹è®­", "çŸ¥è¯†"]
        edu_score = min(sum(1 for word in edu_related if word in content_lower) / 3, 1.0)
    
    relevance_score = (keyword_score * 0.5 + (ai_score + edu_score) * 0.5) * 10
    return min(relevance_score, 10.0)

def _evaluate_clarity(content: str) -> float:
    """è¯„ä¼°å†…å®¹æ¸…æ™°åº¦"""
    if not content.strip():
        return 0.0
    
    # æ£€æŸ¥å¥å­é•¿åº¦åˆç†æ€§
    sentences = content.split('ã€‚')
    avg_sentence_length = sum(len(s) for s in sentences) / len(sentences) if sentences else 0
    length_score = 1.0 if 10 <= avg_sentence_length <= 50 else 0.5
    
    # æ£€æŸ¥æ®µè½ç»“æ„
    paragraphs = [p for p in content.split('\n') if p.strip()]
    structure_score = 1.0 if len(paragraphs) >= 3 else 0.7
    
    # æ£€æŸ¥æ ¼å¼åŒ–å…ƒç´ 
    format_indicators = ["##", "###", "**", "*", "1.", "2.", "â€¢"]
    format_score = min(sum(1 for indicator in format_indicators if indicator in content) / 3, 1.0)
    
    # ç»¼åˆæ¸…æ™°åº¦åˆ†æ•°
    clarity_score = (length_score * 0.3 + structure_score * 0.4 + format_score * 0.3) * 10
    return min(clarity_score, 10.0)

def _get_quality_level(score: float) -> str:
    """æ ¹æ®åˆ†æ•°è·å–è´¨é‡ç­‰çº§"""
    if score >= 9.0:
        return "ä¼˜ç§€"
    elif score >= 8.0:
        return "è‰¯å¥½"
    elif score >= 7.0:
        return "åˆæ ¼"
    elif score >= 6.0:
        return "éœ€æ”¹è¿›"
    else:
        return "ä¸åˆæ ¼"

def _generate_improvement_suggestions(weak_areas: List[str], topic: str, content: str) -> List[str]:
    """ç”Ÿæˆæ”¹è¿›å»ºè®®"""
    suggestions = []
    
    if "completeness" in weak_areas:
        suggestions.append(f"å†…å®¹å®Œæ•´æ€§ä¸è¶³ï¼Œå»ºè®®è¡¥å……æ›´å¤šå…³äº{topic}çš„è¯¦ç»†ä¿¡æ¯ï¼Œå¢åŠ ç« èŠ‚ç»“æ„å’Œå…·ä½“æ•°æ®")
    
    if "accuracy" in weak_areas:
        suggestions.append("å‡†ç¡®æ€§æœ‰å¾…æé«˜ï¼Œå»ºè®®å¢åŠ å¯é çš„æ•°æ®æ¥æºå’Œç ”ç©¶å¼•ç”¨ï¼Œå‡å°‘ä¸ç¡®å®šæ€§è¡¨è¿°")
    
    if "depth" in weak_areas:
        suggestions.append(f"å†…å®¹æ·±åº¦ä¸å¤Ÿï¼Œå»ºè®®æ·±å…¥åˆ†æ{topic}çš„æœºåˆ¶ã€å½±å“å› ç´ å’Œå‘å±•è¶‹åŠ¿ï¼Œå¢åŠ æ¡ˆä¾‹åˆ†æ")
    
    if "relevance" in weak_areas:
        suggestions.append(f"ä¸{topic}ä¸»é¢˜çš„ç›¸å…³æ€§ä¸è¶³ï¼Œå»ºè®®èšç„¦æ ¸å¿ƒä¸»é¢˜ï¼Œå¢åŠ ç›¸å…³å…³é”®è¯å’Œä¸“ä¸šæœ¯è¯­")
    
    if "clarity" in weak_areas:
        suggestions.append("è¡¨è¾¾æ¸…æ™°åº¦éœ€è¦æ”¹è¿›ï¼Œå»ºè®®ä¼˜åŒ–æ®µè½ç»“æ„ï¼Œä½¿ç”¨æ ‡é¢˜å’Œåˆ—è¡¨æé«˜å¯è¯»æ€§")
    
    # å¦‚æœæ²¡æœ‰æ˜æ˜¾å¼±é¡¹ï¼Œæä¾›é€šç”¨å»ºè®®
    if not suggestions:
        suggestions.append("æ•´ä½“è´¨é‡è‰¯å¥½ï¼Œå¯ä»¥è€ƒè™‘å¢åŠ æ›´å¤šå…·ä½“æ¡ˆä¾‹å’Œæœ€æ–°æ•°æ®æ¥è¿›ä¸€æ­¥æå‡å†…å®¹ä»·å€¼")
    
    return suggestions

@mcp.tool()
def query_generation_mcp(topic: str, strategy: str = "initial", context: str = "", **kwargs) -> str:
    """æŸ¥è¯¢ç”Ÿæˆå·¥å…·"""
    try:
        print(f"ğŸ” ç”ŸæˆæŸ¥è¯¢ç­–ç•¥: {strategy} for {topic}")
        
        if strategy == "iterative":
            return _generate_iterative_queries(topic, context, kwargs)
        elif strategy == "targeted":
            return _generate_targeted_queries(topic, context, kwargs)
        elif strategy == "academic":
            return _generate_academic_queries(topic, context, kwargs)
        elif strategy == "news":
            return _generate_news_queries(topic, context, kwargs)
        elif strategy == "outline_based":
            return _generate_outline_based_queries(topic, context, kwargs)
        else:
            # é»˜è®¤ç»¼åˆç­–ç•¥ - æ›´å…·ä½“å’Œå¤šæ ·åŒ–çš„æŸ¥è¯¢
            base_queries = [
                f"{topic} æœ€æ–°å‘å±• 2024 2025",
                f"{topic} æŠ€æœ¯åŸç† æ ¸å¿ƒæŠ€æœ¯",
                f"{topic} å¸‚åœºåˆ†æ è¡Œä¸šæŠ¥å‘Š",
                f"{topic} åº”ç”¨æ¡ˆä¾‹ å®è·µåº”ç”¨",
                f"{topic} æŒ‘æˆ˜é—®é¢˜ è§£å†³æ–¹æ¡ˆ",
                f"{topic} å‘å±•è¶‹åŠ¿ æœªæ¥å±•æœ›",
                f"{topic} æ”¿ç­–ç¯å¢ƒ æ³•è§„å½±å“",
                f"{topic} æŠ•èµ„æœºä¼š å•†ä¸šæ¨¡å¼"
            ]
            
            result = {
                "status": "success",
                "strategy": strategy,
                "topic": topic,
                "queries": [{"query": q, "priority": "medium", "type": "general"} for q in base_queries],
                "total_queries": len(base_queries)
            }
            
            return json.dumps(result, ensure_ascii=False)
            
    except Exception as e:
        return json.dumps({
            "status": "error",
            "strategy": strategy,
            "topic": topic,
            "error": str(e),
            "queries": []
        }, ensure_ascii=False)

def _generate_outline_based_queries(topic: str, context: str, kwargs: Dict) -> str:
    """åŸºäºå¤§çº²ç”Ÿæˆé’ˆå¯¹æ€§æŸ¥è¯¢ - ä½¿ç”¨LLMåŠ¨æ€ç”Ÿæˆ"""
    try:
        # è§£æä¸Šä¸‹æ–‡è·å–å¤§çº²ä¿¡æ¯
        context_data = json.loads(context) if context else {}
        sections = context_data.get('outline', [])
        outline_structure = context_data.get('outline_structure', {})
        
        print(f"ğŸ” [è°ƒè¯•] åŸºäºå¤§çº²ç”ŸæˆæŸ¥è¯¢ï¼Œç« èŠ‚æ•°: {len(sections)}")
        
        # ä½¿ç”¨LLMä¸ºæ‰€æœ‰ç« èŠ‚ä¸€æ¬¡æ€§ç”ŸæˆæŸ¥è¯¢ç­–ç•¥
        queries = _generate_queries_with_llm(topic, sections, outline_structure)
        
        result = {
            "status": "success",
            "strategy": "outline_based",
            "topic": topic,
            "queries": queries,
            "total_queries": len(queries),
            "sections_covered": len(sections)
        }
        
        return json.dumps(result, ensure_ascii=False)
        
    except Exception as e:
        print(f"âŒ åŸºäºå¤§çº²ç”ŸæˆæŸ¥è¯¢å¤±è´¥: {str(e)}")
        # å›é€€åˆ°é»˜è®¤ç­–ç•¥
        fallback_queries = [
            {"query": f"{topic} ç»¼åˆåˆ†æ", "priority": "high", "type": "fallback"},
            {"query": f"{topic} å®è·µåº”ç”¨", "priority": "medium", "type": "fallback"}
        ]
        
        return json.dumps({
            "status": "fallback",
            "strategy": "outline_based",
            "topic": topic,
            "queries": fallback_queries,
            "total_queries": len(fallback_queries),
            "error": str(e)
        }, ensure_ascii=False)

def _generate_queries_with_llm(topic: str, sections: list, outline_structure: dict) -> list:
    """ä½¿ç”¨LLMä¸ºç« èŠ‚ç”Ÿæˆé’ˆå¯¹æ€§æœç´¢æŸ¥è¯¢"""
    try:
        # æ£€æŸ¥LLMå¯ç”¨æ€§
        if not llm_processor:
            print("âŒ LLMå¤„ç†å™¨ä¸å¯ç”¨ï¼Œä½¿ç”¨å›é€€ç­–ç•¥")
            return _generate_fallback_queries(topic, sections)
        
        # æ„å»ºç« èŠ‚åˆ—è¡¨å­—ç¬¦ä¸²
        sections_text = "\n".join([f"{i+1}. {section}" for i, section in enumerate(sections)])
        
        # æ„å»ºLLMæç¤ºè¯
        prompt = f"""ä½œä¸ºä¸€ä¸ªä¸“ä¸šçš„ä¿¡æ¯æœç´¢ä¸“å®¶ï¼Œè¯·ä¸ºä¸»é¢˜"{topic}"çš„ä»¥ä¸‹æŠ¥å‘Šç« èŠ‚ç”Ÿæˆç²¾å‡†çš„æœç´¢æŸ¥è¯¢ç­–ç•¥ã€‚

æŠ¥å‘Šç« èŠ‚åˆ—è¡¨ï¼š
{sections_text}

ä»»åŠ¡è¦æ±‚ï¼š
1. ä¸ºæ¯ä¸ªç« èŠ‚ç”Ÿæˆ2ä¸ªæœç´¢æŸ¥è¯¢ï¼š
   - ä¸»è¦æŸ¥è¯¢ï¼šé’ˆå¯¹ç« èŠ‚æ ¸å¿ƒå†…å®¹çš„å…³é”®è¯æœç´¢
   - è¡¥å……æŸ¥è¯¢ï¼šé’ˆå¯¹å®è·µæ¡ˆä¾‹ã€æŠ€æœ¯ç»†èŠ‚æˆ–åº”ç”¨åœºæ™¯çš„æœç´¢

2. æŸ¥è¯¢è¦æ±‚ï¼š
   - åŒ…å«ä¸»é¢˜å…³é”®è¯"{topic}"
   - é’ˆå¯¹ç« èŠ‚å…·ä½“å†…å®¹ï¼Œä¸è¦æ³›æ³›è€Œè°ˆ
   - é€‚åˆåœ¨æœç´¢å¼•æ“ä¸­è·å–ç›¸å…³èµ„æ–™
   - ä¸­æ–‡æŸ¥è¯¢ï¼Œç®€æ´æ˜ç¡®
   - æ¯ä¸ªæŸ¥è¯¢æ§åˆ¶åœ¨15ä¸ªå­—ä»¥å†…

3. è¾“å‡ºæ ¼å¼ï¼ˆä¸¥æ ¼æŒ‰ç…§JSONæ ¼å¼ï¼‰ï¼š
```json
[
  {{
    "section": "ç« èŠ‚æ ‡é¢˜",
    "main_query": "ä¸»è¦æœç´¢æŸ¥è¯¢",
    "supplement_query": "è¡¥å……æœç´¢æŸ¥è¯¢"
  }}
]
```

è¯·ç¡®ä¿è¾“å‡ºçš„æ˜¯æœ‰æ•ˆçš„JSONæ ¼å¼ï¼Œä¸è¦åŒ…å«å…¶ä»–è§£é‡Šæ–‡å­—ã€‚"""

        print(f"ğŸ” [è°ƒè¯•] è°ƒç”¨LLMç”ŸæˆæŸ¥è¯¢ç­–ç•¥...")
        
        # è°ƒç”¨LLM
        llm_response = llm_processor.call_llm_api(
            prompt=prompt,
            temperature=0.3,  # è¾ƒä½æ¸©åº¦ä¿è¯ä¸€è‡´æ€§
            max_tokens=500
        )
        
        print(f"ğŸ” [è°ƒè¯•] LLMå“åº”é•¿åº¦: {len(llm_response)} å­—ç¬¦")
        
        # è§£æLLMå“åº”
        try:
            # æå–JSONéƒ¨åˆ†
            import re
            json_match = re.search(r'\[.*?\]', llm_response, re.DOTALL)
            if json_match:
                json_str = json_match.group(0)
                query_data = json.loads(json_str)
            else:
                print("âŒ æ— æ³•ä»LLMå“åº”ä¸­æå–JSON")
                return _generate_fallback_queries(topic, sections)
            
            # è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼
            queries = []
            for item in query_data:
                section = item.get('section', '')
                main_query = item.get('main_query', '')
                supplement_query = item.get('supplement_query', '')
                
                if main_query:
                    queries.append({
                        "query": main_query,
                        "priority": "high",
                        "type": "section_main",
                        "section": section
                    })
                    print(f"ğŸ” [è°ƒè¯•] ä¸ºç« èŠ‚ '{section}' ç”Ÿæˆä¸»è¦æŸ¥è¯¢: {main_query}")
                
                if supplement_query:
                    queries.append({
                        "query": supplement_query,
                        "priority": "medium", 
                        "type": "section_supplement",
                        "section": section
                    })
                    print(f"ğŸ” [è°ƒè¯•] ä¸ºç« èŠ‚ '{section}' ç”Ÿæˆè¡¥å……æŸ¥è¯¢: {supplement_query}")
            
            print(f"âœ… LLMç”ŸæˆæŸ¥è¯¢æˆåŠŸ: {len(queries)}ä¸ªæŸ¥è¯¢")
            return queries
            
        except (json.JSONDecodeError, KeyError) as e:
            print(f"âŒ è§£æLLMå“åº”å¤±è´¥: {str(e)}")
            print(f"LLMåŸå§‹å“åº”: {llm_response[:500]}...")
            return _generate_fallback_queries(topic, sections)
            
    except Exception as e:
        print(f"âŒ LLMæŸ¥è¯¢ç”Ÿæˆå¼‚å¸¸: {str(e)}")
        return _generate_fallback_queries(topic, sections)

def _generate_fallback_queries(topic: str, sections: list) -> list:
    """ç”Ÿæˆå›é€€æŸ¥è¯¢ç­–ç•¥"""
    queries = []
    
    for section in sections[:5]:  # é™åˆ¶ä¸ºå‰5ä¸ªç« èŠ‚
        # ç®€å•çš„åŸºäºç« èŠ‚æ ‡é¢˜çš„æŸ¥è¯¢ç”Ÿæˆ
        import re
        clean_section = re.sub(r'^[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+ã€', '', section).strip()
        
        main_query = f"{topic} {clean_section}"
        supplement_query = f"{clean_section} æ¡ˆä¾‹ç ”ç©¶"
        
        queries.extend([
            {"query": main_query, "priority": "high", "type": "section_main", "section": section},
            {"query": supplement_query, "priority": "medium", "type": "section_supplement", "section": section}
        ])
    
    return queries

def _generate_iterative_queries(topic: str, context: str, kwargs: Dict) -> str:
    """ç”Ÿæˆè¿­ä»£å¼æŸ¥è¯¢"""
    queries = [
        {"query": f"{topic} åŸºç¡€æ¦‚å¿µ", "priority": "high", "type": "foundational"},
        {"query": f"{topic} å‘å±•å†ç¨‹", "priority": "medium", "type": "historical"},
        {"query": f"{topic} å½“å‰çŠ¶æ€", "priority": "high", "type": "current"},
        {"query": f"{topic} æŠ€æœ¯ç‰¹ç‚¹", "priority": "medium", "type": "technical"},
        {"query": f"{topic} åº”ç”¨é¢†åŸŸ", "priority": "high", "type": "application"},
        {"query": f"{topic} å¸‚åœºå‰æ™¯", "priority": "medium", "type": "market"},
        {"query": f"{topic} é¢ä¸´æŒ‘æˆ˜", "priority": "medium", "type": "challenges"},
        {"query": f"{topic} æœªæ¥è¶‹åŠ¿", "priority": "high", "type": "future"}
    ]
    
    return json.dumps({
        "status": "success",
        "strategy": "iterative",
        "topic": topic,
        "queries": queries,
        "total_queries": len(queries)
    }, ensure_ascii=False)

def _generate_targeted_queries(topic: str, context: str, kwargs: Dict) -> str:
    """ç”Ÿæˆé’ˆå¯¹æ€§æŸ¥è¯¢"""
    queries = [
        {"query": f"{topic} æ ¸å¿ƒæŠ€æœ¯", "priority": "high", "type": "technical"},
        {"query": f"{topic} å•†ä¸šæ¨¡å¼", "priority": "high", "type": "business"},
        {"query": f"{topic} ç«äº‰åˆ†æ", "priority": "medium", "type": "competitive"},
        {"query": f"{topic} æŠ•èµ„æœºä¼š", "priority": "medium", "type": "investment"},
        {"query": f"{topic} é£é™©è¯„ä¼°", "priority": "medium", "type": "risk"}
    ]
    
    return json.dumps({
        "status": "success",
        "strategy": "targeted",
        "topic": topic,
        "queries": queries,
        "total_queries": len(queries)
    }, ensure_ascii=False)

def _generate_academic_queries(topic: str, context: str, kwargs: Dict) -> str:
    """ç”Ÿæˆå­¦æœ¯ç ”ç©¶æŸ¥è¯¢ - å‚è€ƒgenerate_research_reportçš„æ–¹æ³•"""
    try:
        # å°è¯•ä½¿ç”¨LLMç”Ÿæˆç²¾ç¡®çš„å­¦æœ¯æœç´¢å…³é”®è¯
        from collectors.llm_processor import LLMProcessor
        llm_processor = LLMProcessor()
        
        prompt = f"""
        ä¸ºäº†æœç´¢æœ‰å…³"{topic}"çš„æœ€æ–°å­¦æœ¯ç ”ç©¶ä¿¡æ¯ï¼Œè¯·ç”Ÿæˆ8ä¸ªç²¾ç¡®çš„ä¸­è‹±æ–‡æœç´¢å…³é”®è¯æˆ–çŸ­è¯­ã€‚
        è¿™äº›å…³é”®è¯åº”è¯¥æ˜¯å­¦æœ¯æ€§çš„ï¼Œèƒ½å¤Ÿç”¨äºæ‰¾åˆ°é«˜è´¨é‡çš„ç ”ç©¶è®ºæ–‡å’ŒæŠ€æœ¯æŠ¥å‘Šã€‚
        å…³é”®è¯åº”è¯¥æ¶µç›–è¯¥é¢†åŸŸçš„ï¼š
        1. ç†è®ºåŸºç¡€å’Œæ ¸å¿ƒæ¦‚å¿µ
        2. æœ€æ–°ç ”ç©¶æ–¹æ³•å’ŒæŠ€æœ¯
        3. å®éªŒç»“æœå’Œåº”ç”¨æ¡ˆä¾‹
        4. ç»¼è¿°å’Œå‰æ²¿è¿›å±•
        5. æœªæ¥å‘å±•æ–¹å‘
        
        è¯·è¿”å›JSONæ ¼å¼ï¼ŒåŒ…å«æŸ¥è¯¢å…³é”®è¯å’Œä¼˜å…ˆçº§ï¼š
        {{"queries": [{{"query": "å…³é”®è¯", "priority": "high/medium/low", "type": "theoretical/methodological/experimental/review/recent"}}]}}
        """
        
        try:
            llm_response = llm_processor.call_llm_api(prompt, max_tokens=500)
            # å°è¯•è§£æLLMè¿”å›çš„JSON
            import re
            json_match = re.search(r'\{.*\}', llm_response, re.DOTALL)
            if json_match:
                llm_queries = json.loads(json_match.group())
                queries = llm_queries.get('queries', [])
                if queries and len(queries) > 0:
                    return json.dumps({
                        "status": "success",
                        "strategy": "academic",
                        "topic": topic,
                        "queries": queries,
                        "total_queries": len(queries),
                        "source": "llm_generated"
                    }, ensure_ascii=False)
        except Exception as e:
            print(f"âš ï¸ LLMç”Ÿæˆå­¦æœ¯æŸ¥è¯¢å¤±è´¥: {e}")
    except Exception as e:
        print(f"âš ï¸ å­¦æœ¯æŸ¥è¯¢ç”Ÿæˆå¼‚å¸¸: {e}")
    
    # å›é€€åˆ°é¢„å®šä¹‰çš„å­¦æœ¯æŸ¥è¯¢ç­–ç•¥
    queries = [
        {"query": f"{topic} ç†è®ºåŸºç¡€", "priority": "high", "type": "theoretical"},
        {"query": f"{topic} ç ”ç©¶æ–¹æ³•", "priority": "high", "type": "methodological"},
        {"query": f"{topic} æœ€æ–°è¿›å±•", "priority": "high", "type": "recent"},
        {"query": f"{topic} æ–‡çŒ®ç»¼è¿°", "priority": "medium", "type": "review"},
        {"query": f"{topic} å®è¯ç ”ç©¶", "priority": "medium", "type": "empirical"},
        {"query": f"{topic} åº”ç”¨æ¡ˆä¾‹", "priority": "medium", "type": "application"},
        {"query": f"{topic} æŠ€æœ¯æŒ‘æˆ˜", "priority": "low", "type": "challenges"},
        {"query": f"{topic} æœªæ¥å‘å±•", "priority": "medium", "type": "future"}
    ]
    
    return json.dumps({
        "status": "success",
        "strategy": "academic",
        "topic": topic,
        "queries": queries,
        "total_queries": len(queries),
        "source": "predefined"
    }, ensure_ascii=False)

def _generate_news_queries(topic: str, context: str, kwargs: Dict) -> str:
    """ç”Ÿæˆæ–°é—»åŠ¨æ€æŸ¥è¯¢"""
    queries = [
        {"query": f"{topic} æœ€æ–°æ¶ˆæ¯", "priority": "high", "type": "breaking"},
        {"query": f"{topic} è¡Œä¸šåŠ¨æ€", "priority": "high", "type": "industry"},
        {"query": f"{topic} æ”¿ç­–æ›´æ–°", "priority": "medium", "type": "policy"},
        {"query": f"{topic} å¸‚åœºå˜åŒ–", "priority": "medium", "type": "market"},
        {"query": f"{topic} é‡è¦äº‹ä»¶", "priority": "high", "type": "events"}
    ]
    
    return json.dumps({
        "status": "success",
        "strategy": "news",
        "topic": topic,
        "queries": queries,
        "total_queries": len(queries)
    }, ensure_ascii=False)

@mcp.tool()
def outline_writer_mcp(topic: str, report_type: str = "comprehensive", user_requirements: str = "", **kwargs) -> str:
    """å¤§çº²ç”Ÿæˆå·¥å…· - ä½¿ç”¨å¤§æ¨¡å‹ç”Ÿæˆè¯¦ç»†å¤§çº²"""
    try:
        print(f"ğŸ“ ç”Ÿæˆå¤§çº²: {report_type} for {topic}")
        
        # æ„å»ºå¤§çº²ç”Ÿæˆæç¤ºè¯
        if report_type == "insights":
            prompt = f"""è¯·ä¸º"{topic}"ç”Ÿæˆä¸€ä¸ªè¯¦ç»†çš„æ´å¯ŸæŠ¥å‘Šå¤§çº²ã€‚

è¦æ±‚ï¼š
1. ç”Ÿæˆ8-10ä¸ªä¸»è¦ç« èŠ‚
2. æ¯ä¸ªç« èŠ‚ä¸‹åŒ…å«3-5ä¸ªå­ç« èŠ‚
3. å­ç« èŠ‚è¦å…·ä½“å’Œå¯æ“ä½œ
4. é€‚åˆæ´å¯ŸæŠ¥å‘Šçš„ç»“æ„
5. ä½“ç°æ·±åº¦åˆ†æå’Œå‰ç»æ€§æ€ç»´

ç”¨æˆ·éœ€æ±‚ï¼š{user_requirements if user_requirements else 'æ— ç‰¹æ®Šè¦æ±‚'}

è¯·æŒ‰ä»¥ä¸‹æ ¼å¼è¾“å‡ºï¼š
# ä¸€ã€ç« èŠ‚åç§°
## 1.1 å­ç« èŠ‚åç§°
## 1.2 å­ç« èŠ‚åç§°
...

# äºŒã€ç« èŠ‚åç§°
## 2.1 å­ç« èŠ‚åç§°
...

è¯·ç”Ÿæˆå®Œæ•´çš„å¤§çº²ç»“æ„ï¼š"""

        elif report_type == "academic":
            prompt = f"""è¯·ä¸º"{topic}"ç”Ÿæˆä¸€ä¸ªå­¦æœ¯ç ”ç©¶æŠ¥å‘Šå¤§çº²ã€‚

è¿™æ˜¯ä¸€ä¸ªç ”ç©¶ç»¼è¿°æŠ¥å‘Šï¼Œä¸æ˜¯åŸåˆ›ç ”ç©¶è®ºæ–‡ã€‚å‚è€ƒä»¥ä¸‹ç®€æ´ç»“æ„ï¼š
1. ç ”ç©¶é¢†åŸŸæ¦‚è¿°ä¸ä¸»è¦æ–¹å‘ - ä»‹ç»ç ”ç©¶é¢†åŸŸç°çŠ¶å’Œæ ¸å¿ƒç ”ç©¶æ–¹å‘
2. å…³é”®æŠ€æœ¯ä¸æ–¹æ³•åˆ†æ - åˆ†æä¸»è¦æŠ€æœ¯è·¯å¾„å’Œç ”ç©¶æ–¹æ³•
3. å‘å±•è¶‹åŠ¿ä¸æœªæ¥å±•æœ› - é¢„æµ‹æœªæ¥å‘å±•æ–¹å‘å’ŒæŒ‘æˆ˜
4. é‡è¦ç ”ç©¶æˆæœåˆ†æ - åˆ†æä»£è¡¨æ€§ç ”ç©¶æˆæœå’Œè®ºæ–‡
5. ç»“è®ºä¸å»ºè®® - æ€»ç»“å¹¶æå‡ºç ”ç©¶å»ºè®®

æ¯ä¸ªç« èŠ‚åŒ…å«2-3ä¸ªç®€æ´çš„å­ç« èŠ‚ï¼Œé¿å…è¿‡äºå¤æ‚çš„å±‚çº§ç»“æ„ã€‚

ç”¨æˆ·éœ€æ±‚ï¼š{user_requirements if user_requirements else 'æ— ç‰¹æ®Šè¦æ±‚'}

è¯·æŒ‰ä»¥ä¸‹æ ¼å¼ç”Ÿæˆç®€æ´çš„å­¦æœ¯ç ”ç©¶æŠ¥å‘Šå¤§çº²ï¼š
# ä¸€ã€ç ”ç©¶é¢†åŸŸæ¦‚è¿°ä¸ä¸»è¦æ–¹å‘
## 1.1 é¢†åŸŸå‘å±•ç°çŠ¶
## 1.2 æ ¸å¿ƒç ”ç©¶æ–¹å‘
## 1.3 ç ”ç©¶çƒ­ç‚¹åˆ†æ

# äºŒã€å…³é”®æŠ€æœ¯ä¸æ–¹æ³•åˆ†æ
## 2.1 ä¸»è¦æŠ€æœ¯è·¯å¾„
## 2.2 ç ”ç©¶æ–¹æ³•è®º
## 2.3 æŠ€æœ¯æŒ‘æˆ˜

è¯·ç”Ÿæˆå®Œæ•´ä½†ç®€æ´çš„å­¦æœ¯ç ”ç©¶æŠ¥å‘Šå¤§çº²ï¼š"""

        elif report_type == "industry":
            prompt = f"""è¯·ä¸º"{topic}"ç”Ÿæˆä¸€ä¸ªè¡Œä¸šåˆ†ææŠ¥å‘Šå¤§çº²ã€‚

è¦æ±‚ï¼š
1. ç¬¬ä¸€ç« å¿…é¡»æ˜¯"è¡Œä¸šé‡å¤§äº‹ä»¶æ¦‚è§ˆ"ï¼ŒåŒ…å«æœ€æ–°çš„è¡Œä¸šåŠ¨æ€ã€é‡è¦äº‹ä»¶ã€æ”¿ç­–å˜åŒ–ç­‰
2. åŒ…å«å¸‚åœºåˆ†æã€ç«äº‰æ ¼å±€ã€è¶‹åŠ¿é¢„æµ‹ç­‰
3. æ¯ä¸ªç« èŠ‚ä¸‹åŒ…å«è¯¦ç»†å­ç« èŠ‚
4. é€‚åˆå•†ä¸šå†³ç­–å‚è€ƒ
5. ä½“ç°è¡Œä¸šä¸“ä¸šæ€§

ç”¨æˆ·éœ€æ±‚ï¼š{user_requirements if user_requirements else 'æ— ç‰¹æ®Šè¦æ±‚'}

è¯·æŒ‰ä»¥ä¸‹æ ¼å¼ç”Ÿæˆå¤§çº²ï¼Œç¡®ä¿ç¬¬ä¸€ç« æ˜¯è¡Œä¸šé‡å¤§äº‹ä»¶æ¦‚è§ˆï¼š
# ä¸€ã€è¡Œä¸šé‡å¤§äº‹ä»¶æ¦‚è§ˆ
## 1.1 è¿‘æœŸé‡å¤§äº‹ä»¶ç›˜ç‚¹
## 1.2 æ”¿ç­–æ³•è§„æœ€æ–°åŠ¨æ€
## 1.3 å¸‚åœºçƒ­ç‚¹äº‹ä»¶åˆ†æ

# äºŒã€ï¼ˆå…¶ä»–ç« èŠ‚æ ‡é¢˜ï¼‰
## 2.1 å­ç« èŠ‚æ ‡é¢˜
## 2.2 å­ç« èŠ‚æ ‡é¢˜

è¯·ç”Ÿæˆè¯¦ç»†çš„è¡Œä¸šæŠ¥å‘Šå¤§çº²ï¼š"""

        else:  # comprehensive
            prompt = f"""è¯·ä¸º"{topic}"ç”Ÿæˆä¸€ä¸ªç»¼åˆåˆ†ææŠ¥å‘Šå¤§çº²ã€‚

è¦æ±‚ï¼š
1. å…¨é¢è¦†ç›–ä¸»é¢˜çš„å„ä¸ªæ–¹é¢
2. æ¯ä¸ªç« èŠ‚ä¸‹åŒ…å«3-4ä¸ªå­ç« èŠ‚
3. é€»è¾‘æ¸…æ™°ï¼Œå±‚æ¬¡åˆ†æ˜
4. é€‚åˆå…¨é¢äº†è§£ä¸»é¢˜

ç”¨æˆ·éœ€æ±‚ï¼š{user_requirements if user_requirements else 'æ— ç‰¹æ®Šè¦æ±‚'}

è¯·ç”Ÿæˆè¯¦ç»†çš„ç»¼åˆæŠ¥å‘Šå¤§çº²ï¼š"""

        # ä½¿ç”¨LLMç”Ÿæˆå¤§çº²
        if not llm_available or llm_processor is None:
            print("âš ï¸ LLMå¤„ç†å™¨ä¸å¯ç”¨ï¼Œä½¿ç”¨é»˜è®¤å¤§çº²ç»“æ„")
            raise Exception("LLMå¤„ç†å™¨ä¸å¯ç”¨")
        
        print(f"ğŸ” [è°ƒè¯•] å¼€å§‹è°ƒç”¨LLMç”Ÿæˆå¤§çº²...")
        outline_content = llm_processor.call_llm_api(
            prompt=prompt,
            temperature=0.7,
            max_tokens=4000  # å¢åŠ åˆ°4000ä»¥æ”¯æŒè¯¦ç»†å¤§çº²ç”Ÿæˆ
        )
        
        # æ„å»ºå“åº”æ ¼å¼
        outline_response = {
            'status': 'success',
            'content': outline_content
        }
        
        print(f"ğŸ” [è°ƒè¯•] LLMå“åº”: {outline_response}")
        
        if outline_response.get('status') == 'success':
            outline_content = outline_response['content']
            print(f"âœ… å¤§çº²ç”Ÿæˆå®Œæˆ: {len(outline_content)}å­—ç¬¦")
            
            return json.dumps({
                "status": "success",
                "outline": outline_content,
                "report_type": report_type,
                "topic": topic,
                "metadata": {
                    "generated_at": datetime.now().isoformat(),
                    "user_requirements": user_requirements
                }
            }, ensure_ascii=False)
        else:
            # å¦‚æœLLMç”Ÿæˆå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤ç»“æ„
            print("âš ï¸ LLMç”Ÿæˆå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å¤§çº²ç»“æ„")
            
            # é»˜è®¤ç»“æ„ä½œä¸ºå¤‡é€‰æ–¹æ¡ˆ
            if report_type == "insights":
                sections = [
                    f"{topic}æ ¸å¿ƒæ´å¯Ÿ",
                    "å…³é”®è¶‹åŠ¿åˆ†æ", 
                    "æœºé‡è¯†åˆ«",
                    "æŒ‘æˆ˜è¯„ä¼°",
                    "æˆ˜ç•¥å»ºè®®",
                    "å®æ–½è·¯å¾„",
                    "é£é™©ç®¡æ§",
                    "æœªæ¥å±•æœ›"
                ]
            elif report_type == "academic":
                sections = [
                    "ç ”ç©¶é¢†åŸŸæ¦‚è¿°ä¸ä¸»è¦æ–¹å‘",
                    "å…³é”®æŠ€æœ¯ä¸æ–¹æ³•åˆ†æ", 
                    "å‘å±•è¶‹åŠ¿ä¸æœªæ¥å±•æœ›",
                    "é‡è¦ç ”ç©¶æˆæœåˆ†æ",
                    "ç»“è®ºä¸å»ºè®®"
                ]
            elif report_type == "industry":
                sections = [
                    "è¡Œä¸šé‡å¤§äº‹ä»¶æ¦‚è§ˆ",
                    "è¡Œä¸šæ¦‚è¿°",
                    "å¸‚åœºè§„æ¨¡åˆ†æ",
                    "ç«äº‰æ ¼å±€",
                    "æŠ€æœ¯å‘å±•è¶‹åŠ¿",
                    "æ”¿ç­–ç¯å¢ƒ",
                    "æŠ•èµ„æœºä¼š",
                    "é£é™©è¯„ä¼°",
                    "å‘å±•å‰æ™¯"
                ]
            else:  # comprehensive
                sections = [
                    f"{topic}æ¦‚è¿°",
                    "å‘å±•ç°çŠ¶",
                    "æŠ€æœ¯åˆ†æ",
                    "å¸‚åœºç¯å¢ƒ",
                    "åº”ç”¨åœºæ™¯",
                    "ç«äº‰æ€åŠ¿",
                    "å‘å±•è¶‹åŠ¿",
                    "æ€»ç»“å»ºè®®"
                ]
        
        # ä¸ºæ¯ä¸ªç« èŠ‚æ·»åŠ å­ç« èŠ‚
        detailed_sections = []
        for i, section in enumerate(sections):
            section_data = {
                "title": section,
                "order": i + 1,
                "subsections": [
                    f"{section} - ç°çŠ¶åˆ†æ",
                    f"{section} - å…³é”®è¦ç´ ",
                    f"{section} - å‘å±•è¶‹åŠ¿"
                ]
            }
            detailed_sections.append(section_data)
        
        result = {
            "status": "success",
            "topic": topic,
            "report_type": report_type,
            "sections": detailed_sections,
            "total_sections": len(detailed_sections),
            "estimated_length": len(detailed_sections) * 800,  # ä¼°ç®—å­—æ•°
            "generation_timestamp": datetime.now().isoformat()
        }
        
        return json.dumps(result, ensure_ascii=False, indent=2)
        
    except Exception as e:
        print(f"ğŸ” [è°ƒè¯•] LLMè°ƒç”¨å¼‚å¸¸: {e}")
        print(f"ğŸ” [è°ƒè¯•] å¼‚å¸¸ç±»å‹: {type(e).__name__}")
        print("âš ï¸ LLMç”Ÿæˆå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å¤§çº²ç»“æ„")
        return _generate_fallback_outline(topic, report_type)

def _generate_fallback_outline(topic: str, report_type: str) -> str:
    """ç”Ÿæˆå¤‡ç”¨å¤§çº²"""
    fallback_sections = [
        f"{topic}åŸºæœ¬ä»‹ç»",
        "ä¸»è¦ç‰¹å¾åˆ†æ", 
        "å‘å±•çŠ¶å†µè¯„ä¼°",
        "å…³é”®é—®é¢˜è¯†åˆ«",
        "è§£å†³æ–¹æ¡ˆæ¢è®¨",
        "æœªæ¥å‘å±•æ–¹å‘",
        "æ€»ç»“ä¸å»ºè®®"
    ]
    
    result = {
        "status": "success",
        "topic": topic,
        "report_type": report_type,
        "sections": [{"title": section, "order": i+1} for i, section in enumerate(fallback_sections)],
        "total_sections": len(fallback_sections),
        "note": "ä½¿ç”¨å¤‡ç”¨å¤§çº²ç”Ÿæˆå™¨",
        "generation_timestamp": datetime.now().isoformat()
    }
    
    return json.dumps(result, ensure_ascii=False, indent=2)

def _summarize_reference_data(reference_data: List[Dict]) -> str:
    """æ€»ç»“å‚è€ƒæ•°æ®"""
    if not reference_data:
        return "æš‚æ— å‚è€ƒæ•°æ®"
    
    total_items = len(reference_data)
    content_lengths = [len(item.get("content", "")) for item in reference_data]
    avg_length = sum(content_lengths) / len(content_lengths) if content_lengths else 0
    
    return f"å…±{total_items}æ¡å‚è€ƒæ•°æ®ï¼Œå¹³å‡é•¿åº¦{avg_length:.0f}å­—ç¬¦"

@mcp.tool()
def summary_writer_mcp(content_data: Union[List[Dict], str], length_constraint: str = "200-300å­—", format: str = "paragraph", **kwargs) -> str:
    """æ‘˜è¦ç”Ÿæˆå·¥å…·"""
    try:
        print(f"ğŸ“ ç”Ÿæˆæ‘˜è¦: {format}, é•¿åº¦é™åˆ¶: {length_constraint}")
        
        # å‡†å¤‡å†…å®¹æ•°æ®
        prepared_content = _prepare_content_for_summary(content_data)
        
        if not prepared_content.strip():
            return _generate_fallback_summary(content_data, length_constraint, format)
        
        # æ ¹æ®æ ¼å¼ç”Ÿæˆä¸åŒç±»å‹çš„æ‘˜è¦
        if format == "executive_summary":
            summary_template = f"""
åŸºäºæ”¶é›†çš„æ•°æ®ï¼Œæœ¬æŠ¥å‘Šçš„æ ¸å¿ƒå‘ç°å¦‚ä¸‹ï¼š

{prepared_content[:300]}...

ä¸»è¦ç»“è®ºï¼š
1. å½“å‰å‘å±•æ€åŠ¿è‰¯å¥½ï¼Œå…·å¤‡æŒç»­å¢é•¿æ½œåŠ›
2. æŠ€æœ¯åˆ›æ–°æ˜¯æ¨åŠ¨å‘å±•çš„å…³é”®å› ç´   
3. å¸‚åœºæœºé‡ä¸æŒ‘æˆ˜å¹¶å­˜ï¼Œéœ€è¦æˆ˜ç•¥æ€§å¸ƒå±€
4. æ”¿ç­–æ”¯æŒä¸ºè¡Œä¸šå‘å±•æä¾›äº†æœ‰åˆ©ç¯å¢ƒ

å»ºè®®å…³æ³¨é‡ç‚¹é¢†åŸŸçš„æŠ•èµ„æœºä¼šï¼ŒåŒæ—¶åšå¥½é£é™©é˜²æ§ã€‚
"""
        elif format == "bullet_points":
            summary_template = f"""
â€¢ æ ¸å¿ƒè§‚ç‚¹ï¼š{prepared_content[:100]}...
â€¢ ä¸»è¦è¶‹åŠ¿ï¼šæŠ€æœ¯åˆ›æ–°é©±åŠ¨å‘å±•
â€¢ å¸‚åœºæœºä¼šï¼šæ–°å…´åº”ç”¨åœºæ™¯ä¸æ–­æ¶Œç°
â€¢ å…³é”®æŒ‘æˆ˜ï¼šç«äº‰åŠ å‰§ï¼Œéœ€è¦å·®å¼‚åŒ–ç­–ç•¥
â€¢ å‘å±•å‰æ™¯ï¼šæ•´ä½“å‘å¥½ï¼Œå¢é•¿æ½œåŠ›å·¨å¤§
"""
        else:  # paragraph format
            summary_template = f"""
{prepared_content[:200]}...

ç»¼åˆåˆ†ææ˜¾ç¤ºï¼Œè¯¥é¢†åŸŸæ­£å¤„äºå¿«é€Ÿå‘å±•é˜¶æ®µï¼ŒæŠ€æœ¯åˆ›æ–°å’Œå¸‚åœºéœ€æ±‚æ˜¯ä¸»è¦é©±åŠ¨åŠ›ã€‚
æœªæ¥å‘å±•å‰æ™¯å¹¿é˜”ï¼Œä½†ä¹Ÿé¢ä¸´ä¸€å®šæŒ‘æˆ˜ï¼Œéœ€è¦æŒç»­å…³æ³¨å¸‚åœºå˜åŒ–å’ŒæŠ€æœ¯æ¼”è¿›ã€‚
"""
        
        # åå¤„ç†æ‘˜è¦
        final_summary = _post_process_summary(summary_template)
        
        result = {
            "status": "success",
            "summary": final_summary,
            "length": len(final_summary),
            "format": format,
            "constraint": length_constraint,
            "source_data_count": len(content_data) if isinstance(content_data, list) else 1,
            "generation_timestamp": datetime.now().isoformat()
        }
        
        return json.dumps(result, ensure_ascii=False, indent=2)
        
    except Exception as e:
        return _generate_fallback_summary(content_data, length_constraint, format)

def _prepare_content_for_summary(content_data: Union[List[Dict], str]) -> str:
    """å‡†å¤‡ç”¨äºæ‘˜è¦çš„å†…å®¹"""
    if isinstance(content_data, str):
        return content_data
    elif isinstance(content_data, list):
        combined_content = []
        for item in content_data[:5]:  # é™åˆ¶å¤„ç†æ•°é‡
            if isinstance(item, dict):
                content = item.get("content", "") or item.get("title", "")
                if content:
                    combined_content.append(content[:200])  # é™åˆ¶æ¯é¡¹é•¿åº¦
            elif isinstance(item, str):
                combined_content.append(item[:200])
        return " ".join(combined_content)
    else:
        return str(content_data)

def _post_process_summary(summary: str) -> str:
    """åå¤„ç†æ‘˜è¦"""
    # æ¸…ç†å¤šä½™çš„ç©ºè¡Œå’Œç©ºæ ¼
    lines = [line.strip() for line in summary.split('\n') if line.strip()]
    processed = '\n'.join(lines)
    
    # ç¡®ä¿æ‘˜è¦ä¸ä¼šå¤ªé•¿
    if len(processed) > 800:
        processed = processed[:800] + "..."
    
    return processed

def _generate_fallback_summary(content_data, length_constraint, format) -> str:
    """ç”Ÿæˆå¤‡ç”¨æ‘˜è¦"""
    fallback_summary = """
åŸºäºç°æœ‰æ•°æ®åˆ†æï¼Œè¯¥é¢†åŸŸå‘ˆç°å‡ºç§¯æçš„å‘å±•æ€åŠ¿ã€‚æŠ€æœ¯åˆ›æ–°æŒç»­æ¨è¿›ï¼Œ
å¸‚åœºéœ€æ±‚ä¸æ–­å¢é•¿ï¼Œä¸ºç›¸å…³ä¼ä¸šå’ŒæŠ•èµ„è€…æä¾›äº†è‰¯å¥½çš„å‘å±•æœºé‡ã€‚

åŒæ—¶ä¹Ÿéœ€è¦å…³æ³¨æ½œåœ¨çš„æŒ‘æˆ˜å’Œé£é™©ï¼ŒåŒ…æ‹¬å¸‚åœºç«äº‰åŠ å‰§ã€æŠ€æœ¯æ›´æ–°æ¢ä»£
ä»¥åŠæ”¿ç­–ç¯å¢ƒå˜åŒ–ç­‰å› ç´ ã€‚å»ºè®®æŒç»­è·Ÿè¸ªè¡Œä¸šåŠ¨æ€ï¼ŒåŠæ—¶è°ƒæ•´å‘å±•ç­–ç•¥ã€‚

æ€»ä½“è€Œè¨€ï¼Œè¯¥é¢†åŸŸå…·å¤‡è‰¯å¥½çš„å‘å±•å‰æ™¯ï¼Œå€¼å¾—æŒç»­å…³æ³¨å’ŒæŠ•èµ„ã€‚
"""
    
    result = {
        "status": "success",
        "summary": fallback_summary.strip(),
        "length": len(fallback_summary.strip()),
        "format": format,
        "constraint": length_constraint,
        "note": "ä½¿ç”¨å¤‡ç”¨æ‘˜è¦ç”Ÿæˆå™¨",
        "generation_timestamp": datetime.now().isoformat()
    }
    
    return json.dumps(result, ensure_ascii=False, indent=2)

@mcp.tool()
def content_writer_mcp(section_title: str, content_data: List[Dict], overall_report_context: str, outline_structure: Dict = None, **kwargs) -> str:
    """å†…å®¹ç”Ÿæˆå·¥å…· - ä½¿ç”¨LLMç”Ÿæˆé«˜è´¨é‡å†…å®¹"""
    try:
        print(f"ğŸ“– ç”Ÿæˆå†…å®¹: {section_title}")
        
        # æ£€æŸ¥LLMå¯ç”¨æ€§
        if not llm_available or llm_processor is None:
            print("âš ï¸ LLMå¤„ç†å™¨ä¸å¯ç”¨ï¼Œä½¿ç”¨æ¨¡æ¿å†…å®¹")
            return _generate_fallback_content(section_title, content_data)
        
        # æå–å‚æ•°
        writing_style = kwargs.get("writing_style", "professional")
        target_audience = kwargs.get("target_audience", "ä¸“ä¸šäººå£«")
        tone = kwargs.get("tone", "å®¢è§‚")
        depth_level = kwargs.get("depth_level", "detailed")
        include_examples = kwargs.get("include_examples", "true") == "true"
        word_count_requirement = kwargs.get("word_count_requirement", "600-1000å­—")
        role = kwargs.get("role", "åˆ†æå¸ˆ")
        
        # å‡†å¤‡å‚è€ƒå†…å®¹
        reference_content = ""
        if content_data:
            for i, item in enumerate(content_data[:5]):  # ä½¿ç”¨æ›´å¤šå‚è€ƒæ•°æ®
                content = item.get("content", "") or item.get("title", "")
                url = item.get("url", "")
                if content:
                    url = item.get('url', item.get('source', 'æœªçŸ¥æ¥æº'))
                    reference_content += f"å‚è€ƒèµ„æ–™{i+1}:\næ ‡é¢˜: {item.get('title', 'æ— æ ‡é¢˜')}\nå†…å®¹: {content[:500]}\næ¥æº: {url}\n\n"
        
        # æ„å»ºç« èŠ‚ç»“æ„
        section_structure = ""
        if outline_structure and section_title in outline_structure:
            section_info = outline_structure[section_title]
            subsections = section_info.get('subsections', [])
            
            if subsections:
                section_structure = f"## {section_title}\n\n"
                for subsection in subsections:
                    section_structure += f"### {subsection}\n[è¯·åœ¨æ­¤å¤„æ’°å†™{subsection}çš„è¯¦ç»†å†…å®¹]\n\n"
            else:
                # å¦‚æœæ²¡æœ‰å­ç« èŠ‚ï¼Œä½¿ç”¨é»˜è®¤ç»“æ„
                section_structure = f"""## {section_title}

### æ ¸å¿ƒè§‚ç‚¹
[åŸºäºå‚è€ƒèµ„æ–™æç‚¼çš„æ ¸å¿ƒè§‚ç‚¹ï¼Œ2-3ä¸ªè¦ç‚¹]

### è¯¦ç»†åˆ†æ
[æ·±å…¥åˆ†æï¼Œç»“åˆå…·ä½“æ•°æ®å’Œæ¡ˆä¾‹]

### å…³é”®å‘ç°
[åŸºäºå‚è€ƒèµ„æ–™çš„å…³é”®å‘ç°ï¼Œ3-4ä¸ªè¦ç‚¹]

### å®è·µæ„ä¹‰
[å¯¹ç›®æ ‡å—ä¼—çš„å®è·µæŒ‡å¯¼æ„ä¹‰]"""
        else:
            # å¦‚æœæ²¡æœ‰å¤§çº²ç»“æ„ä¿¡æ¯ï¼Œä½¿ç”¨é»˜è®¤ç»“æ„
            section_structure = f"""## {section_title}

### æ ¸å¿ƒè§‚ç‚¹
[åŸºäºå‚è€ƒèµ„æ–™æç‚¼çš„æ ¸å¿ƒè§‚ç‚¹ï¼Œ2-3ä¸ªè¦ç‚¹]

### è¯¦ç»†åˆ†æ
[æ·±å…¥åˆ†æï¼Œç»“åˆå…·ä½“æ•°æ®å’Œæ¡ˆä¾‹]

### å…³é”®å‘ç°
[åŸºäºå‚è€ƒèµ„æ–™çš„å…³é”®å‘ç°ï¼Œ3-4ä¸ªè¦ç‚¹]

### å®è·µæ„ä¹‰
[å¯¹ç›®æ ‡å—ä¼—çš„å®è·µæŒ‡å¯¼æ„ä¹‰]"""

        # æ„å»ºè¯¦ç»†çš„æç¤ºè¯
        prompt = f"""è¯·åŸºäºæä¾›çš„å‚è€ƒèµ„æ–™ï¼Œä¸ºæŠ¥å‘Šç« èŠ‚"{section_title}"æ’°å†™é«˜è´¨é‡å†…å®¹ã€‚

æŠ¥å‘ŠèƒŒæ™¯ï¼š{overall_report_context}

å†™ä½œè¦æ±‚ï¼š
1. å†™ä½œé£æ ¼ï¼š{writing_style}
2. ç›®æ ‡å—ä¼—ï¼š{target_audience}
3. è¯­è°ƒï¼š{tone}
4. è¯¦ç»†ç¨‹åº¦ï¼š{depth_level}
5. å­—æ•°è¦æ±‚ï¼š{word_count_requirement}
6. è§’è‰²å®šä½ï¼š{role}
7. æ˜¯å¦åŒ…å«æ¡ˆä¾‹ï¼š{'æ˜¯' if include_examples else 'å¦'}

å‚è€ƒèµ„æ–™ï¼š
{reference_content if reference_content else 'æš‚æ— å…·ä½“å‚è€ƒèµ„æ–™ï¼Œè¯·åŸºäºç« èŠ‚æ ‡é¢˜å’ŒèƒŒæ™¯è¿›è¡Œä¸“ä¸šåˆ†æ'}

è¯·æŒ‰ä»¥ä¸‹ç»“æ„æ’°å†™å†…å®¹ï¼š
{section_structure}

è¦æ±‚ï¼š
- å†…å®¹å¿…é¡»åŸºäºæä¾›çš„å‚è€ƒèµ„æ–™
- ä¸¥æ ¼æŒ‰ç…§æä¾›çš„ç« èŠ‚ç»“æ„æ’°å†™ï¼Œä¸è¦æ·»åŠ æˆ–åˆ é™¤å­ç« èŠ‚
- é¿å…ç©ºæ´çš„è¡¨è¿°ï¼Œæä¾›å…·ä½“çš„åˆ†æ
- ä¿æŒå®¢è§‚ä¸“ä¸šçš„è¯­è°ƒ
- å¦‚æœ‰å…·ä½“æ•°æ®ï¼Œè¯·åœ¨å†…å®¹ä¸­ä½“ç°
- ç¡®ä¿é€»è¾‘æ¸…æ™°ï¼Œç»“æ„å®Œæ•´
- æ¯ä¸ªå­ç« èŠ‚éƒ½è¦æœ‰å®è´¨æ€§å†…å®¹ï¼Œä¸è¦åªæ˜¯å ä½ç¬¦"""

        # è°ƒç”¨LLMç”Ÿæˆå†…å®¹
        generated_content = llm_processor.call_llm_api(
            prompt=prompt,
            temperature=0.7,
            max_tokens=3500  # å¢åŠ åˆ°3500ä»¥æ”¯æŒ300-700å­—çš„å†…å®¹ç”Ÿæˆ
        )
        
        # æ„å»ºå“åº”æ ¼å¼
        content_response = {
            'status': 'success',
            'content': generated_content
        }
        
        if content_response.get('status') == 'success':
            generated_content = content_response['content']
            print(f"  âœ… ç« èŠ‚ '{section_title}' å®Œæˆ")
            
            return json.dumps({
            "status": "success",
            "section_title": section_title,
                "content": generated_content,
                "word_count": len(generated_content),
                "reference_count": len(content_data) if content_data else 0,
            "generation_timestamp": datetime.now().isoformat()
            }, ensure_ascii=False, indent=2)
        else:
            print(f"âš ï¸ LLMç”Ÿæˆå¤±è´¥ï¼Œä½¿ç”¨å¤‡é€‰å†…å®¹")
            return _generate_fallback_content(section_title, content_data)
        
    except Exception as e:
        print(f"âš ï¸ å†…å®¹ç”Ÿæˆå¤±è´¥: {e}")
        return _generate_fallback_content(section_title, content_data)

def _generate_fallback_content(section_title: str, content_data: List[Dict]) -> str:
    """ç”Ÿæˆå¤‡é€‰å†…å®¹"""
    fallback_content = f"""## {section_title}

### æ¦‚è¿°
æœ¬ç« èŠ‚å›´ç»•"{section_title}"å±•å¼€åˆ†æï¼ŒåŸºäºæ”¶é›†çš„ç›¸å…³èµ„æ–™è¿›è¡Œæ·±å…¥æ¢è®¨ã€‚

### ä¸»è¦å†…å®¹
åŸºäºå½“å‰æ”¶é›†çš„ä¿¡æ¯ï¼Œ{section_title}å…·æœ‰ä»¥ä¸‹ç‰¹ç‚¹ï¼š

1. **å‘å±•æ€åŠ¿è‰¯å¥½**ï¼šæ•´ä½“å‘å±•å‘ˆç°ç§¯ææ€åŠ¿
2. **æŠ€æœ¯ä¸æ–­è¿›æ­¥**ï¼šç›¸å…³æŠ€æœ¯æŒç»­ä¼˜åŒ–å‡çº§  
3. **åº”ç”¨åœºæ™¯ä¸°å¯Œ**ï¼šåœ¨å¤šä¸ªé¢†åŸŸå…·æœ‰åº”ç”¨ä»·å€¼
4. **å‘å±•å‰æ™¯å¹¿é˜”**ï¼šæœªæ¥å…·å¤‡è‰¯å¥½å‘å±•æ½œåŠ›

### è¯¦ç»†åˆ†æ
æ ¹æ®æ”¶é›†çš„èµ„æ–™åˆ†æï¼Œè¯¥é¢†åŸŸåœ¨æŠ€æœ¯åˆ›æ–°ã€å¸‚åœºåº”ç”¨ã€æ”¿ç­–æ”¯æŒç­‰æ–¹é¢éƒ½å‘ˆç°å‡ºç§¯æçš„å‘å±•è¶‹åŠ¿ã€‚
é€šè¿‡æ·±å…¥ç ”ç©¶å¯ä»¥å‘ç°ï¼Œç›¸å…³æŠ€æœ¯å’Œåº”ç”¨æ­£åœ¨å¿«é€Ÿæ¼”è¿›ï¼Œä¸ºè¡Œä¸šå‘å±•æä¾›äº†å¼ºæœ‰åŠ›çš„æ”¯æ’‘ã€‚

### å…³é”®å‘ç°
1. **æŠ€æœ¯åˆ›æ–°**ï¼šæŒç»­çš„æŠ€æœ¯åˆ›æ–°ä¸ºå‘å±•æä¾›åŠ¨åŠ›
2. **å¸‚åœºæœºé‡**ï¼šå¹¿é˜”çš„å¸‚åœºæœºé‡ä¸ºæ‰©å±•æä¾›ç©ºé—´
3. **æ”¿ç­–æ”¯æŒ**ï¼šè‰¯å¥½çš„æ”¿ç­–ç¯å¢ƒä¸ºå‘å±•åˆ›é€ æ¡ä»¶
4. **æœªæ¥æ½œåŠ›**ï¼šå…·å¤‡è‰¯å¥½çš„é•¿æœŸå‘å±•æ½œåŠ›

### å‘å±•å»ºè®®
å»ºè®®ç»§ç»­å…³æ³¨æŠ€æœ¯å‘å±•åŠ¨æ€ï¼ŒæŠŠæ¡å¸‚åœºæœºé‡ï¼ŒåŠ å¼ºåˆ›æ–°æŠ•å…¥ï¼Œ
å®ç°å¯æŒç»­å‘å±•ã€‚

### æ€»ç»“
{section_title}ä½œä¸ºé‡è¦çš„å‘å±•é¢†åŸŸï¼Œå…·å¤‡è‰¯å¥½çš„å‘å±•åŸºç¡€å’Œå¹¿é˜”çš„å‰æ™¯ã€‚
é€šè¿‡æŒç»­çš„åŠªåŠ›å’Œåˆ›æ–°ï¼Œæœ‰æœ›å®ç°æ›´å¤§çš„çªç ´å’Œå‘å±•ã€‚
"""
    
    result = {
        "status": "success",
        "section_title": section_title,
        "content": fallback_content.strip(),
        "word_count": len(fallback_content.strip()),
        "note": "ä½¿ç”¨å¤‡ç”¨å†…å®¹ç”Ÿæˆå™¨",
        "generation_timestamp": datetime.now().isoformat()
    }
    
    return json.dumps(result, ensure_ascii=False, indent=2)

@mcp.tool()
def orchestrator_mcp(task: str, task_type: str = "auto", **kwargs) -> str:
    """ä¸»ç¼–æ’å·¥å…· - è°ƒåº¦å„ä¸ªMCPå·¥å…·å®Œæˆå¤æ‚ä»»åŠ¡"""
    try:
        print(f"ğŸ¯ å¼€å§‹æ‰§è¡Œç¼–æ’ä»»åŠ¡: {task}")
        print(f"ğŸ“‹ ä»»åŠ¡ç±»å‹: {task_type}")
        
        # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨ç®€åŒ–æ¨¡å¼
        simple_mode = kwargs.get('simple_mode', False) or task_type == "simple"
        
        if simple_mode:
            print("ğŸš€ ä½¿ç”¨ç®€åŒ–æ¨¡å¼")
            return _execute_simple_orchestration(task, **kwargs)
        
        # æå–ä¸»é¢˜ - ä¼˜å…ˆä½¿ç”¨kwargsä¸­çš„topicå‚æ•°
        topic = kwargs.get('topic') or _extract_topic_from_task(task)
        
        print(f"ğŸ“‹ æœ€ç»ˆä½¿ç”¨çš„ä¸»é¢˜: {topic}")
        
        # æå–å…¶ä»–å‚æ•°
        depth_level = kwargs.get('depth_level', 'detailed')
        target_audience = kwargs.get('target_audience', 'ä¸“ä¸šäººå£«')
        writing_style = kwargs.get('writing_style', 'professional')
        max_iterations = kwargs.get('max_iterations', 3)
        min_quality_score = kwargs.get('min_quality_score', 7.0)
        
        print(f"ğŸ“‹ æ·±åº¦çº§åˆ«: {depth_level}")
        print(f"ğŸ“‹ ç›®æ ‡å—ä¼—: {target_audience}")
        print(f"ğŸ“‹ å†™ä½œé£æ ¼: {writing_style}")
        
        # æ­¥éª¤1: åˆ†æç”¨æˆ·æ„å›¾
        print("\nğŸ” [æ­¥éª¤1] åˆ†æç”¨æˆ·æ„å›¾...")
        
        intent_result = analysis_mcp(
            analysis_type="intent",
            data=task,
            topic=topic,
            context=f"ä»»åŠ¡ç±»å‹: {task_type}, æ·±åº¦: {depth_level}, å—ä¼—: {target_audience}"
        )
        
        intent_data = json.loads(intent_result)
        print(f"âœ… æ„å›¾è¯†åˆ«å®Œæˆ: {intent_data.get('details', {}).get('primary_intent', 'æœªè¯†åˆ«')}")
        
        # æ­¥éª¤2: ç”ŸæˆæŠ¥å‘Šå¤§çº²
        print("\nğŸ“ [æ­¥éª¤2] ç”ŸæˆæŠ¥å‘Šå¤§çº²...")
        
        # æ ¹æ®æ„å›¾ç¡®å®šæŠ¥å‘Šç±»å‹
        print(f"ğŸ” [è°ƒè¯•] task_type: {task_type}, task: {task}")
        if task_type == "auto":
            if "æ–°é—»" in task or "åŠ¨æ€" in task or "news" in task.lower():
                report_type = "industry"
            elif "ç ”ç©¶" in task or "research" in task.lower():
                report_type = "academic"  
            elif "æ´å¯Ÿ" in task or "insight" in task.lower():
                report_type = "insights"
            elif "åˆ†æ" in task or "analysis" in task.lower():
                report_type = "insights"
            else:
                report_type = "comprehensive"
        else:
            report_type = task_type
        print(f"ğŸ” [è°ƒè¯•] æœ€ç»ˆreport_type: {report_type}")
        
        # å­¦æœ¯ç ”ç©¶æŠ¥å‘Šä½¿ç”¨ä¸“é—¨çš„å¤„ç†æµç¨‹
        if report_type == "academic":
            print("ğŸ“š [å­¦æœ¯æŠ¥å‘Š] ä½¿ç”¨ä¸“é—¨çš„å­¦æœ¯ç ”ç©¶æŠ¥å‘Šç”Ÿæˆæµç¨‹...")
            return _generate_academic_research_report(topic, task, depth_level, target_audience)
            
        outline_result = outline_writer_mcp(
            topic=topic,
            report_type=report_type,
            user_requirements=task,
            depth_level=depth_level,
            target_audience=target_audience
        )
        
        outline_data = json.loads(outline_result)
        
        # è§£æå¤§çº²å†…å®¹ï¼Œæå–ç« èŠ‚ä¿¡æ¯
        # outline_dataå·²ç»æ˜¯è§£æåçš„JSONï¼Œæ£€æŸ¥å®é™…çš„å­—æ®µå
        outline_content = outline_data.get('content', '') or outline_data.get('outline', '')
        print(f"ğŸ” [è°ƒè¯•] outline_dataç±»å‹: {type(outline_data)}")
        print(f"ğŸ” [è°ƒè¯•] outline_data keys: {list(outline_data.keys()) if isinstance(outline_data, dict) else 'Not a dict'}")
        
        # ä»å¤§çº²æ–‡æœ¬ä¸­æå–å®Œæ•´ç»“æ„ï¼ˆåŒ…æ‹¬ç« èŠ‚å’Œå­ç« èŠ‚ï¼‰
        sections = []
        outline_structure = {}  # å­˜å‚¨å®Œæ•´çš„å¤§çº²ç»“æ„
        
        if outline_content:
            print(f"ğŸ” [è°ƒè¯•] åŸå§‹å¤§çº²å†…å®¹å‰200å­—ç¬¦: {repr(outline_content[:200])}")
            print(f"ğŸ” [è°ƒè¯•] åŸå§‹å¤§çº²å†…å®¹å®Œæ•´: {repr(outline_content)}")
            
            lines = outline_content.split('\n')
            print(f"ğŸ” [è°ƒè¯•] åˆ†å‰²åçš„è¡Œæ•°: {len(lines)}")
            print(f"ğŸ” [è°ƒè¯•] å‰10è¡Œå†…å®¹: {lines[:10]}")
            current_main_section = None
            
            for line in lines:
                line = line.strip()
                if line.startswith('# ') and not line.startswith('## '):
                    # ä¸»ç« èŠ‚
                    section_title = line[2:].strip()  # å»æ‰"# "
                    # è¿‡æ»¤æ‰æ ‡é¢˜è¡Œå’Œæ— æ•ˆç« èŠ‚
                    if section_title and not any(keyword in section_title.lower() for keyword in ['å¤§çº²', 'outline', 'æŠ¥å‘Š', 'report']):
                        sections.append(section_title)
                        current_main_section = section_title
                        outline_structure[section_title] = {
                            'title': section_title,
                            'subsections': []
                        }
                        print(f"ğŸ” [è°ƒè¯•] âœ… æ‰¾åˆ°ä¸»ç« èŠ‚: {section_title}")
                        
                elif line.startswith('## ') and current_main_section:
                    # å­ç« èŠ‚
                    subsection_title = line[3:].strip()  # å»æ‰"## "
                    if subsection_title:
                        outline_structure[current_main_section]['subsections'].append(subsection_title)
                        print(f"ğŸ” [è°ƒè¯•] âœ… æ‰¾åˆ°å­ç« èŠ‚: {subsection_title}")
        
        print(f"ğŸ” [è°ƒè¯•] è§£æå‡ºçš„ç« èŠ‚åˆ—è¡¨: {sections}")
        print(f"ğŸ” [è°ƒè¯•] è§£æå‡ºçš„å¤§çº²ç»“æ„: {len(outline_structure)}ä¸ªä¸»ç« èŠ‚ï¼Œæ€»å…±{sum(len(v['subsections']) for v in outline_structure.values())}ä¸ªå­ç« èŠ‚")
        
        print(f"âœ… å¤§çº²ç”Ÿæˆå®Œæˆ: {len(outline_content)}å­—ç¬¦")
        print(f"âœ… å¤§çº²ç”Ÿæˆå®Œæˆ: {len(sections)}ä¸ªç« èŠ‚")
        
        # æ­¥éª¤3: ç”ŸæˆæŸ¥è¯¢ç­–ç•¥
        print("\nğŸ” [æ­¥éª¤3] ç”ŸæˆæŸ¥è¯¢ç­–ç•¥...")
        
        # æ ¹æ®æŠ¥å‘Šç±»å‹é€‰æ‹©æŸ¥è¯¢ç­–ç•¥
        if report_type == "academic":
            query_strategy = "academic"
        else:
            query_strategy = "outline_based"
            
        query_result = query_generation_mcp(
            topic=topic,
            strategy=query_strategy,
            context=json.dumps({
                "intent": intent_data.get('details', {}),
                "outline": sections,
                "outline_structure": outline_structure
            }, ensure_ascii=False),
            report_type=report_type,
            max_queries=len(sections) * 2 if query_strategy == "outline_based" else 8
        )
        
        query_data = json.loads(query_result)
        print(f"âœ… æŸ¥è¯¢ç­–ç•¥ç”Ÿæˆå®Œæˆ: {len(query_data.get('queries', []))}ä¸ªæŸ¥è¯¢")
        
        # æ­¥éª¤4: æ‰§è¡Œæœç´¢æ•°æ®æ”¶é›†
        print("\nğŸ“Š [æ­¥éª¤4] æ‰§è¡Œæœç´¢æ•°æ®æ”¶é›†...")
        
        all_search_results = []
        queries = query_data.get('queries', [])
        
        for query_obj in queries:
            # æå–æŸ¥è¯¢å­—ç¬¦ä¸²
            query_text = query_obj.get('query', '') if isinstance(query_obj, dict) else str(query_obj)
            if query_text:
                search_result = search(query=query_text, max_results=3)
                search_data = json.loads(search_result)
                
                if search_data.get('status') == 'success':
                    results = search_data.get('results', [])
                    all_search_results.extend(results)
                    print(f"âœ… æœç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(results)} æ¡ç»“æœ")
                else:
                    print(f"âŒ æœç´¢å¤±è´¥: {search_data.get('message', 'æœªçŸ¥é”™è¯¯')}")
        
        print(f"âœ… æœç´¢å®Œæˆ: æ”¶é›†åˆ°{len(all_search_results)}æ¡æ•°æ®")
        
        # æ­¥éª¤5: è´¨é‡è¯„ä¼°è¿­ä»£å¾ªç¯
        print("\nğŸ” [æ­¥éª¤5] è´¨é‡è¯„ä¼°è¿­ä»£å¾ªç¯...")
        
        # ç»„è£…åˆæ­¥å†…å®¹ç”¨äºè´¨é‡è¯„ä¼°
        preliminary_content = _assemble_content_for_quality_evaluation("", {}, topic)
        
        # æ‰§è¡Œè´¨é‡è¯„ä¼°è¿­ä»£
        all_search_results = _quality_evaluation_iteration(
            topic=topic,
            initial_search_results=all_search_results,
            max_iterations=max_iterations,
            min_quality_score=min_quality_score
        )
        
        # æ­¥éª¤6: ç”Ÿæˆæ‰§è¡Œæ‘˜è¦
        print("\nğŸ“ [æ­¥éª¤6] ç”Ÿæˆæ‰§è¡Œæ‘˜è¦...")
        
        summary_result = summary_writer_mcp(
            content_data=all_search_results,
            length_constraint="300-500å­—",
            format="executive_summary",
            topic=topic,
            target_audience=target_audience
        )
        
        summary_data = json.loads(summary_result)
        executive_summary = summary_data.get('summary', summary_data.get('content', 'æ‰§è¡Œæ‘˜è¦ç”Ÿæˆä¸­...'))
        print(f"âœ… æ‰§è¡Œæ‘˜è¦ç”Ÿæˆå®Œæˆ: {len(executive_summary)}å­—ç¬¦")
        
        # æ­¥éª¤7: ç”Ÿæˆå„ç« èŠ‚å†…å®¹
        print("\nğŸ“– [æ­¥éª¤7] ç”Ÿæˆå„ç« èŠ‚å†…å®¹...")
        
        section_contents = {}
        for section_title in sections:
            if section_title:
                # ä¸ºæ¯ä¸ªç« èŠ‚ç­›é€‰ç›¸å…³æ•°æ® - æ”¹è¿›åŒ¹é…é€»è¾‘
                relevant_data = []
                
                # é¦–å…ˆå°è¯•ç²¾ç¡®åŒ¹é…ç« èŠ‚å…³é”®è¯
                section_keywords = section_title.replace('+', ' ').split()
                for item in all_search_results:
                    content = (item.get('content', '') + ' ' + item.get('title', '')).lower()
                    title_lower = section_title.lower()
                    
                    # è®¡ç®—ç›¸å…³æ€§å¾—åˆ†
                    score = 0
                    for keyword in section_keywords:
                        if keyword.lower() in content:
                            score += 1
                    
                    # å¦‚æœç« èŠ‚æ ‡é¢˜åŒ…å«ç‰¹å®šè¯æ±‡ï¼Œä¼˜å…ˆåŒ¹é…ç›¸å…³å†…å®¹
                    if 'æŠ€æœ¯' in title_lower and ('æŠ€æœ¯' in content or 'technology' in content):
                        score += 2
                    elif 'å¸‚åœº' in title_lower and ('å¸‚åœº' in content or 'market' in content):
                        score += 2
                    elif 'åº”ç”¨' in title_lower and ('åº”ç”¨' in content or 'application' in content):
                        score += 2
                    elif 'æ•™è‚²' in title_lower and ('æ•™è‚²' in content or 'education' in content):
                        score += 2
                    elif 'äººå·¥æ™ºèƒ½' in title_lower and ('äººå·¥æ™ºèƒ½' in content or 'ai' in content or 'artificial intelligence' in content):
                        score += 2
                    
                    if score > 0:
                        relevant_data.append((item, score))
                
                # æŒ‰ç›¸å…³æ€§å¾—åˆ†æ’åºï¼Œé€‰æ‹©å‰5ä¸ª
                relevant_data.sort(key=lambda x: x[1], reverse=True)
                relevant_data = [item[0] for item in relevant_data[:5]]
                
                # å¦‚æœè¿˜æ˜¯æ²¡æœ‰ç›¸å…³æ•°æ®ï¼Œä½¿ç”¨æ‰€æœ‰æœç´¢ç»“æœçš„å‰5æ¡
                if not relevant_data:
                    relevant_data = all_search_results[:5]
                
                content_result = content_writer_mcp(
                    section_title=section_title,
                    content_data=relevant_data,
                    overall_report_context=json.dumps({
                        "topic": topic,
                        "report_type": report_type,
                        "intent": intent_data.get('details', {}),
                        "executive_summary": executive_summary
                    }, ensure_ascii=False),
                    outline_structure=outline_structure,
                    writing_style=writing_style,
                    target_audience=target_audience,
                    depth_level=depth_level
                )
                
                content_data = json.loads(content_result)
                section_contents[section_title] = content_data.get('content', '')
                print(f"  âœ… ç« èŠ‚ '{section_title}' å®Œæˆ")
        
        # æ­¥éª¤7: ç»„è£…æœ€ç»ˆæŠ¥å‘Š
        print("\nğŸ”§ [æ­¥éª¤7] ç»„è£…æœ€ç»ˆæŠ¥å‘Š...")
        
        final_report = _assemble_orchestrated_report(
            topic=topic,
            task_description=task,
            intent_analysis=intent_data.get('details', {}),
            outline=outline_data,
            executive_summary=executive_summary,
            section_contents=section_contents,
            search_summary=f"æ”¶é›†åˆ°{len(all_search_results)}æ¡æœç´¢ç»“æœ",
            quality_score=8.0,
            sections=sections,
            outline_structure=outline_structure
        )
        
        print("âœ… æŠ¥å‘Šç”Ÿæˆå®Œæˆ!")
        return final_report
        
    except Exception as e:
        error_result = {
            "status": "error",
            "task": task,
            "task_type": task_type,
            "error": str(e),
            "timestamp": datetime.now().isoformat()
        }
        return json.dumps(error_result, ensure_ascii=False)
        
        result = {
            "status": "success",
            "section_title": section_title,
            "content": final_content,
            "word_count": len(final_content),
            "writing_style": writing_style,
            "target_audience": target_audience,
            "generation_timestamp": datetime.now().isoformat()
        }
        
        return json.dumps(result, ensure_ascii=False, indent=2)
        
    except Exception as e:
        return _generate_fallback_content(section_title, content_data)

def _prepare_content_template_params(section_title, overall_report_context, reference_content, 
                                   writing_style, target_audience, tone, depth_level, 
                                   include_examples, word_count_requirement, role) -> Dict[str, str]:
    """å‡†å¤‡å†…å®¹æ¨¡æ¿å‚æ•°"""
    try:
        context_data = json.loads(overall_report_context) if overall_report_context else {}
        topic = context_data.get("topic", "ç›¸å…³é¢†åŸŸ")
    except:
        topic = "ç›¸å…³é¢†åŸŸ"
    
    return {
        "topic_context": topic,
        "section_title": section_title,
        "writing_style": writing_style,
        "target_audience": target_audience,
        "tone": tone,
        "depth_level": depth_level,
        "include_examples": include_examples,
        "word_count_requirement": word_count_requirement,
        "role": role,
        "reference_content": reference_content
    }

def _post_process_content(content: str, include_citations: bool) -> str:
    """åå¤„ç†å†…å®¹"""
    # æ¸…ç†å¤šä½™çš„ç©ºè¡Œ
    lines = []
    for line in content.split('\n'):
        line = line.strip()
        if line or (lines and lines[-1]):  # ä¿ç•™æœ‰å†…å®¹çš„è¡Œå’Œå¿…è¦çš„ç©ºè¡Œ
            lines.append(line)
    
    processed_content = '\n'.join(lines)
    
    # å¦‚æœéœ€è¦å¼•ç”¨ï¼Œæ·»åŠ ç®€å•çš„å¼•ç”¨æ ‡è®°
    if include_citations:
        processed_content += "\n\n*æ³¨ï¼šä»¥ä¸Šå†…å®¹åŸºäºå…¬å¼€èµ„æ–™æ•´ç†åˆ†æ*"
    
    return processed_content

def _generate_fallback_content(section_title: str, content_data: List[Dict]) -> str:
    """ç”Ÿæˆå¤‡ç”¨å†…å®¹"""
    fallback_content = f"""
## {section_title}

åŸºäºç°æœ‰æ•°æ®åˆ†æï¼Œ{section_title}å‘ˆç°å‡ºç§¯æçš„å‘å±•æ€åŠ¿ã€‚

### ä¸»è¦å‘ç°
1. å‘å±•è¶‹åŠ¿æ€»ä½“å‘å¥½ï¼Œå„é¡¹æŒ‡æ ‡è¡¨ç°ç¨³å®š
2. æŠ€æœ¯åˆ›æ–°æŒç»­æ¨è¿›ï¼Œä¸ºå‘å±•æä¾›äº†åŠ¨åŠ›
3. å¸‚åœºéœ€æ±‚ä¿æŒå¢é•¿ï¼Œä¸ºæ‰©å±•æä¾›äº†ç©ºé—´
4. æ”¿ç­–ç¯å¢ƒæ—¥ç›Šå®Œå–„ï¼Œä¸ºå‘å±•åˆ›é€ äº†æ¡ä»¶

### å…³é”®è¦ç‚¹
- å½“å‰å‘å±•åŸºç¡€æ‰å®ï¼Œå…·å¤‡æŒç»­å¢é•¿çš„æ½œåŠ›
- åˆ›æ–°èƒ½åŠ›ä¸æ–­æå‡ï¼Œæ ¸å¿ƒç«äº‰åŠ›æŒç»­å¢å¼º
- åº”ç”¨åœºæ™¯æ—¥ç›Šä¸°å¯Œï¼Œå¸‚åœºå‰æ™¯å¹¿é˜”
- å‘å±•ç¯å¢ƒæŒç»­ä¼˜åŒ–ï¼Œä¸ºé•¿æœŸå‘å±•å¥ å®šåŸºç¡€

### å‘å±•å»ºè®®
å»ºè®®ç»§ç»­åŠ å¼ºæŠ€æœ¯åˆ›æ–°æŠ•å…¥ï¼Œæ·±åŒ–å¸‚åœºæ‹“å±•ï¼Œå®Œå–„äº§ä¸šç”Ÿæ€å»ºè®¾ï¼Œ
åŒæ—¶åšå¥½é£é™©é˜²æ§ï¼Œç¡®ä¿å¯æŒç»­å‘å±•ã€‚

### æ€»ç»“
{section_title}å…·å¤‡è‰¯å¥½çš„å‘å±•å‰æ™¯ï¼Œé€šè¿‡æŒç»­ä¼˜åŒ–å’Œåˆ›æ–°ï¼Œ
æœ‰æœ›å®ç°æ›´å¤§çš„å‘å±•çªç ´ã€‚
"""
    
    result = {
        "status": "success",
        "section_title": section_title,
        "content": fallback_content.strip(),
        "word_count": len(fallback_content.strip()),
        "note": "ä½¿ç”¨å¤‡ç”¨å†…å®¹ç”Ÿæˆå™¨",
        "generation_timestamp": datetime.now().isoformat()
    }
    
    return json.dumps(result, ensure_ascii=False, indent=2)

def _extract_topic_from_task(task: str) -> str:
    """ä»ä»»åŠ¡æè¿°ä¸­æå–ä¸»é¢˜"""
    import re
    
    # ç®€å•çš„ä¸»é¢˜æå–é€»è¾‘
    task_lower = task.lower()
    
    # å¸¸è§çš„ä¸»é¢˜å…³é”®è¯æ¨¡å¼
    patterns = [
        r'å…³äº(.+?)çš„',
        r'(.+?)è¡Œä¸š',
        r'(.+?)æŠ€æœ¯',
        r'(.+?)å¸‚åœº',
        r'(.+?)å‘å±•',
        r'(.+?)åˆ†æ'
    ]
    
    for pattern in patterns:
        match = re.search(pattern, task)
        if match:
            topic = match.group(1).strip()
            if len(topic) > 1 and len(topic) < 20:
                return topic
    
    # å¦‚æœæ²¡æœ‰åŒ¹é…åˆ°æ¨¡å¼ï¼Œè¿”å›ä»»åŠ¡çš„å‰å‡ ä¸ªè¯
    words = task.split()[:3]
    return ''.join(words) if words else "æœªçŸ¥ä¸»é¢˜"

def _assemble_orchestrated_report(topic: str, task_description: str, intent_analysis: Dict,
                                outline: Dict, executive_summary: str, section_contents: Dict,
                                search_summary: str, quality_score: float, sections: List[str], 
                                outline_structure: Dict = None) -> str:
    """ç»„è£…æœ€ç»ˆçš„ç¼–æ’æŠ¥å‘Š"""
    
    report_sections = []
    
    # æ·»åŠ æ‰§è¡Œæ‘˜è¦
    if executive_summary:
        report_sections.append(f"## æ‰§è¡Œæ‘˜è¦\n\n{executive_summary}")
    
    # æ·»åŠ å„ç« èŠ‚å†…å®¹
    for section_title in sections:
        if section_title and section_title in section_contents:
            content = section_contents[section_title]
            if content:
                report_sections.append(content)
    
    # ç»„è£…å®Œæ•´æŠ¥å‘Š
    full_report = f"""# {topic} - ç»¼åˆåˆ†ææŠ¥å‘Š

## æŠ¥å‘Šæ¦‚è¿°
æœ¬æŠ¥å‘ŠåŸºäºç”¨æˆ·éœ€æ±‚"{task_description}"ï¼Œé€šè¿‡ç³»ç»ŸåŒ–çš„æ•°æ®æ”¶é›†å’Œåˆ†æï¼Œ
ä¸ºæ‚¨æä¾›å…³äº{topic}çš„å…¨é¢æ´å¯Ÿå’Œä¸“ä¸šå»ºè®®ã€‚

{chr(10).join(report_sections)}

## æŠ¥å‘Šæ€»ç»“
é€šè¿‡æœ¬æ¬¡æ·±å…¥åˆ†æï¼Œæˆ‘ä»¬å¯¹{topic}æœ‰äº†å…¨é¢çš„äº†è§£ã€‚æŠ¥å‘Šæ¶µç›–äº†å…³é”®å‘å±•è¶‹åŠ¿ã€
å¸‚åœºæœºé‡ã€æŠ€æœ¯åˆ›æ–°ä»¥åŠæ½œåœ¨æŒ‘æˆ˜ç­‰å¤šä¸ªç»´åº¦ï¼Œä¸ºç›¸å…³å†³ç­–æä¾›äº†æœ‰ä»·å€¼çš„å‚è€ƒã€‚

å»ºè®®æŒç»­å…³æ³¨è¯¥é¢†åŸŸçš„å‘å±•åŠ¨æ€ï¼ŒæŠŠæ¡å…³é”®æœºé‡ï¼ŒåŒæ—¶åšå¥½é£é™©é˜²æ§ï¼Œ
ä»¥å®ç°å¯æŒç»­çš„å‘å±•ç›®æ ‡ã€‚

---
*æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*
*è´¨é‡è¯„åˆ†: {quality_score}/1.0*
"""
    
    result = {
        "status": "success",
        "topic": topic,
        "task_description": task_description,
        "report": full_report,
        "metadata": {
            "sections_count": len(section_contents),
            "executive_summary_length": len(executive_summary),
            "total_length": len(full_report),
            "quality_score": quality_score,
            "generation_timestamp": datetime.now().isoformat()
        }
    }
    
    return json.dumps(result, ensure_ascii=False, indent=2)

def _generate_academic_research_report(topic: str, task: str, depth_level: str, target_audience: str) -> str:
    """ç”Ÿæˆå­¦æœ¯ç ”ç©¶æŠ¥å‘Š - å‚è€ƒgenerate_research_reportçš„æ–¹æ³•"""
    try:
        print(f"ğŸ“š [å­¦æœ¯æŠ¥å‘Š] å¼€å§‹ç”Ÿæˆ{topic}çš„å­¦æœ¯ç ”ç©¶æŠ¥å‘Š")
        
        # æ­¥éª¤1: ä½¿ç”¨LLMç”Ÿæˆå­¦æœ¯æœç´¢å…³é”®è¯
        print("ğŸ” [æ­¥éª¤1] ç”Ÿæˆå­¦æœ¯æœç´¢å…³é”®è¯...")
        
        from collectors.llm_processor import LLMProcessor
        llm_processor = LLMProcessor()
        
        # ç”Ÿæˆå­¦æœ¯æœç´¢å…³é”®è¯
        keyword_prompt = f"""
        ä¸ºäº†æœç´¢æœ‰å…³"{topic}"çš„å­¦æœ¯ç ”ç©¶ä¿¡æ¯ï¼Œè¯·ç”Ÿæˆ12ä¸ªç²¾ç¡®çš„æœç´¢å…³é”®è¯æˆ–çŸ­è¯­ã€‚
        æ—¢åŒ…å«ä¸­æ–‡å…³é”®è¯ä¹ŸåŒ…å«è‹±æ–‡å…³é”®è¯ï¼Œä»¥æé«˜æœç´¢è¦†ç›–ç‡å’Œè®ºæ–‡å‘ç°æ•°é‡ã€‚
        
        å…³é”®è¯åº”è¯¥æ¶µç›–ï¼š
        1. åŸºæœ¬æ¦‚å¿µå’Œå®šä¹‰ï¼ˆä¸­è‹±æ–‡ï¼‰
        2. æ ¸å¿ƒæŠ€æœ¯æ–¹æ³•å’Œç®—æ³•
        3. å…·ä½“åº”ç”¨åœºæ™¯å’Œæ¡ˆä¾‹
        4. æœ€æ–°ç ”ç©¶è¿›å±•å’Œç»¼è¿°
        5. ç›¸å…³æŠ€æœ¯å’Œäº¤å‰é¢†åŸŸ
        6. å…·ä½“çš„æŠ€æœ¯æœ¯è¯­å’Œä¸“ä¸šåè¯
        
        æ ¼å¼ç¤ºä¾‹ï¼š
        {topic} åŸºç¡€ç†è®º
        {topic} architecture
        {topic} reinforcement learning
        multi-agent systems
        {topic} åº”ç”¨ç ”ç©¶
        {topic} latest research 2024
        {topic} deep learning
        {topic} natural language processing
        æ™ºèƒ½ä»£ç†æŠ€æœ¯
        autonomous agents
        {topic} survey
        {topic} ç»¼è¿°
        
        è¯·ç”Ÿæˆ12ä¸ªä¸åŒè§’åº¦çš„æœç´¢å…³é”®è¯ï¼Œæ¯è¡Œä¸€ä¸ªï¼š
        """
        
        try:
            search_keywords_response = llm_processor.call_llm_api(keyword_prompt, max_tokens=500)
            # å¤„ç†è¿”å›çš„å…³é”®è¯ï¼Œç§»é™¤æ•°å­—å‰ç¼€å’Œé¢å¤–ç©ºç™½
            import re
            search_keywords_response = re.sub(r'^\d+\.\s*', '', search_keywords_response, flags=re.MULTILINE)
            search_keywords = [k.strip() for k in search_keywords_response.split('\n') if k.strip()]
            
            if len(search_keywords) < 3:  # å¦‚æœå…³é”®è¯å¤ªå°‘ï¼Œä½¿ç”¨é»˜è®¤å…³é”®è¯
                search_keywords = [
                    f"{topic} ç ”ç©¶", f"{topic} æŠ€æœ¯", f"{topic} åº”ç”¨",
                    f"{topic} latest research", f"{topic} review", f"{topic} advances",
                    f"{topic} methods", f"{topic} applications", f"{topic} survey"
                ]
            
            print(f"âœ… ç”Ÿæˆçš„å­¦æœ¯æœç´¢å…³é”®è¯: {search_keywords[:8]}")
        except Exception as e:
            print(f"âš ï¸ å…³é”®è¯ç”Ÿæˆå¤±è´¥: {e}ï¼Œä½¿ç”¨é»˜è®¤å…³é”®è¯")
            search_keywords = [
                f"{topic} ç ”ç©¶", f"{topic} æŠ€æœ¯", f"{topic} åº”ç”¨",
                f"{topic} latest research", f"{topic} review", f"{topic} advances",
                f"{topic} methods", f"{topic} applications", f"{topic} survey"
            ]
        
        # æ­¥éª¤2: æ‰§è¡Œå­¦æœ¯æœç´¢
        print("ğŸ” [æ­¥éª¤2] æ‰§è¡Œå­¦æœ¯æ–‡çŒ®æœç´¢...")
        
        all_search_results = []
        
        # ä½¿ç”¨ç”Ÿæˆçš„å…³é”®è¯è¿›è¡Œæœç´¢
        for i, keyword in enumerate(search_keywords[:10]):  # å¢åŠ åˆ°10ä¸ªå…³é”®è¯
            try:
                print(f"ğŸ” æ‰§è¡Œæœç´¢æŸ¥è¯¢ ({i+1}/10): {keyword}")
                
                # è°ƒç”¨searchå·¥å…·
                search_result = search(
                    query=keyword,
                    max_results=15,  # å¤§å¹…å¢åŠ æ¯ä¸ªå…³é”®è¯çš„æœç´¢ç»“æœåˆ°15æ¡
                    search_type="academic"  # æŒ‡å®šå­¦æœ¯æœç´¢
                )
                
                search_data = json.loads(search_result)
                if search_data.get('status') == 'success':
                    results = search_data.get('results', [])
                    all_search_results.extend(results)
                    print(f"âœ… æœç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(results)} æ¡ç»“æœ")
                else:
                    print(f"âš ï¸ æœç´¢å¤±è´¥: {search_data.get('error', 'æœªçŸ¥é”™è¯¯')}")
                    
            except Exception as e:
                print(f"âš ï¸ æœç´¢å…³é”®è¯'{keyword}'æ—¶å‡ºé”™: {e}")
        
        print(f"âœ… å­¦æœ¯æœç´¢å®Œæˆï¼Œæ€»å…±æ”¶é›†åˆ° {len(all_search_results)} æ¡ç ”ç©¶èµ„æ–™")
        
        # æ­¥éª¤3: åˆ†æå’Œç»„ç»‡ç ”ç©¶æ•°æ®
        print("ğŸ“Š [æ­¥éª¤3] åˆ†æå’Œç»„ç»‡ç ”ç©¶æ•°æ®...")
        
        # å³ä½¿æœç´¢ç»“æœæœ‰é™ï¼Œä¹Ÿè¦æ‰§è¡Œåˆ†æ­¥éª¤ç”Ÿæˆä»¥ç¡®ä¿åŒ…å«"ä¸»è¦ç ”ç©¶è®ºæ–‡åˆ†æ"ç« èŠ‚
        if not all_search_results:
            print("âš ï¸ æœç´¢ç»“æœæœ‰é™ï¼Œä½†ä»å°†æ‰§è¡Œåˆ†æ­¥éª¤ç”Ÿæˆä»¥ç¡®ä¿æŠ¥å‘Šå®Œæ•´æ€§")
        
        # åˆ†æ­¥éª¤ç”Ÿæˆå­¦æœ¯æŠ¥å‘Š - ä¸“é—¨ä¸ºå­¦æœ¯æŠ¥å‘Šè®¾è®¡çš„è°ƒåº¦æµç¨‹
        print("ğŸ“ [æ­¥éª¤3.1] ç”ŸæˆæŠ¥å‘Šå‰åŠéƒ¨åˆ†...")
        
        # ç¬¬ä¸€æ­¥ï¼šç”ŸæˆæŠ¥å‘Šå‰åŠéƒ¨åˆ†ï¼ˆæ¦‚è¿° + æŠ€æœ¯åˆ†æï¼‰
        first_part_prompt = f"""
        è¯·ç”Ÿæˆ{topic}å­¦æœ¯ç ”ç©¶æŠ¥å‘Šçš„å‰åŠéƒ¨åˆ†ï¼ŒåŒ…å«ä»¥ä¸‹ä¸¤ä¸ªç« èŠ‚ï¼š

        # {topic}å­¦æœ¯ç ”ç©¶æŠ¥å‘Š

        ## ç ”ç©¶é¢†åŸŸæ¦‚è¿°ä¸ä¸»è¦æ–¹å‘
        - è¯¦ç»†åˆ†æ{topic}é¢†åŸŸçš„å‘å±•å†ç¨‹å’Œç°çŠ¶
        - è¯†åˆ«å½“å‰ä¸»è¦ç ”ç©¶æ–¹å‘å’Œçƒ­ç‚¹é¢†åŸŸ  
        - æ€»ç»“è¯¥é¢†åŸŸé¢ä¸´çš„æ ¸å¿ƒç ”ç©¶é—®é¢˜å’ŒæŒ‘æˆ˜
        - åˆ†æå›½å†…å¤–ç ”ç©¶å·®è·å’Œå‘å±•æ°´å¹³
        (å­—æ•°è¦æ±‚ï¼š800-1000å­—)

        ## å…³é”®æŠ€æœ¯ä¸æ–¹æ³•åˆ†æ  
        - æ·±å…¥åˆ†æ{topic}é¢†åŸŸçš„ä¸»è¦æŠ€æœ¯è·¯å¾„å’Œæ ¸å¿ƒç®—æ³•
        - æ¯”è¾ƒä¸åŒæŠ€æœ¯æ–¹æ³•çš„ä¼˜ç¼ºç‚¹å’Œé€‚ç”¨åœºæ™¯
        - è¯†åˆ«æŠ€æœ¯å‘å±•çš„ä¸»è¦è¶‹åŠ¿å’Œçªç ´æ–¹å‘
        - åˆ†ææŠ€æœ¯å®ç°çš„éš¾ç‚¹å’Œè§£å†³æ–¹æ¡ˆ
        (å­—æ•°è¦æ±‚ï¼š800-1000å­—)

        è¯·åŸºäºä»¥ä¸‹ç ”ç©¶èµ„æ–™ç”Ÿæˆå†…å®¹ï¼š
        {json.dumps(all_search_results[:30], ensure_ascii=False, indent=2) if all_search_results else "æœç´¢èµ„æ–™æœ‰é™"}

        è¦æ±‚ï¼šå†…å®¹è¯¦å®ï¼Œé€»è¾‘æ¸…æ™°ï¼Œä½¿ç”¨ä¸“ä¸šæœ¯è¯­ï¼Œæ€»å­—æ•°æ§åˆ¶åœ¨1600-2000å­—ã€‚
        """
        
        try:
            first_part = llm_processor.call_llm_api(
                first_part_prompt, 
                max_tokens=4000,
                temperature=0.7
            )
            print(f"âœ… å‰åŠéƒ¨åˆ†ç”Ÿæˆå®Œæˆ: {len(first_part)}å­—ç¬¦")
        except Exception as e:
            print(f"âŒ å‰åŠéƒ¨åˆ†ç”Ÿæˆå¤±è´¥: {e}")
            first_part = f"# {topic}å­¦æœ¯ç ”ç©¶æŠ¥å‘Š\n\n## ç ”ç©¶é¢†åŸŸæ¦‚è¿°ä¸ä¸»è¦æ–¹å‘\n\n{topic}é¢†åŸŸå‘å±•è¿…é€Ÿ...\n\n## å…³é”®æŠ€æœ¯ä¸æ–¹æ³•åˆ†æ\n\n{topic}æŠ€æœ¯æ–¹æ³•å¤šæ ·..."
        
        # ç¬¬äºŒæ­¥ï¼šä¸“é—¨ç”Ÿæˆ"ä¸»è¦ç ”ç©¶è®ºæ–‡åˆ†æ"ç« èŠ‚
        print("ğŸ“ [æ­¥éª¤3.2] ä¸“é—¨ç”Ÿæˆä¸»è¦ç ”ç©¶è®ºæ–‡åˆ†æç« èŠ‚...")
        
        # ç¡®ä¿æœ‰è¶³å¤Ÿçš„æœç´¢èµ„æ–™
        if not all_search_results:
            print("âš ï¸ æ²¡æœ‰æœç´¢èµ„æ–™ï¼Œè·³è¿‡è®ºæ–‡åˆ†æç« èŠ‚")
            paper_analysis = f"## ä¸»è¦ç ”ç©¶è®ºæ–‡åˆ†æ\n\nç”±äºæœç´¢èµ„æ–™æœ‰é™ï¼Œæ— æ³•è¿›è¡Œè¯¦ç»†çš„è®ºæ–‡åˆ†æã€‚"
        else:
            # é€‰æ‹©æœ€å…·ä»£è¡¨æ€§çš„è®ºæ–‡ï¼ˆæœ€å¤š30ç¯‡ï¼‰
            selected_papers = all_search_results[:30]
            print(f"ğŸ“š é€‰æ‹©{len(selected_papers)}ç¯‡è®ºæ–‡è¿›è¡Œè¯¦ç»†åˆ†æ")
            
            paper_analysis_prompt = f"""
            **ã€æ ¸å¿ƒä»»åŠ¡ã€‘**ï¼šä¸º{topic}å­¦æœ¯ç ”ç©¶æŠ¥å‘Šç”Ÿæˆç‹¬ç«‹çš„"ä¸»è¦ç ”ç©¶è®ºæ–‡åˆ†æ"ç« èŠ‚

            **ã€å…³é”®è¦æ±‚ã€‘**ï¼š
            1. å¿…é¡»ä»æä¾›çš„{len(selected_papers)}ç¯‡è®ºæ–‡ä¸­é€‰æ‹©25-30ç¯‡æœ€å…·ä»£è¡¨æ€§çš„è®ºæ–‡
            2. æ¯ç¯‡è®ºæ–‡åˆ†æä¸å°‘äº200å­—ï¼Œå¿…é¡»åŒ…å«å®Œæ•´çš„ä½œè€…ä¿¡æ¯ã€å‘è¡¨æ¥æºã€è¯¦ç»†å†…å®¹åˆ†æ
            3. æŒ‰è®ºæ–‡ç±»å‹åˆ†ç±»ç»„ç»‡ï¼šåŸºç¡€ç†è®ºç±»ã€æŠ€æœ¯åº”ç”¨ç±»ã€ç»¼è¿°å‰æ²¿ç±»
            4. å¿…é¡»åŸºäºå®é™…æä¾›çš„è®ºæ–‡èµ„æ–™ï¼Œä¸èƒ½ç¼–é€ ä»»ä½•ä¿¡æ¯
            5. æ€»å­—æ•°å¿…é¡»è¾¾åˆ°4000-5000å­—
            6. è¿™æ˜¯ç‹¬ç«‹çš„ç« èŠ‚ï¼Œä¸è¦åŒ…å«å…¶ä»–å†…å®¹

            **ã€è®ºæ–‡èµ„æ–™ã€‘**ï¼š
            {json.dumps(selected_papers, ensure_ascii=False, indent=2)}

            **ã€å¿…é¡»ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹æ ¼å¼ç”Ÿæˆï¼Œå‚è€ƒç¤ºä¾‹ç»“æ„ã€‘**ï¼š

            ## ä¸»è¦ç ”ç©¶è®ºæ–‡åˆ†æ

            ### åŸºç¡€ç†è®ºä¸æ–¹æ³•åˆ›æ–°ç±»è®ºæ–‡ï¼ˆ8-10ç¯‡è®ºæ–‡çš„è¯¦ç»†åˆ†æï¼‰

            **è®ºæ–‡1ï¼š[ä»èµ„æ–™ä¸­é€‰æ‹©çš„è®ºæ–‡æ ‡é¢˜]**
            - **ä¸»è¦ä½œè€…**ï¼š[ä»èµ„æ–™ä¸­æå–çš„çœŸå®ä½œè€…å§“åï¼Œå¦‚æœèµ„æ–™ä¸­æ²¡æœ‰åˆ™å†™"ä½œè€…ä¿¡æ¯æœªæä¾›"]
            - **å‘è¡¨æ¥æº**ï¼š[ä»èµ„æ–™ä¸­æå–çš„çœŸå®æœŸåˆŠ/ä¼šè®®åç§°ï¼Œå¦‚æœåªæœ‰arXivåˆ™å†™"arXivé¢„å°æœ¬"]
            - **å‘å¸ƒæ—¥æœŸ**ï¼š[ä»èµ„æ–™ä¸­æå–çš„å‘è¡¨æ—¥æœŸ]
            - **æ ¸å¿ƒåˆ›æ–°ç‚¹**ï¼š[åŸºäºèµ„æ–™å†…å®¹è¯¦ç»†æè¿°è¯¥è®ºæ–‡çš„ä¸»è¦ç†è®ºè´¡çŒ®å’Œåˆ›æ–°ä¹‹å¤„ï¼Œä¸å°‘äº50å­—]
            - **æŠ€æœ¯æ–¹æ³•**ï¼š[åŸºäºèµ„æ–™å†…å®¹æè¿°å…·ä½“çš„ç ”ç©¶æ–¹æ³•ã€ç®—æ³•è®¾è®¡ã€ç†è®ºæ¡†æ¶ï¼Œä¸å°‘äº50å­—]
            - **å®éªŒéªŒè¯**ï¼š[åŸºäºèµ„æ–™å†…å®¹æè¿°ä¸»è¦å®éªŒè®¾ç½®ã€æ•°æ®é›†ã€è¯„ä¼°æŒ‡æ ‡å’Œç»“æœï¼Œä¸å°‘äº50å­—]
            - **å­¦æœ¯ä»·å€¼**ï¼š[åŸºäºèµ„æ–™å†…å®¹åˆ†æå¯¹{topic}é¢†åŸŸç†è®ºå‘å±•çš„å…·ä½“æ¨åŠ¨ä½œç”¨ï¼Œä¸å°‘äº50å­—]

            **è®ºæ–‡2ï¼š[ç¬¬äºŒç¯‡è®ºæ–‡æ ‡é¢˜]**
            [æŒ‰åŒæ ·æ ¼å¼è¯¦ç»†åˆ†æï¼Œæ¯ç¯‡è®ºæ–‡åˆ†æä¸å°‘äº200å­—]

            [ç»§ç»­åˆ†æå…¶ä½™6-8ç¯‡åŸºç¡€ç†è®ºè®ºæ–‡...]

            ### æŠ€æœ¯åº”ç”¨ä¸å®è·µç±»è®ºæ–‡ï¼ˆ10-12ç¯‡è®ºæ–‡çš„è¯¦ç»†åˆ†æï¼‰

            **è®ºæ–‡1ï¼š[åº”ç”¨ç±»è®ºæ–‡æ ‡é¢˜]**
            - **ä¸»è¦ä½œè€…**ï¼š[ä»èµ„æ–™ä¸­æå–çš„çœŸå®ä½œè€…å§“å]
            - **å‘è¡¨æ¥æº**ï¼š[ä»èµ„æ–™ä¸­æå–çš„çœŸå®æœŸåˆŠ/ä¼šè®®åç§°]
            - **å‘å¸ƒæ—¥æœŸ**ï¼š[ä»èµ„æ–™ä¸­æå–çš„å‘è¡¨æ—¥æœŸ]
            - **è§£å†³é—®é¢˜**ï¼š[åŸºäºèµ„æ–™å†…å®¹æè¿°è®ºæ–‡è¦è§£å†³çš„å…·ä½“æŠ€æœ¯é—®é¢˜å’Œåº”ç”¨åœºæ™¯ï¼Œä¸å°‘äº50å­—]
            - **æŠ€æœ¯æ–¹æ¡ˆ**ï¼š[åŸºäºèµ„æ–™å†…å®¹æè¿°è¯¦ç»†çš„ç³»ç»Ÿæ¶æ„ã€ç®—æ³•å®ç°ã€æŠ€æœ¯è·¯çº¿ï¼Œä¸å°‘äº50å­—]
            - **å®éªŒè¯„ä¼°**ï¼š[åŸºäºèµ„æ–™å†…å®¹æè¿°æ€§èƒ½æµ‹è¯•ã€å¯¹æ¯”å®éªŒã€è¯„ä¼°ç»“æœå’Œæ•°æ®ï¼Œä¸å°‘äº50å­—]
            - **åˆ›æ–°ä¼˜åŠ¿**ï¼š[åŸºäºèµ„æ–™å†…å®¹åˆ†æä¸ç°æœ‰æ–¹æ³•çš„å…·ä½“å¯¹æ¯”å’Œæ”¹è¿›ä¹‹å¤„ï¼Œä¸å°‘äº50å­—]
            - **åº”ç”¨æ½œåŠ›**ï¼š[åŸºäºèµ„æ–™å†…å®¹åˆ†æå®é™…éƒ¨ç½²å¯èƒ½æ€§å’Œäº§ä¸šåŒ–å‰æ™¯ï¼Œä¸å°‘äº50å­—]

            [ç»§ç»­è¯¦ç»†åˆ†æå…¶ä½™9-11ç¯‡åº”ç”¨å®è·µè®ºæ–‡...]

            ### ç»¼è¿°ä¸å‰æ²¿æ¢ç´¢ç±»è®ºæ–‡ï¼ˆ7-8ç¯‡è®ºæ–‡çš„è¯¦ç»†åˆ†æï¼‰

            **è®ºæ–‡1ï¼š[ç»¼è¿°ç±»è®ºæ–‡æ ‡é¢˜]**
            - **ä¸»è¦ä½œè€…**ï¼š[ä»èµ„æ–™ä¸­æå–çš„çœŸå®ä½œè€…å§“å]
            - **å‘è¡¨æ¥æº**ï¼š[ä»èµ„æ–™ä¸­æå–çš„çœŸå®æœŸåˆŠ/ä¼šè®®åç§°]
            - **å‘å¸ƒæ—¥æœŸ**ï¼š[ä»èµ„æ–™ä¸­æå–çš„å‘è¡¨æ—¥æœŸ]
            - **ç»¼è¿°èŒƒå›´**ï¼š[åŸºäºèµ„æ–™å†…å®¹æè¿°è®ºæ–‡è¦†ç›–çš„ç ”ç©¶é¢†åŸŸå’Œæ—¶é—´èŒƒå›´ï¼Œä¸å°‘äº50å­—]
            - **åˆ†ç±»ä½“ç³»**ï¼š[åŸºäºèµ„æ–™å†…å®¹æè¿°è®ºæ–‡æå‡ºçš„æŠ€æœ¯åˆ†ç±»æˆ–ç†è®ºæ¡†æ¶ï¼Œä¸å°‘äº50å­—]
            - **å‘å±•è„‰ç»œ**ï¼š[åŸºäºèµ„æ–™å†…å®¹æè¿°æ¢³ç†çš„é¢†åŸŸå‘å±•å†ç¨‹å’Œå…³é”®èŠ‚ç‚¹ï¼Œä¸å°‘äº50å­—]
            - **ç ”ç©¶çƒ­ç‚¹**ï¼š[åŸºäºèµ„æ–™å†…å®¹æè¿°è¯†åˆ«çš„å½“å‰ç ”ç©¶çƒ­ç‚¹å’Œè¶‹åŠ¿ï¼Œä¸å°‘äº50å­—]
            - **æœªæ¥æ–¹å‘**ï¼š[åŸºäºèµ„æ–™å†…å®¹æè¿°æå‡ºçš„æœªæ¥ç ”ç©¶æ–¹å‘å’ŒæŒ‘æˆ˜ï¼Œä¸å°‘äº50å­—]

            [ç»§ç»­è¯¦ç»†åˆ†æå…¶ä½™6-7ç¯‡ç»¼è¿°å‰æ²¿è®ºæ–‡...]

            ### ç ”ç©¶è„‰ç»œå’Œå‘å±•è¶‹åŠ¿åˆ†æ
            - **å­¦æœ¯ä¼ æ‰¿å…³ç³»**ï¼šåŸºäºä¸Šè¿°è®ºæ–‡åˆ†æï¼Œæ¢³ç†{topic}é¢†åŸŸçš„å­¦æœ¯å‘å±•è„‰ç»œ
            - **å…³é”®ç ”ç©¶æœºæ„**ï¼šè¯†åˆ«åœ¨è¯¥é¢†åŸŸæœ‰é‡è¦è´¡çŒ®çš„å¤§å­¦ã€ç ”ç©¶æ‰€å’Œå›¢é˜Ÿ
            - **æŠ€æœ¯æ¼”è¿›è·¯å¾„**ï¼šæ€»ç»“ä»æ—©æœŸç ”ç©¶åˆ°æœ€æ–°è¿›å±•çš„æŠ€æœ¯å‘å±•è½¨è¿¹
            - **ç ”ç©¶çƒ­ç‚¹åˆ†å¸ƒ**ï¼šåˆ†æå½“å‰ç ”ç©¶çš„çƒ­ç‚¹é¢†åŸŸå’Œæ–°å…´æ–¹å‘
            - **æœªæ¥ç ”ç©¶ç©ºç™½**ï¼šå‘ç°å°šæœªå……åˆ†æ¢ç´¢çš„ç ”ç©¶æ–¹å‘å’ŒæŠ€æœ¯æŒ‘æˆ˜

            **ã€é‡è¦æé†’ã€‘ï¼š
            1. å¿…é¡»é€ç¯‡è¯¦ç»†åˆ†æè®ºæ–‡ï¼Œæ¯ç¯‡è®ºæ–‡åˆ†æä¸å°‘äº200å­—
            2. å¿…é¡»åŸºäºæä¾›çš„å®é™…è®ºæ–‡èµ„æ–™ï¼Œä¸èƒ½ç¼–é€ ä»»ä½•ä¿¡æ¯
            3. å¦‚æœèµ„æ–™ä¸­ç¼ºå°‘ä½œè€…ä¿¡æ¯ï¼Œæ˜ç¡®æ ‡æ³¨"ä½œè€…ä¿¡æ¯æœªæä¾›"
            4. å¦‚æœèµ„æ–™ä¸­ç¼ºå°‘æœŸåˆŠä¿¡æ¯ï¼Œæ˜ç¡®æ ‡æ³¨"arXivé¢„å°æœ¬"
            5. æ€»å­—æ•°å¿…é¡»è¾¾åˆ°4000-5000å­—
            6. è¿™æ˜¯æ•´ä¸ªæŠ¥å‘Šçš„æ ¸å¿ƒç« èŠ‚ï¼Œè¯·åŠ¡å¿…è¯¦ç»†å±•å¼€
            7. åªç”Ÿæˆè¿™ä¸€ä¸ªç« èŠ‚ï¼Œä¸è¦åŒ…å«å…¶ä»–å†…å®¹
            """
            
            try:
                paper_analysis = llm_processor.call_llm_api(
                    paper_analysis_prompt, 
                    max_tokens=15000,  # å¤§å¹…å¢åŠ tokensä»¥æ”¯æŒè¯¦ç»†çš„è®ºæ–‡åˆ†æ
                    temperature=0.7
                )
                print(f"âœ… è®ºæ–‡åˆ†æç« èŠ‚ç”Ÿæˆå®Œæˆ: {len(paper_analysis)}å­—ç¬¦")
                
                # éªŒè¯ç”Ÿæˆçš„ç« èŠ‚æ˜¯å¦åŒ…å«"ä¸»è¦ç ”ç©¶è®ºæ–‡åˆ†æ"
                if "ä¸»è¦ç ”ç©¶è®ºæ–‡åˆ†æ" not in paper_analysis:
                    print("âš ï¸ ç”Ÿæˆçš„ç« èŠ‚ç¼ºå°‘'ä¸»è¦ç ”ç©¶è®ºæ–‡åˆ†æ'æ ‡é¢˜ï¼Œä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆ")
                    paper_analysis = f"## ä¸»è¦ç ”ç©¶è®ºæ–‡åˆ†æ\n\n{paper_analysis}"
                    
            except Exception as e:
                print(f"âŒ è®ºæ–‡åˆ†æç« èŠ‚ç”Ÿæˆå¤±è´¥: {e}")
                paper_analysis = f"## ä¸»è¦ç ”ç©¶è®ºæ–‡åˆ†æ\n\nåŸºäºæ”¶é›†çš„ç ”ç©¶èµ„æ–™ï¼Œä»¥ä¸‹æ˜¯{topic}é¢†åŸŸçš„ä¸»è¦è®ºæ–‡åˆ†æ...\n\n### åŸºç¡€ç†è®ºä¸æ–¹æ³•åˆ›æ–°ç±»è®ºæ–‡\n\n### æŠ€æœ¯åº”ç”¨ä¸å®è·µç±»è®ºæ–‡\n\n### ç»¼è¿°ä¸å‰æ²¿æ¢ç´¢ç±»è®ºæ–‡"
        
        # ç¬¬ä¸‰æ­¥ï¼šç”ŸæˆæŠ¥å‘ŠååŠéƒ¨åˆ†ï¼ˆè¶‹åŠ¿å±•æœ› + ç»“è®ºï¼‰
        print("ğŸ“ [æ­¥éª¤3.3] ç”ŸæˆæŠ¥å‘ŠååŠéƒ¨åˆ†...")
        
        second_part_prompt = f"""
        è¯·ç”Ÿæˆ{topic}å­¦æœ¯ç ”ç©¶æŠ¥å‘Šçš„ååŠéƒ¨åˆ†ï¼ŒåŒ…å«ä»¥ä¸‹ä¸¤ä¸ªç« èŠ‚ï¼š

        ## å‘å±•è¶‹åŠ¿ä¸æœªæ¥å±•æœ›
        - é¢„æµ‹{topic}é¢†åŸŸæœªæ¥3-5å¹´çš„å‘å±•æ–¹å‘
        - åˆ†æè¯¥é¢†åŸŸé¢ä¸´çš„ä¸»è¦æŒ‘æˆ˜å’Œå‘å±•æœºé‡
        - æå‡ºå‰æ²¿ç ”ç©¶é—®é¢˜å’Œæ½œåœ¨çªç ´ç‚¹
        - æ¢è®¨è·¨å­¦ç§‘èåˆçš„å¯èƒ½æ€§å’Œå‘å±•å‰æ™¯
        (å­—æ•°è¦æ±‚ï¼š800-1000å­—)

        ## ç»“è®ºä¸å»ºè®®
        - æ€»ç»“{topic}é¢†åŸŸçš„ä¸»è¦ç ”ç©¶å‘ç°å’Œå‘å±•è§„å¾‹
        - å¯¹ç ”ç©¶è€…æå‡ºå…·ä½“çš„ç ”ç©¶æ–¹å‘å»ºè®®
        - å¯¹äº§ä¸šå‘å±•æå‡ºæˆ˜ç•¥æ€§æŒ‡å¯¼æ„è§
        - å±•æœ›è¯¥é¢†åŸŸçš„é•¿è¿œå‘å±•å‰æ™¯
        (å­—æ•°è¦æ±‚ï¼š600-800å­—)

        è¯·åŸºäºä»¥ä¸‹ç ”ç©¶èµ„æ–™ç”Ÿæˆå†…å®¹ï¼š
        {json.dumps(all_search_results[:20], ensure_ascii=False, indent=2) if all_search_results else "æœç´¢èµ„æ–™æœ‰é™"}

        è¦æ±‚ï¼šå†…å®¹è¯¦å®ï¼Œé€»è¾‘æ¸…æ™°ï¼Œä½¿ç”¨ä¸“ä¸šæœ¯è¯­ï¼Œæ€»å­—æ•°æ§åˆ¶åœ¨1400-1800å­—ã€‚
        """
        
        try:
            second_part = llm_processor.call_llm_api(
                second_part_prompt, 
                max_tokens=3000,
                temperature=0.7
            )
            print(f"âœ… ååŠéƒ¨åˆ†ç”Ÿæˆå®Œæˆ: {len(second_part)}å­—ç¬¦")
        except Exception as e:
            print(f"âŒ ååŠéƒ¨åˆ†ç”Ÿæˆå¤±è´¥: {e}")
            second_part = f"## å‘å±•è¶‹åŠ¿ä¸æœªæ¥å±•æœ›\n\n{topic}é¢†åŸŸæœªæ¥å‘å±•...\n\n## ç»“è®ºä¸å»ºè®®\n\nåŸºäºä»¥ä¸Šåˆ†æ..."
        
        # ç¬¬å››æ­¥ï¼šç»„è£…å®Œæ•´æŠ¥å‘Š
        print("ğŸ“ [æ­¥éª¤3.4] ç»„è£…å®Œæ•´å­¦æœ¯æŠ¥å‘Š...")
        
        # éªŒè¯å„ä¸ªéƒ¨åˆ†æ˜¯å¦ç”ŸæˆæˆåŠŸ
        print(f"ğŸ” [è°ƒè¯•] å‰åŠéƒ¨åˆ†é•¿åº¦: {len(first_part)}å­—ç¬¦")
        print(f"ğŸ” [è°ƒè¯•] è®ºæ–‡åˆ†æéƒ¨åˆ†é•¿åº¦: {len(paper_analysis)}å­—ç¬¦")
        print(f"ğŸ” [è°ƒè¯•] ååŠéƒ¨åˆ†é•¿åº¦: {len(second_part)}å­—ç¬¦")
        
        # ç»„è£…å®Œæ•´æŠ¥å‘Š
        academic_report = f"{first_part}\n\n{paper_analysis}\n\n{second_part}"
        
        # éªŒè¯æœ€ç»ˆæŠ¥å‘Šæ˜¯å¦åŒ…å«"ä¸»è¦ç ”ç©¶è®ºæ–‡åˆ†æ"
        if "ä¸»è¦ç ”ç©¶è®ºæ–‡åˆ†æ" not in academic_report:
            print("âš ï¸ æœ€ç»ˆæŠ¥å‘Šç¼ºå°‘'ä¸»è¦ç ”ç©¶è®ºæ–‡åˆ†æ'ç« èŠ‚ï¼Œå¼ºåˆ¶æ·»åŠ ...")
            # å¦‚æœç¼ºå°‘è®ºæ–‡åˆ†æç« èŠ‚ï¼Œå¼ºåˆ¶æ·»åŠ ä¸€ä¸ª
            if not all_search_results:
                paper_analysis_fallback = f"## ä¸»è¦ç ”ç©¶è®ºæ–‡åˆ†æ\n\nç”±äºæœç´¢èµ„æ–™æœ‰é™ï¼Œæ— æ³•è¿›è¡Œè¯¦ç»†çš„è®ºæ–‡åˆ†æã€‚"
            else:
                # åŸºäºæœç´¢ç»“æœç”Ÿæˆç®€å•çš„è®ºæ–‡åˆ†æ
                paper_analysis_fallback = f"## ä¸»è¦ç ”ç©¶è®ºæ–‡åˆ†æ\n\nåŸºäºæ”¶é›†çš„{len(all_search_results)}ç¯‡ç ”ç©¶èµ„æ–™ï¼Œä»¥ä¸‹æ˜¯{topic}é¢†åŸŸçš„ä¸»è¦è®ºæ–‡åˆ†æï¼š\n\n"
                for i, paper in enumerate(all_search_results[:15]):  # åˆ†æå‰15ç¯‡è®ºæ–‡
                    title = paper.get('title', 'æ— æ ‡é¢˜')
                    content = paper.get('content', paper.get('summary', ''))[:200]
                    paper_analysis_fallback += f"### è®ºæ–‡{i+1}: {title}\n\n{content}...\n\n"
            
            academic_report = f"{first_part}\n\n{paper_analysis_fallback}\n\n{second_part}"
            print("âœ… å·²å¼ºåˆ¶æ·»åŠ 'ä¸»è¦ç ”ç©¶è®ºæ–‡åˆ†æ'ç« èŠ‚")
        # æ·»åŠ å‚è€ƒèµ„æ–™éƒ¨åˆ†
        references_section = "\n\n## å‚è€ƒèµ„æ–™\n\n"
        for i, source in enumerate(all_search_results[:30]):  # é™åˆ¶å‚è€ƒèµ„æ–™æ•°é‡
            title = source.get('title', 'æ— æ ‡é¢˜')
            url = source.get('url', '#')
            source_name = source.get('source', 'æœªçŸ¥æ¥æº')
            references_section += f"{i+1}. [{title}]({url}) - {source_name}\n"
        
        final_content = academic_report + references_section
        
        result = {
            "status": "success",
            "report_type": "academic", 
            "topic": topic,
            "content": final_content,
            "sections_count": 5,
            "sources_count": len(all_search_results),
            "generated_at": datetime.now().isoformat()
        }
        
        print(f"âœ… å­¦æœ¯ç ”ç©¶æŠ¥å‘Šç”Ÿæˆå®Œæˆ: {len(final_content)}å­—ç¬¦ï¼Œ{len(all_search_results)}ä¸ªå‚è€ƒèµ„æ–™")
        
        return json.dumps(result, ensure_ascii=False, indent=2)
        
    except Exception as e:
        print(f"âŒ å­¦æœ¯ç ”ç©¶æŠ¥å‘Šç”Ÿæˆå¼‚å¸¸: {e}")
        
        error_result = {
            "status": "error",
            "report_type": "academic",
            "topic": topic,
            "error": str(e),
            "timestamp": datetime.now().isoformat()
        }
        
        return json.dumps(error_result, ensure_ascii=False, indent=2)

# generate_insight_report å·²åˆ é™¤ - ä½¿ç”¨ orchestrator_mcp(task_type="insights") æ›¿ä»£

def _execute_simple_orchestration(task: str, **kwargs) -> str:
    """æ‰§è¡Œç®€åŒ–ç¼–æ’æµç¨‹"""
    try:
        # æå–ä¸»é¢˜
        topic = kwargs.get('topic') or _extract_topic_from_task(task)
        print(f"ğŸ“‹ è¯†åˆ«ä¸»é¢˜: {topic}")
        
        # æ‰§è¡Œæœç´¢
        search_result = search(topic, max_results=5)
        search_data = json.loads(search_result)
        
        # ç”Ÿæˆæ‘˜è¦
        if search_data.get('status') == 'success' and search_data.get('results'):
            summary_result = summary_writer_mcp(
                content_data=search_data['results'],
                length_constraint="300-500å­—",
                format="paragraph"
            )
            summary_data = json.loads(summary_result)
            final_summary = summary_data.get('summary', 'æš‚æ— æ‘˜è¦')
        else:
            final_summary = "æœç´¢æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯ï¼Œæ— æ³•ç”Ÿæˆæ‘˜è¦ã€‚"
        
        # ç»„è£…ç»“æœ
        result = {
            "status": "success",
            "task": task,
            "topic": topic,
            "summary": final_summary,
            "search_results_count": len(search_data.get('results', [])),
            "mode": "simple",
            "generation_timestamp": datetime.now().isoformat()
        }
        
        return json.dumps(result, ensure_ascii=False, indent=2)
        
    except Exception as e:
        error_result = {
            "status": "error",
            "task": task,
            "error": str(e),
            "mode": "simple",
            "timestamp": datetime.now().isoformat()
        }
        return json.dumps(error_result, ensure_ascii=False, indent=2)

def _assemble_content_for_evaluation(executive_summary: str, section_contents: Dict[str, str], topic: str) -> str:
    """ç»„è£…å†…å®¹ç”¨äºè´¨é‡è¯„ä¼°"""
    try:
        # æ„å»ºå®Œæ•´çš„æŠ¥å‘Šå†…å®¹æ–‡æœ¬
        content_parts = []
        
        # æ·»åŠ ä¸»é¢˜å’Œæ‘˜è¦
        content_parts.append(f"ä¸»é¢˜: {topic}")
        
        if executive_summary:
            content_parts.append(f"æ‰§è¡Œæ‘˜è¦: {executive_summary}")
        
        # æ·»åŠ å„ç« èŠ‚å†…å®¹
        for section_title, content in section_contents.items():
            if content:
                content_parts.append(f"ç« èŠ‚: {section_title}")
                content_parts.append(content)
        
        return "\n\n".join(content_parts)
    except Exception as e:
        print(f"âš ï¸ ç»„è£…è¯„ä¼°å†…å®¹å¤±è´¥: {e}")
        return f"ä¸»é¢˜: {topic}\nå†…å®¹ç»„è£…å¤±è´¥"

def _generate_targeted_query_for_weakness(weak_area: str, topic: str, suggestions: List[str]) -> str:
    """æ ¹æ®è–„å¼±ç¯èŠ‚ç”Ÿæˆé’ˆå¯¹æ€§æŸ¥è¯¢"""
    try:
        # è–„å¼±ç¯èŠ‚å¯¹åº”çš„æŸ¥è¯¢ç­–ç•¥
        weakness_queries = {
            "completeness": [
                f"{topic} è¯¦ç»†åˆ†æ",
                f"{topic} å…¨é¢ä»‹ç»", 
                f"{topic} å®Œæ•´æŒ‡å—"
            ],
            "accuracy": [
                f"{topic} æƒå¨æŠ¥å‘Š",
                f"{topic} å®˜æ–¹æ•°æ®",
                f"{topic} ç ”ç©¶æŠ¥å‘Š"
            ],
            "depth": [
                f"{topic} æ·±åº¦åˆ†æ",
                f"{topic} æŠ€æœ¯åŸç†",
                f"{topic} æœºåˆ¶ç ”ç©¶"
            ],
            "relevance": [
                f"{topic} æ ¸å¿ƒè¦ç‚¹",
                f"{topic} å…³é”®ç‰¹å¾",
                f"{topic} ä¸»è¦å†…å®¹"
            ],
            "clarity": [
                f"{topic} æ¸…æ™°è§£é‡Š",
                f"{topic} é€šä¿—æ˜“æ‡‚",
                f"{topic} ç»“æ„åŒ–ä»‹ç»"
            ]
        }
        
        # æ ¹æ®è–„å¼±ç¯èŠ‚é€‰æ‹©æŸ¥è¯¢
        if weak_area in weakness_queries:
            queries = weakness_queries[weak_area]
            # é€‰æ‹©ç¬¬ä¸€ä¸ªæŸ¥è¯¢ä½œä¸ºåŸºç¡€ï¼Œå¯ä»¥æ ¹æ®å»ºè®®è¿›è¡Œè°ƒæ•´
            base_query = queries[0]
            
            # å¦‚æœæœ‰å…·ä½“å»ºè®®ï¼Œå°è¯•èå…¥æŸ¥è¯¢ä¸­
            if suggestions:
                first_suggestion = suggestions[0]
                if "æ•°æ®" in first_suggestion:
                    return f"{topic} æœ€æ–°æ•°æ® ç»Ÿè®¡æŠ¥å‘Š"
                elif "æ¡ˆä¾‹" in first_suggestion:
                    return f"{topic} å®é™…æ¡ˆä¾‹ åº”ç”¨å®ä¾‹"
                elif "æŠ€æœ¯" in first_suggestion:
                    return f"{topic} æŠ€æœ¯è¯¦è§£ æ·±åº¦åˆ†æ"
                elif "å¸‚åœº" in first_suggestion:
                    return f"{topic} å¸‚åœºç ”ç©¶ è¡Œä¸šæŠ¥å‘Š"
                elif "åº”ç”¨" in first_suggestion:
                    return f"{topic} åº”ç”¨åœºæ™¯ å®è·µæ¡ˆä¾‹"
                else:
                    # æ ¹æ®è–„å¼±ç¯èŠ‚å’Œä¸»é¢˜ç”Ÿæˆæ›´å…·ä½“çš„æŸ¥è¯¢
                    return f"{topic} {weak_area} ä¸“ä¸šåˆ†æ"
            
            return base_query
        else:
            # é»˜è®¤æŸ¥è¯¢
            return f"{topic} è¯¦ç»†èµ„æ–™"
            
    except Exception as e:
        print(f"âš ï¸ ç”Ÿæˆé’ˆå¯¹æ€§æŸ¥è¯¢å¤±è´¥: {e}")
        return f"{topic} è¡¥å……èµ„æ–™"

def _identify_sections_to_improve(weak_areas: List[str], section_titles: List[str]) -> List[str]:
    """è¯†åˆ«éœ€è¦æ”¹è¿›çš„ç« èŠ‚"""
    try:
        # å¦‚æœè–„å¼±ç¯èŠ‚è¾ƒå¤šï¼Œæ”¹è¿›æ‰€æœ‰ç« èŠ‚
        if len(weak_areas) >= 3:
            return list(section_titles)
        
        # æ ¹æ®è–„å¼±ç¯èŠ‚ç±»å‹é€‰æ‹©ç›¸å…³ç« èŠ‚
        sections_to_improve = set()
        
        for weak_area in weak_areas:
            if weak_area == "completeness":
                # å®Œæ•´æ€§é—®é¢˜ï¼šæ”¹è¿›æ‰€æœ‰ç« èŠ‚
                sections_to_improve.update(section_titles)
            elif weak_area == "accuracy":
                # å‡†ç¡®æ€§é—®é¢˜ï¼šé‡ç‚¹æ”¹è¿›æ•°æ®å’Œåˆ†æç›¸å…³ç« èŠ‚
                accuracy_related = [title for title in section_titles 
                                  if any(keyword in title.lower() for keyword in 
                                        ["åˆ†æ", "æ•°æ®", "ç ”ç©¶", "ç°çŠ¶", "è¶‹åŠ¿"])]
                sections_to_improve.update(accuracy_related)
            elif weak_area == "depth":
                # æ·±åº¦é—®é¢˜ï¼šæ”¹è¿›åˆ†æå’ŒæŠ€æœ¯ç« èŠ‚
                depth_related = [title for title in section_titles 
                               if any(keyword in title.lower() for keyword in 
                                     ["æŠ€æœ¯", "åˆ†æ", "æœºåˆ¶", "åŸç†", "å‘å±•"])]
                sections_to_improve.update(depth_related)
            elif weak_area == "relevance":
                # ç›¸å…³æ€§é—®é¢˜ï¼šæ”¹è¿›æ ¸å¿ƒä¸»é¢˜ç« èŠ‚
                relevance_related = [title for title in section_titles 
                                   if any(keyword in title.lower() for keyword in 
                                         ["æ¦‚è¿°", "æ ¸å¿ƒ", "ä¸»è¦", "å…³é”®"])]
                sections_to_improve.update(relevance_related)
            elif weak_area == "clarity":
                # æ¸…æ™°åº¦é—®é¢˜ï¼šæ”¹è¿›æ‰€æœ‰ç« èŠ‚çš„è¡¨è¾¾
                sections_to_improve.update(section_titles)
        
        # å¦‚æœæ²¡æœ‰åŒ¹é…åˆ°ç‰¹å®šç« èŠ‚ï¼Œæ”¹è¿›å‰2ä¸ªç« èŠ‚
        if not sections_to_improve:
            sections_to_improve = set(list(section_titles)[:2])
        
        return list(sections_to_improve)
        
    except Exception as e:
        print(f"âš ï¸ è¯†åˆ«æ”¹è¿›ç« èŠ‚å¤±è´¥: {e}")
        # å‡ºé”™æ—¶æ”¹è¿›æ‰€æœ‰ç« èŠ‚
        return list(section_titles)

def _quality_evaluation_iteration(topic: str, initial_search_results: List[Dict], 
                                max_iterations: int = 3, min_quality_score: float = 7.0) -> List[Dict]:
    """è´¨é‡è¯„ä¼°è¿­ä»£å¾ªç¯ï¼šè¯„ä¼°æ•°æ®è´¨é‡ï¼Œè¡¥å……æœç´¢ï¼Œå†è¯„ä¼°"""
    try:
        print(f"ğŸ” [è´¨é‡è¯„ä¼°] å¼€å§‹è´¨é‡è¯„ä¼°è¿­ä»£ï¼Œæœ€å¤§è¿­ä»£æ¬¡æ•°: {max_iterations}")
        
        current_search_results = initial_search_results.copy()
        iteration = 0
        
        while iteration < max_iterations:
            iteration += 1
            print(f"\nğŸ” [è´¨é‡è¯„ä¼°] ç¬¬{iteration}è½®è¯„ä¼°...")
            print(f"ğŸ“Š [è°ƒè¯•] å½“å‰æ€»æœç´¢ç»“æœ: {len(current_search_results)}æ¡")
            
            # ç»„è£…å½“å‰æœç´¢ç»“æœç”¨äºè¯„ä¼° - æ¯æ¬¡è¿­ä»£è¯„ä¼°ä¸åŒçš„å†…å®¹åˆ‡ç‰‡
            search_content = ""
            
            # ä¿®å¤ï¼šæ¯è½®éƒ½è¯„ä¼°æ‰€æœ‰æœç´¢ç»“æœï¼Œç¡®ä¿è¯„ä¼°çš„æ˜¯æ•´ä½“è´¨é‡
            eval_results = current_search_results
            eval_desc = f"å…¨éƒ¨æœç´¢ç»“æœï¼ˆç¬¬{iteration}è½®è¿­ä»£åï¼‰"
            
            print(f"ğŸ“Š [è°ƒè¯•] ç¬¬{iteration}è½®è¯„ä¼°èŒƒå›´: {eval_desc} ({len(eval_results)}æ¡)")
            
            # ä¸ºé¿å…å†…å®¹è¿‡é•¿ï¼Œåˆç†é‡‡æ ·è¯„ä¼°å†…å®¹
            sample_size = min(30, len(eval_results))  # æœ€å¤šè¯„ä¼°30æ¡
            if len(eval_results) > sample_size:
                # å‡åŒ€é‡‡æ ·
                step = len(eval_results) // sample_size
                sampled_results = eval_results[::step][:sample_size]
            else:
                sampled_results = eval_results
            
            for i, item in enumerate(sampled_results):
                content = item.get('content', '')[:300]  # æ¯æ¡300å­—ç¬¦
                title = item.get('title', '')
                search_content += f"èµ„æ–™{i+1}: {title}\nå†…å®¹: {content}\n\n"
            
            # æ„å»ºå®Œæ•´çš„è¯„ä¼°å†…å®¹
            full_content = f"""ä¸»é¢˜: {topic}
            
å½“å‰æ”¶é›†çš„èµ„æ–™æ€»æ•°: {len(current_search_results)}æ¡
ç¬¬{iteration}è½®è¯„ä¼°: é‡‡æ ·è¯„ä¼°{len(sampled_results)}æ¡ä»£è¡¨æ€§èµ„æ–™

{search_content}

è¯·å¯¹ä»¥ä¸Šèµ„æ–™çš„è´¨é‡è¿›è¡Œ5ä¸ªç»´åº¦çš„è¯„ä¼°ï¼šå®Œæ•´æ€§ã€å‡†ç¡®æ€§ã€æ·±åº¦ã€ç›¸å…³æ€§ã€æ¸…æ™°åº¦ã€‚"""
            
            # è°ƒç”¨analysis_mcpè¿›è¡Œè´¨é‡è¯„ä¼°
            evaluation_result = analysis_mcp(
                analysis_type="evaluation",
                data=full_content,
                topic=topic,
                context=f"ç¬¬{iteration}è½®è´¨é‡è¯„ä¼°",
                quality_standards={
                    "completeness": {"weight": 0.3, "min_score": 7.0},
                    "accuracy": {"weight": 0.25, "min_score": 8.0},
                    "depth": {"weight": 0.2, "min_score": 6.0},
                    "relevance": {"weight": 0.15, "min_score": 7.0},
                    "clarity": {"weight": 0.1, "min_score": 6.0}
                }
            )
            
            evaluation_data = json.loads(evaluation_result)
            
            if evaluation_data.get('status') != 'success':
                print(f"âŒ ç¬¬{iteration}è½®è¯„ä¼°å¤±è´¥ï¼Œè·³å‡ºè¿­ä»£")
                break
            
            evaluation = evaluation_data.get('evaluation', {})
            total_score = evaluation.get('total_score', 0)
            weak_areas = evaluation.get('weak_areas', [])
            needs_iteration = evaluation.get('needs_iteration', False)
            
            print(f"ğŸ“Š [è´¨é‡è¯„ä¼°] ç¬¬{iteration}è½®è¯„åˆ†: {total_score}/10.0")
            print(f"ğŸ“Š [è´¨é‡è¯„ä¼°] è–„å¼±ç¯èŠ‚: {weak_areas}")
            print(f"ğŸ“Š [è´¨é‡è¯„ä¼°] éœ€è¦è¿­ä»£: {needs_iteration}")
            
            # å¦‚æœè´¨é‡è¾¾æ ‡æˆ–æ²¡æœ‰è–„å¼±ç¯èŠ‚ï¼Œåœæ­¢è¿­ä»£
            if total_score >= min_quality_score and not needs_iteration:
                print(f"âœ… [è´¨é‡è¯„ä¼°] è´¨é‡è¾¾æ ‡ ({total_score} >= {min_quality_score})ï¼Œåœæ­¢è¿­ä»£")
                break
            
            # å¦‚æœæ˜¯æœ€åä¸€è½®è¯„ä¼°ï¼Œä»ç„¶æ‰§è¡Œè¡¥å……æœç´¢ï¼Œç„¶åé€€å‡º
            if iteration >= max_iterations:
                print(f"ğŸ” [è´¨é‡è¯„ä¼°] ç¬¬{iteration}è½®ï¼ˆæœ€åä¸€è½®ï¼‰è¯„ä¼°å®Œæˆï¼Œæ‰§è¡Œæœ€åçš„è¡¥å……æœç´¢...")
                # ç»§ç»­æ‰§è¡Œè¡¥å……æœç´¢ï¼Œç„¶ååœ¨æœç´¢å®Œæˆåé€€å‡º
            else:
                print(f"ğŸ” [è´¨é‡è¯„ä¼°] ç¬¬{iteration}è½®è¯„ä¼°å®Œæˆï¼Œç»§ç»­è¡¥å……æœç´¢...")
            
            # æ ¹æ®è–„å¼±ç¯èŠ‚ç”Ÿæˆè¡¥å……æŸ¥è¯¢
            print(f"ğŸ” [è´¨é‡è¯„ä¼°] ç”Ÿæˆè¡¥å……æŸ¥è¯¢ä»¥æ”¹è¿›è–„å¼±ç¯èŠ‚...")
            supplementary_queries = _generate_quality_evaluation_queries(topic, weak_areas)
            
            if not supplementary_queries:
                print(f"âš ï¸ [è´¨é‡è¯„ä¼°] æ— æ³•ç”Ÿæˆè¡¥å……æŸ¥è¯¢ï¼Œåœæ­¢è¿­ä»£")
                break
            
            # æ‰§è¡Œè¡¥å……æœç´¢ - å¢åŠ æ¯æ¬¡æœç´¢çš„ç»“æœæ•°é‡
            print(f"ğŸ“Š [è´¨é‡è¯„ä¼°] æ‰§è¡Œ{len(supplementary_queries)}ä¸ªè¡¥å……æŸ¥è¯¢...")
            supplementary_results = []
            
            for query in supplementary_queries:
                try:
                    # å¢åŠ æ¯ä¸ªæŸ¥è¯¢çš„ç»“æœæ•°é‡ä»3åˆ°5
                    search_result = search(query=query, max_results=5)
                    search_data = json.loads(search_result)
                    
                    if search_data.get('status') == 'success':
                        results = search_data.get('results', [])
                        supplementary_results.extend(results)
                        print(f"âœ… è¡¥å……æœç´¢ '{query}': {len(results)}æ¡ç»“æœ")
                    else:
                        print(f"âŒ è¡¥å……æœç´¢ '{query}': {search_data.get('message', 'å¤±è´¥')}")
                except Exception as e:
                    print(f"âŒ è¡¥å……æœç´¢å¼‚å¸¸ '{query}': {str(e)}")
            
            # åˆå¹¶è¡¥å……ç»“æœ
            if supplementary_results:
                current_search_results.extend(supplementary_results)
                print(f"âœ… [è´¨é‡è¯„ä¼°] ç¬¬{iteration}è½®è¡¥å……äº†{len(supplementary_results)}æ¡ç»“æœ")
            else:
                print(f"âš ï¸ [è´¨é‡è¯„ä¼°] ç¬¬{iteration}è½®æœªè·å¾—æœ‰æ•ˆè¡¥å……ç»“æœ")
            
            # å¦‚æœæ˜¯æœ€åä¸€è½®ï¼Œåœ¨è¡¥å……æœç´¢å®Œæˆåé€€å‡º
            if iteration >= max_iterations:
                print(f"âš ï¸ [è´¨é‡è¯„ä¼°] è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•° ({max_iterations})ï¼Œè¡¥å……æœç´¢å®Œæˆï¼Œåœæ­¢è¿­ä»£")
                break
        
        print(f"âœ… [è´¨é‡è¯„ä¼°] è¿­ä»£å®Œæˆï¼Œæœ€ç»ˆæ”¶é›†åˆ°{len(current_search_results)}æ¡æœç´¢ç»“æœ")
        return current_search_results
        
    except Exception as e:
        print(f"âŒ [è´¨é‡è¯„ä¼°] è¿­ä»£è¿‡ç¨‹å¼‚å¸¸: {str(e)}")
        return initial_search_results

def _assemble_content_for_quality_evaluation(executive_summary: str, section_contents: Dict[str, str], topic: str) -> str:
    """ç»„è£…å†…å®¹ç”¨äºè´¨é‡è¯„ä¼°"""
    try:
        # æ„å»ºå®Œæ•´çš„æŠ¥å‘Šå†…å®¹æ–‡æœ¬
        content_parts = []
        
        # æ·»åŠ ä¸»é¢˜å’Œæ‘˜è¦
        content_parts.append(f"ä¸»é¢˜: {topic}")
        
        if executive_summary:
            content_parts.append(f"æ‰§è¡Œæ‘˜è¦: {executive_summary}")
        
        # æ·»åŠ å„ç« èŠ‚å†…å®¹
        for section_title, content in section_contents.items():
            if content:
                content_parts.append(f"ç« èŠ‚: {section_title}")
                content_parts.append(content)
        
        return "\n\n".join(content_parts)
    except Exception as e:
        print(f"âš ï¸ ç»„è£…è¯„ä¼°å†…å®¹å¤±è´¥: {e}")
        return f"ä¸»é¢˜: {topic}\nå†…å®¹ç»„è£…å¤±è´¥"

def _generate_quality_evaluation_queries(topic: str, weak_areas: List[str]) -> List[str]:
    """ä½¿ç”¨LLMæ ¹æ®è–„å¼±ç¯èŠ‚ç”Ÿæˆæ›´æœ‰é’ˆå¯¹æ€§çš„è¡¥å……æœç´¢æŸ¥è¯¢"""
    try:
        # æ„å»ºLLM promptæ¥ç”Ÿæˆæ›´å¥½çš„æŸ¥è¯¢
        weak_areas_str = "ã€".join(weak_areas)
        prompt = f"""é’ˆå¯¹ä¸»é¢˜"{topic}"ï¼Œå½“å‰èµ„æ–™åœ¨ä»¥ä¸‹æ–¹é¢å­˜åœ¨ä¸è¶³ï¼š{weak_areas_str}

è¯·ç”Ÿæˆ5-8ä¸ªå…·ä½“çš„æœç´¢æŸ¥è¯¢ï¼Œç”¨äºè¡¥å……è¿™äº›è–„å¼±ç¯èŠ‚ã€‚è¦æ±‚ï¼š
1. æŸ¥è¯¢è¦å…·ä½“ã€æœ‰é’ˆå¯¹æ€§
2. é¿å…è¿‡äºå®½æ³›çš„è¯æ±‡
3. åŒ…å«ä¸“ä¸šæœ¯è¯­å’Œå…³é”®æ¦‚å¿µ
4. æ¯ä¸ªæŸ¥è¯¢åº”è¯¥èƒ½è·å–åˆ°ä¸åŒè§’åº¦çš„ä¿¡æ¯

è¯·ç›´æ¥è¿”å›æŸ¥è¯¢åˆ—è¡¨ï¼Œæ¯è¡Œä¸€ä¸ªæŸ¥è¯¢ï¼š"""

        try:
            response = llm_processor.call_llm_api(
                prompt=prompt,
                system_message="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ä¿¡æ¯æ£€ç´¢ä¸“å®¶ï¼Œæ“…é•¿è®¾è®¡ç²¾å‡†çš„æœç´¢æŸ¥è¯¢ã€‚",
                temperature=0.3,
                max_tokens=300
            )
            
            # è§£æLLMç”Ÿæˆçš„æŸ¥è¯¢
            queries = []
            for line in response.strip().split('\n'):
                line = line.strip()
                if line and not line.startswith(('#', '-', '*', 'â€¢')):
                    # æ¸…ç†å¯èƒ½çš„åºå·
                    import re
                    clean_query = re.sub(r'^\d+[\.\)]\s*', '', line).strip()
                    if clean_query and len(clean_query) > 3:
                        queries.append(clean_query)
            
            # é™åˆ¶æŸ¥è¯¢æ•°é‡å¹¶å»é‡
            unique_queries = list(dict.fromkeys(queries))[:6]
            
            if len(unique_queries) >= 3:
                print(f"ğŸ” [è´¨é‡è¯„ä¼°] LLMç”Ÿæˆ{len(unique_queries)}ä¸ªè¡¥å……æŸ¥è¯¢: {unique_queries}")
                return unique_queries
            else:
                print(f"âš ï¸ [è´¨é‡è¯„ä¼°] LLMç”Ÿæˆçš„æŸ¥è¯¢æ•°é‡ä¸è¶³ï¼Œä½¿ç”¨åå¤‡æ–¹æ¡ˆ")
                return _generate_fallback_queries(topic, weak_areas)
                
        except Exception as e:
            print(f"âš ï¸ [è´¨é‡è¯„ä¼°] LLMæŸ¥è¯¢ç”Ÿæˆå¤±è´¥: {e}ï¼Œä½¿ç”¨åå¤‡æ–¹æ¡ˆ")
            return _generate_fallback_queries(topic, weak_areas)
        
    except Exception as e:
        print(f"âš ï¸ ç”Ÿæˆè´¨é‡è¯„ä¼°æŸ¥è¯¢å¤±è´¥: {e}")
        return [f"{topic} è¡¥å……èµ„æ–™"]

def _generate_fallback_queries(topic: str, weak_areas: List[str]) -> List[str]:
    """åå¤‡æŸ¥è¯¢ç”Ÿæˆæ–¹æ¡ˆ"""
    queries = []
    
    # è–„å¼±ç¯èŠ‚å¯¹åº”çš„æŸ¥è¯¢ç­–ç•¥ - æ›´å…·ä½“åŒ–
    weakness_query_map = {
        "completeness": [
            f"{topic} æŠ€æœ¯åŸç†è¯¦è§£",
            f"{topic} åº”ç”¨æ¡ˆä¾‹åˆ†æ",
            f"{topic} å‘å±•å†ç¨‹æ¢³ç†"
        ],
        "accuracy": [
            f"{topic} æƒå¨ç ”ç©¶æŠ¥å‘Š",
            f"{topic} å®˜æ–¹æŠ€æœ¯æ–‡æ¡£",
            f"{topic} å­¦æœ¯è®ºæ–‡ç»¼è¿°"
        ],
        "depth": [
            f"{topic} åº•å±‚æŠ€æœ¯æœºåˆ¶",
            f"{topic} æ ¸å¿ƒç®—æ³•åŸç†",
            f"{topic} æŠ€æœ¯æ¶æ„è®¾è®¡"
        ],
        "relevance": [
            f"{topic} å®é™…åº”ç”¨åœºæ™¯",
            f"{topic} è¡Œä¸šè§£å†³æ–¹æ¡ˆ",
            f"{topic} å•†ä¸šä»·å€¼åˆ†æ"
        ],
        "clarity": [
            f"{topic} é€šä¿—æ˜“æ‡‚è§£é‡Š",
            f"{topic} å›¾è§£æ•™ç¨‹",
            f"{topic} å…¥é—¨æŒ‡å—"
        ]
    }
    
    # ä¸ºæ¯ä¸ªè–„å¼±ç¯èŠ‚ç”Ÿæˆ2ä¸ªæŸ¥è¯¢
    for weak_area in weak_areas:
        if weak_area in weakness_query_map:
            queries.extend(weakness_query_map[weak_area][:2])
    
    # å¦‚æœæ²¡æœ‰è–„å¼±ç¯èŠ‚ï¼Œç”Ÿæˆé€šç”¨æŸ¥è¯¢
    if not queries:
        queries = [
            f"{topic} æœ€æ–°æŠ€æœ¯è¿›å±•",
            f"{topic} å®é™…åº”ç”¨æ¡ˆä¾‹",
            f"{topic} æŠ€æœ¯æŒ‘æˆ˜åˆ†æ",
            f"{topic} æœªæ¥å‘å±•è¶‹åŠ¿"
        ]
    
    unique_queries = list(dict.fromkeys(queries))[:6]
    print(f"ğŸ” [è´¨é‡è¯„ä¼°] åå¤‡æ–¹æ¡ˆç”Ÿæˆ{len(unique_queries)}ä¸ªè¡¥å……æŸ¥è¯¢: {unique_queries}")
    return unique_queries

# æµå¼å¤„ç†å™¨åˆå§‹åŒ–
try:
    from streaming_orchestrator import StreamingOrchestrator
    streaming_orchestrator = StreamingOrchestrator()
    streaming_available = True
    print("âœ… æµå¼å¤„ç†å™¨åˆå§‹åŒ–æˆåŠŸ")
except Exception as e:
    print(f"âš ï¸ æµå¼å¤„ç†å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
    streaming_orchestrator = None
    streaming_available = False

# HTTP API æœåŠ¡å™¨
from fastapi import FastAPI, HTTPException
from fastapi.responses import StreamingResponse
import uvicorn
import asyncio
import json

# åˆ›å»ºHTTPåº”ç”¨
http_app = FastAPI(title="MCP HTTP API", version="1.0.0")

@http_app.post("/mcp/tools/call")
async def tools_call(request: dict):
    """MCPå·¥å…·è°ƒç”¨ç«¯ç‚¹ - æ”¯æŒSSEæµå¼å“åº”"""
    try:
        # æ”¯æŒä¸¤ç§è¯·æ±‚æ ¼å¼
        if "params" in request:
            # MCPæ ‡å‡†æ ¼å¼: {"jsonrpc": "2.0", "method": "tools/call", "params": {"name": "tool", "arguments": {...}}}
            params = request.get("params", {})
            tool_name = params.get("name")
            arguments = params.get("arguments", {})
            request_id = request.get("id", 1)
        else:
            # ç®€åŒ–æ ¼å¼: {"tool": "tool_name", "arguments": {...}}
            tool_name = request.get("tool")
            arguments = request.get("arguments", {})
            request_id = request.get("id", 1)
        
        # å·¥å…·æ˜ å°„
        tool_functions = {
            "search": search,
            "orchestrator_mcp": orchestrator_mcp,
            "query_generation_mcp": query_generation_mcp,
            "outline_writer_mcp": outline_writer_mcp,
            "summary_writer_mcp": summary_writer_mcp,
            "content_writer_mcp": content_writer_mcp,
            "analysis_mcp": analysis_mcp,
            "user_interaction_mcp": user_interaction_mcp
        }
        
        if tool_name not in tool_functions:
            # è¿”å›SSEæ ¼å¼çš„é”™è¯¯
            async def error_stream():
                error_msg = {
                    "jsonrpc": "2.0",
                    "id": request_id,
                    "error": {
                        "code": -32602,
                        "message": "Unknown tool",
                        "data": {
                            "type": "unknown_tool",
                            "message": f"Unknown tool: {tool_name}"
                        }
                    }
                }
                yield f"data: {json.dumps(error_msg, ensure_ascii=False)}\n\n"
            
            return StreamingResponse(error_stream(), media_type="text/event-stream")
        
        # å¯¹äºorchestrator_mcpï¼Œä½¿ç”¨æµå¼å¤„ç†
        if tool_name == "orchestrator_mcp" and streaming_available:
            async def orchestrator_stream():
                try:
                    # å‘é€å¼€å§‹æ¶ˆæ¯
                    start_msg = {
                        "type": "start",
                        "message": f"å¼€å§‹æ‰§è¡Œ{tool_name}ä»»åŠ¡"
                    }
                    yield f"data: {json.dumps(start_msg, ensure_ascii=False)}\n\n"
                    
                    # ä½¿ç”¨StreamingOrchestrator
                    async for message in streaming_orchestrator.stream_insight_report(**arguments):
                        yield f"data: {json.dumps(message, ensure_ascii=False)}\n\n"
                        
                except Exception as e:
                    error_msg = {
                        "jsonrpc": "2.0",
                        "id": request_id,
                        "error": {
                            "code": -32603,
                            "message": "Internal error",
                            "data": {
                                "type": "execution_failed",
                                "message": str(e)
                            }
                        }
                    }
                    yield f"data: {json.dumps(error_msg, ensure_ascii=False)}\n\n"
            
            return StreamingResponse(orchestrator_stream(), media_type="text/event-stream")
        
        # å¯¹äºå…¶ä»–å·¥å…·ï¼Œæ‰§è¡Œå¹¶è¿”å›ç»“æœ
        else:
            async def tool_stream():
                try:
                    # æ‰§è¡Œå·¥å…·
                    result = await tool_functions[tool_name](**arguments)
                    
                    # å‘é€ç»“æœæ¶ˆæ¯
                    result_msg = {
                        "jsonrpc": "2.0",
                        "id": request_id,
                        "result": {
                            "tool": tool_name,
                            "content": result
                        }
                    }
                    yield f"data: {json.dumps(result_msg, ensure_ascii=False)}\n\n"
                        
                except Exception as e:
                    error_msg = {
                        "jsonrpc": "2.0",
                        "id": request_id,
                        "error": {
                            "code": -32603,
                            "message": "Internal error",
                            "data": {
                                "type": "execution_failed",
                                "message": str(e)
                            }
                        }
                    }
                    yield f"data: {json.dumps(error_msg, ensure_ascii=False)}\n\n"
            
            return StreamingResponse(tool_stream(), media_type="text/event-stream")
        
    except Exception as e:
        # è¿”å›SSEæ ¼å¼çš„é”™è¯¯
        async def error_stream():
            error_msg = {
                "jsonrpc": "2.0",
                "id": request.get("id", 1),
                "error": {
                    "code": -32603,
                    "message": "Internal error",
                    "data": {
                        "type": "request_parsing_failed",
                        "message": str(e)
                    }
                }
            }
            yield f"data: {json.dumps(error_msg, ensure_ascii=False)}\n\n"
        
        return StreamingResponse(error_stream(), media_type="text/event-stream")

@http_app.get("/")
async def root():
    return {"message": "MCP HTTP API Server", "version": "1.0.0"}

@http_app.get("/health")
async def health():
    return {"status": "healthy", "timestamp": datetime.now().isoformat()}

if __name__ == "__main__":
    print("ğŸš€ å¯åŠ¨MCPæœåŠ¡å™¨...")
    print("ğŸ“¡ æ”¯æŒç«¯ç‚¹: /mcp/tools/call (HTTP API)")
    print("ğŸ”§ æ”¯æŒçš„MCPå·¥å…·:")
    print("   - orchestrator_mcp: ä¸»ç¼–æ’å·¥å…·(æ”¯æŒè´¨é‡è¯„ä¼°è¿­ä»£ï¼ŒåŒ…æ‹¬æ´å¯ŸæŠ¥å‘Š)")
    print("   - analysis_mcp: åˆ†æå·¥å…·(æ”¯æŒevaluationè´¨é‡è¯„ä¼°)")
    print("   - search: æœç´¢å·¥å…·")
    print("   - outline_writer_mcp: å¤§çº²ç”Ÿæˆå·¥å…·")
    print("   - content_writer_mcp: å†…å®¹ç”Ÿæˆå·¥å…·")
    print("   - summary_writer_mcp: æ‘˜è¦ç”Ÿæˆå·¥å…·")
    print("   - query_generation_mcp: æŸ¥è¯¢ç”Ÿæˆå·¥å…·")
    print("   - user_interaction_mcp: ç”¨æˆ·äº¤äº’å·¥å…·")
    
    # å¯åŠ¨HTTPæœåŠ¡å™¨
    import threading
    import time

    def start_http_server():
        uvicorn.run(http_app, host="0.0.0.0", port=8001, log_level="info")
    
    http_thread = threading.Thread(target=start_http_server, daemon=True)
    http_thread.start()
    
    # ç­‰å¾…HTTPæœåŠ¡å™¨å¯åŠ¨
    time.sleep(2)
    
    # å¯åŠ¨FastMCPæœåŠ¡å™¨
    print("ğŸš€ å¯åŠ¨FastMCPæœåŠ¡å™¨...")
    mcp.run(transport="streamable-http")
