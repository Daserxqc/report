
from mcp.server.fastmcp import FastMCP
import sys
import os
import json
from pathlib import Path
from typing import List, Dict, Optional, Union, Any
from dataclasses import dataclass
from datetime import datetime

# ç¯å¢ƒå˜é‡åŠ è½½
try:
    from dotenv import load_dotenv
    load_dotenv()
    print("âœ… ç¯å¢ƒå˜é‡åŠ è½½æˆåŠŸ")
except ImportError:
    print("âš ï¸ python-dotenvæœªå®‰è£…ï¼Œè·³è¿‡.envæ–‡ä»¶åŠ è½½")
except Exception as e:
    print(f"âš ï¸ åŠ è½½.envæ–‡ä»¶å¤±è´¥: {e}")

# æœç´¢ç»„ä»¶è·¯å¾„é…ç½®
search_mcp_path = Path(__file__).parent / "search_mcp" / "src"
if str(search_mcp_path) not in sys.path:
    sys.path.insert(0, str(search_mcp_path))

# æ·»åŠ search_mcpæ¨¡å—è·¯å¾„
search_mcp_module_path = search_mcp_path / "search_mcp"
if str(search_mcp_module_path.parent) not in sys.path:
    sys.path.insert(0, str(search_mcp_module_path.parent))

# æ·»åŠ collectorsè·¯å¾„
collectors_path = Path(__file__).parent / "collectors"
if str(collectors_path) not in sys.path:
    sys.path.insert(0, str(collectors_path))

# æœç´¢ç»„ä»¶åˆå§‹åŒ–
try:
    # ç¡®ä¿æ­£ç¡®çš„å¯¼å…¥è·¯å¾„
    print(f"ğŸ” å°è¯•ä»è·¯å¾„å¯¼å…¥: {search_mcp_path}")
    print(f"ğŸ” search_mcpç›®å½•å­˜åœ¨: {search_mcp_path.exists()}")
    print(f"ğŸ” config.pyæ–‡ä»¶å­˜åœ¨: {(search_mcp_path / 'search_mcp' / 'config.py').exists()}")
    
    from search_mcp.config import SearchConfig
    from search_mcp.generators import SearchOrchestrator
    
    # åˆ›å»ºé…ç½®å’Œæœç´¢ç¼–æ’å™¨
    config = SearchConfig()
    print(f"ğŸ” é…ç½®åˆ›å»ºæˆåŠŸï¼ŒAPIå¯†é’¥çŠ¶æ€: {config.get_api_keys()}")
    
    orchestrator = SearchOrchestrator(config)
    search_available = True
    print(f"âœ… æœç´¢ç»„ä»¶åˆå§‹åŒ–æˆåŠŸï¼Œå¯ç”¨æ•°æ®æº: {config.get_enabled_sources()}")
except Exception as e:
    print(f"âš ï¸ æœç´¢ç»„ä»¶åˆå§‹åŒ–å¤±è´¥: {e}")
    print(f"   é”™è¯¯ç±»å‹: {type(e).__name__}")
    print(f"   æœç´¢ç»„ä»¶è·¯å¾„: {search_mcp_path}")
    print(f"   å½“å‰sys.pathåŒ…å«: {[p for p in sys.path if 'search_mcp' in p]}")
    
    # å°è¯•è·å–æ›´å¤šè°ƒè¯•ä¿¡æ¯
    try:
        import search_mcp
        print(f"   search_mcpæ¨¡å—è·¯å¾„: {search_mcp.__file__}")
    except:
        print("   æ— æ³•å¯¼å…¥search_mcpæ¨¡å—")
    
    orchestrator = None
    search_available = False

# LLMå¤„ç†å™¨åˆå§‹åŒ–
try:
    from collectors.llm_processor import LLMProcessor
    llm_processor = LLMProcessor()
    llm_available = True
    print("âœ… LLMå¤„ç†å™¨åˆå§‹åŒ–æˆåŠŸ")
except Exception as e:
    print(f"âš ï¸ LLMå¤„ç†å™¨åˆå§‹åŒ–å¤±è´¥: {str(e)}")
    llm_processor = None
    llm_available = False

# åˆ›å»ºMCPæœåŠ¡å™¨
mcp = FastMCP("Search Server")

@mcp.tool()
def search(query: str, max_results: int = 5) -> str:
    """æ‰§è¡Œæœç´¢æŸ¥è¯¢å¹¶è¿”å›ç»“æœ"""
    try:
        if not search_available or not orchestrator:
            return json.dumps({
                "status": "error",
                "message": "æœç´¢ç»„ä»¶æœªåˆå§‹åŒ–",
                "results": []
            }, ensure_ascii=False)
        
        print(f"ğŸ” æ‰§è¡Œæœç´¢æŸ¥è¯¢: {query}")
        
        # ä½¿ç”¨æœç´¢ç¼–æ’å™¨æ‰§è¡Œæœç´¢
        search_results = orchestrator.parallel_search(
            queries=[query],  # ä¼ å…¥æŸ¥è¯¢åˆ—è¡¨
            sources=["tavily", "brave", "google"],  # ä½¿ç”¨ä¸»è¦æœç´¢æº
            max_results_per_query=max_results,
            days_back=30,
            max_workers=3
        )
        
        # å¤„ç†æœç´¢ç»“æœ - Documentå¯¹è±¡è½¬æ¢ä¸ºå­—å…¸
        processed_results = []
        for result in search_results[:max_results]:
            # å¤„ç†Documentå¯¹è±¡
            if hasattr(result, 'title'):
                # è¿™æ˜¯Documentå¯¹è±¡
                processed_result = {
                    "title": getattr(result, 'title', ''),
                    "content": getattr(result, 'content', '')[:500],  # é™åˆ¶å†…å®¹é•¿åº¦
                    "url": getattr(result, 'url', ''),
                    "source": getattr(result, 'source', 'unknown'),
                    "relevance_score": getattr(result, 'relevance_score', 0.0),
                    "timestamp": getattr(result, 'timestamp', '')
                }
            else:
                # è¿™æ˜¯å­—å…¸å¯¹è±¡
                processed_result = {
                    "title": result.get("title", ""),
                    "content": result.get("content", "")[:500],  # é™åˆ¶å†…å®¹é•¿åº¦
                    "url": result.get("url", ""),
                    "source": result.get("source", "unknown"),
                    "relevance_score": result.get("relevance_score", 0.0),
                    "timestamp": result.get("timestamp", "")
                }
                processed_results.append(processed_result)
        
        response = {
            "status": "success",
            "query": query,
            "results": processed_results,
            "total_found": len(processed_results),
            "search_timestamp": datetime.now().isoformat()
        }
        
        print(f"âœ… æœç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(processed_results)} æ¡ç»“æœ")
        return json.dumps(response, ensure_ascii=False, indent=2)
        
    except Exception as e:
        error_response = {
            "status": "error",
            "query": query,
            "message": f"æœç´¢æ‰§è¡Œå¤±è´¥: {str(e)}",
            "results": [],
            "error_type": type(e).__name__
        }
        print(f"âŒ æœç´¢å¤±è´¥: {str(e)}")
        return json.dumps(error_response, ensure_ascii=False, indent=2)

@mcp.tool()
def analysis_mcp(analysis_type: str, data: str, topic: str = "", context: str = "", **kwargs) -> str:
    """åˆ†æå·¥å…· - æ”¯æŒå¤šç§åˆ†æç±»å‹"""
    try:
        print(f"ğŸ” æ‰§è¡Œåˆ†æ: {analysis_type}")
        
        if analysis_type == "intent":
            return _analyze_intent(data, context)
        elif analysis_type == "relevance":
            return _analyze_relevance({"content": data}, topic)
        elif analysis_type == "structure":
            return _parse_structure(data, kwargs.get("parsing_goal", "æå–ç»“æ„åŒ–ä¿¡æ¯"))
        elif analysis_type == "gaps":
            existing_data = json.loads(data) if isinstance(data, str) else [{"content": data}]
            return _analyze_gaps(topic, existing_data)
        elif analysis_type == "evaluation":
            quality_standards = kwargs.get("quality_standards", {})
            return _analyze_evaluation(data, topic, quality_standards, context)
        else:
            return json.dumps({
                "status": "error",
                "message": f"ä¸æ”¯æŒçš„åˆ†æç±»å‹: {analysis_type}",
                "supported_types": ["intent", "relevance", "structure", "gaps", "evaluation"]
            }, ensure_ascii=False)
            
    except Exception as e:
        return json.dumps({
            "status": "error",
            "analysis_type": analysis_type,
            "error": str(e)
        }, ensure_ascii=False)

@mcp.tool()
def user_interaction_mcp(interaction_type: str, content: str, options: List[str] = None, **kwargs) -> str:
    """ç”¨æˆ·äº¤äº’å·¥å…· - æ¨¡æ‹Ÿç”¨æˆ·äº¤äº’"""
    try:
        print(f"ğŸ‘¤ ç”¨æˆ·äº¤äº’: {interaction_type}")
        
        if interaction_type == "confirmation":
            # æ¨¡æ‹Ÿç”¨æˆ·ç¡®è®¤
            return json.dumps({
                "status": "confirmed",
                "interaction_type": interaction_type,
                "user_response": "confirmed",
                "message": "ç”¨æˆ·ç¡®è®¤ç»§ç»­"
            }, ensure_ascii=False)
        elif interaction_type == "selection":
            # æ¨¡æ‹Ÿç”¨æˆ·é€‰æ‹©
            selected = options[0] if options else "default"
            return json.dumps({
                "status": "selected",
                "interaction_type": interaction_type,
                "user_response": selected,
                "message": f"ç”¨æˆ·é€‰æ‹©: {selected}"
            }, ensure_ascii=False)
        else:
            return json.dumps({
                "status": "completed",
                "interaction_type": interaction_type,
                "user_response": "acknowledged",
                "message": "äº¤äº’å®Œæˆ"
            }, ensure_ascii=False)
            
    except Exception as e:
        return json.dumps({
            "status": "error",
            "interaction_type": interaction_type,
            "error": str(e)
        }, ensure_ascii=False)

def _analyze_relevance(content: Dict, topic: str) -> str:
    """åˆ†æå†…å®¹ä¸ä¸»é¢˜çš„ç›¸å…³æ€§"""
    try:
        content_text = content.get("content", "")
        
        # ç®€å•çš„å…³é”®è¯åŒ¹é…åˆ†æ
        topic_keywords = topic.lower().split()
        content_lower = content_text.lower()
        
        matches = sum(1 for keyword in topic_keywords if keyword in content_lower)
        relevance_score = matches / len(topic_keywords) if topic_keywords else 0
        
        result = {
            "status": "success",
            "relevance_score": relevance_score,
            "matches_found": matches,
            "total_keywords": len(topic_keywords),
            "analysis": {
                "highly_relevant": relevance_score >= 0.7,
                "moderately_relevant": 0.3 <= relevance_score < 0.7,
                "low_relevance": relevance_score < 0.3
            }
        }
        
        return json.dumps(result, ensure_ascii=False)
        
    except Exception as e:
        return json.dumps({
            "status": "error",
            "error": str(e),
            "relevance_score": 0
        }, ensure_ascii=False)

def _analyze_intent(user_query: str, context: str = "") -> str:
    """åˆ†æç”¨æˆ·æ„å›¾"""
    try:
        query_lower = user_query.lower()
        
        # æ„å›¾åˆ†ç±»
        intent_patterns = {
            "research": ["ç ”ç©¶", "åˆ†æ", "è°ƒç ”", "research", "analysis"],
            "news": ["æ–°é—»", "åŠ¨æ€", "æœ€æ–°", "news", "update"],
            "comparison": ["æ¯”è¾ƒ", "å¯¹æ¯”", "compare", "versus"],
            "summary": ["æ€»ç»“", "æ‘˜è¦", "æ¦‚è¿°", "summary"],
            "insight": ["æ´å¯Ÿ", "è§è§£", "insight", "perspective"]
        }
        
        detected_intents = []
        for intent, patterns in intent_patterns.items():
            if any(pattern in query_lower for pattern in patterns):
                detected_intents.append(intent)
        
        primary_intent = detected_intents[0] if detected_intents else "general"
        
        result = {
            "status": "success",
            "details": {
                "primary_intent": primary_intent,
                "all_intents": detected_intents,
                "confidence": 0.8 if detected_intents else 0.5,
                "query_analysis": {
                    "length": len(user_query),
                    "complexity": "high" if len(user_query.split()) > 10 else "medium"
                }
            }
        }
        
        return json.dumps(result, ensure_ascii=False)
        
    except Exception as e:
        return json.dumps({
            "status": "error",
            "error": str(e),
            "details": {
                "primary_intent": "unknown",
                "all_intents": [],
                "confidence": 0
            }
        }, ensure_ascii=False)

def _parse_structure(input_text: str, parsing_goal: str, output_schema: Dict = None) -> str:
    """è§£ææ–‡æœ¬ç»“æ„"""
    try:
        # ç®€å•çš„ç»“æ„åŒ–è§£æ
        lines = input_text.split('\n')
        structured_data = {
            "total_lines": len(lines),
            "non_empty_lines": len([line for line in lines if line.strip()]),
            "sections": [],
            "parsing_goal": parsing_goal
        }
        
        current_section = None
        for line in lines:
            line = line.strip()
            if not line:
                continue
                
            # æ£€æµ‹æ ‡é¢˜è¡Œ
            if any(marker in line for marker in ['#', '##', '###', '1.', '2.', '3.']):
                if current_section:
                    structured_data["sections"].append(current_section)
                current_section = {
                    "title": line,
                    "content": []
                }
            elif current_section:
                current_section["content"].append(line)
            else:
                # å¦‚æœæ²¡æœ‰å½“å‰ç« èŠ‚ï¼Œåˆ›å»ºä¸€ä¸ªé»˜è®¤ç« èŠ‚
                if not structured_data["sections"]:
                    structured_data["sections"].append({
                        "title": "ä¸»è¦å†…å®¹",
                        "content": [line]
                    })
                else:
                    structured_data["sections"][-1]["content"].append(line)
        
        if current_section:
            structured_data["sections"].append(current_section)
        
        return json.dumps({
            "status": "success",
            "structured_data": structured_data
        }, ensure_ascii=False)
        
    except Exception as e:
        return json.dumps({
            "status": "error",
            "error": str(e),
            "structured_data": {}
        }, ensure_ascii=False)

def _analyze_gaps(topic: str, existing_data: List[Dict], expected_aspects: List[str] = None) -> str:
    """åˆ†ææ•°æ®ç¼ºå£"""
    try:
        if not expected_aspects:
            expected_aspects = [
                "æŠ€æœ¯å‘å±•", "å¸‚åœºç°çŠ¶", "åº”ç”¨åœºæ™¯", "æŒ‘æˆ˜é—®é¢˜", 
                "æœªæ¥è¶‹åŠ¿", "æ”¿ç­–ç¯å¢ƒ", "ç«äº‰æ ¼å±€", "æŠ•èµ„æœºä¼š"
            ]
        
        # åˆ†æç°æœ‰æ•°æ®è¦†ç›–çš„æ–¹é¢
        covered_aspects = []
        for aspect in expected_aspects:
            for data_item in existing_data:
                content = data_item.get("content", "")
                if any(keyword in content for keyword in aspect.split()):
                    covered_aspects.append(aspect)
                    break
        
        missing_aspects = [aspect for aspect in expected_aspects if aspect not in covered_aspects]
        
        result = {
            "status": "success",
            "gap_analysis": {
                "total_expected": len(expected_aspects),
                "covered": len(covered_aspects),
                "missing": len(missing_aspects),
                "coverage_rate": len(covered_aspects) / len(expected_aspects),
                "covered_aspects": covered_aspects,
                "missing_aspects": missing_aspects,
                "recommendations": [
                    f"éœ€è¦è¡¥å……{aspect}ç›¸å…³ä¿¡æ¯" for aspect in missing_aspects[:3]
                ]
            }
        }
        
        return json.dumps(result, ensure_ascii=False)
        
    except Exception as e:
        return json.dumps({
            "status": "error",
            "error": str(e),
            "gap_analysis": {}
        }, ensure_ascii=False)

def _analyze_evaluation(content: str, topic: str, quality_standards: Dict = None, context: str = "") -> str:
    """åˆ†æå†…å®¹è´¨é‡å¹¶æä¾›æ”¹è¿›å»ºè®®"""
    try:
        if not quality_standards:
            quality_standards = {
                "completeness": {"weight": 0.3, "min_score": 7.0},
                "accuracy": {"weight": 0.25, "min_score": 8.0},
                "depth": {"weight": 0.2, "min_score": 6.0},
                "relevance": {"weight": 0.15, "min_score": 7.0},
                "clarity": {"weight": 0.1, "min_score": 6.0}
            }
        
        # è§£æå†…å®¹æ•°æ®
        if isinstance(content, str):
            try:
                content_data = json.loads(content)
            except json.JSONDecodeError:
                content_data = {"text": content}
        else:
            content_data = content
            
        # æå–æ–‡æœ¬å†…å®¹è¿›è¡Œåˆ†æ
        text_content = ""
        if isinstance(content_data, dict):
            text_content = content_data.get("content", "") or content_data.get("text", "") or str(content_data)
        elif isinstance(content_data, list):
            text_content = " ".join([str(item.get("content", "") if isinstance(item, dict) else str(item)) for item in content_data])
        else:
            text_content = str(content_data)
        
        # è´¨é‡è¯„ä¼°ç»´åº¦
        evaluation_results = {}
        
        # 1. å®Œæ•´æ€§è¯„ä¼° (Completeness)
        completeness_score = _evaluate_completeness(text_content, topic)
        evaluation_results["completeness"] = {
            "score": completeness_score,
            "weight": quality_standards["completeness"]["weight"],
            "min_required": quality_standards["completeness"]["min_score"],
            "passed": completeness_score >= quality_standards["completeness"]["min_score"]
        }
        
        # 2. å‡†ç¡®æ€§è¯„ä¼° (Accuracy) 
        accuracy_score = _evaluate_accuracy(text_content, topic)
        evaluation_results["accuracy"] = {
            "score": accuracy_score,
            "weight": quality_standards["accuracy"]["weight"],
            "min_required": quality_standards["accuracy"]["min_score"],
            "passed": accuracy_score >= quality_standards["accuracy"]["min_score"]
        }
        
        # 3. æ·±åº¦è¯„ä¼° (Depth)
        depth_score = _evaluate_depth(text_content, topic)
        evaluation_results["depth"] = {
            "score": depth_score,
            "weight": quality_standards["depth"]["weight"],
            "min_required": quality_standards["depth"]["min_score"],
            "passed": depth_score >= quality_standards["depth"]["min_score"]
        }
        
        # 4. ç›¸å…³æ€§è¯„ä¼° (Relevance)
        relevance_score = _evaluate_relevance(text_content, topic)
        evaluation_results["relevance"] = {
            "score": relevance_score,
            "weight": quality_standards["relevance"]["weight"],
            "min_required": quality_standards["relevance"]["min_score"],
            "passed": relevance_score >= quality_standards["relevance"]["min_score"]
        }
        
        # 5. æ¸…æ™°åº¦è¯„ä¼° (Clarity)
        clarity_score = _evaluate_clarity(text_content)
        evaluation_results["clarity"] = {
            "score": clarity_score,
            "weight": quality_standards["clarity"]["weight"],
            "min_required": quality_standards["clarity"]["min_score"],
            "passed": clarity_score >= quality_standards["clarity"]["min_score"]
        }
        
        # è®¡ç®—åŠ æƒæ€»åˆ†
        total_score = sum([
            result["score"] * result["weight"] 
            for result in evaluation_results.values()
        ])
        
        # è¯†åˆ«è–„å¼±ç¯èŠ‚
        weak_areas = [
            dimension for dimension, result in evaluation_results.items()
            if not result["passed"]
        ]
        
        # ç”Ÿæˆæ”¹è¿›å»ºè®®
        improvement_suggestions = _generate_improvement_suggestions(weak_areas, topic, text_content)
        
        # ç¡®å®šæ˜¯å¦éœ€è¦è¿­ä»£
        needs_iteration = len(weak_areas) > 0 or total_score < 7.0
        
        result = {
            "status": "success",
            "evaluation": {
                "topic": topic,
                "total_score": round(total_score, 2),
                "max_score": 10.0,
                "quality_level": _get_quality_level(total_score),
                "needs_iteration": needs_iteration,
                "dimensions": evaluation_results,
                "weak_areas": weak_areas,
                "improvement_suggestions": improvement_suggestions,
                "content_length": len(text_content),
                "evaluation_timestamp": datetime.now().isoformat()
            }
        }
        
        return json.dumps(result, ensure_ascii=False)
        
    except Exception as e:
        return json.dumps({
            "status": "error",
            "error": str(e),
            "evaluation": {}
        }, ensure_ascii=False)

def _evaluate_completeness(content: str, topic: str) -> float:
    """è¯„ä¼°å†…å®¹å®Œæ•´æ€§"""
    if not content.strip():
        return 0.0
    
    # åŸºäºå†…å®¹é•¿åº¦å’Œä¸»é¢˜è¦†ç›–åº¦çš„ç®€å•è¯„ä¼°
    content_lower = content.lower()
    topic_keywords = topic.lower().split()
    
    # æ£€æŸ¥ä¸»é¢˜å…³é”®è¯è¦†ç›–
    keyword_coverage = sum(1 for keyword in topic_keywords if keyword in content_lower) / len(topic_keywords) if topic_keywords else 0
    
    # æ£€æŸ¥å†…å®¹é•¿åº¦é€‚ä¸­æ€§
    length_score = min(len(content) / 1000, 1.0)  # 1000å­—ç¬¦ä¸ºæ»¡åˆ†åŸºå‡†
    
    # æ£€æŸ¥ç»“æ„å®Œæ•´æ€§ï¼ˆæ˜¯å¦æœ‰æ ‡é¢˜ã€æ®µè½ç­‰ï¼‰
    structure_indicators = ["##", "###", "1.", "2.", "3.", "â€¢", "-"]
    structure_score = min(sum(1 for indicator in structure_indicators if indicator in content) / 5, 1.0)
    
    # ç»¼åˆè¯„åˆ†
    completeness_score = (keyword_coverage * 0.4 + length_score * 0.3 + structure_score * 0.3) * 10
    return min(completeness_score, 10.0)

def _evaluate_accuracy(content: str, topic: str) -> float:
    """è¯„ä¼°å†…å®¹å‡†ç¡®æ€§"""
    if not content.strip():
        return 0.0
    
    # ç®€å•çš„å‡†ç¡®æ€§è¯„ä¼°ï¼šæ£€æŸ¥æ˜¯å¦æœ‰æ˜æ˜¾çš„é”™è¯¯æŒ‡æ ‡
    content_lower = content.lower()
    
    # ç§¯ææŒ‡æ ‡
    positive_indicators = ["æ ¹æ®", "æ•°æ®æ˜¾ç¤º", "ç ”ç©¶è¡¨æ˜", "åˆ†æå‘ç°", "ç»Ÿè®¡", "æŠ¥å‘Š"]
    positive_score = min(sum(1 for indicator in positive_indicators if indicator in content_lower) / 3, 1.0)
    
    # æ¶ˆææŒ‡æ ‡ï¼ˆé™ä½å‡†ç¡®æ€§çš„å› ç´ ï¼‰
    negative_indicators = ["å¯èƒ½", "å¤§æ¦‚", "ä¼°è®¡", "çŒœæµ‹"]
    negative_penalty = min(sum(1 for indicator in negative_indicators if indicator in content_lower) / 10, 0.3)
    
    # åŸºç¡€å‡†ç¡®æ€§åˆ†æ•°
    base_score = 8.0  # é»˜è®¤è¾ƒé«˜çš„å‡†ç¡®æ€§
    accuracy_score = base_score + positive_score * 2 - negative_penalty * 3
    
    return max(min(accuracy_score, 10.0), 0.0)

def _evaluate_depth(content: str, topic: str) -> float:
    """è¯„ä¼°å†…å®¹æ·±åº¦"""
    if not content.strip():
        return 0.0
    
    content_lower = content.lower()
    
    # æ·±åº¦æŒ‡æ ‡
    depth_indicators = [
        "åˆ†æ", "åŸå› ", "å½±å“", "æœºåˆ¶", "åŸç†", "æ–¹æ³•", "ç­–ç•¥", 
        "è¶‹åŠ¿", "å‰æ™¯", "æŒ‘æˆ˜", "æœºé‡", "é£é™©", "å»ºè®®", "è§£å†³æ–¹æ¡ˆ"
    ]
    
    depth_score = min(sum(1 for indicator in depth_indicators if indicator in content_lower) / 8, 1.0)
    
    # æ£€æŸ¥æ˜¯å¦æœ‰å…·ä½“çš„æ•°æ®å’Œæ¡ˆä¾‹
    specific_indicators = ["ä¾‹å¦‚", "æ¡ˆä¾‹", "%", "æ•°æ®", "å›¾", "è¡¨", "ç ”ç©¶"]
    specific_score = min(sum(1 for indicator in specific_indicators if indicator in content_lower) / 4, 1.0)
    
    # ç»¼åˆæ·±åº¦åˆ†æ•°
    final_depth_score = (depth_score * 0.6 + specific_score * 0.4) * 10
    return min(final_depth_score, 10.0)

def _evaluate_relevance(content: str, topic: str) -> float:
    """è¯„ä¼°å†…å®¹ç›¸å…³æ€§"""
    if not content.strip():
        return 0.0
    
    content_lower = content.lower()
    topic_lower = topic.lower()
    
    # ä¸»é¢˜å…³é”®è¯åŒ¹é…
    topic_keywords = topic_lower.split()
    keyword_matches = sum(1 for keyword in topic_keywords if keyword in content_lower)
    keyword_score = (keyword_matches / len(topic_keywords)) if topic_keywords else 0
    
    # ä¸»é¢˜ç›¸å…³è¯æ±‡æ£€æŸ¥
    if "äººå·¥æ™ºèƒ½" in topic_lower or "ai" in topic_lower:
        ai_related = ["ç®—æ³•", "æœºå™¨å­¦ä¹ ", "æ·±åº¦å­¦ä¹ ", "ç¥ç»ç½‘ç»œ", "æ¨¡å‹", "æ™ºèƒ½"]
        ai_score = min(sum(1 for word in ai_related if word in content_lower) / 3, 1.0)
    else:
        ai_score = 0
    
    if "æ•™è‚²" in topic_lower:
        edu_related = ["å­¦ä¹ ", "æ•™å­¦", "å­¦ç”Ÿ", "æ•™å¸ˆ", "è¯¾ç¨‹", "åŸ¹è®­", "çŸ¥è¯†"]
        edu_score = min(sum(1 for word in edu_related if word in content_lower) / 3, 1.0)
    else:
        edu_score = 0
    
    # ç»¼åˆç›¸å…³æ€§åˆ†æ•°
    relevance_score = (keyword_score * 0.5 + (ai_score + edu_score) * 0.5) * 10
    return min(relevance_score, 10.0)

def _evaluate_clarity(content: str) -> float:
    """è¯„ä¼°å†…å®¹æ¸…æ™°åº¦"""
    if not content.strip():
        return 0.0
    
    # æ£€æŸ¥å¥å­é•¿åº¦åˆç†æ€§
    sentences = content.split('ã€‚')
    avg_sentence_length = sum(len(s) for s in sentences) / len(sentences) if sentences else 0
    length_score = 1.0 if 10 <= avg_sentence_length <= 50 else 0.5
    
    # æ£€æŸ¥æ®µè½ç»“æ„
    paragraphs = [p for p in content.split('\n') if p.strip()]
    structure_score = 1.0 if len(paragraphs) >= 3 else 0.7
    
    # æ£€æŸ¥æ ¼å¼åŒ–å…ƒç´ 
    format_indicators = ["##", "###", "**", "*", "1.", "2.", "â€¢"]
    format_score = min(sum(1 for indicator in format_indicators if indicator in content) / 3, 1.0)
    
    # ç»¼åˆæ¸…æ™°åº¦åˆ†æ•°
    clarity_score = (length_score * 0.3 + structure_score * 0.4 + format_score * 0.3) * 10
    return min(clarity_score, 10.0)

def _get_quality_level(score: float) -> str:
    """æ ¹æ®åˆ†æ•°è·å–è´¨é‡ç­‰çº§"""
    if score >= 9.0:
        return "ä¼˜ç§€"
    elif score >= 8.0:
        return "è‰¯å¥½"
    elif score >= 7.0:
        return "åˆæ ¼"
    elif score >= 6.0:
        return "éœ€æ”¹è¿›"
    else:
        return "ä¸åˆæ ¼"

def _generate_improvement_suggestions(weak_areas: List[str], topic: str, content: str) -> List[str]:
    """ç”Ÿæˆæ”¹è¿›å»ºè®®"""
    suggestions = []
    
    if "completeness" in weak_areas:
        suggestions.append(f"å†…å®¹å®Œæ•´æ€§ä¸è¶³ï¼Œå»ºè®®è¡¥å……æ›´å¤šå…³äº{topic}çš„è¯¦ç»†ä¿¡æ¯ï¼Œå¢åŠ ç« èŠ‚ç»“æ„å’Œå…·ä½“æ•°æ®")
    
    if "accuracy" in weak_areas:
        suggestions.append("å‡†ç¡®æ€§æœ‰å¾…æé«˜ï¼Œå»ºè®®å¢åŠ å¯é çš„æ•°æ®æ¥æºå’Œç ”ç©¶å¼•ç”¨ï¼Œå‡å°‘ä¸ç¡®å®šæ€§è¡¨è¿°")
    
    if "depth" in weak_areas:
        suggestions.append(f"å†…å®¹æ·±åº¦ä¸å¤Ÿï¼Œå»ºè®®æ·±å…¥åˆ†æ{topic}çš„æœºåˆ¶ã€å½±å“å› ç´ å’Œå‘å±•è¶‹åŠ¿ï¼Œå¢åŠ æ¡ˆä¾‹åˆ†æ")
    
    if "relevance" in weak_areas:
        suggestions.append(f"ä¸{topic}ä¸»é¢˜çš„ç›¸å…³æ€§ä¸è¶³ï¼Œå»ºè®®èšç„¦æ ¸å¿ƒä¸»é¢˜ï¼Œå¢åŠ ç›¸å…³å…³é”®è¯å’Œä¸“ä¸šæœ¯è¯­")
    
    if "clarity" in weak_areas:
        suggestions.append("è¡¨è¾¾æ¸…æ™°åº¦éœ€è¦æ”¹è¿›ï¼Œå»ºè®®ä¼˜åŒ–æ®µè½ç»“æ„ï¼Œä½¿ç”¨æ ‡é¢˜å’Œåˆ—è¡¨æé«˜å¯è¯»æ€§")
    
    # å¦‚æœæ²¡æœ‰æ˜æ˜¾å¼±é¡¹ï¼Œæä¾›é€šç”¨å»ºè®®
    if not suggestions:
        suggestions.append("æ•´ä½“è´¨é‡è‰¯å¥½ï¼Œå¯ä»¥è€ƒè™‘å¢åŠ æ›´å¤šå…·ä½“æ¡ˆä¾‹å’Œæœ€æ–°æ•°æ®æ¥è¿›ä¸€æ­¥æå‡å†…å®¹ä»·å€¼")
    
    return suggestions

@mcp.tool()
def query_generation_mcp(topic: str, strategy: str = "initial", context: str = "", **kwargs) -> str:
    """æŸ¥è¯¢ç”Ÿæˆå·¥å…·"""
    try:
        print(f"ğŸ” ç”ŸæˆæŸ¥è¯¢ç­–ç•¥: {strategy} for {topic}")
        
        if strategy == "iterative":
            return _generate_iterative_queries(topic, context, kwargs)
        elif strategy == "targeted":
            return _generate_targeted_queries(topic, context, kwargs)
        elif strategy == "academic":
            return _generate_academic_queries(topic, context, kwargs)
        elif strategy == "news":
            return _generate_news_queries(topic, context, kwargs)
        elif strategy == "outline_based":
            return _generate_outline_based_queries(topic, context, kwargs)
        else:
            # é»˜è®¤ç»¼åˆç­–ç•¥ - æ›´å…·ä½“å’Œå¤šæ ·åŒ–çš„æŸ¥è¯¢
            base_queries = [
                f"{topic} æœ€æ–°å‘å±• 2024 2025",
                f"{topic} æŠ€æœ¯åŸç† æ ¸å¿ƒæŠ€æœ¯",
                f"{topic} å¸‚åœºåˆ†æ è¡Œä¸šæŠ¥å‘Š",
                f"{topic} åº”ç”¨æ¡ˆä¾‹ å®è·µåº”ç”¨",
                f"{topic} æŒ‘æˆ˜é—®é¢˜ è§£å†³æ–¹æ¡ˆ",
                f"{topic} å‘å±•è¶‹åŠ¿ æœªæ¥å±•æœ›",
                f"{topic} æ”¿ç­–ç¯å¢ƒ æ³•è§„å½±å“",
                f"{topic} æŠ•èµ„æœºä¼š å•†ä¸šæ¨¡å¼"
            ]
            
            result = {
                "status": "success",
                "strategy": strategy,
                "topic": topic,
                "queries": [{"query": q, "priority": "medium", "type": "general"} for q in base_queries],
                "total_queries": len(base_queries)
            }
            
            return json.dumps(result, ensure_ascii=False)
            
    except Exception as e:
        return json.dumps({
            "status": "error",
            "strategy": strategy,
            "topic": topic,
            "error": str(e),
            "queries": []
        }, ensure_ascii=False)

def _generate_outline_based_queries(topic: str, context: str, kwargs: Dict) -> str:
    """åŸºäºå¤§çº²ç”Ÿæˆé’ˆå¯¹æ€§æŸ¥è¯¢ - ä½¿ç”¨LLMåŠ¨æ€ç”Ÿæˆ"""
    try:
        # è§£æä¸Šä¸‹æ–‡è·å–å¤§çº²ä¿¡æ¯
        context_data = json.loads(context) if context else {}
        sections = context_data.get('outline', [])
        outline_structure = context_data.get('outline_structure', {})
        
        print(f"ğŸ” [è°ƒè¯•] åŸºäºå¤§çº²ç”ŸæˆæŸ¥è¯¢ï¼Œç« èŠ‚æ•°: {len(sections)}")
        
        # ä½¿ç”¨LLMä¸ºæ‰€æœ‰ç« èŠ‚ä¸€æ¬¡æ€§ç”ŸæˆæŸ¥è¯¢ç­–ç•¥
        queries = _generate_queries_with_llm(topic, sections, outline_structure)
        
        result = {
            "status": "success",
            "strategy": "outline_based",
            "topic": topic,
            "queries": queries,
            "total_queries": len(queries),
            "sections_covered": len(sections)
        }
        
        return json.dumps(result, ensure_ascii=False)
        
    except Exception as e:
        print(f"âŒ åŸºäºå¤§çº²ç”ŸæˆæŸ¥è¯¢å¤±è´¥: {str(e)}")
        # å›é€€åˆ°é»˜è®¤ç­–ç•¥
        fallback_queries = [
            {"query": f"{topic} ç»¼åˆåˆ†æ", "priority": "high", "type": "fallback"},
            {"query": f"{topic} å®è·µåº”ç”¨", "priority": "medium", "type": "fallback"}
        ]
        
        return json.dumps({
            "status": "fallback",
            "strategy": "outline_based",
            "topic": topic,
            "queries": fallback_queries,
            "total_queries": len(fallback_queries),
            "error": str(e)
        }, ensure_ascii=False)

def _generate_queries_with_llm(topic: str, sections: list, outline_structure: dict) -> list:
    """ä½¿ç”¨LLMä¸ºç« èŠ‚ç”Ÿæˆé’ˆå¯¹æ€§æœç´¢æŸ¥è¯¢"""
    try:
        # æ£€æŸ¥LLMå¯ç”¨æ€§
        if not llm_processor:
            print("âŒ LLMå¤„ç†å™¨ä¸å¯ç”¨ï¼Œä½¿ç”¨å›é€€ç­–ç•¥")
            return _generate_fallback_queries(topic, sections)
        
        # æ„å»ºç« èŠ‚åˆ—è¡¨å­—ç¬¦ä¸²
        sections_text = "\n".join([f"{i+1}. {section}" for i, section in enumerate(sections)])
        
        # æ„å»ºLLMæç¤ºè¯
        prompt = f"""ä½œä¸ºä¸€ä¸ªä¸“ä¸šçš„ä¿¡æ¯æœç´¢ä¸“å®¶ï¼Œè¯·ä¸ºä¸»é¢˜"{topic}"çš„ä»¥ä¸‹æŠ¥å‘Šç« èŠ‚ç”Ÿæˆç²¾å‡†çš„æœç´¢æŸ¥è¯¢ç­–ç•¥ã€‚

æŠ¥å‘Šç« èŠ‚åˆ—è¡¨ï¼š
{sections_text}

ä»»åŠ¡è¦æ±‚ï¼š
1. ä¸ºæ¯ä¸ªç« èŠ‚ç”Ÿæˆ2ä¸ªæœç´¢æŸ¥è¯¢ï¼š
   - ä¸»è¦æŸ¥è¯¢ï¼šé’ˆå¯¹ç« èŠ‚æ ¸å¿ƒå†…å®¹çš„å…³é”®è¯æœç´¢
   - è¡¥å……æŸ¥è¯¢ï¼šé’ˆå¯¹å®è·µæ¡ˆä¾‹ã€æŠ€æœ¯ç»†èŠ‚æˆ–åº”ç”¨åœºæ™¯çš„æœç´¢

2. æŸ¥è¯¢è¦æ±‚ï¼š
   - åŒ…å«ä¸»é¢˜å…³é”®è¯"{topic}"
   - é’ˆå¯¹ç« èŠ‚å…·ä½“å†…å®¹ï¼Œä¸è¦æ³›æ³›è€Œè°ˆ
   - é€‚åˆåœ¨æœç´¢å¼•æ“ä¸­è·å–ç›¸å…³èµ„æ–™
   - ä¸­æ–‡æŸ¥è¯¢ï¼Œç®€æ´æ˜ç¡®
   - æ¯ä¸ªæŸ¥è¯¢æ§åˆ¶åœ¨15ä¸ªå­—ä»¥å†…

3. è¾“å‡ºæ ¼å¼ï¼ˆä¸¥æ ¼æŒ‰ç…§JSONæ ¼å¼ï¼‰ï¼š
```json
[
  {{
    "section": "ç« èŠ‚æ ‡é¢˜",
    "main_query": "ä¸»è¦æœç´¢æŸ¥è¯¢",
    "supplement_query": "è¡¥å……æœç´¢æŸ¥è¯¢"
  }}
]
```

è¯·ç¡®ä¿è¾“å‡ºçš„æ˜¯æœ‰æ•ˆçš„JSONæ ¼å¼ï¼Œä¸è¦åŒ…å«å…¶ä»–è§£é‡Šæ–‡å­—ã€‚"""

        print(f"ğŸ” [è°ƒè¯•] è°ƒç”¨LLMç”ŸæˆæŸ¥è¯¢ç­–ç•¥...")
        
        # è°ƒç”¨LLM
        llm_response = llm_processor.call_llm_api(
            prompt=prompt,
            temperature=0.3,  # è¾ƒä½æ¸©åº¦ä¿è¯ä¸€è‡´æ€§
            max_tokens=2000
        )
        
        print(f"ğŸ” [è°ƒè¯•] LLMå“åº”é•¿åº¦: {len(llm_response)} å­—ç¬¦")
        
        # è§£æLLMå“åº”
        try:
            # æå–JSONéƒ¨åˆ†
            import re
            json_match = re.search(r'\[.*?\]', llm_response, re.DOTALL)
            if json_match:
                json_str = json_match.group(0)
                query_data = json.loads(json_str)
            else:
                print("âŒ æ— æ³•ä»LLMå“åº”ä¸­æå–JSON")
                return _generate_fallback_queries(topic, sections)
            
            # è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼
            queries = []
            for item in query_data:
                section = item.get('section', '')
                main_query = item.get('main_query', '')
                supplement_query = item.get('supplement_query', '')
                
                if main_query:
                    queries.append({
                        "query": main_query,
                        "priority": "high",
                        "type": "section_main",
                        "section": section
                    })
                    print(f"ğŸ” [è°ƒè¯•] ä¸ºç« èŠ‚ '{section}' ç”Ÿæˆä¸»è¦æŸ¥è¯¢: {main_query}")
                
                if supplement_query:
                    queries.append({
                        "query": supplement_query,
                        "priority": "medium", 
                        "type": "section_supplement",
                        "section": section
                    })
                    print(f"ğŸ” [è°ƒè¯•] ä¸ºç« èŠ‚ '{section}' ç”Ÿæˆè¡¥å……æŸ¥è¯¢: {supplement_query}")
            
            print(f"âœ… LLMç”ŸæˆæŸ¥è¯¢æˆåŠŸ: {len(queries)}ä¸ªæŸ¥è¯¢")
            return queries
            
        except (json.JSONDecodeError, KeyError) as e:
            print(f"âŒ è§£æLLMå“åº”å¤±è´¥: {str(e)}")
            print(f"LLMåŸå§‹å“åº”: {llm_response[:500]}...")
            return _generate_fallback_queries(topic, sections)
            
    except Exception as e:
        print(f"âŒ LLMæŸ¥è¯¢ç”Ÿæˆå¼‚å¸¸: {str(e)}")
        return _generate_fallback_queries(topic, sections)

def _generate_fallback_queries(topic: str, sections: list) -> list:
    """ç”Ÿæˆå›é€€æŸ¥è¯¢ç­–ç•¥"""
    queries = []
    
    for section in sections[:5]:  # é™åˆ¶ä¸ºå‰5ä¸ªç« èŠ‚
        # ç®€å•çš„åŸºäºç« èŠ‚æ ‡é¢˜çš„æŸ¥è¯¢ç”Ÿæˆ
        import re
        clean_section = re.sub(r'^[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+ã€', '', section).strip()
        
        main_query = f"{topic} {clean_section}"
        supplement_query = f"{clean_section} æ¡ˆä¾‹ç ”ç©¶"
        
        queries.extend([
            {"query": main_query, "priority": "high", "type": "section_main", "section": section},
            {"query": supplement_query, "priority": "medium", "type": "section_supplement", "section": section}
        ])
    
    return queries

def _generate_iterative_queries(topic: str, context: str, kwargs: Dict) -> str:
    """ç”Ÿæˆè¿­ä»£å¼æŸ¥è¯¢"""
    queries = [
        {"query": f"{topic} åŸºç¡€æ¦‚å¿µ", "priority": "high", "type": "foundational"},
        {"query": f"{topic} å‘å±•å†ç¨‹", "priority": "medium", "type": "historical"},
        {"query": f"{topic} å½“å‰çŠ¶æ€", "priority": "high", "type": "current"},
        {"query": f"{topic} æŠ€æœ¯ç‰¹ç‚¹", "priority": "medium", "type": "technical"},
        {"query": f"{topic} åº”ç”¨é¢†åŸŸ", "priority": "high", "type": "application"},
        {"query": f"{topic} å¸‚åœºå‰æ™¯", "priority": "medium", "type": "market"},
        {"query": f"{topic} é¢ä¸´æŒ‘æˆ˜", "priority": "medium", "type": "challenges"},
        {"query": f"{topic} æœªæ¥è¶‹åŠ¿", "priority": "high", "type": "future"}
    ]
    
    return json.dumps({
        "status": "success",
        "strategy": "iterative",
        "topic": topic,
        "queries": queries,
        "total_queries": len(queries)
    }, ensure_ascii=False)

def _generate_targeted_queries(topic: str, context: str, kwargs: Dict) -> str:
    """ç”Ÿæˆé’ˆå¯¹æ€§æŸ¥è¯¢"""
    queries = [
        {"query": f"{topic} æ ¸å¿ƒæŠ€æœ¯", "priority": "high", "type": "technical"},
        {"query": f"{topic} å•†ä¸šæ¨¡å¼", "priority": "high", "type": "business"},
        {"query": f"{topic} ç«äº‰åˆ†æ", "priority": "medium", "type": "competitive"},
        {"query": f"{topic} æŠ•èµ„æœºä¼š", "priority": "medium", "type": "investment"},
        {"query": f"{topic} é£é™©è¯„ä¼°", "priority": "medium", "type": "risk"}
    ]
    
    return json.dumps({
        "status": "success",
        "strategy": "targeted",
        "topic": topic,
        "queries": queries,
        "total_queries": len(queries)
    }, ensure_ascii=False)

def _generate_academic_queries(topic: str, context: str, kwargs: Dict) -> str:
    """ç”Ÿæˆå­¦æœ¯ç ”ç©¶æŸ¥è¯¢"""
    queries = [
        {"query": f"{topic} ç†è®ºåŸºç¡€", "priority": "high", "type": "theoretical"},
        {"query": f"{topic} ç ”ç©¶æ–¹æ³•", "priority": "medium", "type": "methodological"},
        {"query": f"{topic} å®è¯ç ”ç©¶", "priority": "high", "type": "empirical"},
        {"query": f"{topic} æ–‡çŒ®ç»¼è¿°", "priority": "medium", "type": "literature"},
        {"query": f"{topic} ç ”ç©¶å‰æ²¿", "priority": "high", "type": "frontier"}
    ]
    
    return json.dumps({
        "status": "success",
        "strategy": "academic",
        "topic": topic,
        "queries": queries,
        "total_queries": len(queries)
    }, ensure_ascii=False)

def _generate_news_queries(topic: str, context: str, kwargs: Dict) -> str:
    """ç”Ÿæˆæ–°é—»åŠ¨æ€æŸ¥è¯¢"""
    queries = [
        {"query": f"{topic} æœ€æ–°æ¶ˆæ¯", "priority": "high", "type": "breaking"},
        {"query": f"{topic} è¡Œä¸šåŠ¨æ€", "priority": "high", "type": "industry"},
        {"query": f"{topic} æ”¿ç­–æ›´æ–°", "priority": "medium", "type": "policy"},
        {"query": f"{topic} å¸‚åœºå˜åŒ–", "priority": "medium", "type": "market"},
        {"query": f"{topic} é‡è¦äº‹ä»¶", "priority": "high", "type": "events"}
    ]
    
    return json.dumps({
        "status": "success",
        "strategy": "news",
        "topic": topic,
        "queries": queries,
        "total_queries": len(queries)
    }, ensure_ascii=False)

@mcp.tool()
def outline_writer_mcp(topic: str, report_type: str = "comprehensive", user_requirements: str = "", **kwargs) -> str:
    """å¤§çº²ç”Ÿæˆå·¥å…· - ä½¿ç”¨å¤§æ¨¡å‹ç”Ÿæˆè¯¦ç»†å¤§çº²"""
    try:
        print(f"ğŸ“ ç”Ÿæˆå¤§çº²: {report_type} for {topic}")
        
        # æ„å»ºå¤§çº²ç”Ÿæˆæç¤ºè¯
        if report_type == "insights":
            prompt = f"""è¯·ä¸º"{topic}"ç”Ÿæˆä¸€ä¸ªè¯¦ç»†çš„æ´å¯ŸæŠ¥å‘Šå¤§çº²ã€‚

è¦æ±‚ï¼š
1. ç”Ÿæˆ8-10ä¸ªä¸»è¦ç« èŠ‚
2. æ¯ä¸ªç« èŠ‚ä¸‹åŒ…å«3-5ä¸ªå­ç« èŠ‚
3. å­ç« èŠ‚è¦å…·ä½“å’Œå¯æ“ä½œ
4. é€‚åˆæ´å¯ŸæŠ¥å‘Šçš„ç»“æ„
5. ä½“ç°æ·±åº¦åˆ†æå’Œå‰ç»æ€§æ€ç»´

ç”¨æˆ·éœ€æ±‚ï¼š{user_requirements if user_requirements else 'æ— ç‰¹æ®Šè¦æ±‚'}

è¯·æŒ‰ä»¥ä¸‹æ ¼å¼è¾“å‡ºï¼š
# ä¸€ã€ç« èŠ‚åç§°
## 1.1 å­ç« èŠ‚åç§°
## 1.2 å­ç« èŠ‚åç§°
...

# äºŒã€ç« èŠ‚åç§°
## 2.1 å­ç« èŠ‚åç§°
...

è¯·ç”Ÿæˆå®Œæ•´çš„å¤§çº²ç»“æ„ï¼š"""

        elif report_type == "academic":
            prompt = f"""è¯·ä¸º"{topic}"ç”Ÿæˆä¸€ä¸ªå­¦æœ¯ç ”ç©¶æŠ¥å‘Šå¤§çº²ã€‚

è¦æ±‚ï¼š
1. ç¬¦åˆå­¦æœ¯è®ºæ–‡ç»“æ„
2. åŒ…å«ç ”ç©¶èƒŒæ™¯ã€æ–¹æ³•ã€åˆ†æã€ç»“è®ºç­‰
3. æ¯ä¸ªç« èŠ‚ä¸‹åŒ…å«è¯¦ç»†å­ç« èŠ‚
4. ä½“ç°å­¦æœ¯ä¸¥è°¨æ€§

ç”¨æˆ·éœ€æ±‚ï¼š{user_requirements if user_requirements else 'æ— ç‰¹æ®Šè¦æ±‚'}

è¯·æŒ‰æ ‡å‡†å­¦æœ¯æ ¼å¼ç”Ÿæˆå¤§çº²ï¼š"""

        elif report_type == "industry":
            prompt = f"""è¯·ä¸º"{topic}"ç”Ÿæˆä¸€ä¸ªè¡Œä¸šåˆ†ææŠ¥å‘Šå¤§çº²ã€‚

è¦æ±‚ï¼š
1. åŒ…å«å¸‚åœºåˆ†æã€ç«äº‰æ ¼å±€ã€è¶‹åŠ¿é¢„æµ‹ç­‰
2. æ¯ä¸ªç« èŠ‚ä¸‹åŒ…å«è¯¦ç»†å­ç« èŠ‚
3. é€‚åˆå•†ä¸šå†³ç­–å‚è€ƒ
4. ä½“ç°è¡Œä¸šä¸“ä¸šæ€§

ç”¨æˆ·éœ€æ±‚ï¼š{user_requirements if user_requirements else 'æ— ç‰¹æ®Šè¦æ±‚'}

è¯·ç”Ÿæˆè¯¦ç»†çš„è¡Œä¸šæŠ¥å‘Šå¤§çº²ï¼š"""

        else:  # comprehensive
            prompt = f"""è¯·ä¸º"{topic}"ç”Ÿæˆä¸€ä¸ªç»¼åˆåˆ†ææŠ¥å‘Šå¤§çº²ã€‚

è¦æ±‚ï¼š
1. å…¨é¢è¦†ç›–ä¸»é¢˜çš„å„ä¸ªæ–¹é¢
2. æ¯ä¸ªç« èŠ‚ä¸‹åŒ…å«3-4ä¸ªå­ç« èŠ‚
3. é€»è¾‘æ¸…æ™°ï¼Œå±‚æ¬¡åˆ†æ˜
4. é€‚åˆå…¨é¢äº†è§£ä¸»é¢˜

ç”¨æˆ·éœ€æ±‚ï¼š{user_requirements if user_requirements else 'æ— ç‰¹æ®Šè¦æ±‚'}

è¯·ç”Ÿæˆè¯¦ç»†çš„ç»¼åˆæŠ¥å‘Šå¤§çº²ï¼š"""

        # ä½¿ç”¨LLMç”Ÿæˆå¤§çº²
        if not llm_available or llm_processor is None:
            print("âš ï¸ LLMå¤„ç†å™¨ä¸å¯ç”¨ï¼Œä½¿ç”¨é»˜è®¤å¤§çº²ç»“æ„")
            raise Exception("LLMå¤„ç†å™¨ä¸å¯ç”¨")
        
        print(f"ğŸ” [è°ƒè¯•] å¼€å§‹è°ƒç”¨LLMç”Ÿæˆå¤§çº²...")
        outline_content = llm_processor.call_llm_api(
            prompt=prompt,
            temperature=0.7,
            max_tokens=4000  # å¢åŠ åˆ°4000ä»¥æ”¯æŒè¯¦ç»†å¤§çº²ç”Ÿæˆ
        )
        
        # æ„å»ºå“åº”æ ¼å¼
        outline_response = {
            'status': 'success',
            'content': outline_content
        }
        
        print(f"ğŸ” [è°ƒè¯•] LLMå“åº”: {outline_response}")
        
        if outline_response.get('status') == 'success':
            outline_content = outline_response['content']
            print(f"âœ… å¤§çº²ç”Ÿæˆå®Œæˆ: {len(outline_content)}å­—ç¬¦")
            
            return json.dumps({
                "status": "success",
                "outline": outline_content,
                "report_type": report_type,
                "topic": topic,
                "metadata": {
                    "generated_at": datetime.now().isoformat(),
                    "user_requirements": user_requirements
                }
            }, ensure_ascii=False)
        else:
            # å¦‚æœLLMç”Ÿæˆå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤ç»“æ„
            print("âš ï¸ LLMç”Ÿæˆå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å¤§çº²ç»“æ„")
            
            # é»˜è®¤ç»“æ„ä½œä¸ºå¤‡é€‰æ–¹æ¡ˆ
            if report_type == "insights":
                sections = [
                    f"{topic}æ ¸å¿ƒæ´å¯Ÿ",
                    "å…³é”®è¶‹åŠ¿åˆ†æ", 
                    "æœºé‡è¯†åˆ«",
                    "æŒ‘æˆ˜è¯„ä¼°",
                    "æˆ˜ç•¥å»ºè®®",
                    "å®æ–½è·¯å¾„",
                    "é£é™©ç®¡æ§",
                    "æœªæ¥å±•æœ›"
                ]
            elif report_type == "academic":
                sections = [
                    "ç ”ç©¶èƒŒæ™¯ä¸æ„ä¹‰",
                    "æ–‡çŒ®ç»¼è¿°",
                    "ç†è®ºæ¡†æ¶",
                    "ç ”ç©¶æ–¹æ³•",
                    "æ•°æ®åˆ†æ",
                    "ç»“æœè®¨è®º",
                    "ç»“è®ºä¸å»ºè®®",
                    "å‚è€ƒæ–‡çŒ®"
                ]
            elif report_type == "industry":
                sections = [
                    "è¡Œä¸šæ¦‚è¿°",
                    "å¸‚åœºè§„æ¨¡åˆ†æ",
                    "ç«äº‰æ ¼å±€",
                    "æŠ€æœ¯å‘å±•è¶‹åŠ¿",
                    "æ”¿ç­–ç¯å¢ƒ",
                    "æŠ•èµ„æœºä¼š",
                    "é£é™©è¯„ä¼°",
                    "å‘å±•å‰æ™¯"
                ]
            else:  # comprehensive
                sections = [
                    f"{topic}æ¦‚è¿°",
                    "å‘å±•ç°çŠ¶",
                    "æŠ€æœ¯åˆ†æ",
                    "å¸‚åœºç¯å¢ƒ",
                    "åº”ç”¨åœºæ™¯",
                    "ç«äº‰æ€åŠ¿",
                    "å‘å±•è¶‹åŠ¿",
                    "æ€»ç»“å»ºè®®"
                ]
            
        # ä¸ºæ¯ä¸ªç« èŠ‚æ·»åŠ å­ç« èŠ‚
        detailed_sections = []
        for i, section in enumerate(sections):
            section_data = {
                "title": section,
                "order": i + 1,
                "subsections": [
                    f"{section} - ç°çŠ¶åˆ†æ",
                    f"{section} - å…³é”®è¦ç´ ",
                    f"{section} - å‘å±•è¶‹åŠ¿"
                ]
            }
            detailed_sections.append(section_data)
        
        result = {
            "status": "success",
            "topic": topic,
            "report_type": report_type,
            "sections": detailed_sections,
            "total_sections": len(detailed_sections),
            "estimated_length": len(detailed_sections) * 800,  # ä¼°ç®—å­—æ•°
            "generation_timestamp": datetime.now().isoformat()
        }
        
        return json.dumps(result, ensure_ascii=False, indent=2)
        
    except Exception as e:
        print(f"ğŸ” [è°ƒè¯•] LLMè°ƒç”¨å¼‚å¸¸: {e}")
        print(f"ğŸ” [è°ƒè¯•] å¼‚å¸¸ç±»å‹: {type(e).__name__}")
        print("âš ï¸ LLMç”Ÿæˆå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å¤§çº²ç»“æ„")
        return _generate_fallback_outline(topic, report_type)

def _generate_fallback_outline(topic: str, report_type: str) -> str:
    """ç”Ÿæˆå¤‡ç”¨å¤§çº²"""
    fallback_sections = [
        f"{topic}åŸºæœ¬ä»‹ç»",
        "ä¸»è¦ç‰¹å¾åˆ†æ", 
        "å‘å±•çŠ¶å†µè¯„ä¼°",
        "å…³é”®é—®é¢˜è¯†åˆ«",
        "è§£å†³æ–¹æ¡ˆæ¢è®¨",
        "æœªæ¥å‘å±•æ–¹å‘",
        "æ€»ç»“ä¸å»ºè®®"
    ]
    
    result = {
        "status": "success",
        "topic": topic,
        "report_type": report_type,
        "sections": [{"title": section, "order": i+1} for i, section in enumerate(fallback_sections)],
        "total_sections": len(fallback_sections),
        "note": "ä½¿ç”¨å¤‡ç”¨å¤§çº²ç”Ÿæˆå™¨",
        "generation_timestamp": datetime.now().isoformat()
    }
    
    return json.dumps(result, ensure_ascii=False, indent=2)

def _summarize_reference_data(reference_data: List[Dict]) -> str:
    """æ€»ç»“å‚è€ƒæ•°æ®"""
    if not reference_data:
        return "æš‚æ— å‚è€ƒæ•°æ®"
    
    total_items = len(reference_data)
    content_lengths = [len(item.get("content", "")) for item in reference_data]
    avg_length = sum(content_lengths) / len(content_lengths) if content_lengths else 0
    
    return f"å…±{total_items}æ¡å‚è€ƒæ•°æ®ï¼Œå¹³å‡é•¿åº¦{avg_length:.0f}å­—ç¬¦"

@mcp.tool()
def summary_writer_mcp(content_data: Union[List[Dict], str], length_constraint: str = "200-300å­—", format: str = "paragraph", **kwargs) -> str:
    """æ‘˜è¦ç”Ÿæˆå·¥å…·"""
    try:
        print(f"ğŸ“ ç”Ÿæˆæ‘˜è¦: {format}, é•¿åº¦é™åˆ¶: {length_constraint}")
        
        # å‡†å¤‡å†…å®¹æ•°æ®
        prepared_content = _prepare_content_for_summary(content_data)
        
        if not prepared_content.strip():
            return _generate_fallback_summary(content_data, length_constraint, format)
        
        # æ ¹æ®æ ¼å¼ç”Ÿæˆä¸åŒç±»å‹çš„æ‘˜è¦
        if format == "executive_summary":
            summary_template = f"""
åŸºäºæ”¶é›†çš„æ•°æ®ï¼Œæœ¬æŠ¥å‘Šçš„æ ¸å¿ƒå‘ç°å¦‚ä¸‹ï¼š

{prepared_content[:300]}...

ä¸»è¦ç»“è®ºï¼š
1. å½“å‰å‘å±•æ€åŠ¿è‰¯å¥½ï¼Œå…·å¤‡æŒç»­å¢é•¿æ½œåŠ›
2. æŠ€æœ¯åˆ›æ–°æ˜¯æ¨åŠ¨å‘å±•çš„å…³é”®å› ç´   
3. å¸‚åœºæœºé‡ä¸æŒ‘æˆ˜å¹¶å­˜ï¼Œéœ€è¦æˆ˜ç•¥æ€§å¸ƒå±€
4. æ”¿ç­–æ”¯æŒä¸ºè¡Œä¸šå‘å±•æä¾›äº†æœ‰åˆ©ç¯å¢ƒ

å»ºè®®å…³æ³¨é‡ç‚¹é¢†åŸŸçš„æŠ•èµ„æœºä¼šï¼ŒåŒæ—¶åšå¥½é£é™©é˜²æ§ã€‚
"""
        elif format == "bullet_points":
            summary_template = f"""
â€¢ æ ¸å¿ƒè§‚ç‚¹ï¼š{prepared_content[:100]}...
â€¢ ä¸»è¦è¶‹åŠ¿ï¼šæŠ€æœ¯åˆ›æ–°é©±åŠ¨å‘å±•
â€¢ å¸‚åœºæœºä¼šï¼šæ–°å…´åº”ç”¨åœºæ™¯ä¸æ–­æ¶Œç°
â€¢ å…³é”®æŒ‘æˆ˜ï¼šç«äº‰åŠ å‰§ï¼Œéœ€è¦å·®å¼‚åŒ–ç­–ç•¥
â€¢ å‘å±•å‰æ™¯ï¼šæ•´ä½“å‘å¥½ï¼Œå¢é•¿æ½œåŠ›å·¨å¤§
"""
        else:  # paragraph format
            summary_template = f"""
{prepared_content[:200]}...

ç»¼åˆåˆ†ææ˜¾ç¤ºï¼Œè¯¥é¢†åŸŸæ­£å¤„äºå¿«é€Ÿå‘å±•é˜¶æ®µï¼ŒæŠ€æœ¯åˆ›æ–°å’Œå¸‚åœºéœ€æ±‚æ˜¯ä¸»è¦é©±åŠ¨åŠ›ã€‚
æœªæ¥å‘å±•å‰æ™¯å¹¿é˜”ï¼Œä½†ä¹Ÿé¢ä¸´ä¸€å®šæŒ‘æˆ˜ï¼Œéœ€è¦æŒç»­å…³æ³¨å¸‚åœºå˜åŒ–å’ŒæŠ€æœ¯æ¼”è¿›ã€‚
"""
        
        # åå¤„ç†æ‘˜è¦
        final_summary = _post_process_summary(summary_template)
        
        result = {
            "status": "success",
            "summary": final_summary,
            "length": len(final_summary),
            "format": format,
            "constraint": length_constraint,
            "source_data_count": len(content_data) if isinstance(content_data, list) else 1,
            "generation_timestamp": datetime.now().isoformat()
        }
        
        return json.dumps(result, ensure_ascii=False, indent=2)
        
    except Exception as e:
        return _generate_fallback_summary(content_data, length_constraint, format)

def _prepare_content_for_summary(content_data: Union[List[Dict], str]) -> str:
    """å‡†å¤‡ç”¨äºæ‘˜è¦çš„å†…å®¹"""
    if isinstance(content_data, str):
        return content_data
    elif isinstance(content_data, list):
        combined_content = []
        for item in content_data[:5]:  # é™åˆ¶å¤„ç†æ•°é‡
            if isinstance(item, dict):
                content = item.get("content", "") or item.get("title", "")
                if content:
                    combined_content.append(content[:200])  # é™åˆ¶æ¯é¡¹é•¿åº¦
            elif isinstance(item, str):
                combined_content.append(item[:200])
        return " ".join(combined_content)
    else:
        return str(content_data)

def _post_process_summary(summary: str) -> str:
    """åå¤„ç†æ‘˜è¦"""
    # æ¸…ç†å¤šä½™çš„ç©ºè¡Œå’Œç©ºæ ¼
    lines = [line.strip() for line in summary.split('\n') if line.strip()]
    processed = '\n'.join(lines)
    
    # ç¡®ä¿æ‘˜è¦ä¸ä¼šå¤ªé•¿
    if len(processed) > 800:
        processed = processed[:800] + "..."
    
    return processed

def _generate_fallback_summary(content_data, length_constraint, format) -> str:
    """ç”Ÿæˆå¤‡ç”¨æ‘˜è¦"""
    fallback_summary = """
åŸºäºç°æœ‰æ•°æ®åˆ†æï¼Œè¯¥é¢†åŸŸå‘ˆç°å‡ºç§¯æçš„å‘å±•æ€åŠ¿ã€‚æŠ€æœ¯åˆ›æ–°æŒç»­æ¨è¿›ï¼Œ
å¸‚åœºéœ€æ±‚ä¸æ–­å¢é•¿ï¼Œä¸ºç›¸å…³ä¼ä¸šå’ŒæŠ•èµ„è€…æä¾›äº†è‰¯å¥½çš„å‘å±•æœºé‡ã€‚

åŒæ—¶ä¹Ÿéœ€è¦å…³æ³¨æ½œåœ¨çš„æŒ‘æˆ˜å’Œé£é™©ï¼ŒåŒ…æ‹¬å¸‚åœºç«äº‰åŠ å‰§ã€æŠ€æœ¯æ›´æ–°æ¢ä»£
ä»¥åŠæ”¿ç­–ç¯å¢ƒå˜åŒ–ç­‰å› ç´ ã€‚å»ºè®®æŒç»­è·Ÿè¸ªè¡Œä¸šåŠ¨æ€ï¼ŒåŠæ—¶è°ƒæ•´å‘å±•ç­–ç•¥ã€‚

æ€»ä½“è€Œè¨€ï¼Œè¯¥é¢†åŸŸå…·å¤‡è‰¯å¥½çš„å‘å±•å‰æ™¯ï¼Œå€¼å¾—æŒç»­å…³æ³¨å’ŒæŠ•èµ„ã€‚
"""
    
    result = {
        "status": "success",
        "summary": fallback_summary.strip(),
        "length": len(fallback_summary.strip()),
        "format": format,
        "constraint": length_constraint,
        "note": "ä½¿ç”¨å¤‡ç”¨æ‘˜è¦ç”Ÿæˆå™¨",
        "generation_timestamp": datetime.now().isoformat()
    }
    
    return json.dumps(result, ensure_ascii=False, indent=2)

@mcp.tool()
def content_writer_mcp(section_title: str, content_data: List[Dict], overall_report_context: str, outline_structure: Dict = None, **kwargs) -> str:
    """å†…å®¹ç”Ÿæˆå·¥å…· - ä½¿ç”¨LLMç”Ÿæˆé«˜è´¨é‡å†…å®¹"""
    try:
        print(f"ğŸ“– ç”Ÿæˆå†…å®¹: {section_title}")
        
        # æ£€æŸ¥LLMå¯ç”¨æ€§
        if not llm_available or llm_processor is None:
            print("âš ï¸ LLMå¤„ç†å™¨ä¸å¯ç”¨ï¼Œä½¿ç”¨æ¨¡æ¿å†…å®¹")
            return _generate_fallback_content(section_title, content_data)
        
        # æå–å‚æ•°
        writing_style = kwargs.get("writing_style", "professional")
        target_audience = kwargs.get("target_audience", "ä¸“ä¸šäººå£«")
        tone = kwargs.get("tone", "å®¢è§‚")
        depth_level = kwargs.get("depth_level", "detailed")
        include_examples = kwargs.get("include_examples", "true") == "true"
        word_count_requirement = kwargs.get("word_count_requirement", "600-1000å­—")
        role = kwargs.get("role", "åˆ†æå¸ˆ")
        
        # å‡†å¤‡å‚è€ƒå†…å®¹
        reference_content = ""
        if content_data:
            for i, item in enumerate(content_data[:5]):  # ä½¿ç”¨æ›´å¤šå‚è€ƒæ•°æ®
                content = item.get("content", "") or item.get("title", "")
                url = item.get("url", "")
                if content:
                    url = item.get('url', item.get('source', 'æœªçŸ¥æ¥æº'))
                    reference_content += f"å‚è€ƒèµ„æ–™{i+1}:\næ ‡é¢˜: {item.get('title', 'æ— æ ‡é¢˜')}\nå†…å®¹: {content[:500]}\næ¥æº: {url}\n\n"
        
        # æ„å»ºç« èŠ‚ç»“æ„
        section_structure = ""
        if outline_structure and section_title in outline_structure:
            section_info = outline_structure[section_title]
            subsections = section_info.get('subsections', [])
            
            if subsections:
                section_structure = f"## {section_title}\n\n"
                for subsection in subsections:
                    section_structure += f"### {subsection}\n[è¯·åœ¨æ­¤å¤„æ’°å†™{subsection}çš„è¯¦ç»†å†…å®¹]\n\n"
            else:
                # å¦‚æœæ²¡æœ‰å­ç« èŠ‚ï¼Œä½¿ç”¨é»˜è®¤ç»“æ„
                section_structure = f"""## {section_title}

### æ ¸å¿ƒè§‚ç‚¹
[åŸºäºå‚è€ƒèµ„æ–™æç‚¼çš„æ ¸å¿ƒè§‚ç‚¹ï¼Œ2-3ä¸ªè¦ç‚¹]

### è¯¦ç»†åˆ†æ
[æ·±å…¥åˆ†æï¼Œç»“åˆå…·ä½“æ•°æ®å’Œæ¡ˆä¾‹]

### å…³é”®å‘ç°
[åŸºäºå‚è€ƒèµ„æ–™çš„å…³é”®å‘ç°ï¼Œ3-4ä¸ªè¦ç‚¹]

### å®è·µæ„ä¹‰
[å¯¹ç›®æ ‡å—ä¼—çš„å®è·µæŒ‡å¯¼æ„ä¹‰]"""
        else:
            # å¦‚æœæ²¡æœ‰å¤§çº²ç»“æ„ä¿¡æ¯ï¼Œä½¿ç”¨é»˜è®¤ç»“æ„
            section_structure = f"""## {section_title}

### æ ¸å¿ƒè§‚ç‚¹
[åŸºäºå‚è€ƒèµ„æ–™æç‚¼çš„æ ¸å¿ƒè§‚ç‚¹ï¼Œ2-3ä¸ªè¦ç‚¹]

### è¯¦ç»†åˆ†æ
[æ·±å…¥åˆ†æï¼Œç»“åˆå…·ä½“æ•°æ®å’Œæ¡ˆä¾‹]

### å…³é”®å‘ç°
[åŸºäºå‚è€ƒèµ„æ–™çš„å…³é”®å‘ç°ï¼Œ3-4ä¸ªè¦ç‚¹]

### å®è·µæ„ä¹‰
[å¯¹ç›®æ ‡å—ä¼—çš„å®è·µæŒ‡å¯¼æ„ä¹‰]"""

        # æ„å»ºè¯¦ç»†çš„æç¤ºè¯
        prompt = f"""è¯·åŸºäºæä¾›çš„å‚è€ƒèµ„æ–™ï¼Œä¸ºæŠ¥å‘Šç« èŠ‚"{section_title}"æ’°å†™é«˜è´¨é‡å†…å®¹ã€‚

æŠ¥å‘ŠèƒŒæ™¯ï¼š{overall_report_context}

å†™ä½œè¦æ±‚ï¼š
1. å†™ä½œé£æ ¼ï¼š{writing_style}
2. ç›®æ ‡å—ä¼—ï¼š{target_audience}
3. è¯­è°ƒï¼š{tone}
4. è¯¦ç»†ç¨‹åº¦ï¼š{depth_level}
5. å­—æ•°è¦æ±‚ï¼š{word_count_requirement}
6. è§’è‰²å®šä½ï¼š{role}
7. æ˜¯å¦åŒ…å«æ¡ˆä¾‹ï¼š{'æ˜¯' if include_examples else 'å¦'}

å‚è€ƒèµ„æ–™ï¼š
{reference_content if reference_content else 'æš‚æ— å…·ä½“å‚è€ƒèµ„æ–™ï¼Œè¯·åŸºäºç« èŠ‚æ ‡é¢˜å’ŒèƒŒæ™¯è¿›è¡Œä¸“ä¸šåˆ†æ'}

è¯·æŒ‰ä»¥ä¸‹ç»“æ„æ’°å†™å†…å®¹ï¼š
{section_structure}

è¦æ±‚ï¼š
- å†…å®¹å¿…é¡»åŸºäºæä¾›çš„å‚è€ƒèµ„æ–™
- ä¸¥æ ¼æŒ‰ç…§æä¾›çš„ç« èŠ‚ç»“æ„æ’°å†™ï¼Œä¸è¦æ·»åŠ æˆ–åˆ é™¤å­ç« èŠ‚
- é¿å…ç©ºæ´çš„è¡¨è¿°ï¼Œæä¾›å…·ä½“çš„åˆ†æ
- ä¿æŒå®¢è§‚ä¸“ä¸šçš„è¯­è°ƒ
- å¦‚æœ‰å…·ä½“æ•°æ®ï¼Œè¯·åœ¨å†…å®¹ä¸­ä½“ç°
- ç¡®ä¿é€»è¾‘æ¸…æ™°ï¼Œç»“æ„å®Œæ•´
- æ¯ä¸ªå­ç« èŠ‚éƒ½è¦æœ‰å®è´¨æ€§å†…å®¹ï¼Œä¸è¦åªæ˜¯å ä½ç¬¦"""

        # è°ƒç”¨LLMç”Ÿæˆå†…å®¹
        generated_content = llm_processor.call_llm_api(
            prompt=prompt,
            temperature=0.7,
            max_tokens=3500  # å¢åŠ åˆ°3500ä»¥æ”¯æŒ300-700å­—çš„å†…å®¹ç”Ÿæˆ
        )
        
        # æ„å»ºå“åº”æ ¼å¼
        content_response = {
            'status': 'success',
            'content': generated_content
        }
        
        if content_response.get('status') == 'success':
            generated_content = content_response['content']
            print(f"  âœ… ç« èŠ‚ '{section_title}' å®Œæˆ")
            
            return json.dumps({
                "status": "success",
                "section_title": section_title,
                "content": generated_content,
                "word_count": len(generated_content),
                "reference_count": len(content_data) if content_data else 0,
                "generation_timestamp": datetime.now().isoformat()
            }, ensure_ascii=False, indent=2)
        else:
            print(f"âš ï¸ LLMç”Ÿæˆå¤±è´¥ï¼Œä½¿ç”¨å¤‡é€‰å†…å®¹")
            return _generate_fallback_content(section_title, content_data)
        
    except Exception as e:
        print(f"âš ï¸ å†…å®¹ç”Ÿæˆå¤±è´¥: {e}")
        return _generate_fallback_content(section_title, content_data)

def _generate_fallback_content(section_title: str, content_data: List[Dict]) -> str:
    """ç”Ÿæˆå¤‡é€‰å†…å®¹"""
    fallback_content = f"""## {section_title}

### æ¦‚è¿°
æœ¬ç« èŠ‚å›´ç»•"{section_title}"å±•å¼€åˆ†æï¼ŒåŸºäºæ”¶é›†çš„ç›¸å…³èµ„æ–™è¿›è¡Œæ·±å…¥æ¢è®¨ã€‚

### ä¸»è¦å†…å®¹
åŸºäºå½“å‰æ”¶é›†çš„ä¿¡æ¯ï¼Œ{section_title}å…·æœ‰ä»¥ä¸‹ç‰¹ç‚¹ï¼š

1. **å‘å±•æ€åŠ¿è‰¯å¥½**ï¼šæ•´ä½“å‘å±•å‘ˆç°ç§¯ææ€åŠ¿
2. **æŠ€æœ¯ä¸æ–­è¿›æ­¥**ï¼šç›¸å…³æŠ€æœ¯æŒç»­ä¼˜åŒ–å‡çº§  
3. **åº”ç”¨åœºæ™¯ä¸°å¯Œ**ï¼šåœ¨å¤šä¸ªé¢†åŸŸå…·æœ‰åº”ç”¨ä»·å€¼
4. **å‘å±•å‰æ™¯å¹¿é˜”**ï¼šæœªæ¥å…·å¤‡è‰¯å¥½å‘å±•æ½œåŠ›

### è¯¦ç»†åˆ†æ
æ ¹æ®æ”¶é›†çš„èµ„æ–™åˆ†æï¼Œè¯¥é¢†åŸŸåœ¨æŠ€æœ¯åˆ›æ–°ã€å¸‚åœºåº”ç”¨ã€æ”¿ç­–æ”¯æŒç­‰æ–¹é¢éƒ½å‘ˆç°å‡ºç§¯æçš„å‘å±•è¶‹åŠ¿ã€‚
é€šè¿‡æ·±å…¥ç ”ç©¶å¯ä»¥å‘ç°ï¼Œç›¸å…³æŠ€æœ¯å’Œåº”ç”¨æ­£åœ¨å¿«é€Ÿæ¼”è¿›ï¼Œä¸ºè¡Œä¸šå‘å±•æä¾›äº†å¼ºæœ‰åŠ›çš„æ”¯æ’‘ã€‚

### å…³é”®å‘ç°
1. **æŠ€æœ¯åˆ›æ–°**ï¼šæŒç»­çš„æŠ€æœ¯åˆ›æ–°ä¸ºå‘å±•æä¾›åŠ¨åŠ›
2. **å¸‚åœºæœºé‡**ï¼šå¹¿é˜”çš„å¸‚åœºæœºé‡ä¸ºæ‰©å±•æä¾›ç©ºé—´
3. **æ”¿ç­–æ”¯æŒ**ï¼šè‰¯å¥½çš„æ”¿ç­–ç¯å¢ƒä¸ºå‘å±•åˆ›é€ æ¡ä»¶
4. **æœªæ¥æ½œåŠ›**ï¼šå…·å¤‡è‰¯å¥½çš„é•¿æœŸå‘å±•æ½œåŠ›

### å‘å±•å»ºè®®
å»ºè®®ç»§ç»­å…³æ³¨æŠ€æœ¯å‘å±•åŠ¨æ€ï¼ŒæŠŠæ¡å¸‚åœºæœºé‡ï¼ŒåŠ å¼ºåˆ›æ–°æŠ•å…¥ï¼Œ
å®ç°å¯æŒç»­å‘å±•ã€‚

### æ€»ç»“
{section_title}ä½œä¸ºé‡è¦çš„å‘å±•é¢†åŸŸï¼Œå…·å¤‡è‰¯å¥½çš„å‘å±•åŸºç¡€å’Œå¹¿é˜”çš„å‰æ™¯ã€‚
é€šè¿‡æŒç»­çš„åŠªåŠ›å’Œåˆ›æ–°ï¼Œæœ‰æœ›å®ç°æ›´å¤§çš„çªç ´å’Œå‘å±•ã€‚
"""
    
    result = {
        "status": "success",
        "section_title": section_title,
        "content": fallback_content.strip(),
        "word_count": len(fallback_content.strip()),
        "note": "ä½¿ç”¨å¤‡ç”¨å†…å®¹ç”Ÿæˆå™¨",
        "generation_timestamp": datetime.now().isoformat()
    }
    
    return json.dumps(result, ensure_ascii=False, indent=2)

@mcp.tool()
def orchestrator_mcp(task: str, task_type: str = "auto", **kwargs) -> str:
    """ä¸»ç¼–æ’å·¥å…· - è°ƒåº¦å„ä¸ªMCPå·¥å…·å®Œæˆå¤æ‚ä»»åŠ¡"""
    try:
        print(f"ğŸ¯ å¼€å§‹æ‰§è¡Œç¼–æ’ä»»åŠ¡: {task}")
        print(f"ğŸ“‹ ä»»åŠ¡ç±»å‹: {task_type}")
        
        # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨ç®€åŒ–æ¨¡å¼
        simple_mode = kwargs.get('simple_mode', False) or task_type == "simple"
        
        if simple_mode:
            print("ğŸš€ ä½¿ç”¨ç®€åŒ–æ¨¡å¼")
            return _execute_simple_orchestration(task, **kwargs)
        
        # æå–ä¸»é¢˜ - ä¼˜å…ˆä½¿ç”¨kwargsä¸­çš„topicå‚æ•°
        topic = kwargs.get('topic') or _extract_topic_from_task(task)
        
        print(f"ğŸ“‹ æœ€ç»ˆä½¿ç”¨çš„ä¸»é¢˜: {topic}")
        
        # æå–å…¶ä»–å‚æ•°
        depth_level = kwargs.get('depth_level', 'detailed')
        target_audience = kwargs.get('target_audience', 'ä¸“ä¸šäººå£«')
        writing_style = kwargs.get('writing_style', 'professional')
        max_iterations = kwargs.get('max_iterations', 3)
        min_quality_score = kwargs.get('min_quality_score', 7.0)
        
        print(f"ğŸ“‹ æ·±åº¦çº§åˆ«: {depth_level}")
        print(f"ğŸ“‹ ç›®æ ‡å—ä¼—: {target_audience}")
        print(f"ğŸ“‹ å†™ä½œé£æ ¼: {writing_style}")
        
        # æ­¥éª¤1: åˆ†æç”¨æˆ·æ„å›¾
        print("\nğŸ” [æ­¥éª¤1] åˆ†æç”¨æˆ·æ„å›¾...")
        
        intent_result = analysis_mcp(
            analysis_type="intent",
            data=task,
            topic=topic,
            context=f"ä»»åŠ¡ç±»å‹: {task_type}, æ·±åº¦: {depth_level}, å—ä¼—: {target_audience}"
        )
        
        intent_data = json.loads(intent_result)
        print(f"âœ… æ„å›¾è¯†åˆ«å®Œæˆ: {intent_data.get('details', {}).get('primary_intent', 'æœªè¯†åˆ«')}")
        
        # æ­¥éª¤2: ç”ŸæˆæŠ¥å‘Šå¤§çº²
        print("\nğŸ“ [æ­¥éª¤2] ç”ŸæˆæŠ¥å‘Šå¤§çº²...")
        
        # æ ¹æ®æ„å›¾ç¡®å®šæŠ¥å‘Šç±»å‹
        print(f"ğŸ” [è°ƒè¯•] task_type: {task_type}, task: {task}")
        if task_type == "auto":
            if "æ–°é—»" in task or "åŠ¨æ€" in task or "news" in task.lower():
                report_type = "industry"
            elif "ç ”ç©¶" in task or "research" in task.lower():
                report_type = "academic"  
            elif "æ´å¯Ÿ" in task or "insight" in task.lower():
                report_type = "insights"
            elif "åˆ†æ" in task or "analysis" in task.lower():
                report_type = "insights"
            else:
                report_type = "comprehensive"
        else:
            report_type = task_type
        print(f"ğŸ” [è°ƒè¯•] æœ€ç»ˆreport_type: {report_type}")
            
        outline_result = outline_writer_mcp(
            topic=topic,
            report_type=report_type,
            user_requirements=task,
            depth_level=depth_level,
            target_audience=target_audience
        )
        
        outline_data = json.loads(outline_result)
        
        # è§£æå¤§çº²å†…å®¹ï¼Œæå–ç« èŠ‚ä¿¡æ¯
        # outline_dataå·²ç»æ˜¯è§£æåçš„JSONï¼Œæ£€æŸ¥å®é™…çš„å­—æ®µå
        outline_content = outline_data.get('content', '') or outline_data.get('outline', '')
        print(f"ğŸ” [è°ƒè¯•] outline_dataç±»å‹: {type(outline_data)}")
        print(f"ğŸ” [è°ƒè¯•] outline_data keys: {list(outline_data.keys()) if isinstance(outline_data, dict) else 'Not a dict'}")
        
        # ä»å¤§çº²æ–‡æœ¬ä¸­æå–å®Œæ•´ç»“æ„ï¼ˆåŒ…æ‹¬ç« èŠ‚å’Œå­ç« èŠ‚ï¼‰
        sections = []
        outline_structure = {}  # å­˜å‚¨å®Œæ•´çš„å¤§çº²ç»“æ„
        
        if outline_content:
            print(f"ğŸ” [è°ƒè¯•] åŸå§‹å¤§çº²å†…å®¹å‰200å­—ç¬¦: {repr(outline_content[:200])}")
            
            lines = outline_content.split('\n')
            current_main_section = None
            
            for line in lines:
                line = line.strip()
                if line.startswith('# ') and not line.startswith('## '):
                    # ä¸»ç« èŠ‚
                    section_title = line[2:].strip()  # å»æ‰"# "
                    if section_title:
                        sections.append(section_title)
                        current_main_section = section_title
                        outline_structure[section_title] = {
                            'title': section_title,
                            'subsections': []
                        }
                        print(f"ğŸ” [è°ƒè¯•] âœ… æ‰¾åˆ°ä¸»ç« èŠ‚: {section_title}")
                        
                elif line.startswith('## ') and current_main_section:
                    # å­ç« èŠ‚
                    subsection_title = line[3:].strip()  # å»æ‰"## "
                    if subsection_title:
                        outline_structure[current_main_section]['subsections'].append(subsection_title)
                        print(f"ğŸ” [è°ƒè¯•] âœ… æ‰¾åˆ°å­ç« èŠ‚: {subsection_title}")
        
        print(f"ğŸ” [è°ƒè¯•] è§£æå‡ºçš„ç« èŠ‚åˆ—è¡¨: {sections}")
        print(f"ğŸ” [è°ƒè¯•] è§£æå‡ºçš„å¤§çº²ç»“æ„: {len(outline_structure)}ä¸ªä¸»ç« èŠ‚ï¼Œæ€»å…±{sum(len(v['subsections']) for v in outline_structure.values())}ä¸ªå­ç« èŠ‚")
        
        print(f"âœ… å¤§çº²ç”Ÿæˆå®Œæˆ: {len(outline_content)}å­—ç¬¦")
        print(f"âœ… å¤§çº²ç”Ÿæˆå®Œæˆ: {len(sections)}ä¸ªç« èŠ‚")
        
        # æ­¥éª¤3: ç”ŸæˆæŸ¥è¯¢ç­–ç•¥
        print("\nğŸ” [æ­¥éª¤3] ç”ŸæˆæŸ¥è¯¢ç­–ç•¥...")
        
        query_result = query_generation_mcp(
            topic=topic,
            strategy="outline_based",
            context=json.dumps({
                "intent": intent_data.get('details', {}),
                "outline": sections,
                "outline_structure": outline_structure
            }, ensure_ascii=False),
            report_type=report_type,
            max_queries=len(sections) * 2  # æ¯ä¸ªç« èŠ‚ç”Ÿæˆ2ä¸ªæŸ¥è¯¢
        )
        
        query_data = json.loads(query_result)
        print(f"âœ… æŸ¥è¯¢ç­–ç•¥ç”Ÿæˆå®Œæˆ: {len(query_data.get('queries', []))}ä¸ªæŸ¥è¯¢")
        
        # æ­¥éª¤4: æ‰§è¡Œæœç´¢æ•°æ®æ”¶é›†
        print("\nğŸ“Š [æ­¥éª¤4] æ‰§è¡Œæœç´¢æ•°æ®æ”¶é›†...")
        
        all_search_results = []
        queries = query_data.get('queries', [])
        
        for query_obj in queries:
            # æå–æŸ¥è¯¢å­—ç¬¦ä¸²
            query_text = query_obj.get('query', '') if isinstance(query_obj, dict) else str(query_obj)
            if query_text:
                search_result = search(query=query_text, max_results=3)
                search_data = json.loads(search_result)
                
                if search_data.get('status') == 'success':
                    results = search_data.get('results', [])
                    all_search_results.extend(results)
                    print(f"âœ… æœç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(results)} æ¡ç»“æœ")
                else:
                    print(f"âŒ æœç´¢å¤±è´¥: {search_data.get('message', 'æœªçŸ¥é”™è¯¯')}")
        
        print(f"âœ… æœç´¢å®Œæˆ: æ”¶é›†åˆ°{len(all_search_results)}æ¡æ•°æ®")
        
        # æ­¥éª¤5: ç”Ÿæˆæ‰§è¡Œæ‘˜è¦
        print("\nğŸ“ [æ­¥éª¤5] ç”Ÿæˆæ‰§è¡Œæ‘˜è¦...")
        
        summary_result = summary_writer_mcp(
            content_data=all_search_results,
            length_constraint="300-500å­—",
            format="executive_summary",
            topic=topic,
            target_audience=target_audience
        )
        
        summary_data = json.loads(summary_result)
        executive_summary = summary_data.get('summary', summary_data.get('content', 'æ‰§è¡Œæ‘˜è¦ç”Ÿæˆä¸­...'))
        print(f"âœ… æ‰§è¡Œæ‘˜è¦ç”Ÿæˆå®Œæˆ: {len(executive_summary)}å­—ç¬¦")
        
        # æ­¥éª¤6: ç”Ÿæˆå„ç« èŠ‚å†…å®¹
        print("\nğŸ“– [æ­¥éª¤6] ç”Ÿæˆå„ç« èŠ‚å†…å®¹...")
        
        section_contents = {}
        for section_title in sections:
            if section_title:
                # ä¸ºæ¯ä¸ªç« èŠ‚ç­›é€‰ç›¸å…³æ•°æ® - æ”¹è¿›åŒ¹é…é€»è¾‘
                relevant_data = []
                
                # é¦–å…ˆå°è¯•ç²¾ç¡®åŒ¹é…ç« èŠ‚å…³é”®è¯
                section_keywords = section_title.replace('+', ' ').split()
                for item in all_search_results:
                    content = (item.get('content', '') + ' ' + item.get('title', '')).lower()
                    title_lower = section_title.lower()
                    
                    # è®¡ç®—ç›¸å…³æ€§å¾—åˆ†
                    score = 0
                    for keyword in section_keywords:
                        if keyword.lower() in content:
                            score += 1
                    
                    # å¦‚æœç« èŠ‚æ ‡é¢˜åŒ…å«ç‰¹å®šè¯æ±‡ï¼Œä¼˜å…ˆåŒ¹é…ç›¸å…³å†…å®¹
                    if 'æŠ€æœ¯' in title_lower and ('æŠ€æœ¯' in content or 'technology' in content):
                        score += 2
                    elif 'å¸‚åœº' in title_lower and ('å¸‚åœº' in content or 'market' in content):
                        score += 2
                    elif 'åº”ç”¨' in title_lower and ('åº”ç”¨' in content or 'application' in content):
                        score += 2
                    elif 'æ•™è‚²' in title_lower and ('æ•™è‚²' in content or 'education' in content):
                        score += 2
                    elif 'äººå·¥æ™ºèƒ½' in title_lower and ('äººå·¥æ™ºèƒ½' in content or 'ai' in content or 'artificial intelligence' in content):
                        score += 2
                    
                    if score > 0:
                        relevant_data.append((item, score))
                
                # æŒ‰ç›¸å…³æ€§å¾—åˆ†æ’åºï¼Œé€‰æ‹©å‰5ä¸ª
                relevant_data.sort(key=lambda x: x[1], reverse=True)
                relevant_data = [item[0] for item in relevant_data[:5]]
                
                # å¦‚æœè¿˜æ˜¯æ²¡æœ‰ç›¸å…³æ•°æ®ï¼Œä½¿ç”¨æ‰€æœ‰æœç´¢ç»“æœçš„å‰5æ¡
                if not relevant_data:
                    relevant_data = all_search_results[:5]
                
                content_result = content_writer_mcp(
                    section_title=section_title,
                    content_data=relevant_data,
                    overall_report_context=json.dumps({
                        "topic": topic,
                        "report_type": report_type,
                        "intent": intent_data.get('details', {}),
                        "executive_summary": executive_summary
                    }, ensure_ascii=False),
                    outline_structure=outline_structure,
                    writing_style=writing_style,
                    target_audience=target_audience,
                    depth_level=depth_level
                )
                
                content_data = json.loads(content_result)
                section_contents[section_title] = content_data.get('content', '')
                print(f"  âœ… ç« èŠ‚ '{section_title}' å®Œæˆ")
        
        # æ­¥éª¤7: ç»„è£…æœ€ç»ˆæŠ¥å‘Š
        print("\nğŸ”§ [æ­¥éª¤7] ç»„è£…æœ€ç»ˆæŠ¥å‘Š...")
        
        final_report = _assemble_orchestrated_report(
            topic=topic,
            task_description=task,
            intent_analysis=intent_data.get('details', {}),
            outline=outline_data,
            executive_summary=executive_summary,
            section_contents=section_contents,
            search_summary=f"æ”¶é›†åˆ°{len(all_search_results)}æ¡æœç´¢ç»“æœ",
            quality_score=8.0,
            sections=sections,
            outline_structure=outline_structure
        )
        
        print("âœ… æŠ¥å‘Šç”Ÿæˆå®Œæˆ!")
        return final_report
        
    except Exception as e:
        error_result = {
            "status": "error",
            "task": task,
            "task_type": task_type,
            "error": str(e),
            "timestamp": datetime.now().isoformat()
        }
        return json.dumps(error_result, ensure_ascii=False)
        
        result = {
            "status": "success",
            "section_title": section_title,
            "content": final_content,
            "word_count": len(final_content),
            "writing_style": writing_style,
            "target_audience": target_audience,
            "generation_timestamp": datetime.now().isoformat()
        }
        
        return json.dumps(result, ensure_ascii=False, indent=2)
        
    except Exception as e:
        return _generate_fallback_content(section_title, content_data)

def _prepare_content_template_params(section_title, overall_report_context, reference_content, 
                                   writing_style, target_audience, tone, depth_level, 
                                   include_examples, word_count_requirement, role) -> Dict[str, str]:
    """å‡†å¤‡å†…å®¹æ¨¡æ¿å‚æ•°"""
    try:
        context_data = json.loads(overall_report_context) if overall_report_context else {}
        topic = context_data.get("topic", "ç›¸å…³é¢†åŸŸ")
    except:
        topic = "ç›¸å…³é¢†åŸŸ"
    
    return {
        "topic_context": topic,
        "section_title": section_title,
        "writing_style": writing_style,
        "target_audience": target_audience,
        "tone": tone,
        "depth_level": depth_level,
        "include_examples": include_examples,
        "word_count_requirement": word_count_requirement,
        "role": role,
        "reference_content": reference_content
    }

def _post_process_content(content: str, include_citations: bool) -> str:
    """åå¤„ç†å†…å®¹"""
    # æ¸…ç†å¤šä½™çš„ç©ºè¡Œ
    lines = []
    for line in content.split('\n'):
        line = line.strip()
        if line or (lines and lines[-1]):  # ä¿ç•™æœ‰å†…å®¹çš„è¡Œå’Œå¿…è¦çš„ç©ºè¡Œ
            lines.append(line)
    
    processed_content = '\n'.join(lines)
    
    # å¦‚æœéœ€è¦å¼•ç”¨ï¼Œæ·»åŠ ç®€å•çš„å¼•ç”¨æ ‡è®°
    if include_citations:
        processed_content += "\n\n*æ³¨ï¼šä»¥ä¸Šå†…å®¹åŸºäºå…¬å¼€èµ„æ–™æ•´ç†åˆ†æ*"
    
    return processed_content

def _generate_fallback_content(section_title: str, content_data: List[Dict]) -> str:
    """ç”Ÿæˆå¤‡ç”¨å†…å®¹"""
    fallback_content = f"""
## {section_title}

åŸºäºç°æœ‰æ•°æ®åˆ†æï¼Œ{section_title}å‘ˆç°å‡ºç§¯æçš„å‘å±•æ€åŠ¿ã€‚

### ä¸»è¦å‘ç°
1. å‘å±•è¶‹åŠ¿æ€»ä½“å‘å¥½ï¼Œå„é¡¹æŒ‡æ ‡è¡¨ç°ç¨³å®š
2. æŠ€æœ¯åˆ›æ–°æŒç»­æ¨è¿›ï¼Œä¸ºå‘å±•æä¾›äº†åŠ¨åŠ›
3. å¸‚åœºéœ€æ±‚ä¿æŒå¢é•¿ï¼Œä¸ºæ‰©å±•æä¾›äº†ç©ºé—´
4. æ”¿ç­–ç¯å¢ƒæ—¥ç›Šå®Œå–„ï¼Œä¸ºå‘å±•åˆ›é€ äº†æ¡ä»¶

### å…³é”®è¦ç‚¹
- å½“å‰å‘å±•åŸºç¡€æ‰å®ï¼Œå…·å¤‡æŒç»­å¢é•¿çš„æ½œåŠ›
- åˆ›æ–°èƒ½åŠ›ä¸æ–­æå‡ï¼Œæ ¸å¿ƒç«äº‰åŠ›æŒç»­å¢å¼º
- åº”ç”¨åœºæ™¯æ—¥ç›Šä¸°å¯Œï¼Œå¸‚åœºå‰æ™¯å¹¿é˜”
- å‘å±•ç¯å¢ƒæŒç»­ä¼˜åŒ–ï¼Œä¸ºé•¿æœŸå‘å±•å¥ å®šåŸºç¡€

### å‘å±•å»ºè®®
å»ºè®®ç»§ç»­åŠ å¼ºæŠ€æœ¯åˆ›æ–°æŠ•å…¥ï¼Œæ·±åŒ–å¸‚åœºæ‹“å±•ï¼Œå®Œå–„äº§ä¸šç”Ÿæ€å»ºè®¾ï¼Œ
åŒæ—¶åšå¥½é£é™©é˜²æ§ï¼Œç¡®ä¿å¯æŒç»­å‘å±•ã€‚

### æ€»ç»“
{section_title}å…·å¤‡è‰¯å¥½çš„å‘å±•å‰æ™¯ï¼Œé€šè¿‡æŒç»­ä¼˜åŒ–å’Œåˆ›æ–°ï¼Œ
æœ‰æœ›å®ç°æ›´å¤§çš„å‘å±•çªç ´ã€‚
"""
    
    result = {
        "status": "success",
        "section_title": section_title,
        "content": fallback_content.strip(),
        "word_count": len(fallback_content.strip()),
        "note": "ä½¿ç”¨å¤‡ç”¨å†…å®¹ç”Ÿæˆå™¨",
        "generation_timestamp": datetime.now().isoformat()
    }
    
    return json.dumps(result, ensure_ascii=False, indent=2)

def _extract_topic_from_task(task: str) -> str:
    """ä»ä»»åŠ¡æè¿°ä¸­æå–ä¸»é¢˜"""
    import re
    
    # ç®€å•çš„ä¸»é¢˜æå–é€»è¾‘
    task_lower = task.lower()
    
    # å¸¸è§çš„ä¸»é¢˜å…³é”®è¯æ¨¡å¼
    patterns = [
        r'å…³äº(.+?)çš„',
        r'(.+?)è¡Œä¸š',
        r'(.+?)æŠ€æœ¯',
        r'(.+?)å¸‚åœº',
        r'(.+?)å‘å±•',
        r'(.+?)åˆ†æ'
    ]
    
    for pattern in patterns:
        match = re.search(pattern, task)
        if match:
            topic = match.group(1).strip()
            if len(topic) > 1 and len(topic) < 20:
                return topic
    
    # å¦‚æœæ²¡æœ‰åŒ¹é…åˆ°æ¨¡å¼ï¼Œè¿”å›ä»»åŠ¡çš„å‰å‡ ä¸ªè¯
    words = task.split()[:3]
    return ''.join(words) if words else "æœªçŸ¥ä¸»é¢˜"

def _assemble_orchestrated_report(topic: str, task_description: str, intent_analysis: Dict,
                                outline: Dict, executive_summary: str, section_contents: Dict,
                                search_summary: str, quality_score: float, sections: List[str], 
                                outline_structure: Dict = None) -> str:
    """ç»„è£…æœ€ç»ˆçš„ç¼–æ’æŠ¥å‘Š"""
    
    report_sections = []
    
    # æ·»åŠ æ‰§è¡Œæ‘˜è¦
    if executive_summary:
        report_sections.append(f"## æ‰§è¡Œæ‘˜è¦\n\n{executive_summary}")
    
    # æ·»åŠ å„ç« èŠ‚å†…å®¹
    for section_title in sections:
        if section_title and section_title in section_contents:
            content = section_contents[section_title]
            if content:
                report_sections.append(content)
    
    # ç»„è£…å®Œæ•´æŠ¥å‘Š
    full_report = f"""# {topic} - ç»¼åˆåˆ†ææŠ¥å‘Š

## æŠ¥å‘Šæ¦‚è¿°
æœ¬æŠ¥å‘ŠåŸºäºç”¨æˆ·éœ€æ±‚"{task_description}"ï¼Œé€šè¿‡ç³»ç»ŸåŒ–çš„æ•°æ®æ”¶é›†å’Œåˆ†æï¼Œ
ä¸ºæ‚¨æä¾›å…³äº{topic}çš„å…¨é¢æ´å¯Ÿå’Œä¸“ä¸šå»ºè®®ã€‚

{chr(10).join(report_sections)}

## æŠ¥å‘Šæ€»ç»“
é€šè¿‡æœ¬æ¬¡æ·±å…¥åˆ†æï¼Œæˆ‘ä»¬å¯¹{topic}æœ‰äº†å…¨é¢çš„äº†è§£ã€‚æŠ¥å‘Šæ¶µç›–äº†å…³é”®å‘å±•è¶‹åŠ¿ã€
å¸‚åœºæœºé‡ã€æŠ€æœ¯åˆ›æ–°ä»¥åŠæ½œåœ¨æŒ‘æˆ˜ç­‰å¤šä¸ªç»´åº¦ï¼Œä¸ºç›¸å…³å†³ç­–æä¾›äº†æœ‰ä»·å€¼çš„å‚è€ƒã€‚

å»ºè®®æŒç»­å…³æ³¨è¯¥é¢†åŸŸçš„å‘å±•åŠ¨æ€ï¼ŒæŠŠæ¡å…³é”®æœºé‡ï¼ŒåŒæ—¶åšå¥½é£é™©é˜²æ§ï¼Œ
ä»¥å®ç°å¯æŒç»­çš„å‘å±•ç›®æ ‡ã€‚

---
*æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*
*è´¨é‡è¯„åˆ†: {quality_score}/1.0*
"""
    
    result = {
        "status": "success",
        "topic": topic,
        "task_description": task_description,
        "report": full_report,
        "metadata": {
            "sections_count": len(section_contents),
            "executive_summary_length": len(executive_summary),
            "total_length": len(full_report),
            "quality_score": quality_score,
            "generation_timestamp": datetime.now().isoformat()
        }
    }
    
    return json.dumps(result, ensure_ascii=False, indent=2)

# generate_insight_report å·²åˆ é™¤ - ä½¿ç”¨ orchestrator_mcp(task_type="insights") æ›¿ä»£

def _execute_simple_orchestration(task: str, **kwargs) -> str:
    """æ‰§è¡Œç®€åŒ–ç¼–æ’æµç¨‹"""
    try:
        # æå–ä¸»é¢˜
        topic = kwargs.get('topic') or _extract_topic_from_task(task)
        print(f"ğŸ“‹ è¯†åˆ«ä¸»é¢˜: {topic}")
        
        # æ‰§è¡Œæœç´¢
        search_result = search(topic, max_results=5)
        search_data = json.loads(search_result)
        
        # ç”Ÿæˆæ‘˜è¦
        if search_data.get('status') == 'success' and search_data.get('results'):
            summary_result = summary_writer_mcp(
                content_data=search_data['results'],
                length_constraint="300-500å­—",
                format="paragraph"
            )
            summary_data = json.loads(summary_result)
            final_summary = summary_data.get('summary', 'æš‚æ— æ‘˜è¦')
        else:
            final_summary = "æœç´¢æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯ï¼Œæ— æ³•ç”Ÿæˆæ‘˜è¦ã€‚"
        
        # ç»„è£…ç»“æœ
        result = {
            "status": "success",
            "task": task,
            "topic": topic,
            "summary": final_summary,
            "search_results_count": len(search_data.get('results', [])),
            "mode": "simple",
            "generation_timestamp": datetime.now().isoformat()
        }
        
        return json.dumps(result, ensure_ascii=False, indent=2)
        
    except Exception as e:
        error_result = {
            "status": "error",
            "task": task,
            "error": str(e),
            "mode": "simple",
            "timestamp": datetime.now().isoformat()
        }
        return json.dumps(error_result, ensure_ascii=False, indent=2)

def _assemble_content_for_evaluation(executive_summary: str, section_contents: Dict[str, str], topic: str) -> str:
    """ç»„è£…å†…å®¹ç”¨äºè´¨é‡è¯„ä¼°"""
    try:
        # æ„å»ºå®Œæ•´çš„æŠ¥å‘Šå†…å®¹æ–‡æœ¬
        content_parts = []
        
        # æ·»åŠ ä¸»é¢˜å’Œæ‘˜è¦
        content_parts.append(f"ä¸»é¢˜: {topic}")
        
        if executive_summary:
            content_parts.append(f"æ‰§è¡Œæ‘˜è¦: {executive_summary}")
        
        # æ·»åŠ å„ç« èŠ‚å†…å®¹
        for section_title, content in section_contents.items():
            if content:
                content_parts.append(f"ç« èŠ‚: {section_title}")
                content_parts.append(content)
        
        return "\n\n".join(content_parts)
    except Exception as e:
        print(f"âš ï¸ ç»„è£…è¯„ä¼°å†…å®¹å¤±è´¥: {e}")
        return f"ä¸»é¢˜: {topic}\nå†…å®¹ç»„è£…å¤±è´¥"

def _generate_targeted_query_for_weakness(weak_area: str, topic: str, suggestions: List[str]) -> str:
    """æ ¹æ®è–„å¼±ç¯èŠ‚ç”Ÿæˆé’ˆå¯¹æ€§æŸ¥è¯¢"""
    try:
        # è–„å¼±ç¯èŠ‚å¯¹åº”çš„æŸ¥è¯¢ç­–ç•¥
        weakness_queries = {
            "completeness": [
                f"{topic} è¯¦ç»†åˆ†æ",
                f"{topic} å…¨é¢ä»‹ç»", 
                f"{topic} å®Œæ•´æŒ‡å—"
            ],
            "accuracy": [
                f"{topic} æƒå¨æŠ¥å‘Š",
                f"{topic} å®˜æ–¹æ•°æ®",
                f"{topic} ç ”ç©¶æŠ¥å‘Š"
            ],
            "depth": [
                f"{topic} æ·±åº¦åˆ†æ",
                f"{topic} æŠ€æœ¯åŸç†",
                f"{topic} æœºåˆ¶ç ”ç©¶"
            ],
            "relevance": [
                f"{topic} æ ¸å¿ƒè¦ç‚¹",
                f"{topic} å…³é”®ç‰¹å¾",
                f"{topic} ä¸»è¦å†…å®¹"
            ],
            "clarity": [
                f"{topic} æ¸…æ™°è§£é‡Š",
                f"{topic} é€šä¿—æ˜“æ‡‚",
                f"{topic} ç»“æ„åŒ–ä»‹ç»"
            ]
        }
        
        # æ ¹æ®è–„å¼±ç¯èŠ‚é€‰æ‹©æŸ¥è¯¢
        if weak_area in weakness_queries:
            queries = weakness_queries[weak_area]
            # é€‰æ‹©ç¬¬ä¸€ä¸ªæŸ¥è¯¢ä½œä¸ºåŸºç¡€ï¼Œå¯ä»¥æ ¹æ®å»ºè®®è¿›è¡Œè°ƒæ•´
            base_query = queries[0]
            
            # å¦‚æœæœ‰å…·ä½“å»ºè®®ï¼Œå°è¯•èå…¥æŸ¥è¯¢ä¸­
            if suggestions:
                first_suggestion = suggestions[0]
                if "æ•°æ®" in first_suggestion:
                    return f"{topic} æœ€æ–°æ•°æ® ç»Ÿè®¡æŠ¥å‘Š"
                elif "æ¡ˆä¾‹" in first_suggestion:
                    return f"{topic} å®é™…æ¡ˆä¾‹ åº”ç”¨å®ä¾‹"
                elif "æŠ€æœ¯" in first_suggestion:
                    return f"{topic} æŠ€æœ¯è¯¦è§£ æ·±åº¦åˆ†æ"
                elif "å¸‚åœº" in first_suggestion:
                    return f"{topic} å¸‚åœºç ”ç©¶ è¡Œä¸šæŠ¥å‘Š"
                elif "åº”ç”¨" in first_suggestion:
                    return f"{topic} åº”ç”¨åœºæ™¯ å®è·µæ¡ˆä¾‹"
                else:
                    # æ ¹æ®è–„å¼±ç¯èŠ‚å’Œä¸»é¢˜ç”Ÿæˆæ›´å…·ä½“çš„æŸ¥è¯¢
                    return f"{topic} {weak_area} ä¸“ä¸šåˆ†æ"
            
            return base_query
        else:
            # é»˜è®¤æŸ¥è¯¢
            return f"{topic} è¯¦ç»†èµ„æ–™"
            
    except Exception as e:
        print(f"âš ï¸ ç”Ÿæˆé’ˆå¯¹æ€§æŸ¥è¯¢å¤±è´¥: {e}")
        return f"{topic} è¡¥å……èµ„æ–™"

def _identify_sections_to_improve(weak_areas: List[str], section_titles: List[str]) -> List[str]:
    """è¯†åˆ«éœ€è¦æ”¹è¿›çš„ç« èŠ‚"""
    try:
        # å¦‚æœè–„å¼±ç¯èŠ‚è¾ƒå¤šï¼Œæ”¹è¿›æ‰€æœ‰ç« èŠ‚
        if len(weak_areas) >= 3:
            return list(section_titles)
        
        # æ ¹æ®è–„å¼±ç¯èŠ‚ç±»å‹é€‰æ‹©ç›¸å…³ç« èŠ‚
        sections_to_improve = set()
        
        for weak_area in weak_areas:
            if weak_area == "completeness":
                # å®Œæ•´æ€§é—®é¢˜ï¼šæ”¹è¿›æ‰€æœ‰ç« èŠ‚
                sections_to_improve.update(section_titles)
            elif weak_area == "accuracy":
                # å‡†ç¡®æ€§é—®é¢˜ï¼šé‡ç‚¹æ”¹è¿›æ•°æ®å’Œåˆ†æç›¸å…³ç« èŠ‚
                accuracy_related = [title for title in section_titles 
                                  if any(keyword in title.lower() for keyword in 
                                        ["åˆ†æ", "æ•°æ®", "ç ”ç©¶", "ç°çŠ¶", "è¶‹åŠ¿"])]
                sections_to_improve.update(accuracy_related)
            elif weak_area == "depth":
                # æ·±åº¦é—®é¢˜ï¼šæ”¹è¿›åˆ†æå’ŒæŠ€æœ¯ç« èŠ‚
                depth_related = [title for title in section_titles 
                               if any(keyword in title.lower() for keyword in 
                                     ["æŠ€æœ¯", "åˆ†æ", "æœºåˆ¶", "åŸç†", "å‘å±•"])]
                sections_to_improve.update(depth_related)
            elif weak_area == "relevance":
                # ç›¸å…³æ€§é—®é¢˜ï¼šæ”¹è¿›æ ¸å¿ƒä¸»é¢˜ç« èŠ‚
                relevance_related = [title for title in section_titles 
                                   if any(keyword in title.lower() for keyword in 
                                         ["æ¦‚è¿°", "æ ¸å¿ƒ", "ä¸»è¦", "å…³é”®"])]
                sections_to_improve.update(relevance_related)
            elif weak_area == "clarity":
                # æ¸…æ™°åº¦é—®é¢˜ï¼šæ”¹è¿›æ‰€æœ‰ç« èŠ‚çš„è¡¨è¾¾
                sections_to_improve.update(section_titles)
        
        # å¦‚æœæ²¡æœ‰åŒ¹é…åˆ°ç‰¹å®šç« èŠ‚ï¼Œæ”¹è¿›å‰2ä¸ªç« èŠ‚
        if not sections_to_improve:
            sections_to_improve = set(list(section_titles)[:2])
        
        return list(sections_to_improve)
        
    except Exception as e:
        print(f"âš ï¸ è¯†åˆ«æ”¹è¿›ç« èŠ‚å¤±è´¥: {e}")
        # å‡ºé”™æ—¶æ”¹è¿›æ‰€æœ‰ç« èŠ‚
        return list(section_titles)

# æµå¼å¤„ç†å™¨åˆå§‹åŒ–
try:
    from streaming_orchestrator import StreamingOrchestrator
    streaming_orchestrator = StreamingOrchestrator()
    streaming_available = True
    print("âœ… æµå¼å¤„ç†å™¨åˆå§‹åŒ–æˆåŠŸ")
except Exception as e:
    print(f"âš ï¸ æµå¼å¤„ç†å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
    streaming_orchestrator = None
    streaming_available = False

# HTTP API æœåŠ¡å™¨
from fastapi import FastAPI, HTTPException
from fastapi.responses import StreamingResponse
import uvicorn
import asyncio
import json

# åˆ›å»ºHTTPåº”ç”¨
http_app = FastAPI(title="MCP HTTP API", version="1.0.0")

@http_app.post("/mcp/tools/call")
async def tools_call(request: dict):
    """MCPå·¥å…·è°ƒç”¨ç«¯ç‚¹ - æ”¯æŒSSEæµå¼å“åº”"""
    try:
        # æ”¯æŒä¸¤ç§è¯·æ±‚æ ¼å¼
        if "params" in request:
            # MCPæ ‡å‡†æ ¼å¼: {"jsonrpc": "2.0", "method": "tools/call", "params": {"name": "tool", "arguments": {...}}}
            params = request.get("params", {})
            tool_name = params.get("name")
            arguments = params.get("arguments", {})
            request_id = request.get("id", 1)
        else:
            # ç®€åŒ–æ ¼å¼: {"tool": "tool_name", "arguments": {...}}
            tool_name = request.get("tool")
            arguments = request.get("arguments", {})
            request_id = request.get("id", 1)
        
        # å·¥å…·æ˜ å°„
        tool_functions = {
            "search": search,
            "orchestrator_mcp": orchestrator_mcp,
            "query_generation_mcp": query_generation_mcp,
            "outline_writer_mcp": outline_writer_mcp,
            "summary_writer_mcp": summary_writer_mcp,
            "content_writer_mcp": content_writer_mcp,
            "analysis_mcp": analysis_mcp,
            "user_interaction_mcp": user_interaction_mcp
        }
        
        if tool_name not in tool_functions:
            # è¿”å›SSEæ ¼å¼çš„é”™è¯¯
            async def error_stream():
                error_msg = {
                    "jsonrpc": "2.0",
                    "id": request_id,
                    "error": {
                        "code": -32602,
                        "message": "Unknown tool",
                        "data": {
                            "type": "unknown_tool",
                            "message": f"Unknown tool: {tool_name}"
                        }
                    }
                }
                yield f"data: {json.dumps(error_msg, ensure_ascii=False)}\n\n"
            
            return StreamingResponse(error_stream(), media_type="text/event-stream")
        
        # å¯¹äºorchestrator_mcpï¼Œä½¿ç”¨æµå¼å¤„ç†
        if tool_name == "orchestrator_mcp" and streaming_available:
            async def orchestrator_stream():
                try:
                    # å‘é€å¼€å§‹æ¶ˆæ¯
                    start_msg = {
                        "type": "start",
                        "message": f"å¼€å§‹æ‰§è¡Œ{tool_name}ä»»åŠ¡"
                    }
                    yield f"data: {json.dumps(start_msg, ensure_ascii=False)}\n\n"
                    
                    # ä½¿ç”¨StreamingOrchestrator
                    async for message in streaming_orchestrator.stream_insight_report(**arguments):
                        yield f"data: {json.dumps(message, ensure_ascii=False)}\n\n"
                        
                except Exception as e:
                    error_msg = {
                        "jsonrpc": "2.0",
                        "id": request_id,
                        "error": {
                            "code": -32603,
                            "message": "Internal error",
                            "data": {
                                "type": "execution_failed",
                                "message": str(e)
                            }
                        }
                    }
                    yield f"data: {json.dumps(error_msg, ensure_ascii=False)}\n\n"
            
            return StreamingResponse(orchestrator_stream(), media_type="text/event-stream")
        
        # å¯¹äºå…¶ä»–å·¥å…·ï¼Œæ‰§è¡Œå¹¶è¿”å›ç»“æœ
        else:
            async def tool_stream():
                try:
                    # æ‰§è¡Œå·¥å…·
                    result = await tool_functions[tool_name](**arguments)
                    
                    # å‘é€ç»“æœæ¶ˆæ¯
                    result_msg = {
                        "jsonrpc": "2.0",
                        "id": request_id,
                        "result": {
                            "tool": tool_name,
                            "content": result
                        }
                    }
                    yield f"data: {json.dumps(result_msg, ensure_ascii=False)}\n\n"
                
                except Exception as e:
                    error_msg = {
                        "jsonrpc": "2.0",
                        "id": request_id,
                        "error": {
                            "code": -32603,
                            "message": "Internal error",
                            "data": {
                                "type": "execution_failed",
                                "message": str(e)
                            }
                        }
                    }
                    yield f"data: {json.dumps(error_msg, ensure_ascii=False)}\n\n"
            
            return StreamingResponse(tool_stream(), media_type="text/event-stream")
        
    except Exception as e:
        # è¿”å›SSEæ ¼å¼çš„é”™è¯¯
        async def error_stream():
            error_msg = {
                "jsonrpc": "2.0",
                "id": request.get("id", 1),
                "error": {
                    "code": -32603,
                    "message": "Internal error",
                    "data": {
                        "type": "request_parsing_failed",
                        "message": str(e)
                    }
                }
            }
            yield f"data: {json.dumps(error_msg, ensure_ascii=False)}\n\n"
        
        return StreamingResponse(error_stream(), media_type="text/event-stream")

@http_app.get("/")
async def root():
    return {"message": "MCP HTTP API Server", "version": "1.0.0"}

@http_app.get("/health")
async def health():
    return {"status": "healthy", "timestamp": datetime.now().isoformat()}

if __name__ == "__main__":
    print("ğŸš€ å¯åŠ¨MCPæœåŠ¡å™¨...")
    print("ğŸ“¡ æ”¯æŒç«¯ç‚¹: /mcp/tools/call (HTTP API)")
    print("ğŸ”§ æ”¯æŒçš„MCPå·¥å…·:")
    print("   - orchestrator_mcp: ä¸»ç¼–æ’å·¥å…·(æ”¯æŒè´¨é‡è¯„ä¼°è¿­ä»£ï¼ŒåŒ…æ‹¬æ´å¯ŸæŠ¥å‘Š)")
    print("   - analysis_mcp: åˆ†æå·¥å…·(æ”¯æŒevaluationè´¨é‡è¯„ä¼°)")
    print("   - search: æœç´¢å·¥å…·")
    print("   - outline_writer_mcp: å¤§çº²ç”Ÿæˆå·¥å…·")
    print("   - content_writer_mcp: å†…å®¹ç”Ÿæˆå·¥å…·")
    print("   - summary_writer_mcp: æ‘˜è¦ç”Ÿæˆå·¥å…·")
    print("   - query_generation_mcp: æŸ¥è¯¢ç”Ÿæˆå·¥å…·")
    print("   - user_interaction_mcp: ç”¨æˆ·äº¤äº’å·¥å…·")
    
    # å¯åŠ¨HTTPæœåŠ¡å™¨
    import threading
    import time

    def start_http_server():
        uvicorn.run(http_app, host="0.0.0.0", port=8001, log_level="info")
    
    http_thread = threading.Thread(target=start_http_server, daemon=True)
    http_thread.start()
    
    # ç­‰å¾…HTTPæœåŠ¡å™¨å¯åŠ¨
    time.sleep(2)
    
    # å¯åŠ¨FastMCPæœåŠ¡å™¨
    print("ğŸš€ å¯åŠ¨FastMCPæœåŠ¡å™¨...")
    mcp.run(transport="streamable-http")
