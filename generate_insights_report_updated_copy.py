import os
import json
import argparse
from datetime import datetime
from dotenv import load_dotenv
import sys
from fix_md_headings import fix_markdown_headings
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading
import time

from collectors.tavily_collector import TavilyCollector
from collectors.news_collector import NewsCollector
from collectors.brave_search_collector import BraveSearchCollector
from collectors.google_search_collector import GoogleSearchCollector

from generators.report_generator import ReportGenerator
from collectors.llm_processor import LLMProcessor
import config
import logging

# å…³é—­HTTPè¯·æ±‚æ—¥å¿—ï¼Œå‡å°‘å¹²æ‰°
logging.getLogger("httpx").setLevel(logging.WARNING)
logging.getLogger("urllib3").setLevel(logging.WARNING)

class ParallelInsightsCollector:
    """
    å¹¶è¡Œæ´å¯Ÿæ•°æ®æ”¶é›†å™¨
    è´Ÿè´£å¤šå±‚çº§çš„å¹¶è¡Œæ•°æ®æ”¶é›†å’Œå¤„ç†
    """
    
    def __init__(self):
        self.results_lock = threading.Lock()
        
        # åˆå§‹åŒ–æœç´¢æ”¶é›†å™¨
        self.collectors = {}
        
        # Tavilyæ”¶é›†å™¨ï¼ˆå¿…éœ€ï¼‰
        try:
            self.tavily_collector = TavilyCollector()
            self.collectors['tavily'] = self.tavily_collector
            print("âœ… Tavilyæœç´¢æ”¶é›†å™¨å·²åˆå§‹åŒ–")
        except Exception as e:
            print(f"âŒ Tavilyæœç´¢æ”¶é›†å™¨åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            raise
        
        # Braveæ”¶é›†å™¨ï¼ˆå¯é€‰ï¼‰
        try:
            self.brave_collector = BraveSearchCollector()
            if hasattr(self.brave_collector, 'has_api_key') and self.brave_collector.has_api_key:
                self.collectors['brave'] = self.brave_collector
                print("âœ… Braveæœç´¢æ”¶é›†å™¨å·²å¯ç”¨")
            else:
                print("âš ï¸ Braveæœç´¢æ”¶é›†å™¨æœªé…ç½®APIå¯†é’¥ï¼Œå·²è·³è¿‡")
        except Exception as e:
            print(f"âš ï¸ Braveæœç´¢æ”¶é›†å™¨åˆå§‹åŒ–å¤±è´¥: {str(e)}")
    
    def parallel_collect_all_sections(self, topic, sections, llm_processor=None, max_workers=4):
        """
        ğŸš€ å¹¶è¡Œæ”¶é›†æ‰€æœ‰ç« èŠ‚çš„æ•°æ®ï¼ˆç¬¬1å±‚å¹¶è¡Œï¼‰
        
        Args:
            topic: ä¸»é¢˜
            sections: ç« èŠ‚åˆ—è¡¨
            llm_processor: LLMå¤„ç†å™¨
            max_workers: æœ€å¤§å¹¶è¡Œå·¥ä½œæ•°
            
        Returns:
            dict: æŒ‰ç« èŠ‚ç»„ç»‡çš„æ•°æ®ç»“æœ
        """
        print(f"ğŸš€ [ç¬¬1å±‚å¹¶è¡Œ] å¼€å§‹å¹¶è¡Œæ”¶é›†{len(sections)}ä¸ªç« èŠ‚çš„æ•°æ®...")
        start_time = time.time()
        
        sections_data = {}
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # ğŸš€ å¹¶è¡Œæäº¤æ‰€æœ‰ç« èŠ‚ä»»åŠ¡
            future_to_section = {
                executor.submit(
                    self._collect_single_section_data, topic, section, llm_processor
                ): section for section in sections
            }
            
            # ğŸ”„ æ”¶é›†ç« èŠ‚ç»“æœ
            completed_count = 0
            for future in as_completed(future_to_section):
                section = future_to_section[future]
                try:
                    section_data = future.result()
                    completed_count += 1
                    
                    with self.results_lock:
                        sections_data[section] = section_data
                    
                    print(f"  âœ… [{completed_count}/{len(sections)}] ç« èŠ‚'{section}'æ”¶é›†å®Œæˆï¼Œè·å¾—{len(section_data)}æ¡æ•°æ®")
                    
                except Exception as e:
                    print(f"  âŒ ç« èŠ‚'{section}'æ”¶é›†å¤±è´¥: {str(e)}")
                    sections_data[section] = []
        
        total_time = time.time() - start_time
        total_items = sum(len(data) for data in sections_data.values())
        print(f"ğŸ“Š [ç¬¬1å±‚å¹¶è¡Œå®Œæˆ] æ€»è®¡æ”¶é›†{total_items}æ¡æ•°æ®ï¼Œè€—æ—¶{total_time:.1f}ç§’")
        
        return sections_data
    
    def _collect_single_section_data(self, topic, section, llm_processor):
        """æ”¶é›†å•ä¸ªç« èŠ‚çš„æ•°æ®ï¼Œå†…éƒ¨ä½¿ç”¨ç¬¬2å±‚å¹¶è¡Œ"""
        return self.parallel_collect_section_queries(topic, section, llm_processor)
    
    def parallel_collect_section_queries(self, topic, section, llm_processor=None, max_workers=6):
        """
        ğŸ¯ å¹¶è¡Œæ‰§è¡Œå•ä¸ªç« èŠ‚å†…çš„å¤šä¸ªæŸ¥è¯¢ï¼ˆç¬¬2å±‚å¹¶è¡Œï¼‰
        
        Args:
            topic: ä¸»é¢˜
            section: ç« èŠ‚åç§°
            llm_processor: LLMå¤„ç†å™¨
            max_workers: æœ€å¤§å¹¶è¡ŒæŸ¥è¯¢æ•°
            
        Returns:
            list: è¯¥ç« èŠ‚çš„æ•°æ®ç»“æœ
        """
        # é¦–å…ˆæ‰©å±•æœç´¢å…³é”®è¯
        expanded_topics = expand_search_keywords(topic, llm_processor)
        
        # ç”ŸæˆæŸ¥è¯¢åˆ—è¡¨
        all_queries = self._generate_section_queries(expanded_topics, section)
        
        print(f"  ğŸ” [ç¬¬2å±‚å¹¶è¡Œ] ç« èŠ‚'{section}'å¹¶è¡Œæ‰§è¡Œ{len(all_queries)}ä¸ªæŸ¥è¯¢...")
        
        section_results = []
        seen_urls = set()
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # ğŸš€ å¹¶è¡Œæäº¤æ‰€æœ‰æŸ¥è¯¢ä»»åŠ¡
            future_to_query = {
                executor.submit(
                    self._execute_single_query, query_info["query"]
                ): query_info for query_info in all_queries
            }
            
            # ğŸ”„ æ”¶é›†æŸ¥è¯¢ç»“æœ
            completed_count = 0
            for future in as_completed(future_to_query):
                query_info = future_to_query[future]
                try:
                    query_results = future.result()
                    completed_count += 1
                    
                    # å»é‡å¹¶æ·»åŠ åˆ°ç»“æœ
                    new_results = 0
                    for result in query_results:
                        url = result.get("url", "")
                        if url and url not in seen_urls:
                            seen_urls.add(url)
                            result["section"] = section
                            result["source_query"] = query_info["query"]
                            section_results.append(result)
                            new_results += 1
                    
                    if new_results > 0:
                        print(f"    âœ… [{completed_count}/{len(all_queries)}] æŸ¥è¯¢å®Œæˆ: +{new_results}æ¡æ–°æ•°æ®")
                    else:
                        print(f"    â– [{completed_count}/{len(all_queries)}] æŸ¥è¯¢å®Œæˆ: æ— æ–°æ•°æ®")
                        
                except Exception as e:
                    print(f"    âŒ æŸ¥è¯¢å¤±è´¥: {str(e)}")
        
        # ç«‹å³è¿›è¡Œç›¸å…³æ€§è¯„ä¼°å’Œç­›é€‰
        if llm_processor and section_results:
            print(f"    ğŸ” è¯„ä¼°ç« èŠ‚'{section}'çš„{len(section_results)}æ¡æ•°æ®ç›¸å…³æ€§...")
            scored_results = evaluate_insights_relevance(section_results, f"{topic} {section}", llm_processor)
            
            # ä¿ç•™æœ€ç›¸å…³çš„8-12æ¡
            if len(scored_results) > 12:
                final_results = scored_results[:12]
                print(f"    âœ‚ï¸ ä»{len(scored_results)}æ¡ä¸­ç­›é€‰å‡ºæœ€ç›¸å…³çš„12æ¡")
            else:
                final_results = scored_results
                print(f"    âœ… ä¿ç•™{len(final_results)}æ¡é«˜è´¨é‡æ•°æ®")
                
            return final_results
        
        # ç®€å•ç­›é€‰ï¼ˆæ— LLMæ—¶ï¼‰
        if len(section_results) > 10:
            section_results.sort(key=lambda x: len(x.get("content", "")), reverse=True)
            return section_results[:10]
        
        return section_results
    
    def _generate_section_queries(self, expanded_topics, section):
        """ä¸ºç‰¹å®šç« èŠ‚ç”ŸæˆæŸ¥è¯¢åˆ—è¡¨"""
        all_queries = []
        
        for expanded_topic in expanded_topics:
            queries = []
            
            if "è¡Œä¸šå®šä¹‰" in section or "æ ¸å¿ƒç‰¹ç‚¹" in section:
                queries.extend([
                    f"{expanded_topic} è¡Œä¸šå®šä¹‰ æŠ€æœ¯ç‰¹å¾ æ ¸å¿ƒä»·å€¼ è¾¹ç•Œ",
                    f"{expanded_topic} æŠ€æœ¯åŸç† æ ¸å¿ƒåŠŸèƒ½ ç‰¹ç‚¹",
                    f"{expanded_topic} å®šä¹‰ æ¦‚å¿µ èŒƒå›´ ç‰¹ç‚¹",
                    f"{expanded_topic} æŠ€æœ¯æ¶æ„ åŸºç¡€ç»„ä»¶ æ ¸å¿ƒä»·å€¼ä¸»å¼ ",
                    f"{expanded_topic} æŠ€æœ¯æ ‡å‡† å…³é”®ç‰¹å¾ åŒºåˆ«äºä¼ ç»Ÿæ–¹æ³•",
                    f"{expanded_topic} è¡Œä¸šè§£æ æ ¸å¿ƒæŠ€æœ¯ ä»·å€¼æµ"
                ])
            elif "å‘å±•å†ç¨‹" in section or "é˜¶æ®µæ¼”è¿›" in section:
                queries.extend([
                    f"{expanded_topic} å‘å±•å†ç¨‹ å…³é”®é˜¶æ®µ é‡Œç¨‹ç¢‘ æŠ€æœ¯æ¼”è¿›",
                    f"{expanded_topic} å†å²å‘å±• æ¼”è¿›è·¯å¾„ é‡å¤§çªç ´",
                    f"{expanded_topic} å‘å±•å² é˜¶æ®µ å…³é”®äº‹ä»¶",
                    f"{expanded_topic} æŠ€æœ¯è¿­ä»£ è½¬æŠ˜ç‚¹ å¹´è¡¨",
                    f"{expanded_topic} å†å²æ²¿é© ä»£é™…å˜è¿ æŠ€æœ¯æ¼”åŒ–",
                    f"{expanded_topic} å‘å±•æ—¶é—´çº¿ çªç ´æ€§äº‹ä»¶ è¡Œä¸šå˜é©"
                ])
            elif "äº§ä¸šé“¾" in section or "ä»·å€¼åˆ†å¸ƒ" in section:
                queries.extend([
                    f"{expanded_topic} äº§ä¸šé“¾ ä¸Šæ¸¸ ä¸­æ¸¸ ä¸‹æ¸¸ ç»“æ„",
                    f"{expanded_topic} ä»·å€¼åˆ†å¸ƒ æˆæœ¬ç»“æ„ åˆ©æ¶¦åˆ†é…",
                    f"{expanded_topic} äº§ä¸šç”Ÿæ€ ä¾›åº”é“¾ ä»·å€¼é“¾",
                    f"{expanded_topic} ä¸Šä¸‹æ¸¸ä¼ä¸š ä»·å€¼å æ¯” æ ¸å¿ƒç¯èŠ‚",
                    f"{expanded_topic} äº§ä¸šç»“æ„ åˆ©æ¶¦åˆ†å¸ƒ å…³é”®è§’è‰²",
                    f"{expanded_topic} äº§ä¸šåœ°å›¾ ä»·å€¼æµåŠ¨ ç¯èŠ‚åˆ†æ"
                ])
            elif "å¸‚åœºæ ¼å±€" in section or "å‚ä¸è€…" in section:
                queries.extend([
                    f"{expanded_topic} å¸‚åœºæ ¼å±€ ç«äº‰çŠ¶å†µ å¸‚åœºä»½é¢ é¢†å…ˆä¼ä¸š",
                    f"{expanded_topic} ä¸»è¦å‚ä¸è€… ä»£è¡¨æ€§ä¼ä¸š å•†ä¸šæ¨¡å¼",
                    f"{expanded_topic} å¸‚åœºç«äº‰ å¤´éƒ¨ä¼ä¸š æ’å",
                    f"{expanded_topic} å¸‚åœºé›†ä¸­åº¦ ç«äº‰ä¼˜åŠ¿ å•†ä¸šåœ°ä½",
                    f"{expanded_topic} ç»†åˆ†å¸‚åœº åŒºåŸŸæ ¼å±€ å›½å†…å¤–ä¼ä¸šå¯¹æ¯”",
                    f"{expanded_topic} äº§ä¸šå‚ä¸è€… æŠ€æœ¯å£å’ ç«äº‰ç­–ç•¥"
                ])
            elif "æ ¸å¿ƒé©±åŠ¨" in section or "è¶‹åŠ¿" in section:
                queries.extend([
                    f"{expanded_topic} é©±åŠ¨å› ç´  å‘å±•è¶‹åŠ¿ å¸‚åœºéœ€æ±‚ æŠ€æœ¯æ¼”è¿›",
                    f"{expanded_topic} è¶‹åŠ¿é¢„æµ‹ æŠ€æœ¯å‘å±• å•†ä¸šæ¨¡å¼å˜é©",
                    f"{expanded_topic} è¡Œä¸šè¶‹åŠ¿ å‘å±•æ–¹å‘ æ¼”å˜",
                    f"{expanded_topic} ä¸»è¦è¶‹åŠ¿ ç§‘æŠ€çªç ´ æœªæ¥æŠ€æœ¯è·¯çº¿å›¾",
                    f"{expanded_topic} è¡Œä¸šå˜é© åˆ›æ–°é©±åŠ¨ éœ€æ±‚åŠ¨åŠ›",
                    f"{expanded_topic} å¢é•¿é©±åŠ¨åŠ› æ–°å…´æŠ€æœ¯èåˆ äº§ä¸šå‡çº§"
                ])
            elif "æœªæ¥å±•æœ›" in section or "æŒ‘æˆ˜åº”å¯¹" in section:
                queries.extend([
                    f"{expanded_topic} æœªæ¥å±•æœ› æŠ€æœ¯çªç ´ åˆ›æ–°æœºé‡",
                    f"{expanded_topic} è¡Œä¸šæŒ‘æˆ˜ é—®é¢˜ è§£å†³æ–¹æ¡ˆ ç­–ç•¥",
                    f"{expanded_topic} æœªæ¥å‘å±• åˆ›æ–° çªç ´ å‰æ™¯",
                    f"{expanded_topic} æŒ‘æˆ˜ å›°éš¾ åº”å¯¹ç­–ç•¥",
                    f"{expanded_topic} å¢é•¿ç©ºé—´ æœºé‡çª—å£ å‘å±•ç“¶é¢ˆ",
                    f"{expanded_topic} è¡Œä¸šå‰æ™¯ é¢„æµ‹åˆ†æ æˆ˜ç•¥æ–¹å‘"
                ])
            elif "æ”¿ç­–ç¯å¢ƒ" in section:
                queries.extend([
                    f"{expanded_topic} æ”¿ç­–ç¯å¢ƒ æ³•è§„ ç›‘ç®¡ å…¨çƒå¯¹æ¯”",
                    f"{expanded_topic} äº§ä¸šæ”¿ç­– æ‰¶æŒæªæ–½ ç›‘ç®¡è¶‹åŠ¿ å½±å“",
                    f"{expanded_topic} æ³•å¾‹æ³•è§„ æ ‡å‡† åˆè§„è¦æ±‚",
                    f"{expanded_topic} å›½å®¶æ”¿ç­– åœ°æ–¹æ”¯æŒ ç›‘ç®¡æ¡†æ¶",
                    f"{expanded_topic} å›½é™…æ”¿ç­– å›½å†…æ³•è§„ åˆè§„æˆæœ¬",
                    f"{expanded_topic} æ”¿ç­–å¯¼å‘ è¡Œä¸šæ ‡å‡† åˆè§„ä½“ç³»"
                ])
            
            # è½¬æ¢ä¸ºæŸ¥è¯¢ä¿¡æ¯å¯¹è±¡
            for query in queries:
                all_queries.append({"query": query, "section": section})
        
        return all_queries
    
    def _execute_single_query(self, query):
        """æ‰§è¡Œå•ä¸ªæŸ¥è¯¢ï¼Œåœ¨å¤šä¸ªæœç´¢å¼•æ“ä¸­æœç´¢"""
        search_results = []
        used_urls = set()
        
        for name, collector in self.collectors.items():
            try:
                if name == 'tavily':
                    results = collector.search(query, max_results=3)
                elif name == 'brave':
                    results = collector.search(query, count=3)
                else:
                    continue
                    
                if results:
                    for result in results:
                        url = result.get('url', '')
                        if url and url not in used_urls:
                            result['search_source'] = name
                            search_results.append(result)
                            used_urls.add(url)
                            
            except Exception:
                continue
        
        return search_results

class ParallelInsightsProcessor:
    """
    å¹¶è¡Œæ´å¯Ÿå†…å®¹ç”Ÿæˆå™¨
    è´Ÿè´£ç¬¬3å±‚å¹¶è¡Œï¼šå¤šä¸ªç« èŠ‚çš„å†…å®¹ç”Ÿæˆ
    """
    
    def __init__(self, llm_processor):
        self.llm_processor = llm_processor
        self.results_lock = threading.Lock()
    
    def parallel_generate_sections_content(self, sections_data, topic, max_workers=3):
        """
        âš¡ å¹¶è¡Œç”Ÿæˆå¤šä¸ªç« èŠ‚çš„å†…å®¹ï¼ˆç¬¬3å±‚å¹¶è¡Œï¼‰
        
        Args:
            sections_data: æŒ‰ç« èŠ‚ç»„ç»‡çš„æ•°æ®
            topic: ä¸»é¢˜
            max_workers: æœ€å¤§å¹¶è¡Œå·¥ä½œæ•°
            
        Returns:
            dict: ç”Ÿæˆçš„ç« èŠ‚å†…å®¹
        """
        print(f"âš¡ [ç¬¬3å±‚å¹¶è¡Œ] å¼€å§‹å¹¶è¡Œç”Ÿæˆ{len(sections_data)}ä¸ªç« èŠ‚çš„å†…å®¹...")
        start_time = time.time()
        
        generated_sections = {}
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # ğŸš€ å¹¶è¡Œæäº¤ç« èŠ‚å†…å®¹ç”Ÿæˆä»»åŠ¡
            future_to_section = {}
            
            for section_name, section_items in sections_data.items():
                if section_items:  # åªå¤„ç†æœ‰æ•°æ®çš„ç« èŠ‚
                    future_to_section[executor.submit(
                        self._generate_single_section_content, 
                        section_name, section_items, topic
                    )] = section_name
            
            # ğŸ”„ æ”¶é›†ç”Ÿæˆç»“æœ
            completed_count = 0
            for future in as_completed(future_to_section):
                section_name = future_to_section[future]
                try:
                    section_content = future.result()
                    completed_count += 1
                    
                    with self.results_lock:
                        generated_sections[section_name] = section_content
                    
                    content_length = len(section_content) if section_content else 0
                    print(f"  âœ… [{completed_count}/{len(future_to_section)}] ç« èŠ‚'{section_name}'å†…å®¹ç”Ÿæˆå®Œæˆï¼Œé•¿åº¦{content_length}å­—ç¬¦")
                    
                except Exception as e:
                    print(f"  âŒ ç« èŠ‚'{section_name}'å†…å®¹ç”Ÿæˆå¤±è´¥: {str(e)}")
                    generated_sections[section_name] = ""
        
        total_time = time.time() - start_time
        total_length = sum(len(content) for content in generated_sections.values())
        print(f"ğŸ“Š [ç¬¬3å±‚å¹¶è¡Œå®Œæˆ] æ€»è®¡ç”Ÿæˆ{total_length}å­—ç¬¦å†…å®¹ï¼Œè€—æ—¶{total_time:.1f}ç§’")
        
        return generated_sections
    
    def _generate_single_section_content(self, section_name, section_items, topic):
        """ç”Ÿæˆå•ä¸ªç« èŠ‚çš„å†…å®¹"""
        if not self.llm_processor or not section_items:
            return generate_section_content_simple(section_items)
        
        try:
            # æ ¹æ®èµ„æ–™æ•°é‡ä½¿ç”¨ä¸åŒçš„ç”Ÿæˆç­–ç•¥
            if len(section_items) == 1:
                return self._generate_single_item_content(section_items[0], topic, section_name)
            elif len(section_items) == 2:
                return self._generate_two_items_content(section_items, topic, section_name)
            else:
                return self._generate_multiple_items_content(section_items, topic, section_name)
                
        except Exception as e:
            print(f"âš ï¸ LLMç”Ÿæˆç« èŠ‚'{section_name}'å†…å®¹å¤±è´¥: {str(e)}ï¼Œä½¿ç”¨ç®€å•æ–¹æ³•")
            return generate_section_content_simple(section_items)
    
    def _generate_single_item_content(self, item, topic, section_name):
        """å•ä¸ªèµ„æ–™çš„å†…å®¹ç”Ÿæˆ"""
        title = item.get("title", "")
        content = item.get("content", "").strip()
        source = item.get("source", "è¡Œä¸šåˆ†æ")
        url = item.get("url", "#")
        
        prompt = f"""è¯·åŸºäºä»¥ä¸‹å…³äº"{topic}{section_name}"çš„è¯¦ç»†èµ„æ–™ï¼Œåˆ›å»ºä¸€ä¸ªå†…å®¹éå¸¸ä¸°å¯Œã€ç»“æ„æ¸…æ™°çš„åˆ†ææŠ¥å‘Šç« èŠ‚ã€‚åŠ¡å¿…è¯¦å°½å±•å¼€ï¼Œä¸è¦ç®€ç•¥å¤„ç†ï¼š

èµ„æ–™æ ‡é¢˜: {title}
èµ„æ–™å†…å®¹: {content}

è¦æ±‚ï¼š
1. ç”Ÿæˆä¸€ä¸ªæ ‡é¢˜ä¸º"# {title}"çš„markdownæ ¼å¼ç« èŠ‚
2. åˆ†æå¿…é¡»æå…¶æ·±å…¥ä¸”è¯¦å°½ï¼ŒåŒ…å«è‡³å°‘7-10ä¸ªæœ‰å±‚æ¬¡çš„å­æ ‡é¢˜
3. æ¯ä¸ªå°èŠ‚å¿…é¡»æœ‰å……åˆ†å±•å¼€çš„å†…å®¹ï¼Œç¡®ä¿å†…å®¹çš„æ·±åº¦å’Œå¹¿åº¦
4. ä½¿ç”¨Markdownæ ¼å¼ç»„ç»‡å†…å®¹ï¼Œé‡è¦è§‚ç‚¹å’Œæ•°æ®ä½¿ç”¨**ç²—ä½“**æ ‡è®°
5. åˆ†æé•¿åº¦å¿…é¡»åœ¨2500-3500å­—ä»¥ä¸Šï¼Œç¡®ä¿å†…å®¹æå…¶å……å®å’Œæ·±å…¥
6. ä¿ç•™æ‰€æœ‰é‡è¦æ•°æ®ç‚¹å’Œäº‹å®ï¼Œæ•´åˆåˆ°åˆé€‚çš„ä¸Šä¸‹æ–‡ä¸­
7. ä½¿ç”¨å¤šçº§æ ‡é¢˜ï¼ˆ##ã€###ã€####ï¼‰ç»„ç»‡å†…å®¹ï¼Œç¡®ä¿ç»“æ„åˆ†æ˜
8. å¯¹åŸå§‹å†…å®¹è¿›è¡Œå……åˆ†æ‰©å±•å’Œæ·±å…¥æŒ–æ˜ï¼Œç»ä¸ç®€å•å¤è¿°
9. åœ¨æ–‡æœ«æ·»åŠ æ•°æ®æ¥æº: {source} - {url}
10. å†…å®¹å¿…é¡»ä¸“ä¸šã€æƒå¨ä¸”æœ‰æé«˜çš„åˆ†ææ·±åº¦ï¼Œå½»åº•é¿å…æµ…å°è¾„æ­¢
11. æ¯ä¸ªå°èŠ‚å»ºè®®åŒ…å«4-6ä¸ªæ®µè½ï¼Œç¡®ä¿å……åˆ†å±•å¼€è®ºè¿°
12. ä½¿ç”¨ç®€çŸ­å°æ ‡é¢˜+è¯¦å°½å†…å®¹çš„ç»„ç»‡æ–¹å¼ï¼Œä½¿å†…å®¹æ—¢æœ‰å±‚æ¬¡åˆä¾¿äºé˜…è¯»
"""
        
        system_message = f"ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„{topic}è¡Œä¸šåˆ†æå¸ˆå’Œå†…å®¹ç»„ç»‡ä¸“å®¶ï¼Œæ“…é•¿åˆ›å»ºç»“æ„æ¸…æ™°çš„ä¸“ä¸šæŠ¥å‘Šã€‚"
        
        return self.llm_processor.call_llm_api(prompt, system_message, temperature=0.2, max_tokens=8000)
    
    def _generate_two_items_content(self, section_items, topic, section_name):
        """ä¸¤ä¸ªèµ„æ–™çš„å†…å®¹ç”Ÿæˆ"""
        item1, item2 = section_items[0], section_items[1]
        
        prompt = f"""è¯·åŸºäºä»¥ä¸‹ä¸¤æ¡èµ„æ–™ï¼Œä¸º'{topic}è¡Œä¸šæ´å¯ŸæŠ¥å‘Š'çš„'{section_name}'ç« èŠ‚ç”Ÿæˆæå…¶è¯¦å°½ã€ç»“æ„æ¸…æ™°çš„å†…å®¹ã€‚

èµ„æ–™1æ ‡é¢˜: {item1.get("title", "")}
èµ„æ–™1å†…å®¹: {item1.get("content", "")[:3000]}...

èµ„æ–™2æ ‡é¢˜: {item2.get("title", "")}
èµ„æ–™2å†…å®¹: {item2.get("content", "")[:3000]}...

è¦æ±‚ï¼š
1. æ·±å…¥åˆ†æå’Œæ•´åˆè¿™ä¸¤æ¡èµ„æ–™ï¼Œæå–æ‰€æœ‰å…³é”®ä¿¡æ¯å’Œè§è§£
2. å†…å®¹å¿…é¡»æå…¶è¯¦å°½å…¨é¢ï¼Œè‡³å°‘åŒ…å«8-10ä¸ªä¸»è¦å°èŠ‚
3. ä½¿ç”¨å±‚çº§æ ‡é¢˜ç»„ç»‡å†…å®¹ï¼š
   - ä½¿ç”¨ä¸‰çº§æ ‡é¢˜(###)ä½œä¸ºä¸»è¦åˆ†å—ï¼Œè‡³å°‘åˆ›å»º8-10ä¸ªä¸‰çº§æ ‡é¢˜
   - åœ¨æ¯ä¸ªä¸‰çº§æ ‡é¢˜ä¸‹ï¼Œä½¿ç”¨å››çº§æ ‡é¢˜(####)è¿›ä¸€æ­¥ç»†åˆ†å†…å®¹
4. æ¯ä¸ªå°èŠ‚éƒ½å¿…é¡»æœ‰å……åˆ†å±•å¼€çš„å†…å®¹
5. åœ¨ç›¸åº”å†…å®¹åæ ‡æ³¨æ¥æºä¿¡æ¯
6. å†…å®¹è¦ç»å¯¹è¯¦å°½ï¼Œæ€»ä½“é•¿åº¦ä¸å°‘äº4000å­—
7. å¯¹äºæ•°æ®å’Œå…³é”®è§‚ç‚¹ï¼Œä½¿ç”¨**ç²—ä½“**æ ‡è®°æˆ–é¡¹ç›®ç¬¦å·å‘ˆç°
8. å¿…é¡»åŒ…å«è¡Œä¸šæœ€æ–°æ•°æ®ã€æ·±åº¦åˆ†æå’Œä¸“ä¸šæ´è§
"""
        
        system_message = f"ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„{topic}è¡Œä¸šåˆ†æå¸ˆå’Œå†…å®¹ç»„ç»‡ä¸“å®¶ã€‚"
        
        return self.llm_processor.call_llm_api(prompt, system_message, temperature=0.2, max_tokens=8000)
    
    def _generate_multiple_items_content(self, section_items, topic, section_name):
        """å¤šä¸ªèµ„æ–™çš„å†…å®¹ç”Ÿæˆ"""
        resource_texts = []
        source_references = []
        
        for i, item in enumerate(section_items):
            title = item.get("title", "")
            content = item.get("content", "").strip()
            source = item.get("source", "è¡Œä¸šåˆ†æ")
            url = item.get("url", "#")
            
            resource_texts.append(f"èµ„æ–™{i+1}æ ‡é¢˜: {title}\nèµ„æ–™{i+1}å†…å®¹: {content[:1200]}...")
            source_references.append(f"[æ•°æ®æ¥æº{i+1}: {source} - {url}]")
        
        all_resources = "\n\n".join(resource_texts)
        source_reference_text = "\n".join(source_references)
        
        prompt = f"""è¯·åŸºäºä»¥ä¸‹å…³äº"{topic}{section_name}"çš„å¤šä¸ªèµ„æ–™æ¥æºï¼Œåˆ›å»ºä¸€ä¸ªæå…¶è¯¦å°½ã€ä¸“ä¸šä¸”ç»“æ„æ¸…æ™°çš„è¡Œä¸šåˆ†æç« èŠ‚ï¼š

{all_resources}

è¦æ±‚ï¼š
1. åˆ›å»ºä¸€ä¸ªå†…å®¹æå…¶ä¸°å¯Œçš„ä¸“ä¸šè¡Œä¸šåˆ†æç« èŠ‚ï¼Œæ•´åˆæ‰€æœ‰èµ„æ–™çš„æ ¸å¿ƒè§‚ç‚¹å’Œæ•°æ®
2. åˆ†æå¿…é¡»éå¸¸æ·±å…¥ä¸”å…¨é¢ï¼Œä½¿ç”¨å¤šçº§æ ‡é¢˜ç»„ç»‡å†…å®¹ï¼ˆ##ã€###ã€####ï¼‰
3. å¿…é¡»è¯¦å°½è¦†ç›–æ‰€æœ‰èµ„æ–™ä¸­çš„é‡è¦è§‚ç‚¹ï¼Œè¿›è¡Œç³»ç»Ÿæ€§æ•´åˆä¸æ·±åº¦æ‹“å±•
4. ç« èŠ‚åº”åˆ†ä¸ºè‡³å°‘7-10ä¸ªå­æ ‡é¢˜ï¼Œæ¯ä¸ªå­æ ‡é¢˜ä¸‹å†…å®¹è¯¦å°½å……å®
5. æ€»ä½“å†…å®¹é•¿åº¦åº”è¾¾åˆ°4000-6000å­—ï¼Œç¡®ä¿åˆ†ææ·±åº¦è¿œè¶…æ™®é€šæŠ¥å‘Š
6. åœ¨é€‚å½“ä½ç½®æ·»åŠ æ¥æºå¼•ç”¨ï¼š
{source_reference_text}
7. æ¯ä¸ªå°èŠ‚æ ‡é¢˜åº”å…·ä½“æ˜ç¡®ï¼Œå¹¶èƒ½å‡†ç¡®æ¦‚æ‹¬å…¶å†…å®¹
8. ä¸è¦ç®€å•å †ç Œèµ„æ–™ï¼Œå¿…é¡»å½¢æˆæœ‰æ·±åº¦çš„åˆ†ææ¡†æ¶å’Œç‹¬åˆ°è§è§£
"""
        
        system_message = f"ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„{topic}è¡Œä¸šåˆ†æå¸ˆå’Œå†…å®¹ç»„ç»‡ä¸“å®¶ã€‚"
        
        return self.llm_processor.call_llm_api(prompt, system_message, temperature=0.2, max_tokens=8000)

def generate_industry_insights_without_api(topic, subtopics=None):
    """
    åœ¨æ²¡æœ‰APIå¯†é’¥çš„æƒ…å†µä¸‹ç”Ÿæˆè¡Œä¸šæ´å¯Ÿ
    ä½¿ç”¨ä¼˜åŒ–åçš„è¡Œä¸šæ´å¯ŸæŠ¥å‘Šç»“æ„
    """
    # ä¼˜åŒ–åçš„è¡Œä¸šæ´å¯ŸæŠ¥å‘Šç»“æ„
    insights_sections = {
        f"{topic}è¡Œä¸šå®šä¹‰ä¸æ ¸å¿ƒç‰¹ç‚¹": f"æœ¬èŠ‚å°†ä»‹ç»{topic}çš„è¡Œä¸šè¾¹ç•Œã€æŠ€æœ¯ç‰¹å¾å’Œæ ¸å¿ƒä»·å€¼ä¸»å¼ ã€‚",
        f"{topic}å‘å±•å†ç¨‹ä¸é˜¶æ®µæ¼”è¿›": f"åˆ†æ{topic}çš„å…³é”®å‘å±•é˜¶æ®µã€æŠ€æœ¯é‡Œç¨‹ç¢‘å’Œæ¼”è¿›è·¯å¾„ã€‚",
        f"{topic}äº§ä¸šé“¾ä¸ä»·å€¼åˆ†å¸ƒ": f"æ¢è®¨{topic}çš„ä¸Šä¸­ä¸‹æ¸¸ç»“æ„åŠå„ç¯èŠ‚ä»·å€¼å æ¯”åˆ†æã€‚",
        f"{topic}å¸‚åœºæ ¼å±€ä¸å‚ä¸è€…": f"ç ”ç©¶{topic}å¸‚åœºçš„ç«äº‰æ ¼å±€ã€å¸‚åœºä»½é¢å’Œä»£è¡¨æ€§ä¼ä¸šã€‚",
        f"{topic}æ ¸å¿ƒé©±åŠ¨ä¸è¶‹åŠ¿": f"åˆ†ææ¨åŠ¨{topic}å‘å±•çš„å†…å¤–éƒ¨å› ç´ åŠä¸»è¦è¶‹åŠ¿é¢„æµ‹ã€‚",
        f"{topic}æœªæ¥å±•æœ›ä¸æŒ‘æˆ˜åº”å¯¹": f"é¢„æµ‹{topic}çš„æŠ€æœ¯çªç ´ç‚¹ã€æ½œåœ¨æŒ‘æˆ˜åŠè§£å†³æ–¹æ¡ˆï¼Œå¹¶æå‡ºæˆ˜ç•¥å»ºè®®ã€‚",
        f"{topic}æ”¿ç­–ç¯å¢ƒåˆ†æ": f"å¯¹æ¯”å…¨çƒå’ŒåŒºåŸŸ{topic}ç›¸å…³æ”¿ç­–ï¼Œåˆ†æç›‘ç®¡è¶‹åŠ¿åŠå½±å“ã€‚"
    }
    
    # è½¬æ¢ä¸ºæ–‡ç« æ ¼å¼
    current_date = datetime.now().strftime('%Y-%m-%d')
    articles = []
    
    for section, content in insights_sections.items():
        article = {
            'title': section,
            'authors': ['è¡Œä¸šåˆ†æ'],
            'summary': content,
            'published': current_date,
            'url': '#',
            'source': 'ç³»ç»Ÿåˆ†æ',
            'content': content
        }
        articles.append(article)
    
    return articles

def evaluate_insights_relevance(raw_insights, topic, llm_processor=None):
    """
    è¯„ä¼°åŸå§‹æ´å¯Ÿå†…å®¹ä¸ä¸»é¢˜çš„ç›¸å…³æ€§ï¼Œå¹¶è¿›è¡Œç­›é€‰å’Œæ’åº
    
    Args:
        raw_insights (list): åŸå§‹æ´å¯Ÿæ•°æ®åˆ—è¡¨
        topic (str): ä¸»é¢˜
        llm_processor: LLMå¤„ç†å™¨å®ä¾‹ï¼Œç”¨äºé«˜çº§è¯„åˆ†
        
    Returns:
        list: ç­›é€‰å¹¶æ’åºåçš„æ´å¯Ÿæ•°æ®
    """
    if not raw_insights or not llm_processor:
        return raw_insights
    
    print(f"æ­£åœ¨è¯„ä¼°{len(raw_insights)}æ¡åŸå§‹æ´å¯Ÿæ•°æ®ä¸'{topic}'çš„ç›¸å…³æ€§...")
    
    # è¯„ä¼°æ ‡å‡†
    criteria = {
        "ä¸»é¢˜ç›¸å…³æ€§": 0.5,   # å†…å®¹ä¸ä¸»é¢˜çš„ç›´æ¥ç›¸å…³æ€§
        "ä¿¡æ¯è´¨é‡": 0.3,     # å†…å®¹çš„å®Œæ•´æ€§ã€æ·±åº¦å’Œä¿¡æ¯é‡
        "æ—¶æ•ˆæ€§": 0.1,       # å†…å®¹çš„æ–°é²œåº¦
        "å¯æ“ä½œæ€§": 0.1      # å†…å®¹çš„å®ç”¨æ€§å’ŒæŒ‡å¯¼ä»·å€¼
    }
    
    scored_insights = []
    
    try:
        for item in raw_insights:
            # ç¡®ä¿å†…å®¹å­—æ®µçš„å­˜åœ¨
            title = item.get('title', '')
            content = item.get('content', '')
            if not content and not title:
                continue
                
            # åˆ›å»ºè¯„ä¼°æç¤º
            prompt = f"""
            è¯·è¯„ä¼°ä»¥ä¸‹åŸå§‹å†…å®¹ä¸'{topic}'ä¸»é¢˜çš„ç›¸å…³æ€§å’Œä¿¡æ¯è´¨é‡ï¼Œæ ¹æ®ä»¥ä¸‹æ ‡å‡†ç»™å‡º1-10åˆ†çš„è¯„åˆ†ï¼š
            
            æ ‡é¢˜: {title}
            å†…å®¹: {content[:800]}...
            
            è¯„åˆ†æ ‡å‡†:
            1. ä¸»é¢˜ç›¸å…³æ€§ (1-10åˆ†): å†…å®¹ä¸'{topic}'ä¸»é¢˜çš„ç›´æ¥ç›¸å…³ç¨‹åº¦ï¼Œæ˜¯å¦æ¶µç›–ä¸»é¢˜çš„æ ¸å¿ƒæ–¹é¢
            2. ä¿¡æ¯è´¨é‡ (1-10åˆ†): å†…å®¹çš„å®Œæ•´æ€§ã€æ·±åº¦ã€ä¿¡æ¯å¯†åº¦å’Œå‡†ç¡®æ€§
            3. æ—¶æ•ˆæ€§ (1-10åˆ†): å†…å®¹çš„æ–°é²œåº¦å’Œå¯¹å½“å‰æƒ…å†µçš„åæ˜ ç¨‹åº¦
            4. å¯æ“ä½œæ€§ (1-10åˆ†): å†…å®¹æ˜¯å¦æä¾›æœ‰å®ç”¨ä»·å€¼çš„è§è§£æˆ–å»ºè®®
            
            è¯·ä»¥JSONæ ¼å¼è¿”å›è¯„åˆ†å’Œä¸€å¥è¯ç†ç”±ï¼ŒåªåŒ…å«ä»¥ä¸‹å­—æ®µ:
            {{
                "ä¸»é¢˜ç›¸å…³æ€§": åˆ†æ•°,
                "ä¿¡æ¯è´¨é‡": åˆ†æ•°,
                "æ—¶æ•ˆæ€§": åˆ†æ•°,
                "å¯æ“ä½œæ€§": åˆ†æ•°,
                "æ€»åˆ†": åŠ æƒæ€»åˆ†,
                "æ¨èç†ç”±": "ä¸€å¥è¯è¯´æ˜è¿™æ¡æ´å¯Ÿçš„ä»·å€¼æˆ–æ¨è/ä¸æ¨èç†ç”±",
                "é€‚åˆç« èŠ‚": "è¿™æ¡å†…å®¹æœ€é€‚åˆæ”¾åœ¨å“ªä¸ªç« èŠ‚ï¼ˆè¡Œä¸šå®šä¹‰/å‘å±•å†ç¨‹/äº§ä¸šé“¾/å¸‚åœºæ ¼å±€/æ ¸å¿ƒé©±åŠ¨/æœªæ¥å±•æœ›/æ”¿ç­–ç¯å¢ƒï¼‰"
            }}
            """
            
            try:
                # ä½¿ç”¨ä¸“é—¨çš„JSON APIè°ƒç”¨æ–¹æ³•
                system_message = "ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„è¡Œä¸šåˆ†æå¸ˆï¼Œæ“…é•¿è¯„ä¼°å†…å®¹çš„ç›¸å…³æ€§ã€è´¨é‡å’Œå®ç”¨ä»·å€¼ã€‚ä½ çš„å›ç­”å¿…é¡»æ˜¯ä¸¥æ ¼çš„JSONæ ¼å¼ï¼Œä¸åŒ…å«ä»»ä½•å…¶ä»–æ–‡æœ¬ã€‚"
                
                scores = llm_processor.call_llm_api_json(prompt, system_message)
                
                # è®¡ç®—åŠ æƒå¾—åˆ†
                weighted_score = 0
                for criterion, weight in criteria.items():
                    if criterion in scores:
                        weighted_score += scores[criterion] * weight
                        
                # ä½¿ç”¨è®¡ç®—çš„åŠ æƒåˆ†æ•°æˆ–APIè¿”å›çš„æ€»åˆ†
                final_score = scores.get("æ€»åˆ†", weighted_score)
                
                # å°†åˆ†æ•°å­˜å‚¨åˆ°å†…å®¹é¡¹ä¸­
                item["relevance_score"] = final_score
                item["detailed_scores"] = scores
                item["recommendation_reason"] = scores.get("æ¨èç†ç”±", "")
                item["suggested_section"] = scores.get("é€‚åˆç« èŠ‚", "")
                
                scored_insights.append(item)
                
            except Exception as e:
                print(f"è¯„ä¼°æ´å¯Ÿå†…å®¹æ—¶å‡ºé”™: {str(e)}")
                # å‡ºé”™æ—¶ç»™äºˆé»˜è®¤åˆ†æ•°
                item["relevance_score"] = 5.0
                item["detailed_scores"] = {
                    "ä¸»é¢˜ç›¸å…³æ€§": 5.0,
                    "ä¿¡æ¯è´¨é‡": 5.0,
                    "æ—¶æ•ˆæ€§": 5.0,
                    "å¯æ“ä½œæ€§": 5.0,
                    "æ€»åˆ†": 5.0
                }
                scored_insights.append(item)
        
        # æŒ‰ç›¸å…³æ€§å¾—åˆ†æ’åº
        scored_insights.sort(key=lambda x: x.get("relevance_score", 0), reverse=True)
        
        # ç­›é€‰è¯„åˆ†é«˜äºé˜ˆå€¼çš„å†…å®¹ï¼ˆä¾‹å¦‚7åˆ†ä»¥ä¸Šï¼‰
        high_quality_insights = [item for item in scored_insights if item.get("relevance_score", 0) >= 7.0]
        
        # å¦‚æœé«˜è´¨é‡å†…å®¹å¤ªå°‘ï¼Œæ”¾å®½æ ‡å‡†
        if len(high_quality_insights) < 3:
            high_quality_insights = scored_insights[:5]  # è‡³å°‘å–å‰5ä¸ª
        
        print(f"å®Œæˆæ´å¯Ÿå†…å®¹ç›¸å…³æ€§è¯„åˆ†ï¼Œä»{len(scored_insights)}æ¡ä¸­ç­›é€‰å‡º{len(high_quality_insights)}æ¡é«˜è´¨é‡å†…å®¹")
        return high_quality_insights
        
    except Exception as e:
        print(f"LLMç›¸å…³æ€§è¯„ä¼°å¤±è´¥: {str(e)}ï¼Œè¿”å›æœªç­›é€‰çš„åŸå§‹æ•°æ®")
        return raw_insights

def expand_search_keywords(topic, llm_processor=None):
    """
    ä½¿ç”¨LLMæ‰©å±•æœç´¢å…³é”®è¯ï¼ŒåŒ…æ‹¬ä¸­è‹±æ–‡ã€ç›¸å…³æœ¯è¯­ç­‰
    
    Args:
        topic (str): åŸå§‹ä¸»é¢˜
        llm_processor: LLMå¤„ç†å™¨å®ä¾‹
        
    Returns:
        list: æ‰©å±•åçš„å…³é”®è¯åˆ—è¡¨ï¼ˆæœ€å¤š5ä¸ªï¼‰
    """
    if not llm_processor:
        # å¦‚æœæ²¡æœ‰LLMå¤„ç†å™¨ï¼Œè¿”å›åŸºæœ¬çš„ä¸­è‹±æ–‡ç»„åˆ
        english_topic = topic.replace('AI', 'Artificial Intelligence').replace('+', ' ')
        return [topic, english_topic]
    
    try:
        prompt = f"""è¯·åŸºäºä¸»é¢˜"{topic}"ï¼Œç”Ÿæˆæœ€ç›¸å…³çš„5ä¸ªæœç´¢å…³é”®è¯ï¼ŒåŒ…æ‹¬ï¼š
1. ä¸­è‹±æ–‡å¯¹ç…§ï¼ˆå¿…é¡»åŒ…å«ï¼‰
2. ç›¸å…³æœ¯è¯­å’Œæ¦‚å¿µ
3. è¡Œä¸šé€šç”¨è¯´æ³•

è¦æ±‚ï¼š
1. å…³é”®è¯è¦ä¸“ä¸šå‡†ç¡®
2. ç¡®ä¿ç›¸å…³æ€§ä»é«˜åˆ°ä½æ’åº
3. åªè¿”å›æœ€ç›¸å…³çš„5ä¸ªå…³é”®è¯
4. ç¡®ä¿ä¸­è‹±æ–‡éƒ½æœ‰è¦†ç›–
5. é€‚åˆæœç´¢å¼•æ“ä½¿ç”¨

è¯·ä»¥JSONæ ¼å¼è¿”å›ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
{{
    "keywords": [
        "å…³é”®è¯1",
        "å…³é”®è¯2",
        "å…³é”®è¯3",
        "å…³é”®è¯4",
        "å…³é”®è¯5"
    ]
}}

æ³¨æ„ï¼šä¸¥æ ¼éµå®ˆJSONæ ¼å¼ï¼Œç¡®ä¿åŒå¼•å·æ­£ç¡®ä½¿ç”¨ï¼Œç¡®ä¿JSONå¯ä»¥è¢«æ­£ç¡®è§£æã€‚
"""
        
        system_message = f"""ä½ æ˜¯ä¸€ä½ç²¾é€š{topic}é¢†åŸŸçš„ä¸“å®¶ï¼Œå¯¹è¯¥é¢†åŸŸçš„å„ç§ä¸“ä¸šæœ¯è¯­å’Œè¡¨è¾¾æ–¹å¼éƒ½éå¸¸ç†Ÿæ‚‰ã€‚
ä½ éœ€è¦å¸®åŠ©ç”Ÿæˆä¸€ä¸ªç®€çŸ­ä½†å‡†ç¡®çš„æœç´¢å…³é”®è¯åˆ—è¡¨ï¼Œè¿™äº›å…³é”®è¯å°†ç”¨äºæœç´¢å¼•æ“æ£€ç´¢ç›¸å…³å†…å®¹ã€‚
è¯·ç¡®ä¿ç”Ÿæˆçš„å…³é”®è¯ä¸“ä¸šã€å‡†ç¡®ã€ç›¸å…³æ€§é«˜ï¼Œå¹¶ä¸¥æ ¼éµå®ˆJSONæ ¼å¼è§„èŒƒã€‚"""

        # ä½¿ç”¨LLMç”Ÿæˆå…³é”®è¯åˆ—è¡¨
        response = llm_processor.call_llm_api_json(prompt, system_message)
        
        if isinstance(response, dict) and "keywords" in response:
            expanded_keywords = response["keywords"]
            # ç¡®ä¿åŸå§‹å…³é”®è¯åœ¨åˆ—è¡¨ä¸­
            if topic not in expanded_keywords:
                expanded_keywords.insert(0, topic)
                # å¦‚æœæ’å…¥åè¶…è¿‡5ä¸ªï¼Œåˆ é™¤æœ€åä¸€ä¸ª
                if len(expanded_keywords) > 5:
                    expanded_keywords = expanded_keywords[:5]
            print(f"æˆåŠŸæ‰©å±•å…³é”®è¯ï¼šä»1ä¸ªæ‰©å±•åˆ°{len(expanded_keywords)}ä¸ª")
            return expanded_keywords
            
    except Exception as e:
        print(f"æ‰©å±•å…³é”®è¯æ—¶å‡ºé”™: {str(e)}")
    
    # å‡ºé”™æ—¶è¿”å›åŸºæœ¬çš„ä¸­è‹±æ–‡ç»„åˆ
    english_topic = topic.replace('AI', 'Artificial Intelligence').replace('+', ' ')
    return [topic, english_topic]

# å·²è¢« ParallelInsightsCollector æ›¿ä»£çš„æ—§å‡½æ•°
# def get_raw_industry_data_by_section(topic, section, llm_processor=None):
#     """
#     [å·²åºŸå¼ƒ] è·å–å•ä¸ªç« èŠ‚çš„åŸå§‹æ•°æ®ï¼Œå¹¶ç«‹å³è¯„ä¼°ç­›é€‰ - å¤šæ¸ é“æ•´åˆç‰ˆæœ¬
#     æ­¤å‡½æ•°å·²è¢« ParallelInsightsCollector.parallel_collect_section_queries æ›¿ä»£
#     """
#     pass

def get_industry_insights(topic, subtopics=None, parallel_config="balanced"):
    """
    ğŸš€ å¹¶è¡Œè·å–è¡Œä¸šæ´å¯Ÿæ•°æ®
    
    Args:
        topic (str): ä¸»é¢˜
        subtopics (list): å­ä¸»é¢˜åˆ—è¡¨
        parallel_config (str): å¹¶è¡Œé…ç½® ("conservative", "balanced", "aggressive")
        
    Returns:
        dict: åŒ…å«è¡Œä¸šæ´å¯Ÿå†…å®¹å’Œæ¥æºçš„å­—å…¸
    """
    print(f"\nğŸš€ === å¼€å§‹å¹¶è¡Œæ”¶é›†{topic}è¡Œä¸šæ´å¯Ÿ ===")
    overall_start_time = time.time()
    
    try:
        # åˆå§‹åŒ–LLMå¤„ç†å™¨
        llm_processor = None
        try:
            llm_processor = LLMProcessor()
            print("âœ… å·²åˆå§‹åŒ–LLMå¤„ç†å™¨ç”¨äºå†…å®¹ç›¸å…³æ€§è¯„ä¼°å’Œç”Ÿæˆ")
        except Exception as e:
            print(f"âŒ åˆå§‹åŒ–LLMå¤„ç†å™¨å¤±è´¥: {str(e)}ï¼Œå°†è·³è¿‡æ™ºèƒ½åŠŸèƒ½")
        
        # ä½¿ç”¨æ ‡å‡†ç« èŠ‚ç»“æ„
        if not subtopics:
            subtopics = [
                "è¡Œä¸šå®šä¹‰ä¸æ ¸å¿ƒç‰¹ç‚¹",
                "å‘å±•å†ç¨‹ä¸é˜¶æ®µæ¼”è¿›",
                "äº§ä¸šé“¾ä¸ä»·å€¼åˆ†å¸ƒ",
                "å¸‚åœºæ ¼å±€ä¸å‚ä¸è€…",
                "æ ¸å¿ƒé©±åŠ¨ä¸è¶‹åŠ¿",
                "æœªæ¥å±•æœ›ä¸æŒ‘æˆ˜åº”å¯¹",
                "æ”¿ç­–ç¯å¢ƒåˆ†æ"
            ]
        
        # ğŸš€ åˆå§‹åŒ–å¹¶è¡Œæ•°æ®æ”¶é›†å™¨
        try:
            parallel_collector = ParallelInsightsCollector()
        except Exception as e:
            print(f"âŒ åˆå§‹åŒ–å¹¶è¡Œæ”¶é›†å™¨å¤±è´¥: {str(e)}ï¼Œä½¿ç”¨å¤‡ç”¨æ–¹æ³•")
            return _fallback_insights_generation(topic, subtopics)
        
        # ğŸ¯ é…ç½®å¹¶è¡Œå‚æ•°
        parallel_configs = {
            "conservative": {"section_workers": 3, "query_workers": 4, "content_workers": 2},
            "balanced": {"section_workers": 4, "query_workers": 6, "content_workers": 3},
            "aggressive": {"section_workers": 6, "query_workers": 8, "content_workers": 4}
        }
        
        config = parallel_configs.get(parallel_config, parallel_configs["balanced"])
        print(f"âš™ï¸ ä½¿ç”¨å¹¶è¡Œé…ç½®: {parallel_config}")
        print(f"   - ç« èŠ‚å¹¶è¡Œæ•°: {config['section_workers']}")
        print(f"   - æŸ¥è¯¢å¹¶è¡Œæ•°: {config['query_workers']}")
        print(f"   - å†…å®¹ç”Ÿæˆå¹¶è¡Œæ•°: {config['content_workers']}")
        
        # ğŸš€ ç¬¬1-2å±‚å¹¶è¡Œï¼šå¹¶è¡Œæ”¶é›†æ‰€æœ‰ç« èŠ‚æ•°æ®
        sections_data = parallel_collector.parallel_collect_all_sections(
            topic, subtopics, llm_processor, max_workers=config['section_workers']
        )
        
        # æ£€æŸ¥æ•°æ®æ”¶é›†ç»“æœ
        total_items = sum(len(data) for data in sections_data.values())
        if total_items == 0:
            print("âš ï¸ æ‰€æœ‰ç« èŠ‚å‡æœªæ”¶é›†åˆ°æœ‰æ•ˆæ•°æ®ï¼Œä½¿ç”¨å¤‡ç”¨ç”Ÿæˆæ–¹æ³•")
            return _fallback_insights_generation(topic, subtopics)
        
        print(f"ğŸ“Š æ•°æ®æ”¶é›†é˜¶æ®µå®Œæˆï¼Œæ€»è®¡æ”¶é›†åˆ° {total_items} æ¡é«˜è´¨é‡æ•°æ®")
        
        # âš¡ ç¬¬3å±‚å¹¶è¡Œï¼šå¹¶è¡Œç”Ÿæˆç« èŠ‚å†…å®¹
        if llm_processor:
            parallel_processor = ParallelInsightsProcessor(llm_processor)
            generated_sections = parallel_processor.parallel_generate_sections_content(
                sections_data, topic, max_workers=config['content_workers']
            )
        else:
            print("âš ï¸ æ— LLMå¤„ç†å™¨ï¼Œä½¿ç”¨ç®€å•å†…å®¹ç”Ÿæˆ")
            generated_sections = {}
            for section_name, section_items in sections_data.items():
                generated_sections[section_name] = generate_section_content_simple(section_items)
        
        # ğŸ“ ç»„ç»‡æœ€ç»ˆæŠ¥å‘Š
        insights_data = _organize_parallel_insights_report(
            generated_sections, sections_data, topic, subtopics
        )
        
        # ğŸ“Š æ€§èƒ½ç»Ÿè®¡
        total_time = time.time() - overall_start_time
        estimated_sequential_time = total_time * 3.5  # ä¼°ç®—ä¸²è¡Œæ—¶é—´
        time_saved = estimated_sequential_time - total_time
        speedup_ratio = estimated_sequential_time / total_time
        
        print("\n" + "=" * 60)
        print("ğŸ“Š å¹¶è¡Œæ´å¯ŸæŠ¥å‘Šç”Ÿæˆæ€§èƒ½ç»Ÿè®¡:")
        print(f"â±ï¸  å®é™…è€—æ—¶: {total_time:.1f}ç§’")
        print(f"ğŸŒ ä¸²è¡Œé¢„ä¼°: {estimated_sequential_time:.1f}ç§’")
        print(f"âš¡ æ—¶é—´èŠ‚çœ: {time_saved:.1f}ç§’")
        print(f"ğŸš€ æ€§èƒ½æå‡: {speedup_ratio:.1f}x")
        print(f"ğŸ“„ ç”Ÿæˆç« èŠ‚: {len(insights_data.get('sections', []))}ä¸ª")
        print(f"ğŸ“š æ•°æ®æ¥æº: {len(insights_data.get('sources', []))}ä¸ª")
        print(f"ğŸ”§ å¹¶è¡Œé…ç½®: {parallel_config}")
        print("=" * 60)
        
        print(f"ğŸ‰ å¹¶è¡Œè¡Œä¸šæ´å¯ŸæŠ¥å‘Šç”Ÿæˆå®Œæˆï¼")
        return insights_data
            
    except Exception as e:
        print(f"âŒ å¹¶è¡Œç”Ÿæˆè¡Œä¸šæ´å¯ŸæŠ¥å‘Šæ—¶å‡ºé”™: {str(e)}ï¼Œä½¿ç”¨å¤‡ç”¨æ–¹æ³•...")
        return _fallback_insights_generation(topic, subtopics)

def _fallback_insights_generation(topic, subtopics):
    """å¤‡ç”¨çš„æ´å¯Ÿç”Ÿæˆæ–¹æ³•"""
    print("ğŸ›¡ï¸ ä½¿ç”¨å¤‡ç”¨æ´å¯Ÿç”Ÿæˆæ–¹æ³•...")
    
    fallback_insights = generate_industry_insights_without_api(topic, subtopics)
    
    sections = []
    sources = []
    
    for article in fallback_insights:
        section = {
            "title": article.get('title', "æœªçŸ¥éƒ¨åˆ†"),
            "content": article.get('content', "æ— å†…å®¹")
        }
        sections.append(section)
    
    return {
        "title": f"{topic}è¡Œä¸šæ´å¯Ÿ (ç³»ç»Ÿç”Ÿæˆ)",
        "sections": sections,
        "sources": sources,
        "date": datetime.now().strftime('%Y-%m-%d')
    }

def _organize_parallel_insights_report(generated_sections, sections_data, topic, subtopics):
    """ç»„ç»‡å¹¶è¡Œç”Ÿæˆçš„æ´å¯ŸæŠ¥å‘Š"""
    structured_sections = []
    sources = []
    
    # æŒ‰æ ‡å‡†ç« èŠ‚é¡ºåºç»„ç»‡å†…å®¹
    for section_name in subtopics:
        if section_name in generated_sections and generated_sections[section_name]:
            section_title = f"{topic}{section_name}"
            section_content = generated_sections[section_name]
            
            structured_sections.append({
                "title": section_title,
                "content": section_content
            })
            
            # æ”¶é›†è¯¥ç« èŠ‚çš„æ•°æ®æ¥æº
            if section_name in sections_data:
                for item in sections_data[section_name]:
                    source_title = item.get("title", "æœªçŸ¥æ ‡é¢˜")
                    source_url = item.get("url", "#")
                    source_name = item.get("source", "æœªçŸ¥æ¥æº")
                    
                    sources.append({
                        "title": source_title,
                        "url": source_url,
                        "source": source_name
                    })
    
    # ç”Ÿæˆå®Œæ•´æŠ¥å‘Šå†…å®¹
    report_content = f"# {topic}è¡Œä¸šæ´å¯ŸæŠ¥å‘Š\n\n"
    
    for section in structured_sections:
        report_content += f"## {section['title']}\n\n{section['content']}\n\n"
    
    # æ·»åŠ å‚è€ƒèµ„æ–™ï¼ˆå»é‡ï¼‰
    if sources:
        report_content += "## å‚è€ƒèµ„æ–™\n\n"
        seen_urls = set()
        for source in sources:
            url = source.get("url", "#")
            title = source.get("title", "æœªçŸ¥æ ‡é¢˜")
            source_name = source.get("source", "æœªçŸ¥æ¥æº")
            
            if url not in seen_urls:
                report_content += f"- [{title}]({url}) - {source_name}\n"
                seen_urls.add(url)
    
    return {
        "title": f"{topic}è¡Œä¸šæ´å¯ŸæŠ¥å‘Š",
        "content": report_content,
        "sections": structured_sections,
        "sources": sources,
        "date": datetime.now().strftime('%Y-%m-%d')
    }

# å·²è¢« _organize_parallel_insights_report æ›¿ä»£çš„æ—§å‡½æ•°
# def organize_industry_insights_with_sources(filtered_data, topic, subtopics, llm_processor=None, sections_data=None):
#     """
#     [å·²åºŸå¼ƒ] ä½¿ç”¨ç­›é€‰åçš„æ•°æ®ç»„ç»‡è¡Œä¸šæ´å¯ŸæŠ¥å‘Š
#     æ­¤å‡½æ•°å·²è¢« _organize_parallel_insights_report æ›¿ä»£
#     """
#     pass

def generate_section_content_simple(section_items):
    """
    ä½¿ç”¨ç®€å•æ–¹æ³•ç”Ÿæˆç« èŠ‚å†…å®¹ï¼Œç¡®ä¿åœ¨æ¯ä¸ªå°ç‚¹ä¸‹æ·»åŠ æ¥æº
    æ³¨æ„ï¼šç°åœ¨æ¯ä¸ªç« èŠ‚æœ‰8-15æ¡èµ„æ–™ï¼Œéœ€è¦åˆ†æˆæ›´å°çš„å°èŠ‚ï¼Œå¹¶ç¡®ä¿å†…å®¹è¯¦å°½
    
    Args:
        section_items: ç« èŠ‚æ•°æ®é¡¹åˆ—è¡¨ï¼ˆé€šå¸¸æœ‰8-15ä¸ªå…ƒç´ ï¼‰
        
    Returns:
        str: ç”Ÿæˆçš„ç« èŠ‚å†…å®¹
    """
    content = ""
    
    # æŒ‰ç›¸å…³æ€§è¯„åˆ†æ’åºï¼ˆç¡®ä¿æœ€å¥½çš„å†…å®¹åœ¨å‰ï¼‰
    section_items.sort(key=lambda x: x.get("relevance_score", 0), reverse=True)
    
    # æ·»åŠ ç« èŠ‚æ‘˜è¦
    if len(section_items) > 0:
        content += "### ç« èŠ‚æ¦‚è¿°\n\n"
        overview = "æœ¬ç« èŠ‚åŸºäºå¯¹å¤šç§æƒå¨èµ„æ–™çš„æ•´åˆåˆ†æï¼Œæä¾›äº†å…¨é¢ä¸”æ·±å…¥çš„è¡Œä¸šæ´å¯Ÿã€‚ä»¥ä¸‹å†…å®¹å°†ä»å¤šä¸ªç»´åº¦å±•å¼€è¯¦ç»†åˆ†æï¼Œæ¶µç›–äº†æœ€æ–°æ•°æ®ã€å…³é”®è¶‹åŠ¿å’Œä¸“ä¸šè§‚ç‚¹ã€‚\n\n"
        content += overview
    
    # å¤„ç†æ¯ä¸ªé¡¹ç›®
    for i, item in enumerate(section_items):
        title = item.get("title", f"è¦ç‚¹{i+1}")
        item_content = item.get("content", "").strip()
        
        # è·å–æ¥æºä¿¡æ¯
        source_name = item.get("source", "è¡Œä¸šåˆ†æ")
        source_url = item.get("url", "#")
        
        # åˆ›å»ºä¸‰çº§æ ‡é¢˜
        content += f"### {title}\n\n"
        
        # å°†å†…å®¹åˆ†æˆæ›´å°çš„æ®µè½ï¼Œä½†ä¿ç•™æ›´å¤šå†…å®¹å¹¶ç¡®ä¿è¯¦å°½
        if len(item_content) > 500:
            # å°è¯•æŒ‰æ®µè½åˆ†å‰²
            paragraphs = item_content.split('\n\n')
            
            # å¦‚æœæ²¡æœ‰åˆé€‚çš„æ®µè½åˆ†éš”ï¼Œå°è¯•æŒ‰å¥å·åˆ†å‰²
            if len(paragraphs) < 3:
                sentences = item_content.replace('ã€‚', 'ã€‚\n').replace('ï¼', 'ï¼\n').replace('ï¼Ÿ', 'ï¼Ÿ\n').split('\n')
                # é‡æ–°ç»„ç»‡æˆæ›´å°çš„æ®µè½ï¼Œæ¯3-5ä¸ªå¥å­ä¸€ä¸ªæ®µè½
                paragraphs = []
                current_para = []
                
                for sentence in sentences:
                    if not sentence.strip():
                        continue
                    current_para.append(sentence)
                    if len(current_para) >= 3:
                        paragraphs.append(''.join(current_para))
                        current_para = []
                
                # æ·»åŠ å‰©ä½™å¥å­
                if current_para:
                    paragraphs.append(''.join(current_para))
            
            # åˆ›å»ºå››çº§æ ‡é¢˜å’Œå°èŠ‚
            # æ ¹æ®å†…å®¹æ¨æ–­å¯èƒ½çš„å°èŠ‚æ ‡é¢˜
            section_keywords = ["æ¦‚è¿°", "å®šä¹‰", "ç‰¹ç‚¹", "å†å²", "å‘å±•", "åº”ç”¨", "æ¡ˆä¾‹", "æŒ‘æˆ˜", "å‰æ™¯", "è¶‹åŠ¿", 
                              "åŸç†", "æ–¹æ³•", "åˆ†æ", "å½±å“", "è¯„ä¼°", "ç°çŠ¶", "æœºåˆ¶", "æ¯”è¾ƒ", "ä¼˜åŠ¿", "åŠ£åŠ¿", 
                              "æŠ€æœ¯è·¯çº¿", "å¸‚åœºæ•°æ®", "åŒºåŸŸåˆ†å¸ƒ", "å…³é”®æŒ‡æ ‡", "ä¸»è¦å‚ä¸è€…"]
            
            # åˆ›å»ºåˆ†æ®µå†…å®¹ï¼Œå¤§å¹…å¢åŠ ä¿ç•™å†…å®¹é‡
            for j, para in enumerate(paragraphs[:18]):  # å¢åŠ å°èŠ‚æ•°é‡é™åˆ¶
                if j < len(section_keywords):
                    subtitle = f"{title}çš„{section_keywords[j]}"
                else:
                    subtitle = f"{title}çš„æ‰©å±•åˆ†æ({j+1})"
                
                # æ·»åŠ å››çº§æ ‡é¢˜
                content += f"#### {subtitle}\n\n"
                
                # æ·»åŠ æ®µè½å†…å®¹ï¼Œå¢åŠ å­—ç¬¦é™åˆ¶
                if len(para) > 8000:  # å¢åŠ å­—ç¬¦é™åˆ¶
                    para = para[:8000] + "..."
                
                content += f"{para}\n\n"
                # æ¯ä¸ªå°èŠ‚éƒ½åŠ å¼•ç”¨
                content += f"[æ•°æ®æ¥æº: {source_name} - {source_url}]\n\n"
        else:
            # å¦‚æœå†…å®¹æœ¬èº«å°±å¾ˆçŸ­ï¼Œç›´æ¥æ·»åŠ 
            content += f"{item_content}\n\n"
            content += f"[æ•°æ®æ¥æº: {source_name} - {source_url}]\n\n"
    
    # æ·»åŠ ç« èŠ‚æ€»ç»“
    if len(section_items) > 0:
        content += "### ç« èŠ‚å°ç»“\n\n"
        summary = "ç»¼åˆä»¥ä¸Šåˆ†æï¼Œæœ¬ç« èŠ‚å…¨é¢é˜è¿°äº†ç›¸å…³é¢†åŸŸçš„æ ¸å¿ƒè¦ç‚¹å’Œæœ€æ–°å‘å±•ã€‚é€šè¿‡å¤šç»´åº¦çš„æ•°æ®å’Œæ¡ˆä¾‹åˆ†æï¼Œä¸ºè¯»è€…æä¾›äº†æ·±å…¥ç†è§£è¡Œä¸šç°çŠ¶ä¸è¶‹åŠ¿çš„åŸºç¡€ã€‚åç»­ç« èŠ‚å°†è¿›ä¸€æ­¥æ¢è®¨å…¶ä»–å…³é”®æ–¹é¢ï¼Œå½¢æˆå®Œæ•´çš„è¡Œä¸šæ´å¯Ÿä½“ç³»ã€‚\n\n"
        content += summary
    
    return content

def generate_insights_report(topic, subtopics=None, output_file=None, parallel_config="balanced"):
    """
    ğŸš€ ç”Ÿæˆå¹¶è¡Œè¡Œä¸šæ´å¯ŸæŠ¥å‘Š
    
    Args:
        topic (str): ä¸»é¢˜
        subtopics (list): å­ä¸»é¢˜åˆ—è¡¨
        output_file (str): è¾“å‡ºæ–‡ä»¶åæˆ–è·¯å¾„
        parallel_config (str): å¹¶è¡Œé…ç½® ("conservative", "balanced", "aggressive")
        
    Returns:
        tuple: (æŠ¥å‘Šæ–‡ä»¶è·¯å¾„, æŠ¥å‘Šæ•°æ®)
    """
    print(f"ğŸš€ å¼€å§‹ç”Ÿæˆå¹¶è¡Œè¡Œä¸šæ´å¯ŸæŠ¥å‘Š: {topic}")
    print(f"âš™ï¸ å¹¶è¡Œé…ç½®: {parallel_config}")
    
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    os.makedirs(config.OUTPUT_DIR, exist_ok=True)
    
    # ğŸš€ ä½¿ç”¨å¹¶è¡Œæ–¹æ³•è·å–è¡Œä¸šæ´å¯Ÿæ•°æ®
    insights_data = get_industry_insights(topic, subtopics, parallel_config)
    
    # æå–å†…å®¹
    if "content" in insights_data and insights_data["content"]:
        content = insights_data["content"]
    else:
        # å¤‡ç”¨æ–¹æ³•ï¼šä»sectionsç»„ç»‡å†…å®¹
        title = insights_data.get("title", f"{topic}è¡Œä¸šæ´å¯Ÿ")
        
        content = f"# {title}\n\n"
        
        # æ·»åŠ ç« èŠ‚å†…å®¹
        for section in insights_data.get("sections", []):
            section_title = section.get("title", "æœªçŸ¥éƒ¨åˆ†")
            section_content = section.get("content", "æ— å†…å®¹")
            content += f"## {section_title}\n\n{section_content}\n\n"
        
        # æ·»åŠ å‚è€ƒèµ„æ–™
        sources = insights_data.get("sources", [])
        if sources:
            content += "## å‚è€ƒèµ„æ–™\n\n"
            seen_urls = set()
            for source in sources:
                url = source.get('url', '#')
                title = source.get('title', 'æœªçŸ¥æ ‡é¢˜')
                source_name = source.get('source', 'æœªçŸ¥æ¥æº')
                
                if url not in seen_urls:
                    content += f"- [{title}]({url}) - {source_name}\n"
                    seen_urls.add(url)
    
    # ç¡®å®šè¾“å‡ºæ–‡ä»¶è·¯å¾„
    if not output_file:
        # å¦‚æœæ²¡æœ‰æä¾›è¾“å‡ºæ–‡ä»¶ï¼Œä½¿ç”¨é»˜è®¤å‘½å
        date_str = datetime.now().strftime('%Y%m%d')
        clean_topic = topic.replace(' ', '_').replace('/', '_').replace('\\', '_').lower()
        output_file = os.path.join(config.OUTPUT_DIR, f"{clean_topic}_insights_parallel_{date_str}.md")
    elif not os.path.isabs(output_file):
        # å¦‚æœæä¾›çš„æ˜¯ç›¸å¯¹è·¯å¾„ï¼Œç¡®ä¿æ­£ç¡®æ‹¼æ¥
        output_dir = os.path.dirname(output_file)
        if output_dir:
            os.makedirs(output_dir, exist_ok=True)
    
    # å†™å…¥æŠ¥å‘Š
    with open(output_file, 'w', encoding='utf-8-sig') as f:
        f.write(content)
    
    print(f"\nğŸ‰ === å¹¶è¡Œè¡Œä¸šæ´å¯ŸæŠ¥å‘Šç”Ÿæˆå®Œæˆ ===")
    print(f"ğŸ“„ æŠ¥å‘Šå·²ä¿å­˜è‡³: {output_file}")
    print(f"ğŸ“Š æŠ¥å‘Šç»Ÿè®¡:")
    print(f"   - ç« èŠ‚æ•°é‡: {len(insights_data.get('sections', []))}")
    print(f"   - æ•°æ®æ¥æº: {len(insights_data.get('sources', []))}")
    print(f"   - æ–‡ä»¶å¤§å°: {len(content)} å­—ç¬¦")
    
    # ä¿®å¤æŠ¥å‘Šä¸­çš„æ ‡é¢˜é—®é¢˜
    print("ğŸ”§ æ­£åœ¨ä¼˜åŒ–æŠ¥å‘Šæ ‡é¢˜æ ¼å¼...")
    fix_markdown_headings(output_file)
    
    return output_file, insights_data

if __name__ == "__main__":
    # åŠ è½½ç¯å¢ƒå˜é‡
    load_dotenv()
    
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(
        description='ğŸš€ ç”Ÿæˆå¹¶è¡Œè¡Œä¸šæ´å¯ŸæŠ¥å‘Š',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
å¹¶è¡Œé…ç½®è¯´æ˜:
  conservative  - ä¿å®ˆæ¨¡å¼ (3/4/2): èŠ‚çœç³»ç»Ÿèµ„æºï¼Œé€‚åˆé…ç½®è¾ƒä½çš„æœºå™¨
  balanced      - å¹³è¡¡æ¨¡å¼ (4/6/3): é»˜è®¤æ¨¡å¼ï¼Œå¹³è¡¡æ€§èƒ½ä¸èµ„æºæ¶ˆè€—
  aggressive    - æ¿€è¿›æ¨¡å¼ (6/8/4): æœ€å¤§åŒ–å¹¶è¡Œæ€§èƒ½ï¼Œéœ€è¦è¾ƒé«˜é…ç½®

ä½¿ç”¨ç¤ºä¾‹:
  python generate_insights_report_updated_copy.py --topic "äººå·¥æ™ºèƒ½" --parallel balanced
  python generate_insights_report_updated_copy.py --topic "åŒºå—é“¾" --parallel aggressive --output my_report.md
  python generate_insights_report_updated_copy.py --topic "æ–°èƒ½æºæ±½è½¦" --subtopics "ç”µæ± æŠ€æœ¯" "å……ç”µè®¾æ–½" --parallel conservative
        """
    )
    
    parser.add_argument('--topic', type=str, required=True, help='æŠ¥å‘Šçš„ä¸»é¢˜')
    parser.add_argument('--subtopics', type=str, nargs='*', help='ä¸ä¸»é¢˜ç›¸å…³çš„å­ä¸»é¢˜')
    parser.add_argument('--output', type=str, help='è¾“å‡ºæ–‡ä»¶å')
    parser.add_argument('--parallel', type=str, choices=['conservative', 'balanced', 'aggressive'], 
                       default='balanced', help='å¹¶è¡Œé…ç½® (é»˜è®¤: balanced)')
    
    args = parser.parse_args()
    
    # æ˜¾ç¤ºå¯åŠ¨ä¿¡æ¯
    print("ğŸš€ " + "=" * 50)
    print("ğŸš€ å¹¶è¡Œè¡Œä¸šæ´å¯ŸæŠ¥å‘Šç”Ÿæˆå™¨")
    print("ğŸš€ " + "=" * 50)
    print(f"ğŸ“ ä¸»é¢˜: {args.topic}")
    if args.subtopics:
        print(f"ğŸ“ å­ä¸»é¢˜: {', '.join(args.subtopics)}")
    print(f"âš™ï¸ å¹¶è¡Œé…ç½®: {args.parallel}")
    if args.output:
        print(f"ğŸ“„ è¾“å‡ºæ–‡ä»¶: {args.output}")
    print("ğŸš€ " + "=" * 50)
    
    # ç”ŸæˆæŠ¥å‘Š
    try:
        output_file, insights_data = generate_insights_report(
            args.topic, 
            args.subtopics, 
            args.output, 
            args.parallel
        )
        
        print(f"\nâœ… æŠ¥å‘Šç”ŸæˆæˆåŠŸ!")
        print(f"ğŸ“„ æ–‡ä»¶è·¯å¾„: {output_file}")
        
    except KeyboardInterrupt:
        print("\nâŒ ç”¨æˆ·ä¸­æ–­äº†æŠ¥å‘Šç”Ÿæˆè¿‡ç¨‹")
        sys.exit(1)
    except Exception as e:
        print(f"\nâŒ æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {str(e)}")
        sys.exit(1) 