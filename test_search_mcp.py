#!/usr/bin/env python3
"""
SearchMcpæµ‹è¯•å’Œæ¼”ç¤ºæ–‡ä»¶
å±•ç¤ºå¦‚ä½•ä½¿ç”¨ç»Ÿä¸€çš„æœç´¢MCPè¿›è¡Œå¤šæºå¹¶è¡Œæœç´¢
"""

import sys
import os
from dotenv import load_dotenv

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# å¯¼å…¥SearchMcp
from collectors.search_mcp import SearchMcp, Document


def test_basic_search():
    """æµ‹è¯•åŸºç¡€æœç´¢åŠŸèƒ½"""
    print("ğŸ§ª æµ‹è¯•1: åŸºç¡€å¹¶è¡Œæœç´¢")
    print("=" * 50)
    
    # åˆå§‹åŒ–SearchMcp
    search_mcp = SearchMcp()
    
    # æ£€æŸ¥å¯ç”¨æ•°æ®æº
    available_sources = search_mcp.get_available_sources()
    print("ğŸ“Š å¯ç”¨æ•°æ®æº:")
    for category, sources in available_sources.items():
        print(f"  {category}: {sources}")
    
    # æ‰§è¡Œæœç´¢
    queries = [
        "ç”Ÿæˆå¼äººå·¥æ™ºèƒ½æœ€æ–°å‘å±•",
        "ChatGPT GPT-4æŠ€æœ¯åŸç†",
        "å¤§è¯­è¨€æ¨¡å‹å•†ä¸šåº”ç”¨"
    ]
    
    results = search_mcp.parallel_search(
        queries=queries,
        max_results_per_query=3,
        days_back=30,
        max_workers=4
    )
    
    print(f"\nğŸ“‹ æœç´¢ç»“æœç»Ÿè®¡:")
    print(f"  æ€»ç»“æœæ•°: {len(results)}")
    print(f"  å»é‡URLæ•°: {len(set(doc.url for doc in results))}")
    
    # æŒ‰æ•°æ®æºç±»å‹ç»Ÿè®¡
    source_stats = {}
    for doc in results:
        source_stats[doc.source_type] = source_stats.get(doc.source_type, 0) + 1
    
    print(f"  æŒ‰ç±»å‹ç»Ÿè®¡:")
    for source_type, count in source_stats.items():
        print(f"    {source_type}: {count}æ¡")
    
    # æ˜¾ç¤ºå‰3ä¸ªç»“æœ
    print(f"\nğŸ” å‰3ä¸ªæœç´¢ç»“æœ:")
    for i, doc in enumerate(results[:3]):
        print(f"\n[{i+1}] æ ‡é¢˜: {doc.title}")
        print(f"    æ¥æº: {doc.source} ({doc.source_type})")
        print(f"    URL: {doc.url}")
        print(f"    å†…å®¹: {doc.content[:100]}...")
        if doc.authors:
            print(f"    ä½œè€…: {', '.join(doc.authors)}")
        if doc.publish_date:
            print(f"    å‘å¸ƒæ—¶é—´: {doc.publish_date}")


def test_category_search():
    """æµ‹è¯•æŒ‰ç±»åˆ«æœç´¢"""
    print("\n\nğŸ§ª æµ‹è¯•2: æŒ‰ç±»åˆ«æœç´¢")
    print("=" * 50)
    
    search_mcp = SearchMcp()
    
    # æµ‹è¯•å­¦æœ¯æœç´¢
    print("ğŸ“š å­¦æœ¯æœç´¢æµ‹è¯•:")
    academic_results = search_mcp.search_by_category(
        queries=["machine learning transformer attention mechanism"],
        category='academic',
        max_results_per_query=2,
        days_back=365
    )
    
    print(f"  å­¦æœ¯æœç´¢ç»“æœ: {len(academic_results)}æ¡")
    for doc in academic_results[:2]:
        print(f"    - {doc.title} ({doc.source})")
        if doc.authors:
            print(f"      ä½œè€…: {', '.join(doc.authors[:3])}...")
    
    # æµ‹è¯•æ–°é—»æœç´¢
    print("\nğŸ“° æ–°é—»æœç´¢æµ‹è¯•:")
    news_results = search_mcp.search_by_category(
        queries=["äººå·¥æ™ºèƒ½æœ€æ–°åŠ¨æ€", "AIè¡Œä¸šå‘å±•"],
        category='news',
        max_results_per_query=2,
        days_back=7
    )
    
    print(f"  æ–°é—»æœç´¢ç»“æœ: {len(news_results)}æ¡")
    for doc in news_results[:2]:
        print(f"    - {doc.title} ({doc.source})")
        if doc.publish_date:
            print(f"      å‘å¸ƒæ—¶é—´: {doc.publish_date}")


def test_fallback_search():
    """æµ‹è¯•é™çº§æœç´¢"""
    print("\n\nğŸ§ª æµ‹è¯•3: é™çº§æœç´¢æµ‹è¯•")
    print("=" * 50)
    
    search_mcp = SearchMcp()
    
    # ä½¿ç”¨é™çº§æœç´¢
    results = search_mcp.search_with_fallback(
        queries=["é‡å­è®¡ç®—æœ€æ–°çªç ´", "quantum computing breakthrough"],
        preferred_sources=['tavily', 'brave'],  # é¦–é€‰ç½‘ç»œæœç´¢
        fallback_sources=['arxiv', 'academic'],  # å¤‡é€‰å­¦æœ¯æœç´¢
        max_results_per_query=3,
        days_back=30
    )
    
    print(f"ğŸ“Š é™çº§æœç´¢ç»“æœ: {len(results)}æ¡")
    
    # ç»Ÿè®¡æ•°æ®æºåˆ†å¸ƒ
    source_distribution = {}
    for doc in results:
        source_distribution[doc.source] = source_distribution.get(doc.source, 0) + 1
    
    print("ğŸ“ˆ æ•°æ®æºåˆ†å¸ƒ:")
    for source, count in source_distribution.items():
        print(f"  {source}: {count}æ¡")


def test_document_standardization():
    """æµ‹è¯•Documentæ ‡å‡†åŒ–"""
    print("\n\nğŸ§ª æµ‹è¯•4: Documentå¯¹è±¡æ ‡å‡†åŒ–")
    print("=" * 50)
    
    # åˆ›å»ºç¤ºä¾‹Document
    doc = Document(
        title="ç”Ÿæˆå¼AIçš„æœªæ¥å‘å±•è¶‹åŠ¿",
        content="ç”Ÿæˆå¼äººå·¥æ™ºèƒ½æ­£åœ¨å¿«é€Ÿå‘å±•ï¼ŒåŒ…æ‹¬æ–‡æœ¬ç”Ÿæˆã€å›¾åƒç”Ÿæˆã€ä»£ç ç”Ÿæˆç­‰å¤šä¸ªé¢†åŸŸ...",
        url="https://example.com/ai-future",
        source="tavily",
        source_type="web",
        publish_date="2024-01-15",
        authors=["å¼ ä¸‰", "æå››"],
        venue="AI Research Journal",
        score=0.95
    )
    
    print("ğŸ“‹ Documentå¯¹è±¡ç¤ºä¾‹:")
    print(f"  æ ‡é¢˜: {doc.title}")
    print(f"  æ¥æº: {doc.source} ({doc.source_type})")
    print(f"  åŸŸå: {doc.domain}")
    print(f"  ä½œè€…: {', '.join(doc.authors)}")
    print(f"  å‘å¸ƒæ—¶é—´: {doc.publish_date}")
    print(f"  ç›¸å…³æ€§è¯„åˆ†: {doc.score}")
    
    # è½¬æ¢ä¸ºå­—å…¸
    doc_dict = doc.to_dict()
    print(f"\nğŸ“„ è½¬æ¢ä¸ºå­—å…¸åçš„é”®: {list(doc_dict.keys())}")


def demonstrate_integration_potential():
    """æ¼”ç¤ºä¸ç°æœ‰agentçš„é›†æˆæ½œåŠ›"""
    print("\n\nğŸš€ é›†æˆæ½œåŠ›æ¼”ç¤º")
    print("=" * 50)
    
    print("ğŸ“ SearchMcpå¯ä»¥æ›¿ä»£ç°æœ‰agentä¸­çš„ä»¥ä¸‹æœç´¢é€»è¾‘:")
    
    integration_examples = {
        "generate_outline_report.py": [
            "OutlineDataCollectorç±»ä¸­çš„å¹¶è¡Œæœç´¢",
            "_execute_single_queryæ–¹æ³•",
            "å¤šæ”¶é›†å™¨ç®¡ç†é€»è¾‘"
        ],
        "generate_news_report_parallel.py": [
            "ParallelDataCollectorç±»",
            "parallel_comprehensive_searchæ–¹æ³•",
            "å¤šæ¸ é“æœç´¢åˆå¹¶é€»è¾‘"
        ],
        "generate_insights_report_updated_copy.py": [
            "ParallelInsightsCollectorç±»",
            "parallel_collect_section_queriesæ–¹æ³•",
            "æœç´¢ç»“æœå»é‡é€»è¾‘"
        ],
        "intent_search_agent.py": [
            "parallel_content_searchæ–¹æ³•",
            "_execute_single_searchæ–¹æ³•",
            "_deduplicate_resultsæ–¹æ³•"
        ],
        "proposal_report_agent.py": [
            "search_academic_contentæ–¹æ³•",
            "_search_academic_papersæ–¹æ³•",
            "_search_section_contentæ–¹æ³•"
        ]
    }
    
    for agent_file, components in integration_examples.items():
        print(f"\nğŸ”§ {agent_file}:")
        for component in components:
            print(f"  âœ… {component}")
    
    print(f"\nğŸ’¡ é›†æˆä¼˜åŠ¿:")
    print(f"  ğŸ¯ ç»Ÿä¸€æ¥å£: æ‰€æœ‰agentä½¿ç”¨ç›¸åŒçš„æœç´¢API")
    print(f"  âš¡ å¹¶è¡Œä¼˜åŒ–: å†…ç½®çº¿ç¨‹æ± ç®¡ç†å’Œé”™è¯¯å¤„ç†")
    print(f"  ğŸ”„ è‡ªåŠ¨å»é‡: URLçº§åˆ«çš„å»é‡æœºåˆ¶")
    print(f"  ğŸ“Š æ ‡å‡†åŒ–: ç»Ÿä¸€çš„Documentæ•°æ®ç»“æ„")
    print(f"  ğŸ›¡ï¸ å®¹é”™æœºåˆ¶: è‡ªåŠ¨é™çº§å’Œé”™è¯¯æ¢å¤")
    print(f"  ğŸ“ˆ å¯æ‰©å±•: æ˜“äºæ·»åŠ æ–°çš„æ•°æ®æº")


def main():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    print("ğŸš€ SearchMcp ç»Ÿä¸€æœç´¢ç³»ç»Ÿæµ‹è¯•")
    print("=" * 60)
    
    try:
        # åŸºç¡€åŠŸèƒ½æµ‹è¯•
        test_basic_search()
        
        # ç±»åˆ«æœç´¢æµ‹è¯•
        test_category_search()
        
        # é™çº§æœç´¢æµ‹è¯•
        test_fallback_search()
        
        # Documentæ ‡å‡†åŒ–æµ‹è¯•
        test_document_standardization()
        
        # é›†æˆæ½œåŠ›æ¼”ç¤º
        demonstrate_integration_potential()
        
        print(f"\nğŸ‰ æ‰€æœ‰æµ‹è¯•å®Œæˆ!")
        
    except KeyboardInterrupt:
        print(f"\nâŒ ç”¨æˆ·ä¸­æ–­äº†æµ‹è¯•")
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main() 