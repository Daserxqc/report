import os
import json
import argparse
from datetime import datetime
from dotenv import load_dotenv
import sys
import re
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading
import time
import importlib.util

from collectors.tavily_collector import TavilyCollector
from collectors.brave_search_collector import BraveSearchCollector
from collectors.llm_processor import LLMProcessor
from fix_md_headings import fix_markdown_headings
import config
import logging

# å…³é—­HTTPè¯·æ±‚æ—¥å¿—ï¼Œå‡å°‘å¹²æ‰°
logging.getLogger("httpx").setLevel(logging.WARNING)
logging.getLogger("urllib3").setLevel(logging.WARNING)

class OutlineParser:
    """
    æ™ºèƒ½å¤§çº²è§£æå™¨
    ä½¿ç”¨å¤§æ¨¡å‹ç†è§£å’Œè§£æç”¨æˆ·è¾“å…¥çš„å±‚çº§ç»“æ„å¤§çº²
    """
    
    def __init__(self, llm_processor=None):
        self.outline_structure = {}
        self.main_sections = []
        self.llm_processor = llm_processor
        
    def parse_outline(self, outline_text, extracted_topic=None):
        """
        æ™ºèƒ½è§£æå¤§çº²æ–‡æœ¬ï¼Œä¼˜å…ˆä½¿ç”¨LLMç†è§£
        
        Args:
            outline_text (str): å¤§çº²æ–‡æœ¬
            extracted_topic (str): å·²æå–çš„ä¸»é¢˜ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
            
        Returns:
            dict: è§£æåçš„å¤§çº²ç»“æ„
        """
        print("ğŸ” å¼€å§‹æ™ºèƒ½è§£æå¤§çº²ç»“æ„...")
        
        if self.llm_processor:
            print("ğŸ“– ä½¿ç”¨LLMç›´æ¥ç†è§£å¤§çº²å†…å®¹...")
            try:
                # ä¼˜å…ˆä½¿ç”¨LLMå®Œæ•´ç†è§£å¤§çº²
                structure = self._parse_outline_with_llm_complete(outline_text, extracted_topic)
                if structure:
                    self.outline_structure = structure
                    self.main_sections = list(structure.keys())
                    
                    print(f"âœ… LLMå¤§çº²è§£æå®Œæˆï¼Œå‘ç°{len(self.main_sections)}ä¸ªä¸»è¦ç« èŠ‚")
                    for section in self.main_sections:
                        sub_count = len(structure[section]['subsections'])
                        item_count = len(structure[section]['content'])
                        print(f"   - {section}: {sub_count}ä¸ªå­ç« èŠ‚, {item_count}ä¸ªç›´æ¥å†…å®¹é¡¹")
                    
                    return structure
            except Exception as e:
                print(f"âš ï¸ LLMè§£æå¤±è´¥: {str(e)}ï¼Œå°è¯•å¤‡ç”¨æ–¹æ³•...")
        
        # å¦‚æœLLMä¸å¯ç”¨æˆ–å¤±è´¥ï¼Œä½¿ç”¨å¤‡ç”¨æ–¹æ³•
        print("ğŸ”„ LLMä¸å¯ç”¨ï¼Œä½¿ç”¨å¤‡ç”¨è§£ææ–¹æ³•...")
        structure = self._parse_outline_fallback(outline_text, extracted_topic)
        
        self.outline_structure = structure
        self.main_sections = list(structure.keys())
        
        print(f"âœ… å¤‡ç”¨è§£æå®Œæˆï¼Œå‘ç°{len(self.main_sections)}ä¸ªä¸»è¦ç« èŠ‚")
        for section in self.main_sections:
            sub_count = len(structure[section]['subsections'])
            item_count = len(structure[section]['content'])
            print(f"   - {section}: {sub_count}ä¸ªå­ç« èŠ‚, {item_count}ä¸ªç›´æ¥å†…å®¹é¡¹")
        
        return structure
    
    def _parse_outline_with_llm_complete(self, outline_text, extracted_topic=None):
        """ä½¿ç”¨LLMå®Œæ•´ç†è§£å¤§çº²å†…å®¹"""
        topic_hint = ""
        if extracted_topic:
            topic_hint = f"\næ³¨æ„ï¼šä¸»é¢˜ '{extracted_topic}' å·²ç»è¢«è¯†åˆ«ï¼Œè¯·åœ¨è§£ææ—¶ä¸è¦å°†å…¶é‡å¤ä½œä¸ºç« èŠ‚å¤„ç†ã€‚"
        
        prompt = f"""è¯·ä»”ç»†é˜…è¯»ä»¥ä¸‹å¤§çº²å†…å®¹ï¼Œç†è§£å…¶å®Œæ•´çš„å±‚çº§ç»“æ„å’Œæ‰€æœ‰è¦ç‚¹ã€‚{topic_hint}

å¤§çº²å†…å®¹ï¼š
{outline_text}

è¯·æŒ‰ç…§ä»¥ä¸‹è¦æ±‚è§£æï¼š

1. **å®Œæ•´ç†è§£**ï¼šè¯·ç†è§£å¤§çº²çš„æ‰€æœ‰å†…å®¹ï¼Œä¸è¦é—æ¼ä»»ä½•è¦ç‚¹
2. **å±‚çº§è¯†åˆ«**ï¼šè¯†åˆ«ä¸»è¦ç« èŠ‚å’Œå­è¦ç‚¹çš„å±‚çº§å…³ç³»
3. **æ ¼å¼ç»Ÿä¸€**ï¼šå°†æ‰€æœ‰å†…å®¹è½¬æ¢ä¸ºæ ‡å‡†çš„JSONç»“æ„
4. **è¦ç‚¹ä¿ç•™**ï¼šæ¯ä¸ªå°è¦ç‚¹éƒ½å¾ˆé‡è¦ï¼Œéƒ½éœ€è¦åç»­æœç´¢èµ„æ–™

è¯·è¿”å›ä»¥ä¸‹JSONæ ¼å¼ï¼š
{{
    "ä¸»è¦ç« èŠ‚1": {{
        "title": "ä¸»è¦ç« èŠ‚1",
        "subsections": {{
            "å­è¦ç‚¹1": {{
                "title": "å­è¦ç‚¹1",
                "items": []
            }},
            "å­è¦ç‚¹2": {{
                "title": "å­è¦ç‚¹2", 
                "items": []
            }}
        }},
        "content": []
    }},
    "ä¸»è¦ç« èŠ‚2": {{
        "title": "ä¸»è¦ç« èŠ‚2",
        "subsections": {{
            "å­è¦ç‚¹3": {{
                "title": "å­è¦ç‚¹3",
                "items": []
            }}
        }},
        "content": []
    }}
}}

**é‡è¦è¯´æ˜**ï¼š
- è¯·è¯†åˆ«æ‰€æœ‰çš„ä¸»è¦ç« èŠ‚ï¼ˆå¦‚"ä¸€ã€äºŒã€ä¸‰"æˆ–ç±»ä¼¼æ ‡è®°çš„éƒ¨åˆ†ï¼‰
- å°†æ¯ä¸ªä¸»è¦ç« èŠ‚ä¸‹çš„å°è¦ç‚¹éƒ½è¯†åˆ«ä¸ºsubsections
- å³ä½¿æ˜¯ç®€çŸ­çš„è¯è¯­ï¼ˆå¦‚"éè¯­è¨€æ—¶ä»£"ã€"ç¬¦å·ä¸å›¾åƒ"ï¼‰ä¹Ÿæ˜¯é‡è¦çš„å­è¦ç‚¹
- ç¡®ä¿ä¸é—æ¼ä»»ä½•å†…å®¹
- åªè¿”å›æœ‰æ•ˆçš„JSONï¼Œä¸è¦åŒ…å«å…¶ä»–æ–‡å­—

æˆ‘éœ€è¦ä¸ºæ¯ä¸ªsubsectionæœç´¢ç›¸å…³èµ„æ–™ï¼Œæ‰€ä»¥è¯·ç¡®ä¿å®Œæ•´æå–æ‰€æœ‰è¦ç‚¹ã€‚
"""
        
        system_message = """ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„æ–‡æ¡£ç»“æ„åˆ†æå¸ˆï¼Œæ“…é•¿ç†è§£å„ç§æ ¼å¼çš„å¤§çº²å’Œæ–‡æ¡£ç»“æ„ã€‚
ä½ éœ€è¦å®Œæ•´ç†è§£ç”¨æˆ·æä¾›çš„å¤§çº²å†…å®¹ï¼Œå‡†ç¡®è¯†åˆ«å…¶å±‚çº§å…³ç³»ï¼Œå¹¶è½¬æ¢ä¸ºæ ‡å‡†çš„JSONæ ¼å¼ã€‚
è¯·ç¡®ä¿ä¸é—æ¼ä»»ä½•è¦ç‚¹ï¼Œæ¯ä¸ªè¦ç‚¹éƒ½æ˜¯åç»­ç ”ç©¶çš„é‡è¦åŸºç¡€ã€‚
ä½ çš„å›ç­”å¿…é¡»æ˜¯ä¸¥æ ¼çš„JSONæ ¼å¼ï¼Œä¸åŒ…å«ä»»ä½•å…¶ä»–æ–‡æœ¬ã€‚"""
        
        try:
            response = self.llm_processor.call_llm_api_json(prompt, system_message)
            
            # éªŒè¯å“åº”æ ¼å¼
            if isinstance(response, dict) and self._validate_outline_structure(response):
                print(f"ğŸ¯ LLMæˆåŠŸç†è§£å¤§çº²ï¼Œè¯†åˆ«äº†{len(response)}ä¸ªä¸»è¦ç« èŠ‚")
                # æ‰“å°è¯¦ç»†çš„è§£æç»“æœ
                for section, info in response.items():
                    subsection_count = len(info.get('subsections', {}))
                    print(f"   ğŸ“ {section}: {subsection_count}ä¸ªå­è¦ç‚¹")
                    for sub_title in info.get('subsections', {}):
                        print(f"      - {sub_title}")
                
                return response
            else:
                print("âŒ LLMè¿”å›çš„ç»“æ„æ ¼å¼ä¸æ­£ç¡®")
                return None
                
        except Exception as e:
            print(f"âŒ LLMå®Œæ•´è§£æå‡ºé”™: {str(e)}")
            return None
    
    def _parse_outline_with_llm(self, outline_text, extracted_topic=None):
        """ä½¿ç”¨å¤§æ¨¡å‹è§£æå¤§çº²"""
        topic_hint = ""
        if extracted_topic:
            topic_hint = f"\næ³¨æ„ï¼šæ ‡é¢˜/ä¸»é¢˜ '{extracted_topic}' å·²ç»è¢«è¯†åˆ«ï¼Œè¯·åœ¨è§£ææ—¶å¿½ç•¥å®ƒï¼Œä¸è¦å°†å…¶ä½œä¸ºç« èŠ‚å¤„ç†ã€‚"
        
        prompt = f"""è¯·è§£æä»¥ä¸‹å¤§çº²æ–‡æœ¬ï¼Œæå–å…¶å±‚çº§ç»“æ„ã€‚æ— è®ºç”¨æˆ·ä½¿ç”¨ä»€ä¹ˆç¬¦å·æˆ–æ ¼å¼ï¼Œè¯·ç†è§£å…¶å«ä¹‰å¹¶è½¬æ¢ä¸ºæ ‡å‡†ç»“æ„ã€‚{topic_hint}

å¤§çº²æ–‡æœ¬ï¼š
{outline_text}

è¯·å°†å¤§çº²è§£æä¸ºJSONæ ¼å¼ï¼ŒåŒ…å«ä»¥ä¸‹ç»“æ„ï¼š
{{
    "ä¸»è¦ç« èŠ‚æ ‡é¢˜1": {{
        "title": "ä¸»è¦ç« èŠ‚æ ‡é¢˜1",
        "subsections": {{
            "å­ç« èŠ‚æ ‡é¢˜1": {{
                "title": "å­ç« èŠ‚æ ‡é¢˜1",
                "items": ["æ¡ç›®1", "æ¡ç›®2", "æ¡ç›®3"]
            }},
            "å­ç« èŠ‚æ ‡é¢˜2": {{
                "title": "å­ç« èŠ‚æ ‡é¢˜2", 
                "items": ["æ¡ç›®1", "æ¡ç›®2"]
            }}
        }},
        "content": ["ç›´æ¥éš¶å±äºä¸»ç« èŠ‚çš„å†…å®¹é¡¹"]
    }},
    "ä¸»è¦ç« èŠ‚æ ‡é¢˜2": {{
        "title": "ä¸»è¦ç« èŠ‚æ ‡é¢˜2",
        "subsections": {{}},
        "content": ["å†…å®¹é¡¹1", "å†…å®¹é¡¹2"]
    }}
}}

è§£æè§„åˆ™ï¼š
1. è¯†åˆ«ä¸»è¦ç« èŠ‚ï¼ˆé€šå¸¸æ˜¯ä¸€çº§æ ‡é¢˜ï¼Œå¦‚"ä¸€ã€"ã€"1."ã€"ç¬¬ä¸€ç« "ç­‰ï¼‰
2. è¯†åˆ«å­ç« èŠ‚ï¼ˆé€šå¸¸æ˜¯äºŒçº§æ ‡é¢˜ï¼Œå¦‚"ï¼ˆä¸€ï¼‰"ã€"1.1"ã€"<ä¸€>"ç­‰ï¼‰
3. è¯†åˆ«å†…å®¹é¡¹ï¼ˆé€šå¸¸æ˜¯é¡¹ç›®ç¬¦å·ã€æ•°å­—åˆ—è¡¨æˆ–ç¼©è¿›å†…å®¹ï¼‰
4. å¿½ç•¥æ ¼å¼ç¬¦å·ï¼Œæå–çº¯å†…å®¹
5. ä¿æŒå±‚çº§å…³ç³»æ¸…æ™°

æ³¨æ„ï¼š
- åªè¿”å›æœ‰æ•ˆçš„JSONæ ¼å¼
- ä¸è¦åŒ…å«ä»»ä½•è§£é‡Šæ€§æ–‡å­—
- ç¡®ä¿JSONç»“æ„å®Œæ•´ä¸”å¯è§£æ
- å¦‚æœæŸä¸ªç« èŠ‚æ²¡æœ‰å­ç« èŠ‚ï¼Œsubsectionsè®¾ä¸ºç©ºå¯¹è±¡{{}}
- å¦‚æœæŸä¸ªç« èŠ‚æ²¡æœ‰ç›´æ¥å†…å®¹ï¼Œcontentè®¾ä¸ºç©ºæ•°ç»„[]
"""
        
        system_message = """ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„æ–‡æ¡£ç»“æ„åˆ†æå¸ˆï¼Œæ“…é•¿ç†è§£å„ç§æ ¼å¼çš„å¤§çº²å’Œå±‚çº§ç»“æ„ã€‚
ä½ éœ€è¦å‡†ç¡®è¯†åˆ«æ–‡æ¡£çš„å±‚çº§å…³ç³»ï¼Œæ— è®ºç”¨æˆ·ä½¿ç”¨ä»€ä¹ˆæ ·çš„ç¬¦å·æˆ–æ ¼å¼ã€‚
ä½ çš„å›ç­”å¿…é¡»æ˜¯ä¸¥æ ¼çš„JSONæ ¼å¼ï¼Œä¸åŒ…å«ä»»ä½•å…¶ä»–æ–‡æœ¬ã€‚"""
        
        try:
            response = self.llm_processor.call_llm_api_json(prompt, system_message)
            
            # éªŒè¯å“åº”æ ¼å¼
            if isinstance(response, dict) and self._validate_outline_structure(response):
                return response
            else:
                print("âŒ LLMè¿”å›çš„ç»“æ„æ ¼å¼ä¸æ­£ç¡®")
                return None
                
        except Exception as e:
            print(f"âŒ LLMè§£æå‡ºé”™: {str(e)}")
            return None
    
    def _validate_outline_structure(self, structure):
        """éªŒè¯è§£æåçš„å¤§çº²ç»“æ„æ˜¯å¦ç¬¦åˆé¢„æœŸæ ¼å¼"""
        if not isinstance(structure, dict):
            return False
            
        for section_title, section_data in structure.items():
            if not isinstance(section_data, dict):
                return False
            
            required_fields = ['title', 'subsections', 'content']
            if not all(field in section_data for field in required_fields):
                return False
                
            if not isinstance(section_data['subsections'], dict):
                return False
                
            if not isinstance(section_data['content'], list):
                return False
                
            # éªŒè¯å­ç« èŠ‚ç»“æ„
            for sub_title, sub_data in section_data['subsections'].items():
                if not isinstance(sub_data, dict):
                    return False
                if not all(field in sub_data for field in ['title', 'items']):
                    return False
                if not isinstance(sub_data['items'], list):
                    return False
        
        return True
    
    def _parse_outline_fallback(self, outline_text, extracted_topic=None):
        """å¤‡ç”¨è§£ææ–¹æ³• - ä½¿ç”¨æ›´çµæ´»çš„è§„åˆ™"""
        lines = outline_text.strip().split('\n')
        structure = {}
        current_main = None
        current_sub = None
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
            
            # å¦‚æœè¿™è¡Œæ˜¯å·²æå–çš„ä¸»é¢˜ï¼Œè·³è¿‡å®ƒ
            # åªè·³è¿‡å®Œå…¨åŒ¹é…çš„ä¸»é¢˜è¡Œï¼Œè€Œä¸æ˜¯åŒ…å«ä¸»é¢˜è¯çš„ç« èŠ‚æ ‡é¢˜
            if extracted_topic and (line == extracted_topic or 
                                   line.replace('#', '').strip() == extracted_topic):
                print(f"  ğŸ” è·³è¿‡ä¸»é¢˜è¡Œ: {line}")
                continue
            
            # æ›´çµæ´»çš„ä¸»ç« èŠ‚åŒ¹é…
            # åŒ¹é…å„ç§å¯èƒ½çš„ä¸»ç« èŠ‚æ ¼å¼
            main_patterns = [
                r'^[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+[ã€ï¼\.]?\s*(.+)',  # ä¸€ã€äºŒã€ä¸‰
                r'^[â‘ â‘¡â‘¢â‘£â‘¤â‘¥â‘¦â‘§â‘¨â‘©]+[ã€ï¼\.]?\s*(.+)',     # â‘ â‘¡â‘¢
                r'^[1-9]\d*[ã€ï¼\.]?\s*(.+)',                # 1ã€2ã€3
                r'^ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+[ç« èŠ‚éƒ¨åˆ†][ã€ï¼\.]?\s*(.+)',  # ç¬¬ä¸€ç« 
                r'^[1-9]\d*\.\d*[ã€ï¼\.]?\s*(.+)',           # 1.1ã€1.2
                r'^[A-Z][ã€ï¼\.]?\s*(.+)',                   # Aã€Bã€C
            ]
            
            main_match = None
            for pattern in main_patterns:
                main_match = re.match(pattern, line)
                if main_match:
                    break
            
            if main_match:
                current_main = main_match.group(1).strip()
                structure[current_main] = {
                    'title': current_main,
                    'subsections': {},
                    'content': []
                }
                current_sub = None
                continue
            
            # æ›´çµæ´»çš„å­ç« èŠ‚åŒ¹é…
            sub_patterns = [
                r'^[ï¼ˆ\(]\s*[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+\s*[ï¼‰\)]\s*(.+)',  # ï¼ˆä¸€ï¼‰
                r'^[ï¼ˆ\(]\s*[1-9]\d*\s*[ï¼‰\)]\s*(.+)',              # ï¼ˆ1ï¼‰
                r'^[<ï¼œ]\s*[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+\s*[>ï¼]\s*(.+)',    # <ä¸€>
                r'^[ã€\[]\s*[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+\s*[ã€‘\]]\s*(.+)',  # ã€ä¸€ã€‘
                r'^[1-9]\d*\.[1-9]\d*[ã€ï¼\.]?\s*(.+)',             # 1.1ã€1.2
                r'^[a-zA-Z]\)\s*(.+)',                             # a) b) c)
            ]
            
            sub_match = None
            for pattern in sub_patterns:
                sub_match = re.match(pattern, line)
                if sub_match:
                    break
                    
            if sub_match and current_main:
                current_sub = sub_match.group(1).strip()
                structure[current_main]['subsections'][current_sub] = {
                    'title': current_sub,
                    'items': []
                }
                continue
            
            # æ›´çµæ´»çš„é¡¹ç›®ç¬¦å·åŒ¹é…
            item_patterns = [
                r'^[\*\-\+â€¢Â·]\s*(.+)',                 # * - + â€¢ Â·
                r'^[1-9]\d*[\.ã€‚)]\s*(.+)',            # 1. 2. 3.
                r'^[a-zA-Z][\.ã€‚)]\s*(.+)',            # a. b. c.
                r'^[â‘ â‘¡â‘¢â‘£â‘¤â‘¥â‘¦â‘§â‘¨â‘©]+\s*(.+)',           # â‘ â‘¡â‘¢
                r'^â†’\s*(.+)',                          # â†’
                r'^â–ª\s*(.+)',                          # â–ª
                r'^â—‹\s*(.+)',                          # â—‹
            ]
            
            item_match = None
            for pattern in item_patterns:
                item_match = re.match(pattern, line)
                if item_match:
                    break
                    
            if item_match and current_main:
                item_text = item_match.group(1).strip()
                if current_sub and current_sub in structure[current_main]['subsections']:
                    structure[current_main]['subsections'][current_sub]['items'].append(item_text)
                else:
                    structure[current_main]['content'].append(item_text)
                continue
            
            # å¤„ç†ç¼©è¿›å†…å®¹ï¼ˆæ›´çµæ´»ï¼‰
            if (line.startswith('ã€€') or line.startswith('    ') or 
                line.startswith('\t') or line.startswith('  ')):
                cleaned_line = line.lstrip('ã€€ \t')
                if current_main and cleaned_line:
                    if current_sub and current_sub in structure[current_main]['subsections']:
                        structure[current_main]['subsections'][current_sub]['items'].append(cleaned_line)
                    else:
                        structure[current_main]['content'].append(cleaned_line)
                continue
            
            # å¦‚æœéƒ½ä¸åŒ¹é…ï¼Œä½†æœ‰å½“å‰ä¸»ç« èŠ‚ï¼Œåˆ™ä½œä¸ºå†…å®¹é¡¹æ·»åŠ 
            if current_main and line:
                # å¢å¼ºå¯¹ç®€å•åˆ—è¡¨æ ¼å¼çš„æ”¯æŒ
                # å¦‚æœè¡Œçœ‹èµ·æ¥åƒæ˜¯ä¸€ä¸ªè¦ç‚¹ï¼ˆçŸ­ä¸”æè¿°æ€§ï¼‰ï¼Œåˆ›å»ºä¸ºå­ç« èŠ‚
                if (len(line) <= 20 and 
                    not any(char in line for char in ['ã€‚', 'ï¼', 'ï¼Ÿ', 'ï¼Œ', 'ï¼›']) and
                    not line.startswith(('ç¬¬', 'æœ¬', 'è¯¥', 'è¿™', 'é‚£', 'åœ¨', 'æ˜¯', 'æœ‰', 'ä»', 'å¯¹', 'ç”±', 'è¢«', 'ä¸', 'ä¸º', 'å°±', 'éƒ½', 'ä¹Ÿ', 'è¿˜', 'åª', 'æ›´', 'æœ€', 'å¾ˆ', 'éå¸¸'))):
                    # åˆ›å»ºä¸ºå­ç« èŠ‚
                    structure[current_main]['subsections'][line] = {
                        'title': line,
                        'items': []
                    }
                    print(f"  ğŸ“ è¯†åˆ«ä¸ºå­ç« èŠ‚: {line}")
                else:
                    # å¦åˆ™ä½œä¸ºå†…å®¹é¡¹
                    if current_sub and current_sub in structure[current_main]['subsections']:
                        structure[current_main]['subsections'][current_sub]['items'].append(line)
                    else:
                        structure[current_main]['content'].append(line)
        
        return structure

class OutlineDataCollector:
    """
    åŸºäºå¤§çº²çš„æ•°æ®æ”¶é›†å™¨
    è´Ÿè´£ä¸ºæ¯ä¸ªç« èŠ‚æ”¶é›†ç›¸å…³æ•°æ®
    """
    
    def __init__(self, llm_processor=None):
        self.results_lock = threading.Lock()
        self.llm_processor = llm_processor
        
        # åˆå§‹åŒ–æœç´¢æ”¶é›†å™¨
        self.collectors = {}
        
        # Tavilyæ”¶é›†å™¨ï¼ˆå¿…éœ€ï¼‰
        try:
            self.tavily_collector = TavilyCollector()
            self.collectors['tavily'] = self.tavily_collector
            print("âœ… Tavilyæœç´¢æ”¶é›†å™¨å·²åˆå§‹åŒ–")
        except Exception as e:
            print(f"âŒ Tavilyæœç´¢æ”¶é›†å™¨åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            raise
        
        # Braveæ”¶é›†å™¨ï¼ˆå¯é€‰ï¼‰
        try:
            self.brave_collector = BraveSearchCollector()
            if hasattr(self.brave_collector, 'has_api_key') and self.brave_collector.has_api_key:
                self.collectors['brave'] = self.brave_collector
                print("âœ… Braveæœç´¢æ”¶é›†å™¨å·²å¯ç”¨")
            else:
                print("âš ï¸ Braveæœç´¢æ”¶é›†å™¨æœªé…ç½®APIå¯†é’¥ï¼Œå·²è·³è¿‡")
        except Exception as e:
            print(f"âš ï¸ Braveæœç´¢æ”¶é›†å™¨åˆå§‹åŒ–å¤±è´¥: {str(e)}")
        
        # åˆå§‹åŒ–æ•°æ®ç­›é€‰å¤„ç†å™¨
        from collectors.data_filter_processor import DataFilterProcessor
        self.data_filter = DataFilterProcessor(llm_processor=llm_processor)
        print("âœ… æ•°æ®ç­›é€‰å¤„ç†å™¨å·²åˆå§‹åŒ–")
    
    def parallel_collect_main_sections(self, outline_structure, topic, target_audience="é€šç”¨", max_workers=4):
        """
        å¹¶è¡Œæ”¶é›†ä¸»è¦ç« èŠ‚çš„æ•°æ®
        
        Args:
            outline_structure (dict): å¤§çº²ç»“æ„
            topic (str): ä¸»é¢˜
            target_audience (str): ç›®æ ‡å—ä¼—
            max_workers (int): æœ€å¤§å¹¶è¡Œå·¥ä½œæ•°
            
        Returns:
            dict: æŒ‰ç« èŠ‚ç»„ç»‡çš„æ•°æ®ç»“æœ
        """
        print(f"ğŸš€ [å¹¶è¡Œæ”¶é›†] å¼€å§‹å¹¶è¡Œæ”¶é›†{len(outline_structure)}ä¸ªä¸»è¦ç« èŠ‚çš„æ•°æ®...")
        start_time = time.time()
        
        sections_data = {}
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # ğŸš€ å¹¶è¡Œæäº¤ä¸»è¦ç« èŠ‚ä»»åŠ¡
            future_to_section = {
                executor.submit(
                    self._collect_section_data, topic, section_title, section_info, target_audience
                ): section_title for section_title, section_info in outline_structure.items()
            }
            
            # ğŸ”„ æ”¶é›†ç« èŠ‚ç»“æœ
            completed_count = 0
            for future in as_completed(future_to_section):
                section_title = future_to_section[future]
                try:
                    section_data = future.result()
                    completed_count += 1
                    
                    with self.results_lock:
                        sections_data[section_title] = section_data
                    
                    print(f"  âœ… [{completed_count}/{len(outline_structure)}] ç« èŠ‚'{section_title}'æ”¶é›†å®Œæˆï¼Œè·å¾—{len(section_data)}æ¡æ•°æ®")
                    
                except Exception as e:
                    print(f"  âŒ ç« èŠ‚'{section_title}'æ”¶é›†å¤±è´¥: {str(e)}")
                    sections_data[section_title] = []
        
        total_time = time.time() - start_time
        total_items = sum(len(data) for data in sections_data.values())
        print(f"ğŸ“Š [å¹¶è¡Œæ”¶é›†å®Œæˆ] æ€»è®¡æ”¶é›†{total_items}æ¡æ•°æ®ï¼Œè€—æ—¶{total_time:.1f}ç§’")
        
        return sections_data
    
    def _collect_section_data(self, topic, section_title, section_info, target_audience):
        """æ”¶é›†å•ä¸ªç« èŠ‚çš„æ•°æ®"""
        print(f"  ğŸ” å¼€å§‹æ”¶é›†ç« èŠ‚'{section_title}'çš„æ•°æ®...")
        
        # ç”Ÿæˆæœç´¢æŸ¥è¯¢
        queries = self._generate_section_queries(topic, section_title, section_info, target_audience)
        
        raw_results = []
        seen_urls = set()
        
        # ä¸²è¡Œæ‰§è¡ŒæŸ¥è¯¢ï¼ˆé¿å…è¿‡åº¦å¹¶è¡Œï¼‰
        for query in queries:
            try:
                query_results = self._execute_single_query(query)
                
                # å»é‡å¹¶æ·»åŠ åˆ°ç»“æœ
                for result in query_results:
                    url = result.get("url", "")
                    if url and url not in seen_urls:
                        seen_urls.add(url)
                        result["section"] = section_title
                        result["source_query"] = query
                        raw_results.append(result)
                        
            except Exception as e:
                print(f"    âŒ æŸ¥è¯¢å¤±è´¥: {str(e)}")
                continue
        
        print(f"  ğŸ“Š æ”¶é›†åˆ° {len(raw_results)} æ¡åŸå§‹æ•°æ®ï¼Œå¼€å§‹è´¨é‡ç­›é€‰...")
        
        # è½¬æ¢ä¸ºDataSourceå¯¹è±¡
        data_sources = self._convert_to_data_sources(raw_results)
        
        # ä½¿ç”¨DataFilterProcessorç­›é€‰æ•°æ®
        try:
            filtered_data = self.data_filter.filter_and_score_data(
                data_sources=data_sources,
                topic=topic,
                section_title=section_title,
                min_score=0.6  # å¯è°ƒæ•´çš„æœ€ä½åˆ†æ•°é˜ˆå€¼
            )
            
            print(f"  âœ… ç­›é€‰å®Œæˆï¼Œ{len(filtered_data)} æ¡æ•°æ®é€šè¿‡ç­›é€‰")
            return filtered_data
            
        except Exception as e:
            print(f"  âš ï¸ æ•°æ®ç­›é€‰å¤±è´¥: {str(e)}ï¼Œè¿”å›åŸå§‹æ•°æ®")
            # å¦‚æœç­›é€‰å¤±è´¥ï¼Œè¿”å›ç®€å•å¤„ç†çš„åŸå§‹æ•°æ®
            if len(raw_results) > 10:
                raw_results.sort(key=lambda x: len(x.get("content", "")), reverse=True)
                raw_results = raw_results[:10]
            return raw_results
    
    def _generate_section_queries(self, topic, section_title, section_info, target_audience):
        """ä¸ºç‰¹å®šç« èŠ‚ç”Ÿæˆæœç´¢æŸ¥è¯¢"""
        if self.llm_processor:
            return self._generate_queries_with_llm(topic, section_title, section_info, target_audience)
        else:
            return self._generate_queries_fallback(topic, section_title, section_info, target_audience)
    
    def _generate_queries_with_llm(self, topic, section_title, section_info, target_audience):
        """ä½¿ç”¨LLMç”Ÿæˆæ™ºèƒ½æœç´¢æŸ¥è¯¢"""
        try:
            # æ„å»ºå­ç« èŠ‚ä¿¡æ¯
            subsections_text = ""
            if section_info.get('subsections'):
                subsections_text = "å­ç« èŠ‚ï¼š\n"
                for sub_title, sub_data in section_info['subsections'].items():
                    subsections_text += f"- {sub_title}\n"
                    if sub_data.get('items'):
                        for item in sub_data['items'][:3]:  # é™åˆ¶æ˜¾ç¤ºæ•°é‡
                            subsections_text += f"  * {item}\n"
            
            # æ„å»ºç›´æ¥å†…å®¹ä¿¡æ¯
            content_text = ""
            if section_info.get('content'):
                content_text = "ä¸»è¦å†…å®¹ï¼š\n"
                for item in section_info['content'][:5]:  # é™åˆ¶æ˜¾ç¤ºæ•°é‡
                    content_text += f"- {item}\n"
            
            # è®¡ç®—æŸ¥è¯¢æ•°é‡
            subsection_count = len(section_info.get('subsections', {}))
            content_count = len(section_info.get('content', []))
            
            # åŠ¨æ€è°ƒæ•´æŸ¥è¯¢æ•°é‡
            if subsection_count > 3 or content_count > 3:
                query_count = "12-15"
                max_queries = 15
            elif subsection_count > 1 or content_count > 1:
                query_count = "10-12"
                max_queries = 12
            else:
                query_count = "8-10"
                max_queries = 10
            
            prompt = f"""è¯·ä¸ºä»¥ä¸‹ç« èŠ‚ç”Ÿæˆ{query_count}ä¸ªæœ€æœ‰æ•ˆçš„æœç´¢æŸ¥è¯¢ï¼Œç”¨äºæ”¶é›†ç›¸å…³èµ„æ–™ï¼š

ä¸»é¢˜ï¼š{topic}
ç« èŠ‚æ ‡é¢˜ï¼š{section_title}
ç›®æ ‡å—ä¼—ï¼š{target_audience}

{subsections_text}
{content_text}

**é‡è¦è¦æ±‚**ï¼š
- å¿…é¡»ä¸ºæ¯ä¸ªå­ç« èŠ‚/è¦ç‚¹ç”Ÿæˆè‡³å°‘2ä¸ªä¸“é—¨çš„æŸ¥è¯¢
- ç¡®ä¿æ‰€æœ‰åˆ—å‡ºçš„å­ç« èŠ‚å’Œå†…å®¹é¡¹éƒ½æœ‰å¯¹åº”çš„æœç´¢æŸ¥è¯¢
- ä¸è¦é—æ¼ä»»ä½•è¦ç‚¹

è¯·ç”Ÿæˆé’ˆå¯¹æ€§å¼ºã€å¤šæ ·åŒ–çš„æœç´¢æŸ¥è¯¢ï¼ŒåŒ…æ‹¬ï¼š
1. åŸºç¡€æ¦‚å¿µå’Œå®šä¹‰ç±»æŸ¥è¯¢
2. æŠ€æœ¯åŸç†å’Œæœºåˆ¶ç±»æŸ¥è¯¢
3. åº”ç”¨æ¡ˆä¾‹å’Œå®è·µç±»æŸ¥è¯¢
4. å‘å±•è¶‹åŠ¿å’Œå‰æ²¿ç±»æŸ¥è¯¢
5. é’ˆå¯¹ç‰¹å®šå—ä¼—çš„æŸ¥è¯¢
6. **æ¯ä¸ªå­ç« èŠ‚/è¦ç‚¹çš„ä¸“é—¨æŸ¥è¯¢**

ç›®æ ‡å—ä¼—ç‰¹ç‚¹ï¼š
- é«˜æ ¡å­¦ç”Ÿï¼šæ³¨é‡æ•™å­¦ã€å­¦ä¹ ã€åŸºç¡€åŸç†
- ä¼ä¸šä»ä¸šè€…ï¼šæ³¨é‡å®è·µã€åº”ç”¨ã€å•†ä¸šä»·å€¼
- AIçˆ±å¥½è€…ï¼šæ³¨é‡å‰æ²¿æŠ€æœ¯ã€æœ€æ–°å‘å±•
- é€šç”¨ï¼šå¹³è¡¡å„æ–¹é¢éœ€æ±‚

è¯·ä»¥JSONæ ¼å¼è¿”å›æŸ¥è¯¢åˆ—è¡¨ï¼š
{{
    "queries": [
        "æŸ¥è¯¢1",
        "æŸ¥è¯¢2",
        "æŸ¥è¯¢3",
        "æŸ¥è¯¢4",
        "æŸ¥è¯¢5",
        "æŸ¥è¯¢6",
        "æŸ¥è¯¢7",
        "æŸ¥è¯¢8",
        "æŸ¥è¯¢9",
        "æŸ¥è¯¢10",
        "æŸ¥è¯¢11",
        "æŸ¥è¯¢12"
    ]
}}

æ³¨æ„ï¼š
- æŸ¥è¯¢åº”ç®€æ´æ˜ç¡®ï¼Œé€‚åˆæœç´¢å¼•æ“
- é¿å…é‡å¤ï¼Œç¡®ä¿å¤šæ ·æ€§
- è€ƒè™‘ä¸­è‹±æ–‡æœç´¢çš„éœ€è¦
- æŸ¥è¯¢é•¿åº¦æ§åˆ¶åœ¨10-30ä¸ªå­—ç¬¦
- ç¡®ä¿æ¯ä¸ªè¦ç‚¹éƒ½æœ‰å¯¹åº”çš„æŸ¥è¯¢
"""
            
            system_message = f"""ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„{topic}é¢†åŸŸä¸“å®¶å’Œæœç´¢ç­–ç•¥ä¸“å®¶ã€‚
ä½ éœ€è¦æ ¹æ®ç»™å®šçš„ç« èŠ‚ä¿¡æ¯å’Œç›®æ ‡å—ä¼—ï¼Œç”Ÿæˆæœ€æœ‰æ•ˆçš„æœç´¢æŸ¥è¯¢ã€‚
ä½ çš„å›ç­”å¿…é¡»æ˜¯ä¸¥æ ¼çš„JSONæ ¼å¼ï¼Œä¸åŒ…å«ä»»ä½•å…¶ä»–æ–‡æœ¬ã€‚"""
            
            response = self.llm_processor.call_llm_api_json(prompt, system_message)
            
            if isinstance(response, dict) and 'queries' in response:
                queries = response['queries']
                if isinstance(queries, list) and len(queries) > 0:
                    print(f"    ğŸ“Š LLMä¸ºç« èŠ‚'{section_title}'ç”Ÿæˆ{len(queries)}ä¸ªæŸ¥è¯¢")
                    return queries[:max_queries]  # ä½¿ç”¨åŠ¨æ€æ•°é‡
            
        except Exception as e:
            print(f"âŒ LLMç”Ÿæˆæœç´¢æŸ¥è¯¢å¤±è´¥: {str(e)}")
        
        # å¤±è´¥æ—¶ä½¿ç”¨å¤‡ç”¨æ–¹æ³•
        return self._generate_queries_fallback(topic, section_title, section_info, target_audience)
    
    def _generate_queries_fallback(self, topic, section_title, section_info, target_audience):
        """å¤‡ç”¨æŸ¥è¯¢ç”Ÿæˆæ–¹æ³• - ä¼˜åŒ–ç‰ˆï¼Œç¡®ä¿æ¯ä¸ªå°ç‚¹éƒ½æœ‰æŸ¥è¯¢"""
        queries = []
        
        # åŸºäºç« èŠ‚æ ‡é¢˜çš„æŸ¥è¯¢ï¼ˆä¼˜å…ˆçº§æœ€é«˜ï¼‰
        queries.append(f"{topic} {section_title}")
        queries.append(f"{topic} {section_title} è¯¦è§£")
        
        # åŸºäºå­ç« èŠ‚çš„æŸ¥è¯¢ï¼ˆæ¯ä¸ªå­ç« èŠ‚éƒ½è¦æœ‰æŸ¥è¯¢ï¼‰
        for sub_title in section_info.get('subsections', {}):
            queries.append(f"{topic} {sub_title}")
            queries.append(f"{section_title} {sub_title}")
            queries.append(f"{topic} {sub_title} å‘å±•")  # å¢åŠ å˜åŒ–
        
        # åŸºäºå†…å®¹é¡¹çš„æŸ¥è¯¢ï¼ˆç¡®ä¿æ¯ä¸ªè¦ç‚¹éƒ½æœ‰ï¼‰
        for item in section_info.get('content', []):
            if len(item) > 3:  # é™ä½è¿‡æ»¤é˜ˆå€¼
                queries.append(f"{topic} {item}")
                queries.append(f"{section_title} {item}")
        
        # æ ¹æ®ç›®æ ‡å—ä¼—è°ƒæ•´æŸ¥è¯¢
        audience_queries = []
        if target_audience == "é«˜æ ¡å­¦ç”Ÿ":
            audience_queries.extend([
                f"{topic} {section_title} æ•™å­¦ å­¦ä¹ ",
                f"{topic} {section_title} åŸºç¡€ åŸç†"
            ])
        elif target_audience == "ä¼ä¸šä»ä¸šè€…":
            audience_queries.extend([
                f"{topic} {section_title} å®è·µ åº”ç”¨",
                f"{topic} {section_title} å•†ä¸š æ¡ˆä¾‹"
            ])
        elif target_audience == "AIçˆ±å¥½è€…":
            audience_queries.extend([
                f"{topic} {section_title} å‰æ²¿ æŠ€æœ¯",
                f"{topic} {section_title} æœ€æ–° å‘å±•"
            ])
        
        # åˆå¹¶æ‰€æœ‰æŸ¥è¯¢
        all_queries = queries + audience_queries
        
        # å»é‡ä½†ä¿æŒé¡ºåº
        unique_queries = []
        seen = set()
        for q in all_queries:
            if q not in seen:
                unique_queries.append(q)
                seen.add(q)
        
        # æ ¹æ®ç« èŠ‚å¤æ‚åº¦è°ƒæ•´æŸ¥è¯¢æ•°é‡
        subsection_count = len(section_info.get('subsections', {}))
        content_count = len(section_info.get('content', []))
        
        # åŠ¨æ€è°ƒæ•´æŸ¥è¯¢æ•°é‡
        if subsection_count > 3 or content_count > 3:
            max_queries = 15  # å¤æ‚ç« èŠ‚å…è®¸æ›´å¤šæŸ¥è¯¢
        elif subsection_count > 1 or content_count > 1:
            max_queries = 12  # ä¸­ç­‰å¤æ‚åº¦
        else:
            max_queries = 8   # ç®€å•ç« èŠ‚
        
        print(f"    ğŸ“Š ä¸ºç« èŠ‚'{section_title}'ç”Ÿæˆ{len(unique_queries[:max_queries])}ä¸ªæŸ¥è¯¢ï¼ˆå­ç« èŠ‚:{subsection_count}, å†…å®¹é¡¹:{content_count}ï¼‰")
        
        return unique_queries[:max_queries]
    
    def _execute_single_query(self, query):
        """æ‰§è¡Œå•ä¸ªæŸ¥è¯¢"""
        search_results = []
        used_urls = set()
        
        for name, collector in self.collectors.items():
            try:
                if name == 'tavily':
                    results = collector.search(query, max_results=3)
                elif name == 'brave':
                    results = collector.search(query, count=3)
                else:
                    continue
                    
                if results:
                    for result in results:
                        url = result.get('url', '')
                        if url and url not in used_urls:
                            result['search_source'] = name
                            search_results.append(result)
                            used_urls.add(url)
                            
            except Exception:
                continue
        
        return search_results
    
    def _convert_to_data_sources(self, raw_results):
        """å°†åŸå§‹æœç´¢ç»“æœè½¬æ¢ä¸ºDataSourceå¯¹è±¡"""
        from collectors.data_filter_processor import DataSource
        
        data_sources = []
        
        for result in raw_results:
            try:
                # ç¡®å®šæ¥æºç±»å‹
                source_type = self._determine_source_type(result)
                
                # æå–å‘å¸ƒæ—¥æœŸ
                publish_date = self._extract_publish_date(result)
                
                # åˆ›å»ºDataSourceå¯¹è±¡ï¼Œä¿ç•™æœç´¢æ¥æºä¿¡æ¯
                data_source = DataSource(
                    content=result.get('content', ''),
                    url=result.get('url', ''),
                    title=result.get('title', ''),
                    source_type=source_type,
                    publish_date=publish_date,
                    author=result.get('author', None)
                )
                
                # ä¿å­˜æœç´¢æ¥æºä¿¡æ¯åˆ°åŸŸåå­—æ®µï¼Œç”¨äºåç»­åˆ†ç»„
                if 'search_source' in result:
                    data_source.domain = f"{result['search_source']}.search_engine"
                
                data_sources.append(data_source)
                
            except Exception as e:
                print(f"    âš ï¸ è½¬æ¢æ•°æ®æºå¤±è´¥: {str(e)}")
                continue
        
        return data_sources
    
    def _determine_source_type(self, result):
        """ç¡®å®šæ•°æ®æºç±»å‹"""
        url = result.get('url', '').lower()
        
        # å­¦æœ¯æ¥æº
        if any(domain in url for domain in ['arxiv.org', 'ieee.org', 'acm.org', 'springer.com', 'nature.com']):
            return 'academic'
        
        # æ–°é—»æ¥æº
        if any(domain in url for domain in ['news', 'reuters', 'bloomberg', 'xinhua', 'people.com']):
            return 'news'
        
        # å¸‚åœºç ”ç©¶
        if any(domain in url for domain in ['market', 'research', 'report', 'analysis']):
            return 'market'
        
        # é»˜è®¤ä¸ºç½‘é¡µ
        return 'web'
    
    def _extract_publish_date(self, result):
        """æå–å‘å¸ƒæ—¥æœŸ"""
        # å°è¯•ä»å¤šä¸ªå¯èƒ½çš„å­—æ®µæå–æ—¥æœŸ
        date_fields = ['published_date', 'publish_date', 'date', 'published']
        
        for field in date_fields:
            if field in result and result[field]:
                date_str = result[field]
                try:
                    # å°è¯•ä¸åŒçš„æ—¥æœŸæ ¼å¼
                    import re
                    from datetime import datetime
                    
                    # åŒ¹é…YYYY-MM-DDæ ¼å¼
                    date_match = re.search(r'(\d{4})-(\d{2})-(\d{2})', str(date_str))
                    if date_match:
                        return f"{date_match.group(1)}-{date_match.group(2)}-{date_match.group(3)}"
                    
                    # åŒ¹é…YYYY/MM/DDæ ¼å¼
                    date_match = re.search(r'(\d{4})/(\d{2})/(\d{2})', str(date_str))
                    if date_match:
                        return f"{date_match.group(1)}-{date_match.group(2)}-{date_match.group(3)}"
                    
                except:
                    continue
        
        return None

class OutlineContentGenerator:
    """
    åŸºäºå¤§çº²çš„å†…å®¹ç”Ÿæˆå™¨
    è´Ÿè´£ç”Ÿæˆç« èŠ‚å†…å®¹
    """
    
    def __init__(self, llm_processor):
        self.llm_processor = llm_processor
        self.results_lock = threading.Lock()
    
    def parallel_generate_main_sections(self, outline_structure, sections_data, topic, target_audience="é€šç”¨", max_workers=3):
        """
        å¹¶è¡Œç”Ÿæˆä¸»è¦ç« èŠ‚çš„å†…å®¹
        
        Args:
            outline_structure (dict): å¤§çº²ç»“æ„
            sections_data (dict): æ”¶é›†çš„æ•°æ®
            topic (str): ä¸»é¢˜
            target_audience (str): ç›®æ ‡å—ä¼—
            max_workers (int): æœ€å¤§å¹¶è¡Œå·¥ä½œæ•°
            
        Returns:
            dict: ç”Ÿæˆçš„ç« èŠ‚å†…å®¹
        """
        print(f"âš¡ [å¹¶è¡Œç”Ÿæˆ] å¼€å§‹å¹¶è¡Œç”Ÿæˆ{len(outline_structure)}ä¸ªä¸»è¦ç« èŠ‚çš„å†…å®¹...")
        start_time = time.time()
        
        generated_sections = {}
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # ğŸš€ å¹¶è¡Œæäº¤ç« èŠ‚å†…å®¹ç”Ÿæˆä»»åŠ¡
            future_to_section = {}
            
            for section_title, section_info in outline_structure.items():
                section_data = sections_data.get(section_title, [])
                future_to_section[executor.submit(
                    self._generate_section_content, 
                    section_title, section_info, section_data, topic, target_audience
                )] = section_title
            
            # ğŸ”„ æ”¶é›†ç”Ÿæˆç»“æœ
            completed_count = 0
            for future in as_completed(future_to_section):
                section_title = future_to_section[future]
                try:
                    section_content = future.result()
                    completed_count += 1
                    
                    with self.results_lock:
                        generated_sections[section_title] = section_content
                    
                    content_length = len(section_content) if section_content else 0
                    print(f"  âœ… [{completed_count}/{len(future_to_section)}] ç« èŠ‚'{section_title}'å†…å®¹ç”Ÿæˆå®Œæˆï¼Œé•¿åº¦{content_length}å­—ç¬¦")
                    
                except Exception as e:
                    print(f"  âŒ ç« èŠ‚'{section_title}'å†…å®¹ç”Ÿæˆå¤±è´¥: {str(e)}")
                    generated_sections[section_title] = ""
        
        total_time = time.time() - start_time
        total_length = sum(len(content) for content in generated_sections.values())
        print(f"ğŸ“Š [å¹¶è¡Œç”Ÿæˆå®Œæˆ] æ€»è®¡ç”Ÿæˆ{total_length}å­—ç¬¦å†…å®¹ï¼Œè€—æ—¶{total_time:.1f}ç§’")
        
        return generated_sections
    
    def _generate_section_content(self, section_title, section_info, section_data, topic, target_audience):
        """ç”Ÿæˆå•ä¸ªç« èŠ‚çš„å†…å®¹ï¼ˆä¸²è¡Œå¤„ç†å­ç« èŠ‚ï¼‰"""
        if not self.llm_processor:
            return self._generate_simple_section_content(section_title, section_info, section_data)
        
        try:
            # æ ¹æ®ç›®æ ‡å—ä¼—è°ƒæ•´è¯­è¨€é£æ ¼
            audience_style = self._get_audience_style(target_audience)
            
            # ä¸²è¡Œç”Ÿæˆå­ç« èŠ‚å†…å®¹
            subsection_contents = []
            
            # è¯„ä¼°ä¸»ç« èŠ‚å¤æ‚åº¦å¹¶åˆ†é…å­—æ•°
            main_complexity = self._assess_section_complexity(section_title, section_info, topic)
            main_word_req = self._get_word_count_requirements(main_complexity)
            
            # è®¡ç®—å­ç« èŠ‚å­—æ•°åˆ†é…
            subsection_count = len(section_info.get('subsections', {}))
            if subsection_count > 0:
                # ä¸ºæ¦‚è¿°é¢„ç•™å­—æ•°
                overview_words = {"high": 500, "medium": 350, "low": 250}[main_complexity]
                # å‰©ä½™å­—æ•°åˆ†é…ç»™å­ç« èŠ‚
                available_words = main_word_req['min_words'] - overview_words
                words_per_subsection = max(300, available_words // subsection_count)
                
                print(f"  ğŸ“Š ä¸»ç« èŠ‚å¤æ‚åº¦: {main_complexity.upper()}, æ€»é¢„ç®—: {main_word_req['min_words']}å­—")
                print(f"  ğŸ“‹ {subsection_count}ä¸ªå­ç« èŠ‚ï¼Œæ¯ä¸ªåˆ†é…çº¦{words_per_subsection}å­—")
            
            # å¤„ç†å­ç« èŠ‚
            for sub_title, sub_info in section_info.get('subsections', {}).items():
                allocated_words = words_per_subsection if subsection_count > 0 else None
                sub_content = self._generate_subsection_content(
                    sub_title, sub_info, section_data, topic, audience_style, allocated_words
                )
                subsection_contents.append({
                    'title': sub_title,
                    'content': sub_content
                })
                
            # ç”Ÿæˆä¸»ç« èŠ‚å†…å®¹
            main_content = self._generate_main_section_content(
                section_title, section_info, subsection_contents, section_data, topic, audience_style
            )
            
            return main_content
            
        except Exception as e:
            print(f"âš ï¸ LLMç”Ÿæˆç« èŠ‚'{section_title}'å†…å®¹å¤±è´¥: {str(e)}ï¼Œä½¿ç”¨ç®€å•æ–¹æ³•")
            return self._generate_simple_section_content(section_title, section_info, section_data)
    
    def _get_audience_style(self, target_audience):
        """è·å–ç›®æ ‡å—ä¼—çš„è¯­è¨€é£æ ¼"""
        styles = {
            "é«˜æ ¡å­¦ç”Ÿ": {
                "tone": "æ•™å­¦å¼ã€å¾ªåºæ¸è¿›",
                "complexity": "ä¸­ç­‰ï¼Œæ³¨é‡æ¦‚å¿µè§£é‡Š",
                "examples": "å­¦æœ¯æ¡ˆä¾‹å’Œå®éªŒ",
                "language": "æ¸…æ™°å‡†ç¡®ï¼Œé€‚é‡ä¸“ä¸šæœ¯è¯­"
            },
            "ä¼ä¸šä»ä¸šè€…": {
                "tone": "å®ç”¨æ€§å¼ºã€è§£å†³æ–¹æ¡ˆå¯¼å‘",
                "complexity": "é«˜ï¼Œæ³¨é‡å®é™…åº”ç”¨",
                "examples": "å•†ä¸šæ¡ˆä¾‹å’Œæœ€ä½³å®è·µ",
                "language": "å•†ä¸šåŒ–è¡¨è¾¾ï¼Œæ³¨é‡ROIå’Œæ•ˆç›Š"
            },
            "AIçˆ±å¥½è€…": {
                "tone": "å‰æ²¿æ¢ç´¢ã€æŠ€æœ¯æ·±å…¥",
                "complexity": "é«˜ï¼Œæ³¨é‡æŠ€æœ¯ç»†èŠ‚",
                "examples": "æœ€æ–°ç ”ç©¶å’ŒæŠ€æœ¯çªç ´",
                "language": "æŠ€æœ¯æ€§å¼ºï¼Œå¯ä½¿ç”¨ä¸“ä¸šæœ¯è¯­"
            },
            "é€šç”¨": {
                "tone": "å¹³è¡¡æ€§ã€æ˜“äºç†è§£",
                "complexity": "ä¸­ç­‰ï¼Œå…¼é¡¾æ·±åº¦å’Œå¹¿åº¦",
                "examples": "å¤šæ ·åŒ–æ¡ˆä¾‹",
                "language": "é€šä¿—æ˜“æ‡‚ï¼Œé€‚åº¦ä¸“ä¸š"
            }
        }
        
        return styles.get(target_audience, styles["é€šç”¨"])
    
    def _assess_section_complexity(self, section_title, section_info, topic):
        """è¯„ä¼°ç« èŠ‚å¤æ‚åº¦"""
        complexity_score = 0
        
        # 1. å­ç« èŠ‚æ•°é‡ (æƒé‡: 25%) - é™ä½æƒé‡ï¼Œè®©å†…å®¹è¦ç‚¹æ•°é‡æ›´é‡è¦
        subsection_count = len(section_info.get('subsections', {}))
        if subsection_count >= 5:
            complexity_score += 25
        elif subsection_count >= 3:
            complexity_score += 18
        else:
            complexity_score += 10
        
        # 2. å†…å®¹è¦ç‚¹æ•°é‡ (æƒé‡: 25%) - å¢åŠ æƒé‡ï¼Œç»†åŒ–é˜ˆå€¼
        total_items = 0
        for sub_info in section_info.get('subsections', {}).values():
            total_items += len(sub_info.get('items', []))
        
        if total_items >= 20:  # 20ä¸ªä»¥ä¸Šè¦ç‚¹ = é«˜å¤æ‚åº¦
            complexity_score += 25
        elif total_items >= 15:  # 15-19ä¸ªè¦ç‚¹ = ä¸­é«˜å¤æ‚åº¦
            complexity_score += 20
        elif total_items >= 10:  # 10-14ä¸ªè¦ç‚¹ = ä¸­ç­‰å¤æ‚åº¦
            complexity_score += 15
        else:  # <10ä¸ªè¦ç‚¹ = ä½å¤æ‚åº¦
            complexity_score += 10
        
        # 3. æŠ€æœ¯å…³é”®è¯å¯†åº¦ (æƒé‡: 25%)
        tech_keywords = [
            'ç®—æ³•', 'æ¶æ„', 'æ¨¡å‹', 'æŠ€æœ¯', 'æ–¹æ³•', 'ç³»ç»Ÿ', 'æ¡†æ¶', 'æœºåˆ¶', 'åŸç†', 'ç­–ç•¥',
            'ä¼˜åŒ–', 'å®ç°', 'è®¾è®¡', 'å¼€å‘', 'éƒ¨ç½²', 'è¯„ä¼°', 'åˆ†æ', 'å¤„ç†', 'è®¡ç®—', 'æ•°æ®'
        ]
        
        text_content = f"{section_title} {' '.join(section_info.get('content', []))}"
        for sub_info in section_info.get('subsections', {}).values():
            text_content += f" {' '.join(sub_info.get('items', []))}"
        
        tech_count = sum(1 for keyword in tech_keywords if keyword in text_content)
        
        if tech_count >= 8:
            complexity_score += 25
        elif tech_count >= 5:
            complexity_score += 20
        elif tech_count >= 3:
            complexity_score += 15
        else:
            complexity_score += 10
        
        # 4. æ ‡é¢˜é•¿åº¦å’Œå¤æ‚æ€§ (æƒé‡: 25%)
        title_length = len(section_title)
        if title_length >= 15:
            complexity_score += 25
        elif title_length >= 10:
            complexity_score += 20
        else:
            complexity_score += 15
        
        # æ ¹æ®å¤æ‚åº¦åˆ†æ•°åˆ†ç±»
        if complexity_score >= 80:
            return "high"  # é«˜å¤æ‚åº¦
        elif complexity_score >= 60:
            return "medium"  # ä¸­ç­‰å¤æ‚åº¦
        else:
            return "low"  # ä½å¤æ‚åº¦
    
    def _get_word_count_requirements(self, complexity):
        """æ ¹æ®å¤æ‚åº¦è·å–å­—æ•°è¦æ±‚"""
        requirements = {
            "high": {
                "min_words": 3500,
                "max_words": 5000,
                "max_tokens": 8000,
                "description": "é«˜å¤æ‚åº¦å†…å®¹ï¼Œéœ€è¦æ·±å…¥è¯¦ç»†çš„åˆ†æ"
            },
            "medium": {
                "min_words": 2500,
                "max_words": 3500,
                "max_tokens": 6000,
                "description": "ä¸­ç­‰å¤æ‚åº¦å†…å®¹ï¼Œå¹³è¡¡æ·±åº¦ä¸å¹¿åº¦"
            },
            "low": {
                "min_words": 2000,
                "max_words": 3000,
                "max_tokens": 5000,
                "description": "åŸºç¡€å¤æ‚åº¦å†…å®¹ï¼Œæ³¨é‡æ¸…æ™°æ€§"
            }
        }
        
        return requirements.get(complexity, requirements["medium"])
    
    def _generate_subsection_content(self, sub_title, sub_info, section_data, topic, audience_style, allocated_words=None):
        """ç”Ÿæˆå­ç« èŠ‚å†…å®¹"""
        items = sub_info.get('items', [])
        
        # å¦‚æœæœ‰åˆ†é…çš„å­—æ•°ï¼Œä½¿ç”¨åˆ†é…å€¼ï¼›å¦åˆ™ä½¿ç”¨å•ç‹¬è¯„ä¼°
        if allocated_words:
            word_req = {
                'min_words': max(300, int(allocated_words * 0.8)),  # æœ€å°‘300å­—
                'max_words': min(2000, int(allocated_words * 1.2)), # æœ€å¤š2000å­—
                'max_tokens': min(4000, int(allocated_words * 2)),
                'description': f"åˆ†é…å­—æ•°çº¦{allocated_words}å­—"
            }
            complexity = "allocated"  # æ ‡è®°ä¸ºåˆ†é…æ¨¡å¼
        else:
            # åå¤‡æ–¹æ¡ˆï¼šå•ç‹¬è¯„ä¼°å­ç« èŠ‚å¤æ‚åº¦
            section_info = {'subsections': {sub_title: sub_info}, 'content': items}
            complexity = self._assess_section_complexity(sub_title, section_info, topic)
            word_req = self._get_word_count_requirements(complexity)
        
        # æ˜¾ç¤ºå­—æ•°åˆ†é…ä¿¡æ¯
        if allocated_words:
            print(f"  ğŸ“ å­ç« èŠ‚'{sub_title}' - åˆ†é…å­—æ•°: {allocated_words}å­— (èŒƒå›´: {word_req['min_words']}-{word_req['max_words']}å­—)")
        else:
            print(f"  ğŸ“ å­ç« èŠ‚'{sub_title}' - å¤æ‚åº¦: {complexity.upper()}, ç›®æ ‡å­—æ•°: {word_req['min_words']}-{word_req['max_words']}å­—")
        
        # å‡†å¤‡å‚è€ƒæ•°æ®å’Œå¼•ç”¨ä¿¡æ¯
        reference_content = self._prepare_reference_content(section_data, sub_title)
        citations = self._prepare_citations(section_data)
        
        # æ„å»ºå­ç« èŠ‚å†…å®¹ç”Ÿæˆæç¤º
        prompt = f"""è¯·ä¸º"{topic}"ä¸»é¢˜ä¸‹çš„å­ç« èŠ‚"{sub_title}"ç”Ÿæˆè¯¦ç»†å†…å®¹ã€‚

å­ç« èŠ‚è¦ç‚¹ï¼š
{chr(10).join(f"- {item}" for item in items)}

ç›®æ ‡å—ä¼—ï¼š{audience_style['tone']}
å¤æ‚åº¦ï¼š{audience_style['complexity']}
è¯­è¨€é£æ ¼ï¼š{audience_style['language']}

å†…å®¹é•¿åº¦è¦æ±‚ï¼š{word_req['description']}

{reference_content}

è¯·ç”Ÿæˆä¸€ä¸ªç»“æ„æ¸…æ™°ã€å†…å®¹è¯¦å®çš„å­ç« èŠ‚ï¼ŒåŒ…æ‹¬ï¼š
1. æ ¸å¿ƒæ¦‚å¿µè§£é‡Š
2. å…³é”®è¦ç‚¹åˆ†æ
3. å®é™…åº”ç”¨æˆ–æ¡ˆä¾‹
4. å›¾è¡¨å»ºè®®ï¼ˆå¦‚æœé€‚ç”¨ï¼‰
5. ä¸å…¶ä»–æ¦‚å¿µçš„å…³è”

è¦æ±‚ï¼š
- å­—æ•°æ§åˆ¶åœ¨{word_req['min_words']}-{word_req['max_words']}å­—
- ä½¿ç”¨æ ‡å‡†Markdownæ ¼å¼
- æ ¹æ®ç›®æ ‡å—ä¼—è°ƒæ•´è¯­è¨€éš¾åº¦
- åœ¨é€‚å½“ä½ç½®æ ‡æ³¨"[å»ºè®®æ’å…¥å›¾è¡¨ï¼šå›¾è¡¨æè¿°]"
- åœ¨é€‚å½“ä½ç½®æ ‡æ³¨"[å»ºè®®æ’å…¥æ¡ˆä¾‹ï¼šæ¡ˆä¾‹æè¿°]"
- é‡è¦ï¼šä¸è¦åœ¨å†…å®¹å¼€å¤´æ·»åŠ æ ‡é¢˜ï¼Œå†…å®¹å°†è‡ªåŠ¨æ·»åŠ åˆ°ç›¸åº”çš„ç« èŠ‚æ ‡é¢˜ä¸‹
- å¦‚æœéœ€è¦ä½¿ç”¨å°èŠ‚æ ‡é¢˜ï¼Œè¯·ä½¿ç”¨å››çº§æ ‡é¢˜ï¼ˆ####ï¼‰æ ¼å¼
- ä¸¥æ ¼ç¦æ­¢ä½¿ç”¨ä¸­æ–‡ç¬¦å·ä½œä¸ºæ ‡é¢˜æ ‡è®°ï¼š
  * ä¸è¦ä½¿ç”¨ï¼šä¸€ã€äºŒã€ä¸‰ã€å››ã€äº”...
  * ä¸è¦ä½¿ç”¨ï¼šï¼ˆä¸€ï¼‰ã€ï¼ˆäºŒï¼‰ã€ï¼ˆä¸‰ï¼‰...
  * ä¸è¦ä½¿ç”¨ï¼š<ä¸€>ã€<äºŒ>ã€<ä¸‰>...
  * ä¸è¦ä½¿ç”¨ï¼šã€ä¸€ã€‘ã€ã€äºŒã€‘ã€ã€ä¸‰ã€‘...
  * ä¸è¦ä½¿ç”¨ï¼šâ‘ â‘¡â‘¢â‘£â‘¤...
  * ä¸è¦ä½¿ç”¨ï¼š1.ã€2.ã€3.ä½œä¸ºæ ‡é¢˜
- æ‰€æœ‰æ ‡é¢˜å¿…é¡»ä½¿ç”¨Markdownæ ¼å¼ï¼š#### æ ‡é¢˜åç§°
- **é‡è¦**ï¼šåœ¨å¼•ç”¨å‚è€ƒèµ„æ–™æ—¶ï¼Œè¯·ä½¿ç”¨ç›¸åº”çš„å¼•ç”¨æ ‡è®°ï¼Œå¦‚[1]ã€[2]ç­‰

æ ¹æ®å­—æ•°è¦æ±‚ç”Ÿæˆç›¸åº”é•¿åº¦çš„å†…å®¹ï¼Œç¡®ä¿ä¿¡æ¯ä¸°å¯Œä¸”é€»è¾‘æ¸…æ™°ã€‚
"""
        
        system_message = f"ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„{topic}é¢†åŸŸä¸“å®¶å’Œå†…å®¹åˆ›ä½œè€…ï¼Œæ“…é•¿ä¸ºä¸åŒå—ä¼—åˆ›ä½œé«˜è´¨é‡çš„æŠ€æœ¯å†…å®¹ã€‚è¯·ç›´æ¥è¾“å‡ºå†…å®¹ï¼Œä¸è¦æ·»åŠ ç« èŠ‚æ ‡é¢˜ã€‚ä¸¥æ ¼éµå®ˆMarkdownæ ¼å¼è§„èŒƒï¼Œæ‰€æœ‰æ ‡é¢˜å¿…é¡»ä½¿ç”¨#å·æ ‡è®°ã€‚æ ¹æ®æŒ‡å®šçš„å­—æ•°è¦æ±‚ç”Ÿæˆç›¸åº”é•¿åº¦çš„å†…å®¹ã€‚ç¡®ä¿åœ¨é€‚å½“ä½ç½®åŒ…å«å¼•ç”¨æ ‡è®°ã€‚"
        
        content = self.llm_processor.call_llm_api(prompt, system_message, temperature=0.3, max_tokens=word_req['max_tokens'])
        
        # åœ¨å†…å®¹æœ«å°¾æ·»åŠ å¼•ç”¨ä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
        if citations:
            content += f"\n\n{citations}"
        
        return content
    
    def _prepare_reference_content(self, section_data, current_section):
        """å‡†å¤‡å‚è€ƒå†…å®¹ç”¨äºLLMç”Ÿæˆ"""
        if not section_data:
            return ""
        
        # å¤„ç†ä¸åŒçš„æ•°æ®æ ¼å¼
        if isinstance(section_data, list) and len(section_data) > 0:
            # æ£€æŸ¥æ˜¯å¦æ˜¯FilteredDataå¯¹è±¡
            if hasattr(section_data[0], 'selected_excerpts'):
                # æ–°çš„FilteredDataæ ¼å¼
                reference_parts = []
                for i, filtered_data in enumerate(section_data[:5]):  # æœ€å¤š5ä¸ªå‚è€ƒæº
                    if filtered_data.selected_excerpts:
                        source_info = f"æ¥æº{i+1}: {filtered_data.source.title}"
                        excerpts = '\n'.join(f"- {excerpt}" for excerpt in filtered_data.selected_excerpts[:3])
                        reference_parts.append(f"{source_info}\n{excerpts}")
                
                if reference_parts:
                    return f"""
å‚è€ƒèµ„æ–™æ‘˜å½•ï¼š
{chr(10).join(reference_parts)}

è¯·åœ¨ç”Ÿæˆå†…å®¹æ—¶åˆç†å¼•ç”¨è¿™äº›èµ„æ–™ï¼Œä½¿ç”¨[1]ã€[2]ç­‰æ ‡è®°ã€‚
"""
            else:
                # åŸå§‹æ ¼å¼
                reference_parts = []
                for i, data in enumerate(section_data[:5]):
                    if data.get('content'):
                        content_snippet = data['content'][:200] + '...' if len(data['content']) > 200 else data['content']
                        reference_parts.append(f"æ¥æº{i+1}: {data.get('title', 'æœªçŸ¥æ ‡é¢˜')}\n- {content_snippet}")
                
                if reference_parts:
                    return f"""
å‚è€ƒèµ„æ–™æ‘˜å½•ï¼š
{chr(10).join(reference_parts)}

è¯·åœ¨ç”Ÿæˆå†…å®¹æ—¶åˆç†å¼•ç”¨è¿™äº›èµ„æ–™ï¼Œä½¿ç”¨[1]ã€[2]ç­‰æ ‡è®°ã€‚
"""
        
        return ""
    
    def _prepare_citations(self, section_data):
        """å‡†å¤‡å¼•ç”¨ä¿¡æ¯"""
        if not section_data:
            return ""
        
        citations = []
        
        # å¤„ç†ä¸åŒçš„æ•°æ®æ ¼å¼
        if isinstance(section_data, list) and len(section_data) > 0:
            # æ£€æŸ¥æ˜¯å¦æ˜¯FilteredDataå¯¹è±¡
            if hasattr(section_data[0], 'source'):
                # æ–°çš„FilteredDataæ ¼å¼
                for i, filtered_data in enumerate(section_data[:5]):  # æœ€å¤š5ä¸ªå‚è€ƒæº
                    citation = f"[{i+1}] {filtered_data.source.title}"
                    if filtered_data.source.url:
                        citation += f" - {filtered_data.source.url}"
                    if filtered_data.source.publish_date:
                        citation += f" ({filtered_data.source.publish_date})"
                    
                    # æ·»åŠ è´¨é‡è¯„åˆ†ä¿¡æ¯
                    score = filtered_data.quality_score.total_score
                    citation += f" [è¯„åˆ†: {score:.2f}]"
                    
                    citations.append(citation)
            else:
                # åŸå§‹æ ¼å¼
                for i, data in enumerate(section_data[:5]):
                    citation = f"[{i+1}] {data.get('title', 'æœªçŸ¥æ ‡é¢˜')}"
                    if data.get('url'):
                        citation += f" - {data['url']}"
                    citations.append(citation)
        
        if citations:
            return "**å‚è€ƒèµ„æ–™ï¼š**\n" + '\n'.join(citations)
        
        return ""
    
    def _generate_main_section_content(self, section_title, section_info, subsection_contents, section_data, topic, audience_style):
        """ç”Ÿæˆä¸»ç« èŠ‚å†…å®¹"""
        # æ„å»ºä¸»ç« èŠ‚å†…å®¹ - ä½¿ç”¨äºŒçº§æ ‡é¢˜ï¼ˆ##ï¼‰è€Œä¸æ˜¯ä¸€çº§æ ‡é¢˜
        main_content = f"## {section_title}\n\n"
        
        # è¯„ä¼°ç« èŠ‚å¤æ‚åº¦
        complexity = self._assess_section_complexity(section_title, section_info, topic)
        word_req = self._get_word_count_requirements(complexity)
        
        # æ ¹æ®å¤æ‚åº¦è°ƒæ•´æ¦‚è¿°é•¿åº¦
        overview_length = {
            "high": "400-600å­—",
            "medium": "300-400å­—", 
            "low": "200-300å­—"
        }
        
        # æ·»åŠ ç« èŠ‚æ¦‚è¿°
        overview_prompt = f"""è¯·ä¸º"{topic}"ä¸»é¢˜ä¸‹çš„ä¸»ç« èŠ‚"{section_title}"ç”Ÿæˆä¸€ä¸ªè¯¦ç»†çš„æ¦‚è¿°æ®µè½ã€‚

ç« èŠ‚åŒ…å«çš„å­ç« èŠ‚ï¼š
{chr(10).join(f"- {sub['title']}" for sub in subsection_contents)}

ç›®æ ‡å—ä¼—ï¼š{audience_style['tone']}
è¯­è¨€é£æ ¼ï¼š{audience_style['language']}
ç« èŠ‚å¤æ‚åº¦ï¼š{complexity.upper()} ({word_req['description']})

è¯·ç”Ÿæˆä¸€ä¸ª{overview_length[complexity]}çš„æ¦‚è¿°ï¼Œä»‹ç»æœ¬ç« èŠ‚çš„ä¸»è¦å†…å®¹å’Œé‡è¦æ€§ã€‚
æ ¹æ®å¤æ‚åº¦è¦æ±‚ï¼š
{complexity == "high" and "- æ·±å…¥ä»‹ç»æŠ€æœ¯èƒŒæ™¯å’Œå‰æ²¿å‘å±•\n- è¯¦ç»†è¯´æ˜å„å­ç« èŠ‚çš„å…³è”æ€§\n- å¼ºè°ƒæŠ€æœ¯åŸç†å’Œåº”ç”¨ä»·å€¼" or ""}
{complexity == "medium" and "- å¹³è¡¡ä»‹ç»åŸºç¡€æ¦‚å¿µå’Œåº”ç”¨åœºæ™¯\n- è¯´æ˜å„å­ç« èŠ‚çš„é€»è¾‘å…³ç³»\n- çªå‡ºå®é™…ä»·å€¼å’Œå‘å±•è¶‹åŠ¿" or ""}
{complexity == "low" and "- ç®€æ˜ä»‹ç»æ ¸å¿ƒæ¦‚å¿µ\n- æ¦‚è¿°ä¸»è¦å†…å®¹ç»“æ„\n- å¼ºè°ƒåŸºç¡€æ€§å’Œå®ç”¨æ€§" or ""}

æ³¨æ„ï¼šç”Ÿæˆæ™®é€šæ®µè½æ–‡æœ¬ï¼Œä¸è¦æ·»åŠ ä»»ä½•æ ‡é¢˜æ ‡è®°ã€‚
"""
        
        overview_max_tokens = {
            "high": 1200,
            "medium": 800,
            "low": 600
        }
        
        try:
            overview = self.llm_processor.call_llm_api(overview_prompt, 
                f"ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„{topic}é¢†åŸŸä¸“å®¶ï¼Œæ“…é•¿ç”Ÿæˆæ¸…æ™°çš„æ®µè½æ–‡æœ¬ã€‚", 
                temperature=0.3, max_tokens=overview_max_tokens[complexity])
            main_content += f"{overview}\n\n"
        except:
            main_content += f"æœ¬ç« èŠ‚å°†è¯¦ç»†ä»‹ç»{section_title}çš„ç›¸å…³å†…å®¹ã€‚\n\n"
        
        # æ·»åŠ å­ç« èŠ‚å†…å®¹ - ä½¿ç”¨ä¸‰çº§æ ‡é¢˜ï¼ˆ###ï¼‰
        for sub_content in subsection_contents:
            # æ¸…ç†å­ç« èŠ‚æ ‡é¢˜ï¼Œç¡®ä¿æ ¼å¼æ­£ç¡®
            clean_title = self._clean_section_title(sub_content['title'])
            main_content += f"### {clean_title}\n\n"
            
            # å¤„ç†å­ç« èŠ‚å†…å®¹ï¼Œç¡®ä¿å†…éƒ¨æ ‡é¢˜å±‚çº§æ­£ç¡®
            clean_content = self._normalize_content_headings(sub_content['content'])
            main_content += f"{clean_content}\n\n"
        
        # æ·»åŠ ç›´æ¥å†…å®¹é¡¹
        if section_info.get('content'):
            main_content += "### è¡¥å……è¦ç‚¹\n\n"
            for item in section_info['content']:
                main_content += f"- {item}\n"
            main_content += "\n"
        
        # æ·»åŠ ç« èŠ‚çº§åˆ«çš„å‚è€ƒèµ„æ–™
        if section_data:
            main_content += "### å‚è€ƒèµ„æ–™\n\n"
            
            # å¤„ç†ä¸åŒçš„æ•°æ®æ ¼å¼
            if isinstance(section_data, list) and len(section_data) > 0:
                # æ£€æŸ¥æ˜¯å¦æ˜¯FilteredDataå¯¹è±¡
                if hasattr(section_data[0], 'source'):
                    # æ–°çš„FilteredDataæ ¼å¼
                    for i, filtered_data in enumerate(section_data[:5]):  # æœ€å¤š5ä¸ªæ¥æº
                        source = filtered_data.source
                        title = source.title if source.title else f'å‚è€ƒèµ„æ–™{i+1}'
                        url = source.url if source.url else '#'
                        
                        # æ„å»ºå¼•ç”¨ä¿¡æ¯
                        citation_info = f"- [{title}]({url})"
                        
                        # æ·»åŠ æ¥æºç±»å‹
                        if source.source_type:
                            citation_info += f" ({source.source_type})"
                        
                        # æ·»åŠ å‘å¸ƒæ—¥æœŸ
                        if source.publish_date:
                            citation_info += f" - {source.publish_date}"
                        
                        # æ·»åŠ è´¨é‡è¯„åˆ†
                        score = filtered_data.quality_score.total_score
                        citation_info += f" [è¯„åˆ†: {score:.2f}]"
                        
                        # æ·»åŠ è¯„åˆ†ç†ç”±
                        if filtered_data.reasoning:
                            citation_info += f"\n  - è¯„åˆ†ç†ç”±: {filtered_data.reasoning}"
                        
                        main_content += citation_info + "\n"
                else:
                    # åŸå§‹æ ¼å¼
                    for i, data in enumerate(section_data[:5]):
                        title = data.get('title', f'å‚è€ƒèµ„æ–™{i+1}')
                        url = data.get('url', '#')
                        source = data.get('search_source', 'æœªçŸ¥æ¥æº')
                        main_content += f"- [{title}]({url}) - {source}\n"
        
        return main_content
    
    def _clean_section_title(self, title):
        """æ¸…ç†ç« èŠ‚æ ‡é¢˜ï¼Œç§»é™¤ä¸­æ–‡ç¬¦å·å¹¶è§„èŒƒåŒ–"""
        import re
        
        # ç§»é™¤å„ç§ä¸­æ–‡ç¬¦å·
        title = re.sub(r'^[ï¼ˆ(]\s*[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+\s*[ï¼‰)]\s*', '', title)  # ç§»é™¤ï¼ˆä¸€ï¼‰ã€ï¼ˆäºŒï¼‰ç­‰
        title = re.sub(r'^[<ï¼œ]\s*[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+\s*[>ï¼]\s*', '', title)  # ç§»é™¤<ä¸€>ã€<äºŒ>ç­‰
        title = re.sub(r'^[ã€\[]\s*[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+\s*[ã€‘\]]\s*', '', title)  # ç§»é™¤ã€ä¸€ã€‘ã€ã€äºŒã€‘ç­‰
        title = re.sub(r'^[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+[ã€ï¼\.]?\s*', '', title)  # ç§»é™¤ä¸€ã€äºŒã€ç­‰
        title = re.sub(r'^[â‘ â‘¡â‘¢â‘£â‘¤â‘¥â‘¦â‘§â‘¨â‘©]+\s*', '', title)  # ç§»é™¤â‘ â‘¡â‘¢ç­‰
        title = re.sub(r'^[1-9]\d*[\.ã€‚)]\s*', '', title)  # ç§»é™¤1.ã€2.ç­‰
        title = re.sub(r'^[A-Za-z][\.ã€‚)]\s*', '', title)  # ç§»é™¤A.ã€B.ç­‰
        
        return title.strip()
    
    def _normalize_content_headings(self, content):
        """è§„èŒƒåŒ–å†…å®¹ä¸­çš„æ ‡é¢˜å±‚çº§"""
        import re
        
        # å°†ä¸€çº§æ ‡é¢˜é™çº§ä¸ºå››çº§æ ‡é¢˜
        content = re.sub(r'^#\s+', '#### ', content, flags=re.MULTILINE)
        
        # å°†äºŒçº§æ ‡é¢˜é™çº§ä¸ºå››çº§æ ‡é¢˜
        content = re.sub(r'^##\s+', '#### ', content, flags=re.MULTILINE)
        
        # å°†ä¸‰çº§æ ‡é¢˜é™çº§ä¸ºå››çº§æ ‡é¢˜
        content = re.sub(r'^###\s+', '#### ', content, flags=re.MULTILINE)
        
        # å¤„ç†åŠ ç²—æ ‡é¢˜è½¬æ¢ä¸ºå››çº§æ ‡é¢˜
        content = re.sub(r'^\*\*([^*]+)\*\*\s*$', r'#### \1', content, flags=re.MULTILINE)
        
        return content
    
    def _generate_simple_section_content(self, section_title, section_info, section_data):
        """ç®€å•çš„ç« èŠ‚å†…å®¹ç”Ÿæˆ"""
        content = f"## {section_title}\n\n"
        
        # æ·»åŠ å­ç« èŠ‚
        for sub_title, sub_info in section_info.get('subsections', {}).items():
            # æ¸…ç†å­ç« èŠ‚æ ‡é¢˜
            clean_title = self._clean_section_title(sub_title)
            content += f"### {clean_title}\n\n"
            for item in sub_info.get('items', []):
                content += f"- {item}\n"
            content += "\n"
        
        # æ·»åŠ ç›´æ¥å†…å®¹
        if section_info.get('content'):
            content += "### ä¸»è¦è¦ç‚¹\n\n"
            for item in section_info['content']:
                content += f"- {item}\n"
            content += "\n"
        
        return content

def generate_outline_report(topic, outline_text, target_audience="é€šç”¨", output_file=None, parallel_config="balanced", extracted_topic=None):
    """
    æ ¹æ®å¤§çº²ç”ŸæˆæŠ¥å‘Š
    
    Args:
        topic (str): ä¸»é¢˜
        outline_text (str): å¤§çº²æ–‡æœ¬
        target_audience (str): ç›®æ ‡å—ä¼—
        output_file (str): è¾“å‡ºæ–‡ä»¶è·¯å¾„
        parallel_config (str): å¹¶è¡Œé…ç½®
        extracted_topic (str): å·²æå–çš„ä¸»é¢˜ï¼ˆç”¨äºé¿å…é‡å¤å¤„ç†ï¼‰
        
    Returns:
        tuple: (æŠ¥å‘Šæ–‡ä»¶è·¯å¾„, æŠ¥å‘Šæ•°æ®)
    """
    print(f"ğŸš€ å¼€å§‹ç”ŸæˆåŸºäºå¤§çº²çš„æŠ¥å‘Š: {topic}")
    print(f"ğŸ‘¥ ç›®æ ‡å—ä¼—: {target_audience}")
    print(f"âš™ï¸ å¹¶è¡Œé…ç½®: {parallel_config}")
    
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    os.makedirs(config.OUTPUT_DIR, exist_ok=True)
    
    # æ­¥éª¤1ï¼šåˆå§‹åŒ–LLMå¤„ç†å™¨
    llm_processor = None
    try:
        llm_processor = LLMProcessor()
        print("âœ… å·²åˆå§‹åŒ–LLMå¤„ç†å™¨ç”¨äºæ™ºèƒ½å¤§çº²è§£æå’Œå†…å®¹ç”Ÿæˆ")
    except Exception as e:
        print(f"âš ï¸ åˆå§‹åŒ–LLMå¤„ç†å™¨å¤±è´¥: {str(e)}ï¼Œå°†ä½¿ç”¨å¤‡ç”¨è§£ææ–¹æ³•")
    
    # æ­¥éª¤2ï¼šæ™ºèƒ½è§£æå¤§çº²ï¼ˆä¼ é€’å·²æå–çš„ä¸»é¢˜ä»¥é¿å…é‡å¤å¤„ç†ï¼‰
    parser = OutlineParser(llm_processor)
    # ä½¿ç”¨å·²æå–çš„ä¸»é¢˜ï¼Œé¿å…æŠŠä¸»é¢˜å½“ä½œç« èŠ‚å¤„ç†
    outline_structure = parser.parse_outline(outline_text, extracted_topic)
    
    if not outline_structure:
        raise ValueError("å¤§çº²è§£æå¤±è´¥ï¼Œè¯·æ£€æŸ¥å¤§çº²æ ¼å¼")
    
    # æ­¥éª¤3ï¼šå¹¶è¡Œæ”¶é›†æ•°æ®
    try:
        collector = OutlineDataCollector(llm_processor)
        
        # é…ç½®å¹¶è¡Œå‚æ•°
        parallel_configs = {
            "conservative": {"data_workers": 2, "content_workers": 2},
            "balanced": {"data_workers": 3, "content_workers": 3},
            "aggressive": {"data_workers": 4, "content_workers": 4}
        }
        
        config_params = parallel_configs.get(parallel_config, parallel_configs["balanced"])
        
        sections_data = collector.parallel_collect_main_sections(
            outline_structure, topic, target_audience, 
            max_workers=config_params['data_workers']
        )
        
    except Exception as e:
        print(f"âš ï¸ æ•°æ®æ”¶é›†å¤±è´¥: {str(e)}ï¼Œå°†ä½¿ç”¨åŸºç¡€ç”Ÿæˆæ–¹æ³•")
        sections_data = {}
    
    # æ­¥éª¤4ï¼šè¯„ä¼°å„ç« èŠ‚å¤æ‚åº¦
    print("\nğŸ“Š ç« èŠ‚å¤æ‚åº¦è¯„ä¼°:")
    complexity_stats = {"high": 0, "medium": 0, "low": 0}
    
    if llm_processor:
        temp_generator = OutlineContentGenerator(llm_processor)
        for section_title, section_info in outline_structure.items():
            complexity = temp_generator._assess_section_complexity(section_title, section_info, topic)
            word_req = temp_generator._get_word_count_requirements(complexity)
            complexity_stats[complexity] += 1
            print(f"  ğŸ“ {section_title}: {complexity.upper()} ({word_req['min_words']}-{word_req['max_words']}å­—)")
    
    print(f"\nğŸ“ˆ å¤æ‚åº¦ç»Ÿè®¡: é«˜å¤æ‚åº¦ {complexity_stats['high']}ä¸ª, ä¸­ç­‰å¤æ‚åº¦ {complexity_stats['medium']}ä¸ª, ä½å¤æ‚åº¦ {complexity_stats['low']}ä¸ª")
    
    # æ­¥éª¤5ï¼šå¹¶è¡Œç”Ÿæˆå†…å®¹
    try:
        if llm_processor:
            generator = OutlineContentGenerator(llm_processor)
            generated_sections = generator.parallel_generate_main_sections(
                outline_structure, sections_data, topic, target_audience,
                max_workers=config_params['content_workers']
            )
        else:
            generator = OutlineContentGenerator(None)
            generated_sections = generator.parallel_generate_main_sections(
                outline_structure, sections_data, topic, target_audience, max_workers=1
            )
        
    except Exception as e:
        print(f"âš ï¸ å†…å®¹ç”Ÿæˆå¤±è´¥: {str(e)}ï¼Œä½¿ç”¨ç®€å•ç”Ÿæˆæ–¹æ³•")
        generator = OutlineContentGenerator(None)
        generated_sections = generator.parallel_generate_main_sections(
            outline_structure, sections_data, topic, target_audience, max_workers=1
        )
    
    # æ­¥éª¤6ï¼šç»„ç»‡æŠ¥å‘Š
    report_content = _organize_outline_report(
        topic, outline_structure, generated_sections, target_audience
    )
    
    # æ­¥éª¤7ï¼šä¿å­˜æŠ¥å‘Š
    if not output_file:
        date_str = datetime.now().strftime('%Y%m%d')
        clean_topic = topic.replace(' ', '_').replace('/', '_').replace('\\', '_').lower()
        output_file = os.path.join(config.OUTPUT_DIR, f"{clean_topic}_outline_report_{date_str}.md")
    elif not os.path.isabs(output_file):
        output_dir = os.path.dirname(output_file)
        if output_dir:
            os.makedirs(output_dir, exist_ok=True)
    
    with open(output_file, 'w', encoding='utf-8-sig') as f:
        f.write(report_content)
    
    print(f"\nğŸ‰ === å¤§çº²æŠ¥å‘Šç”Ÿæˆå®Œæˆ ===")
    print(f"ğŸ“„ æŠ¥å‘Šå·²ä¿å­˜è‡³: {output_file}")
    print(f"ğŸ“Š æŠ¥å‘Šç»Ÿè®¡:")
    print(f"   - ä¸»è¦ç« èŠ‚: {len(outline_structure)}")
    print(f"   - ç›®æ ‡å—ä¼—: {target_audience}")
    print(f"   - æ–‡ä»¶å¤§å°: {len(report_content)} å­—ç¬¦")
    
    # ä¿®å¤æ ‡é¢˜æ ¼å¼
    print("ğŸ”§ æ­£åœ¨ä¼˜åŒ–æŠ¥å‘Šæ ‡é¢˜æ ¼å¼...")
    try:
        if _fix_outline_report_format(output_file):
            print("âœ… Markdownæ ¼å¼ä¿®å¤æˆåŠŸ")
        else:
            print("âš ï¸ Markdownæ ¼å¼ä¿®å¤å¤±è´¥ï¼Œä½†æŠ¥å‘Šå·²ç”Ÿæˆ")
    except Exception as e:
        print(f"âš ï¸ æ ¼å¼ä¿®å¤å‡ºé”™: {str(e)}")
        import traceback
        print(traceback.format_exc())
    
    return output_file, {
        'title': f"{topic}æŠ¥å‘Š",
        'content': report_content,
        'outline_structure': outline_structure,
        'target_audience': target_audience,
        'date': datetime.now().strftime('%Y-%m-%d')
    }

def _organize_outline_report(topic, outline_structure, generated_sections, target_audience):
    """ç»„ç»‡å¤§çº²æŠ¥å‘Š"""
    # ç”ŸæˆæŠ¥å‘Šæ ‡é¢˜
    report_content = f"# {topic}æŠ¥å‘Š\n\n"
    
    # æ·»åŠ æŠ¥å‘Šä¿¡æ¯
    report_content += f"**ç›®æ ‡å—ä¼—**: {target_audience}\n\n"
    report_content += f"**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n"
    
    # æ·»åŠ ç›®å½•
    report_content += "## ç›®å½•\n\n"
    for i, section_title in enumerate(outline_structure.keys(), 1):
        # æ¸…ç†æ ‡é¢˜ç”¨äºç›®å½•é“¾æ¥
        clean_title = _clean_title_for_link(section_title)
        report_content += f"{i}. [{section_title}](#{clean_title})\n"
    report_content += "\n"
    
    # æ·»åŠ ç« èŠ‚å†…å®¹
    for section_title in outline_structure.keys():
        section_content = generated_sections.get(section_title, "")
        if section_content:
            # ç¡®ä¿ç« èŠ‚å†…å®¹ä¸é‡å¤æ·»åŠ æ ‡é¢˜
            if section_content.strip().startswith(f"## {section_title}"):
                report_content += f"{section_content}\n\n"
            else:
                report_content += f"## {section_title}\n\n{section_content}\n\n"
        else:
            # å¤‡ç”¨å†…å®¹
            report_content += f"## {section_title}\n\n"
            report_content += f"æœ¬ç« èŠ‚å°†è¯¦ç»†ä»‹ç»{section_title}çš„ç›¸å…³å†…å®¹ã€‚\n\n"
    
    # æ·»åŠ ä½¿ç”¨è¯´æ˜
    report_content += "---\n\n"
    report_content += "## ä½¿ç”¨è¯´æ˜\n\n"
    report_content += "- æœ¬æŠ¥å‘ŠåŸºäºAIæŠ€æœ¯ç”Ÿæˆï¼Œå†…å®¹ä»…ä¾›å‚è€ƒ\n"
    report_content += "- å»ºè®®ç»“åˆæœ€æ–°èµ„æ–™è¿›è¡ŒéªŒè¯å’Œæ›´æ–°\n"
    report_content += "- å›¾è¡¨å’Œæ¡ˆä¾‹æ ‡æ³¨ä½ç½®å»ºè®®æ ¹æ®å®é™…éœ€è¦æ’å…¥ç›¸åº”å†…å®¹\n"
    
    return report_content

def _clean_title_for_link(title):
    """æ¸…ç†æ ‡é¢˜ç”¨äºMarkdowné“¾æ¥"""
    import re
    
    # ç§»é™¤å„ç§ä¸­æ–‡ç¬¦å·å’Œç‰¹æ®Šå­—ç¬¦
    clean_title = re.sub(r'[ï¼ˆï¼‰()ã€ã€‘\[\]<>ï¼œï¼ã€ã€‚ï¼Œ,\s]+', '-', title)
    clean_title = re.sub(r'[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åâ‘ â‘¡â‘¢â‘£â‘¤â‘¥â‘¦â‘§â‘¨â‘©]+', '', clean_title)
    clean_title = re.sub(r'-+', '-', clean_title)  # åˆå¹¶å¤šä¸ªè¿å­—ç¬¦
    clean_title = clean_title.strip('-').lower()  # ç§»é™¤é¦–å°¾è¿å­—ç¬¦å¹¶è½¬å°å†™
    
    return clean_title

def _fix_outline_report_format(file_path):
    """ä¿®å¤å¤§çº²æŠ¥å‘Šçš„Markdownæ ¼å¼ - ä½¿ç”¨æ–°çš„æœ‰æ•ˆåå¤„ç†é€»è¾‘"""
    import re
    
    try:
        # è¯»å–æ–‡ä»¶
        with open(file_path, 'r', encoding='utf-8-sig') as f:
            content = f.read()
        
        # åˆ›å»ºå¤‡ä»½
        backup_path = file_path + '.bak'
        with open(backup_path, 'w', encoding='utf-8-sig') as f:
            f.write(content)
        
        print(f"å·²åˆ›å»ºå¤‡ä»½æ–‡ä»¶: {backup_path}")
        
        # ä¿®å¤ä»£ç å—æ ‡è®°å’ŒMermaidè¯­æ³•é—®é¢˜
        print("æ­£åœ¨ä¿®å¤ä»£ç å—æ ‡è®°å’ŒMermaidè¯­æ³•é—®é¢˜...")
        fixed_content = _fix_code_block_issues(content)
        
        # å†™å›æ–‡ä»¶
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(fixed_content)
        
        print(f"æŠ¥å‘Šæ ¼å¼ä¿®å¤å®Œæˆ: {file_path}")
        return True
        
    except Exception as e:
        print(f"ä¿®å¤æŠ¥å‘Šæ ¼å¼æ—¶å‡ºé”™: {str(e)}")
        return False

def _fix_code_block_issues(content):
    """ä¿®å¤ä»£ç å—æ ‡è®°é—®é¢˜å’ŒMermaidè¯­æ³•é—®é¢˜"""
    import re
    
    # 1. é¦–å…ˆä¿®å¤Mermaidè¯­æ³•é—®é¢˜
    content = _fix_mermaid_syntax(content)
    
    # 2. ç§»é™¤é”™è¯¯çš„ ```markdown æ ‡è®°
    content = re.sub(r'^```markdown\s*$', '', content, flags=re.MULTILINE)
    content = re.sub(r'^`````markdown\s*$', '', content, flags=re.MULTILINE)
    
    # 3. ç§»é™¤å­¤ç«‹çš„ ``` æ ‡è®°ï¼ˆä¸æ˜¯çœŸæ­£çš„ä»£ç å—ï¼‰
    lines = content.split('\n')
    fixed_lines = []
    
    for i, line in enumerate(lines):
        stripped = line.strip()
        
        # å¦‚æœæ˜¯å­¤ç«‹çš„ ``` è¡Œ
        if stripped in ['```', '`````']:
            # æ£€æŸ¥å‰åæ˜¯å¦æœ‰çœŸæ­£çš„ä»£ç å†…å®¹
            has_code_before = False
            has_code_after = False
            
            # æ£€æŸ¥å‰é¢10è¡Œ
            for j in range(max(0, i-10), i):
                prev_line = lines[j].strip()
                if prev_line.startswith('```') and prev_line not in ['```', '`````']:
                    has_code_before = True
                    break
            
            # æ£€æŸ¥åé¢10è¡Œ
            for j in range(i+1, min(len(lines), i+11)):
                next_line = lines[j].strip()
                if next_line.startswith('```') and next_line not in ['```', '`````']:
                    has_code_after = True
                    break
            
            # å¦‚æœä¸æ˜¯çœŸæ­£çš„ä»£ç å—ï¼Œå°±ç§»é™¤
            if not (has_code_before or has_code_after):
                continue
        
        fixed_lines.append(line)
    
    return '\n'.join(fixed_lines)

def _fix_mermaid_syntax(content):
    """ä¿®å¤Mermaidå›¾è¡¨çš„è¯­æ³•é—®é¢˜"""
    lines = content.split('\n')
    fixed_lines = []
    in_mermaid_block = False
    
    for i, line in enumerate(lines):
        stripped = line.strip()
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯mermaidå—çš„å¼€å§‹
        if stripped == '```mermaid':
            in_mermaid_block = True
            fixed_lines.append(line)
            continue
        
        # å¦‚æœåœ¨mermaidå—ä¸­ï¼Œæ£€æŸ¥æ˜¯å¦æ˜¯ç»“æŸæ ‡è®°
        if in_mermaid_block:
            # å¦‚æœé‡åˆ°ç»“æŸæ ‡è®°
            if stripped == '```':
                in_mermaid_block = False
                fixed_lines.append(line)
                continue
            
            # å¦‚æœé‡åˆ°ä»¥ # å¼€å¤´çš„è¡Œï¼ˆå¯èƒ½æ˜¯markdownæ ‡é¢˜ï¼‰ï¼Œè¯´æ˜mermaidå—æ²¡æœ‰æ­£ç¡®ç»“æŸ
            if stripped.startswith('#'):
                # æ’å…¥ç¼ºå¤±çš„ç»“æŸæ ‡è®°
                fixed_lines.append('```')
                fixed_lines.append('')  # æ·»åŠ ç©ºè¡Œ
                fixed_lines.append(line)
                in_mermaid_block = False
                continue
        
        fixed_lines.append(line)
    
    # å¦‚æœæ–‡ä»¶ç»“æŸæ—¶ä»åœ¨mermaidå—ä¸­ï¼Œæ·»åŠ ç»“æŸæ ‡è®°
    if in_mermaid_block:
        fixed_lines.append('```')
    
    return '\n'.join(fixed_lines)



def load_outline_from_file(file_path):
    """ä»æ–‡ä»¶åŠ è½½å¤§çº²"""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            return f.read()
    except Exception as e:
        print(f"âŒ è¯»å–å¤§çº²æ–‡ä»¶å¤±è´¥: {str(e)}")
        return None

def extract_topic_from_outline(outline_text, outline_file_path=None):
    """ä»å¤§çº²å†…å®¹ä¸­æå–é¢˜ç›®"""
    import re
    
    if not outline_text:
        return None
    
    # å°è¯•ä»å¤§çº²å†…å®¹çš„å¼€å¤´å‡ è¡Œæå–é¢˜ç›®
    lines = outline_text.strip().split('\n')
    
    for i, line in enumerate(lines[:10]):  # åªæ£€æŸ¥å‰10è¡Œ
        line = line.strip()
        if not line:
            continue
        
        # åŒ¹é…å¯èƒ½çš„é¢˜ç›®æ ¼å¼
        title_patterns = [
            r'^#\s*(.+)$',  # Markdownä¸€çº§æ ‡é¢˜
            r'^##\s*(.+)$',  # MarkdownäºŒçº§æ ‡é¢˜
            r'^[ã€Š<](.+)[ã€‹>]$',  # ä¹¦åå·
            r'^ã€(.+)ã€‘$',  # æ–¹æ‹¬å·
            r'^ã€Œ(.+)ã€$',  # æ—¥å¼å¼•å·
            r'^"(.+)"$',  # åŒå¼•å·
            r'^[\u2018\u2019](.+)[\u2018\u2019]$',  # ä¸­æ–‡å¼•å·
            r'^é¢˜ç›®[:ï¼š]\s*(.+)$',  # é¢˜ç›®: æ ¼å¼
            r'^ä¸»é¢˜[:ï¼š]\s*(.+)$',  # ä¸»é¢˜: æ ¼å¼
            r'^æ ‡é¢˜[:ï¼š]\s*(.+)$',  # æ ‡é¢˜: æ ¼å¼
            r'^æŠ¥å‘Š[:ï¼š]\s*(.+)$',  # æŠ¥å‘Š: æ ¼å¼
        ]
        
        for pattern in title_patterns:
            match = re.match(pattern, line)
            if match:
                title = match.group(1).strip()
                if len(title) > 3 and not _is_structural_text(title):
                    return title
        
        # å¦‚æœæ˜¯ç¬¬ä¸€è¡Œä¸”ä¸æ˜¯æ˜æ˜¾çš„ç»“æ„æ€§æ–‡æœ¬ï¼Œå¯èƒ½æ˜¯é¢˜ç›®
        if i == 0 and len(line) > 3 and not _is_structural_text(line):
            # ç§»é™¤å¯èƒ½çš„åºå·æˆ–ç¬¦å·
            cleaned_line = re.sub(r'^[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+[ã€ï¼\.\s]*', '', line)
            if cleaned_line and len(cleaned_line) > 3:
                return cleaned_line
    
    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°é¢˜ç›®ï¼Œå°è¯•ä»æ–‡ä»¶åæå–
    if outline_file_path:
        file_name = os.path.basename(outline_file_path)
        # ç§»é™¤æ–‡ä»¶æ‰©å±•å
        name_without_ext = os.path.splitext(file_name)[0]
        
        # æ¸…ç†å¸¸è§çš„æ–‡ä»¶åæ¨¡å¼
        cleaned_name = re.sub(r'[_\-\s]', ' ', name_without_ext)
        cleaned_name = re.sub(r'\s+', ' ', cleaned_name).strip()
        
        if len(cleaned_name) > 3:
            return cleaned_name
    
    return None

def _is_structural_text(text):
    """åˆ¤æ–­æ–‡æœ¬æ˜¯å¦æ˜¯ç»“æ„æ€§æ–‡æœ¬ï¼ˆå¦‚ç›®å½•ã€å¤§çº²ç­‰ï¼‰"""
    structural_keywords = [
        'ç›®å½•', 'å¤§çº²', 'æçº²', 'ç»“æ„', 'æ¡†æ¶', 'ç´¢å¼•', 'å†…å®¹',
        'ç¬¬ä¸€', 'ç¬¬äºŒ', 'ç¬¬ä¸‰', 'ç¬¬å››', 'ç¬¬äº”',
        'ä¸€ã€', 'äºŒã€', 'ä¸‰ã€', 'å››ã€', 'äº”ã€',
        'ï¼ˆä¸€ï¼‰', 'ï¼ˆäºŒï¼‰', 'ï¼ˆä¸‰ï¼‰', 'ï¼ˆå››ï¼‰', 'ï¼ˆäº”ï¼‰',
        '1.', '2.', '3.', '4.', '5.',
        'outline', 'contents', 'index', 'structure'
    ]
    
    text_lower = text.lower()
    return any(keyword in text_lower for keyword in structural_keywords)

if __name__ == "__main__":
    # åŠ è½½ç¯å¢ƒå˜é‡
    load_dotenv()
    
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(
        description='ğŸš€ åŸºäºå¤§çº²ç”Ÿæˆå®šåˆ¶åŒ–æŠ¥å‘Š',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  python generate_outline_report.py --topic "ç”Ÿæˆå¼å¤§æ¨¡å‹" --outline-file outline.txt --audience "é«˜æ ¡å­¦ç”Ÿ"
  python generate_outline_report.py --topic "AIåº”ç”¨" --outline "ä¸€ã€æ¦‚è¿°..." --audience "ä¼ä¸šä»ä¸šè€…"
  python generate_outline_report.py --outline-file outline.txt  # è‡ªåŠ¨è§£æé¢˜ç›®å’Œä½¿ç”¨é»˜è®¤å—ä¼—
  python generate_outline_report.py --outline-file outline.txt --test-complexity  # æµ‹è¯•ç« èŠ‚å¤æ‚åº¦
  
ç›®æ ‡å—ä¼—é€‰é¡¹:
  - é«˜æ ¡å­¦ç”Ÿ: æ•™å­¦å¼ã€å¾ªåºæ¸è¿›çš„è¡¨è¾¾æ–¹å¼
  - ä¼ä¸šä»ä¸šè€…: å®ç”¨æ€§å¼ºã€è§£å†³æ–¹æ¡ˆå¯¼å‘
  - AIçˆ±å¥½è€…: å‰æ²¿æ¢ç´¢ã€æŠ€æœ¯æ·±å…¥
  - é€šç”¨: å¹³è¡¡æ€§ã€æ˜“äºç†è§£
        """
    )
    
    parser.add_argument('--topic', type=str, help='æŠ¥å‘Šçš„ä¸»é¢˜ï¼ˆå¦‚æœä¸æŒ‡å®šï¼Œå°†ä»å¤§çº²å†…å®¹æˆ–æ–‡ä»¶åè‡ªåŠ¨è§£æï¼‰')
    parser.add_argument('--outline', type=str, help='å¤§çº²æ–‡æœ¬ï¼ˆç›´æ¥è¾“å…¥ï¼‰')
    parser.add_argument('--outline-file', type=str, help='å¤§çº²æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--audience', type=str, choices=['é«˜æ ¡å­¦ç”Ÿ', 'ä¼ä¸šä»ä¸šè€…', 'AIçˆ±å¥½è€…', 'é€šç”¨'], 
                       default='é€šç”¨', help='ç›®æ ‡å—ä¼— (é»˜è®¤: é€šç”¨)')
    parser.add_argument('--output', type=str, help='è¾“å‡ºæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--parallel', type=str, choices=['conservative', 'balanced', 'aggressive'], 
                       default='balanced', help='å¹¶è¡Œé…ç½® (é»˜è®¤: balanced)')
    parser.add_argument('--test-complexity', action='store_true', help='æµ‹è¯•ç« èŠ‚å¤æ‚åº¦è¯„ä¼°ï¼ˆä½¿ç”¨ç°æœ‰è¯„ä¼°é€»è¾‘ï¼‰')
    
    args = parser.parse_args()
    
    # å¦‚æœæ˜¯æµ‹è¯•å¤æ‚åº¦æ¨¡å¼
    if args.test_complexity:
        # è·å–å¤§çº²å†…å®¹
        outline_text = None
        outline_file_path = None
        if args.outline:
            outline_text = args.outline
        elif args.outline_file:
            outline_file_path = args.outline_file
            outline_text = load_outline_from_file(args.outline_file)
        else:
            print("âŒ è¯·æä¾›å¤§çº²å†…å®¹ï¼ˆ--outlineï¼‰æˆ–å¤§çº²æ–‡ä»¶è·¯å¾„ï¼ˆ--outline-fileï¼‰")
            sys.exit(1)
        
        if not outline_text:
            print("âŒ æ— æ³•è·å–å¤§çº²å†…å®¹")
            sys.exit(1)
        
        # å¦‚æœç”¨æˆ·æ²¡æœ‰æŒ‡å®štopicï¼Œå°è¯•ä»å¤§çº²å†…å®¹æˆ–æ–‡ä»¶åè‡ªåŠ¨è§£æ
        topic = args.topic
        if not topic:
            print("ğŸ” æœªæŒ‡å®šä¸»é¢˜ï¼Œæ­£åœ¨ä»å¤§çº²å†…å®¹è‡ªåŠ¨è§£æ...")
            topic = extract_topic_from_outline(outline_text, outline_file_path)
            if topic:
                print(f"âœ… è‡ªåŠ¨è§£æåˆ°é¢˜ç›®: {topic}")
            else:
                print("âŒ æ— æ³•è‡ªåŠ¨è§£æé¢˜ç›®ï¼Œè¯·æ‰‹åŠ¨æŒ‡å®š --topic å‚æ•°")
                sys.exit(1)
        
        # è§£æå¤§çº²
        print("ğŸ” è§£æå¤§çº²ç»“æ„...")
        parser_obj = OutlineParser()
        outline_structure = parser_obj.parse_outline(outline_text, topic if not args.topic else None)
        
        if not outline_structure:
            print("âŒ å¤§çº²è§£æå¤±è´¥")
            sys.exit(1)
        
        # ä½¿ç”¨ç°æœ‰çš„å¤æ‚åº¦è¯„ä¼°æ–¹æ³•
        print("\nğŸ“Š ä½¿ç”¨ç°æœ‰è¯„ä¼°é€»è¾‘æµ‹è¯•ç« èŠ‚å¤æ‚åº¦:")
        print("=" * 60)
        
        # åˆ›å»ºç”Ÿæˆå™¨å®ä¾‹æ¥ä½¿ç”¨è¯„ä¼°æ–¹æ³•
        generator = OutlineContentGenerator(None)
        
        complexity_stats = {"high": 0, "medium": 0, "low": 0}
        
        for section_title, section_info in outline_structure.items():
            print(f"\nğŸ“ ç« èŠ‚: {section_title}")
            print("-" * 40)
            
            # ä½¿ç”¨ç°æœ‰çš„è¯„ä¼°æ–¹æ³•
            complexity = generator._assess_section_complexity(section_title, section_info, topic)
            word_req = generator._get_word_count_requirements(complexity)
            
            complexity_stats[complexity] += 1
            
            # æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
            subsection_count = len(section_info.get('subsections', {}))
            total_items = sum(len(sub_info.get('items', [])) for sub_info in section_info.get('subsections', {}).values())
            
            print(f"   å­ç« èŠ‚æ•°é‡: {subsection_count}")
            print(f"   å†…å®¹è¦ç‚¹æ•°é‡: {total_items}")
            print(f"   æ ‡é¢˜é•¿åº¦: {len(section_title)} å­—ç¬¦")
            
            # è®¡ç®—æŠ€æœ¯å…³é”®è¯
            tech_keywords = [
                'ç®—æ³•', 'æ¶æ„', 'æ¨¡å‹', 'æŠ€æœ¯', 'æ–¹æ³•', 'ç³»ç»Ÿ', 'æ¡†æ¶', 'æœºåˆ¶', 'åŸç†', 'ç­–ç•¥',
                'ä¼˜åŒ–', 'å®ç°', 'è®¾è®¡', 'å¼€å‘', 'éƒ¨ç½²', 'è¯„ä¼°', 'åˆ†æ', 'å¤„ç†', 'è®¡ç®—', 'æ•°æ®'
            ]
            
            text_content = f"{section_title} {' '.join(section_info.get('content', []))}"
            for sub_info in section_info.get('subsections', {}).values():
                text_content += f" {' '.join(sub_info.get('items', []))}"
            
            tech_count = sum(1 for keyword in tech_keywords if keyword in text_content)
            print(f"   æŠ€æœ¯å…³é”®è¯æ•°é‡: {tech_count}")
            
            print(f"   â†’ å¤æ‚åº¦ç­‰çº§: {complexity.upper()}")
            print(f"   â†’ ç›®æ ‡å­—æ•°: {word_req['min_words']}-{word_req['max_words']} å­—")
            print(f"   â†’ æè¿°: {word_req['description']}")
            
            # æ˜¾ç¤ºå­ç« èŠ‚è¯¦æƒ…
            if section_info.get('subsections'):
                print("   å­ç« èŠ‚è¯¦æƒ…:")
                for sub_title, sub_info in section_info['subsections'].items():
                    items_count = len(sub_info.get('items', []))
                    print(f"     - {sub_title}: {items_count} ä¸ªè¦ç‚¹")
        
        print("\n" + "=" * 60)
        print("ğŸ“ˆ å¤æ‚åº¦ç»Ÿè®¡æ€»ç»“:")
        print(f"   é«˜å¤æ‚åº¦ç« èŠ‚: {complexity_stats['high']} ä¸ª")
        print(f"   ä¸­ç­‰å¤æ‚åº¦ç« èŠ‚: {complexity_stats['medium']} ä¸ª")
        print(f"   ä½å¤æ‚åº¦ç« èŠ‚: {complexity_stats['low']} ä¸ª")
        
        total_sections = sum(complexity_stats.values())
        if total_sections > 0:
            print(f"   å¹³å‡å¤æ‚åº¦åˆ†å¸ƒ: é«˜ {complexity_stats['high']/total_sections*100:.1f}%, ä¸­ {complexity_stats['medium']/total_sections*100:.1f}%, ä½ {complexity_stats['low']/total_sections*100:.1f}%")
        
        sys.exit(0)
    
    # è·å–å¤§çº²å†…å®¹
    outline_text = None
    outline_file_path = None
    if args.outline:
        outline_text = args.outline
    elif args.outline_file:
        outline_file_path = args.outline_file
        outline_text = load_outline_from_file(args.outline_file)
    else:
        print("âŒ è¯·æä¾›å¤§çº²å†…å®¹ï¼ˆ--outlineï¼‰æˆ–å¤§çº²æ–‡ä»¶è·¯å¾„ï¼ˆ--outline-fileï¼‰")
        sys.exit(1)
    
    if not outline_text:
        print("âŒ æ— æ³•è·å–å¤§çº²å†…å®¹")
        sys.exit(1)
    
    # å¦‚æœç”¨æˆ·æ²¡æœ‰æŒ‡å®štopicï¼Œå°è¯•ä»å¤§çº²å†…å®¹æˆ–æ–‡ä»¶åè‡ªåŠ¨è§£æ
    topic = args.topic
    if not topic:
        print("ğŸ” æœªæŒ‡å®šä¸»é¢˜ï¼Œæ­£åœ¨ä»å¤§çº²å†…å®¹è‡ªåŠ¨è§£æ...")
        topic = extract_topic_from_outline(outline_text, outline_file_path)
        if topic:
            print(f"âœ… è‡ªåŠ¨è§£æåˆ°é¢˜ç›®: {topic}")
        else:
            print("âŒ æ— æ³•è‡ªåŠ¨è§£æé¢˜ç›®ï¼Œè¯·æ‰‹åŠ¨æŒ‡å®š --topic å‚æ•°")
            sys.exit(1)
    
    # æ˜¾ç¤ºå¯åŠ¨ä¿¡æ¯
    print("ğŸš€ " + "=" * 50)
    print("ğŸš€ åŸºäºå¤§çº²çš„å®šåˆ¶åŒ–æŠ¥å‘Šç”Ÿæˆå™¨")
    print("ğŸš€ " + "=" * 50)
    print(f"ğŸ“ ä¸»é¢˜: {topic}" + (" (è‡ªåŠ¨è§£æ)" if not args.topic else ""))
    print(f"ğŸ‘¥ ç›®æ ‡å—ä¼—: {args.audience}")
    print(f"âš™ï¸ å¹¶è¡Œé…ç½®: {args.parallel}")
    if args.output:
        print(f"ğŸ“„ è¾“å‡ºæ–‡ä»¶: {args.output}")
    print("ğŸš€ " + "=" * 50)
    
    # ç”ŸæˆæŠ¥å‘Š
    try:
        output_file, report_data = generate_outline_report(
            topic, 
            outline_text, 
            args.audience, 
            args.output, 
            args.parallel,
            extracted_topic=topic if not args.topic else None  # åªæœ‰å½“topicæ˜¯è‡ªåŠ¨è§£ææ—¶æ‰ä¼ é€’
        )
        
        print(f"\nâœ… æŠ¥å‘Šç”ŸæˆæˆåŠŸ!")
        print(f"ğŸ“„ æ–‡ä»¶è·¯å¾„: {output_file}")
        
    except KeyboardInterrupt:
        print("\nâŒ ç”¨æˆ·ä¸­æ–­äº†æŠ¥å‘Šç”Ÿæˆè¿‡ç¨‹")
        sys.exit(1)
    except Exception as e:
        print(f"\nâŒ æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {str(e)}")
        sys.exit(1) 