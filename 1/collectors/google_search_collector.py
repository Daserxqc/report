import requests
import json
import time
from datetime import datetime, timedelta
from typing import List, Dict, Optional
import config
from collectors.llm_processor import LLMProcessor
import urllib3
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

# ç¦ç”¨SSLè­¦å‘Š
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

class GoogleSearchCollector:
    """
    Google Custom Search APIæ”¶é›†å™¨
    ç”¨äºä»Googleæœç´¢è·å–èµ„æ–™å’Œæ•°æ®
    """
    
    def __init__(self, api_key=None, cx=None):
        """
        åˆå§‹åŒ–Googleæœç´¢æ”¶é›†å™¨
        
        Args:
            api_key (str): Google Custom Search APIå¯†é’¥
            cx (str): Google Custom Search Engine ID
        """
        self.api_key = api_key if api_key else getattr(config, 'GOOGLE_SEARCH_API_KEY', None)
        self.cx = cx if cx else getattr(config, 'GOOGLE_SEARCH_CX', None)
        self.base_url = "https://www.googleapis.com/customsearch/v1"
        self.has_api_key = bool(self.api_key and self.cx)
        
        # åˆå§‹åŒ–ä¼šè¯ï¼Œé…ç½®é‡è¯•å’ŒSSL
        self.session = requests.Session()
        
        # é…ç½®é‡è¯•ç­–ç•¥
        retry_strategy = Retry(
            total=3,
            status_forcelist=[429, 500, 502, 503, 504],
            backoff_factor=1,
            respect_retry_after_header=True
        )
        
        # é…ç½®HTTPé€‚é…å™¨
        adapter = HTTPAdapter(max_retries=retry_strategy)
        self.session.mount("https://", adapter)
        self.session.mount("http://", adapter)
        
        # åˆå§‹åŒ–LLMå¤„ç†å™¨ç”¨äºå†…å®¹å¤„ç†
        try:
            self.llm_processor = LLMProcessor()
            self.has_llm = True
        except Exception as e:
            print(f"åˆå§‹åŒ–LLMå¤„ç†å™¨å¤±è´¥: {str(e)}")
            self.has_llm = False
        
        if not self.has_api_key:
            print("Google Search APIå¯†é’¥æˆ–CXæœªé…ç½®ï¼Œå°†æ— æ³•æ‰§è¡Œæœç´¢")
        else:
            pass  # print("âœ… Googleæœç´¢æ”¶é›†å™¨å·²åˆå§‹åŒ–")  # MCPéœ€è¦é™é»˜
    
    def search(self, query: str, days_back: int = 7, max_results: int = 10, 
               site_search: str = None, file_type: str = None) -> List[Dict]:
        """
        æ‰§è¡ŒGoogleæœç´¢
        
        Args:
            query (str): æœç´¢æŸ¥è¯¢
            days_back (int): æœç´¢å¤šå°‘å¤©å†…çš„å†…å®¹
            max_results (int): æœ€å¤§ç»“æœæ•°é‡
            site_search (str): é™åˆ¶åœ¨ç‰¹å®šç½‘ç«™æœç´¢
            file_type (str): é™åˆ¶æ–‡ä»¶ç±»å‹ (pdf, doc, pptç­‰)
            
        Returns:
            List[Dict]: æœç´¢ç»“æœåˆ—è¡¨
        """
        if not self.has_api_key:
            print("Google Search APIæœªé…ç½®ï¼Œæ— æ³•æ‰§è¡Œæœç´¢")
            return []
        
        results = []
        num_pages = (max_results // 10) + (1 if max_results % 10 > 0 else 0)
        
        for page in range(num_pages):
            start_index = page * 10 + 1
            page_results = min(10, max_results - len(results))
            
            if page_results <= 0:
                break
                
            try:
                # æ„å»ºæœç´¢å‚æ•°
                params = {
                    'key': self.api_key,
                    'cx': self.cx,
                    'q': query,
                    'start': start_index,
                    'num': page_results,
                    'dateRestrict': f'd{days_back}' if days_back > 0 else None,
                    'safe': 'active',  # å®‰å…¨æœç´¢
                    'lr': 'lang_zh-CN|lang_en',  # æ”¯æŒä¸­è‹±æ–‡
                }
                
                # æ·»åŠ å¯é€‰å‚æ•°
                if site_search:
                    params['siteSearch'] = site_search
                if file_type:
                    params['fileType'] = file_type
                
                # ç§»é™¤Noneå€¼
                params = {k: v for k, v in params.items() if v is not None}
                
                print(f"Googleæœç´¢ç¬¬{page + 1}é¡µ: {query}")
                response = self.session.get(self.base_url, params=params, timeout=30)
                response.raise_for_status()
                
                data = response.json()
                
                if 'items' not in data:
                    print(f"ç¬¬{page + 1}é¡µæ²¡æœ‰æœç´¢ç»“æœ")
                    break
                
                # å¤„ç†æœç´¢ç»“æœ
                for item in data['items']:
                    result = {
                        'title': item.get('title', ''),
                        'content': item.get('snippet', ''),
                        'url': item.get('link', ''),
                        'source': self._extract_domain(item.get('link', '')),
                        'published_date': self._extract_date(item),
                        'search_engine': 'Google',
                        'query': query
                    }
                    
                    # æ·»åŠ é¢å¤–ä¿¡æ¯
                    if 'pagemap' in item:
                        pagemap = item['pagemap']
                        if 'metatags' in pagemap:
                            metatags = pagemap['metatags'][0] if pagemap['metatags'] else {}
                            result['description'] = metatags.get('og:description', result['content'])
                            result['image'] = metatags.get('og:image', '')
                    
                    results.append(result)
                
                # APIé€Ÿç‡é™åˆ¶ï¼šé¿å…è¿‡å¿«è¯·æ±‚
                time.sleep(0.1)
                
            except requests.exceptions.RequestException as e:
                print(f"Googleæœç´¢APIè¯·æ±‚é”™è¯¯: {str(e)}")
                # å¦‚æœæ˜¯SSLé”™è¯¯ï¼Œå°è¯•ä¸éªŒè¯SSL
                if "SSL" in str(e) or "ssl" in str(e).lower():
                    try:
                        print("å°è¯•è·³è¿‡SSLéªŒè¯é‡æ–°è¯·æ±‚...")
                        response = self.session.get(self.base_url, params=params, timeout=30, verify=False)
                        response.raise_for_status()
                        data = response.json()
                        
                        if 'items' not in data:
                            print(f"ç¬¬{page + 1}é¡µæ²¡æœ‰æœç´¢ç»“æœ")
                            break
                        
                        # å¤„ç†æœç´¢ç»“æœ
                        for item in data['items']:
                            result = {
                                'title': item.get('title', ''),
                                'content': item.get('snippet', ''),
                                'url': item.get('link', ''),
                                'source': self._extract_domain(item.get('link', '')),
                                'published_date': self._extract_date(item),
                                'search_engine': 'Google',
                                'query': query
                            }
                            
                            # æ·»åŠ é¢å¤–ä¿¡æ¯
                            if 'pagemap' in item:
                                pagemap = item['pagemap']
                                if 'metatags' in pagemap:
                                    metatags = pagemap['metatags'][0] if pagemap['metatags'] else {}
                                    result['description'] = metatags.get('og:description', result['content'])
                                    result['image'] = metatags.get('og:image', '')
                            
                            results.append(result)
                        
                        print(f"âœ… Googleæœç´¢SSLé‡è¯•æˆåŠŸ: ç¬¬{page + 1}é¡µ")
                        time.sleep(0.1)
                        continue
                        
                    except Exception as ssl_retry_error:
                        print(f"SSLé‡è¯•ä¹Ÿå¤±è´¥: {str(ssl_retry_error)}")
                        break
                else:
                    break
            except Exception as e:
                print(f"å¤„ç†Googleæœç´¢ç»“æœæ—¶å‡ºé”™: {str(e)}")
                break
        
        print(f"Googleæœç´¢å®Œæˆ: {query}, å…±è·å¾—{len(results)}æ¡ç»“æœ")
        return results
    
    def search_news(self, topic: str, days_back: int = 7, max_results: int = 10) -> List[Dict]:
        """
        æœç´¢æ–°é—»å†…å®¹
        
        Args:
            topic (str): æœç´¢ä¸»é¢˜
            days_back (int): æœç´¢å¤šå°‘å¤©å†…çš„æ–°é—»
            max_results (int): æœ€å¤§ç»“æœæ•°é‡
            
        Returns:
            List[Dict]: æ–°é—»æœç´¢ç»“æœ
        """
        # æ„å»ºæ–°é—»ç›¸å…³çš„æœç´¢æŸ¥è¯¢
        news_queries = [
            f"{topic} æ–°é—» æœ€æ–°",
            f"{topic} news latest",
            f"{topic} åŠ¨æ€ å‘å±•",
            f"{topic} breakthrough announcement"
        ]
        
        all_results = []
        results_per_query = max_results // len(news_queries) + 1
        
        for query in news_queries:
            try:
                # ä¼˜å…ˆæœç´¢æ–°é—»ç½‘ç«™
                results = self.search(
                    query=query,
                    days_back=days_back,
                    max_results=results_per_query,
                    site_search=None  # å¯ä»¥æ·»åŠ ç‰¹å®šæ–°é—»ç½‘ç«™
                )
                
                # ä¸ºç»“æœæ·»åŠ æ–°é—»æ ‡ç­¾
                for result in results:
                    result['content_type'] = 'news'
                    result['topic'] = topic
                
                # å»é‡æ·»åŠ 
                existing_urls = {r['url'] for r in all_results}
                for result in results:
                    if result['url'] not in existing_urls:
                        all_results.append(result)
                        existing_urls.add(result['url'])
                
            except Exception as e:
                print(f"æœç´¢æ–°é—»æ—¶å‡ºé”™ ({query}): {str(e)}")
                continue
        
        # é™åˆ¶æœ€ç»ˆç»“æœæ•°é‡
        return all_results[:max_results]
    
    def search_research_papers(self, topic: str, days_back: int = 30, max_results: int = 10) -> List[Dict]:
        """
        æœç´¢å­¦æœ¯ç ”ç©¶è®ºæ–‡
        
        Args:
            topic (str): ç ”ç©¶ä¸»é¢˜
            days_back (int): æœç´¢å¤šå°‘å¤©å†…çš„è®ºæ–‡
            max_results (int): æœ€å¤§ç»“æœæ•°é‡
            
        Returns:
            List[Dict]: ç ”ç©¶è®ºæ–‡æœç´¢ç»“æœ
        """
        # æ„å»ºå­¦æœ¯æœç´¢æŸ¥è¯¢
        academic_queries = [
            f"{topic} research paper filetype:pdf",
            f"{topic} study analysis",
            f"{topic} academic paper",
            f"{topic} ç ”ç©¶ è®ºæ–‡"
        ]
        
        all_results = []
        results_per_query = max_results // len(academic_queries) + 1
        
        for query in academic_queries:
            try:
                # æœç´¢å­¦æœ¯ç½‘ç«™å’ŒPDFæ–‡ä»¶
                results = self.search(
                    query=query,
                    days_back=days_back,
                    max_results=results_per_query,
                    file_type='pdf' if 'filetype:pdf' in query else None
                )
                
                # ä¸ºç»“æœæ·»åŠ å­¦æœ¯æ ‡ç­¾
                for result in results:
                    result['content_type'] = 'academic'
                    result['topic'] = topic
                
                # å»é‡æ·»åŠ 
                existing_urls = {r['url'] for r in all_results}
                for result in results:
                    if result['url'] not in existing_urls:
                        all_results.append(result)
                        existing_urls.add(result['url'])
                
            except Exception as e:
                print(f"æœç´¢å­¦æœ¯è®ºæ–‡æ—¶å‡ºé”™ ({query}): {str(e)}")
                continue
        
        return all_results[:max_results]
    
    def search_industry_reports(self, topic: str, days_back: int = 90, max_results: int = 10) -> List[Dict]:
        """
        æœç´¢è¡Œä¸šæŠ¥å‘Š
        
        Args:
            topic (str): è¡Œä¸šä¸»é¢˜
            days_back (int): æœç´¢å¤šå°‘å¤©å†…çš„æŠ¥å‘Š
            max_results (int): æœ€å¤§ç»“æœæ•°é‡
            
        Returns:
            List[Dict]: è¡Œä¸šæŠ¥å‘Šæœç´¢ç»“æœ
        """
        # æ„å»ºè¡Œä¸šæŠ¥å‘Šæœç´¢æŸ¥è¯¢
        report_queries = [
            f"{topic} industry report 2024",
            f"{topic} market analysis report",
            f"{topic} è¡Œä¸šæŠ¥å‘Š å¸‚åœºåˆ†æ",
            f"{topic} white paper filetype:pdf",
            f"{topic} industry outlook trends"
        ]
        
        all_results = []
        results_per_query = max_results // len(report_queries) + 1
        
        for query in report_queries:
            try:
                results = self.search(
                    query=query,
                    days_back=days_back,
                    max_results=results_per_query
                )
                
                # ä¸ºç»“æœæ·»åŠ æŠ¥å‘Šæ ‡ç­¾
                for result in results:
                    result['content_type'] = 'industry_report'
                    result['topic'] = topic
                
                # å»é‡æ·»åŠ 
                existing_urls = {r['url'] for r in all_results}
                for result in results:
                    if result['url'] not in existing_urls:
                        all_results.append(result)
                        existing_urls.add(result['url'])
                
            except Exception as e:
                print(f"æœç´¢è¡Œä¸šæŠ¥å‘Šæ—¶å‡ºé”™ ({query}): {str(e)}")
                continue
        
        return all_results[:max_results]
    
    def get_comprehensive_research(self, topic: str, days_back: int = 7) -> Dict:
        """
        è·å–ä¸»é¢˜çš„ç»¼åˆç ”ç©¶èµ„æ–™
        
        Args:
            topic (str): ç ”ç©¶ä¸»é¢˜
            days_back (int): æœç´¢å¤©æ•°èŒƒå›´
            
        Returns:
            Dict: åŒ…å«å„ç±»å‹å†…å®¹çš„å­—å…¸
        """
        print(f"å¼€å§‹Googleæœç´¢ç»¼åˆç ”ç©¶: {topic}")
        
        if not self.has_api_key:
            return {
                'news': [],
                'academic': [],
                'industry_reports': [],
                'general': [],
                'total_count': 0
            }
        
        # å¹¶è¡Œæœç´¢ä¸åŒç±»å‹çš„å†…å®¹
        results = {
            'news': self.search_news(topic, days_back=days_back, max_results=8),
            'academic': self.search_research_papers(topic, days_back=days_back*4, max_results=6),
            'industry_reports': self.search_industry_reports(topic, days_back=days_back*12, max_results=5),
            'general': self.search(topic, days_back=days_back, max_results=10)
        }
        
        # æ·»åŠ æ€»æ•°ç»Ÿè®¡
        total_count = sum(len(v) for v in results.values())
        results['total_count'] = total_count
        
        print(f"Googleæœç´¢å®Œæˆ: {topic}, æ€»å…±è·å¾—{total_count}æ¡ç»“æœ")
        print(f"  - æ–°é—»: {len(results['news'])}æ¡")
        print(f"  - å­¦æœ¯: {len(results['academic'])}æ¡") 
        print(f"  - è¡Œä¸šæŠ¥å‘Š: {len(results['industry_reports'])}æ¡")
        print(f"  - é€šç”¨: {len(results['general'])}æ¡")
        
        return results
    
    def _extract_domain(self, url: str) -> str:
        """ä»URLæå–åŸŸå"""
        try:
            from urllib.parse import urlparse
            parsed = urlparse(url)
            return parsed.netloc
        except:
            return "æœªçŸ¥æ¥æº"
    
    def _extract_date(self, item: Dict) -> str:
        """ä»æœç´¢ç»“æœé¡¹ä¸­æå–æ—¥æœŸ"""
        # Googleæœç´¢ç»“æœé€šå¸¸ä¸åŒ…å«æ˜ç¡®çš„å‘å¸ƒæ—¥æœŸ
        # å¯ä»¥å°è¯•ä»pagemapæˆ–å…¶ä»–å…ƒæ•°æ®ä¸­æå–
        try:
            if 'pagemap' in item and 'metatags' in item['pagemap']:
                metatags = item['pagemap']['metatags'][0] if item['pagemap']['metatags'] else {}
                
                # å°è¯•å¤šä¸ªå¯èƒ½çš„æ—¥æœŸå­—æ®µ
                date_fields = ['article:published_time', 'datePublished', 'publishedDate', 'date']
                for field in date_fields:
                    if field in metatags and metatags[field]:
                        return metatags[field][:10]  # åªè¿”å›æ—¥æœŸéƒ¨åˆ†
            
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°å…·ä½“æ—¥æœŸï¼Œè¿”å›ä»Šå¤©çš„æ—¥æœŸ
            return datetime.now().strftime('%Y-%m-%d')
        except:
            return datetime.now().strftime('%Y-%m-%d')
    
    def _filter_by_date(self, items, days_limit):
        """
        æ ¹æ®æ—¶é—´é™åˆ¶è¿‡æ»¤å†…å®¹é¡¹ - åªä¾èµ–å¯é çš„æ—¶é—´ä¿¡æ¯
        
        Args:
            items (list): å†…å®¹é¡¹åˆ—è¡¨
            days_limit (int): å¤©æ•°é™åˆ¶
            
        Returns:
            list: è¿‡æ»¤åçš„å†…å®¹é¡¹åˆ—è¡¨
        """
        if not items or days_limit <= 0:
            return items
            
        filtered_items = []
        cutoff_date = datetime.now() - timedelta(days=days_limit)
        current_year = datetime.now().year
        
        print(f"  ğŸ” [Google] å¼€å§‹ä¸¥æ ¼æ—¶é—´è¿‡æ»¤ï¼Œè¦æ±‚æœ€è¿‘{days_limit}å¤©å†…çš„å†…å®¹ï¼ˆæˆªæ­¢æ—¥æœŸï¼š{cutoff_date.strftime('%Y-%m-%d')}ï¼‰")
        print(f"  âš ï¸ [Google] æ³¨æ„ï¼šåªæ¥å—æœ‰æ˜ç¡®å‘å¸ƒæ—¥æœŸçš„å†…å®¹ï¼Œå¿½ç•¥'æœ€æ–°'ã€'ä»Šæ—¥'ç­‰å…³é”®è¯")
        
        for item in items:
            should_include = False
            filter_reason = ""
            
            title = item.get('title', '')
            content = item.get('content', '')
            combined_text = f"{title} {content}".lower()
            
            # 1. é¦–å…ˆæ£€æŸ¥æ˜¯å¦åŒ…å«æ˜æ˜¾çš„æ—§å¹´ä»½æ ‡è¯†
            old_year_patterns = ['2024å¹´', '2023å¹´', '2022å¹´', '2021å¹´', '2020å¹´']
            has_old_year = any(pattern in combined_text for pattern in old_year_patterns)
            
            if has_old_year:
                filter_reason = "åŒ…å«æ—§å¹´ä»½æ ‡è¯†"
                should_include = False
            else:
                # 2. æ£€æŸ¥æ˜¯å¦æœ‰å¯é çš„å‘å¸ƒæ—¥æœŸä¿¡æ¯
                published_date = item.get("published_date")
                
                if published_date and published_date != "æœªçŸ¥æ—¥æœŸ":
                    try:
                        # å°è¯•è§£æå‘å¸ƒæ—¥æœŸ
                        pub_date = None
                        
                        if isinstance(published_date, str):
                            # å°è¯•å¤šç§æ—¥æœŸæ ¼å¼
                            date_formats = [
                                "%Y-%m-%d",
                                "%Y-%m-%d %H:%M:%S",
                                "%Y-%m-%dT%H:%M:%S",
                                "%Y-%m-%dT%H:%M:%S.%fZ",
                                "%Y-%m-%dT%H:%M:%SZ",
                                "%Y/%m/%d",
                                "%d/%m/%Y",
                                "%m/%d/%Y"
                            ]
                            
                            for fmt in date_formats:
                                try:
                                    pub_date = datetime.strptime(published_date, fmt)
                                    break
                                except ValueError:
                                    continue
                        
                        if pub_date:
                            # æœ‰æ˜ç¡®çš„å‘å¸ƒæ—¥æœŸï¼Œæ£€æŸ¥æ˜¯å¦åœ¨æ—¶é—´èŒƒå›´å†…
                            if pub_date >= cutoff_date:
                                should_include = True
                                filter_reason = f"å‘å¸ƒæ—¥æœŸç¬¦åˆè¦æ±‚ï¼ˆ{pub_date.strftime('%Y-%m-%d')}ï¼‰"
                            else:
                                should_include = False
                                filter_reason = f"å‘å¸ƒæ—¥æœŸè¿‡æ—©ï¼ˆ{pub_date.strftime('%Y-%m-%d')}ï¼‰"
                        else:
                            # æ— æ³•è§£æå‘å¸ƒæ—¥æœŸ
                            should_include = False
                            filter_reason = "æ— æ³•è§£æå‘å¸ƒæ—¥æœŸæ ¼å¼"
                            
                    except Exception as e:
                        should_include = False
                        filter_reason = f"å‘å¸ƒæ—¥æœŸè§£æå¤±è´¥: {str(e)}"
                        
                else:
                    # 3. æ²¡æœ‰å‘å¸ƒæ—¥æœŸçš„æƒ…å†µ - æ™ºèƒ½å¤„ç†æ¨¡ç³Šæ—¥æœŸ
                    
                    # æ£€æŸ¥æ˜¯å¦åŒ…å«æ¨¡ç³Šçš„æ—¥æœŸæ ¼å¼ï¼ˆå¦‚"3æœˆ27æ—¥"ã€"12æœˆ15æ—¥"ç­‰ï¼‰
                    import re
                    ambiguous_date_patterns = [
                        r'(\d{1,2})æœˆ(\d{1,2})æ—¥',  # 3æœˆ27æ—¥
                        r'(\d{1,2})/(\d{1,2})(?!\d)',  # 3/27 (ä½†ä¸æ˜¯ 3/27/2025)
                        r'(\d{1,2})-(\d{1,2})(?!\d)',  # 3-27 (ä½†ä¸æ˜¯ 3-27-2025)
                    ]
                    
                    ambiguous_date_match = None
                    for pattern in ambiguous_date_patterns:
                        match = re.search(pattern, combined_text)
                        if match:
                            ambiguous_date_match = match
                            break
                    
                    if ambiguous_date_match:
                        # æ‰¾åˆ°æ¨¡ç³Šæ—¥æœŸï¼Œå°è¯•æ™ºèƒ½åˆ¤æ–­
                        try:
                            month = int(ambiguous_date_match.group(1))
                            day = int(ambiguous_date_match.group(2))
                            
                            # å‡è®¾æ˜¯å½“å‰å¹´ä»½ï¼Œæ„é€ æ—¥æœŸ
                            current_date = datetime.now()
                            try:
                                assumed_date = datetime(current_date.year, month, day)
                                
                                # æ£€æŸ¥è¿™ä¸ªæ—¥æœŸæ˜¯å¦åœ¨åˆç†èŒƒå›´å†…
                                days_diff = (current_date - assumed_date).days
                                
                                if days_diff < 0:
                                    # æ—¥æœŸåœ¨æœªæ¥ï¼Œå¯èƒ½æ˜¯å»å¹´çš„æ—¥æœŸ
                                    assumed_date = datetime(current_date.year - 1, month, day)
                                    days_diff = (current_date - assumed_date).days
                                
                                if days_diff <= days_limit:
                                    # åœ¨æ—¶é—´èŒƒå›´å†…
                                    should_include = True
                                    filter_reason = f"æ¨¡ç³Šæ—¥æœŸæ¨æµ‹ä¸º{assumed_date.strftime('%Y-%m-%d')}ï¼Œåœ¨æ—¶é—´èŒƒå›´å†…"
                                else:
                                    # è¶…å‡ºæ—¶é—´èŒƒå›´
                                    should_include = False
                                    filter_reason = f"æ¨¡ç³Šæ—¥æœŸæ¨æµ‹ä¸º{assumed_date.strftime('%Y-%m-%d')}ï¼Œè¶…å‡º{days_limit}å¤©èŒƒå›´"
                                    
                            except ValueError:
                                # æ— æ•ˆæ—¥æœŸï¼ˆå¦‚2æœˆ30æ—¥ï¼‰
                                should_include = False
                                filter_reason = f"æ¨¡ç³Šæ—¥æœŸ{month}æœˆ{day}æ—¥æ— æ•ˆ"
                                
                        except (ValueError, IndexError):
                            # è§£æå¤±è´¥
                            should_include = False
                            filter_reason = "æ¨¡ç³Šæ—¥æœŸè§£æå¤±è´¥"
                    else:
                        # æ²¡æœ‰æ¨¡ç³Šæ—¥æœŸï¼Œæ£€æŸ¥æ˜¯å¦åŒ…å«å½“å‰å¹´ä»½çš„æ˜ç¡®æ ‡è¯†
                        current_year_patterns = [
                            f'{current_year}å¹´',
                            f'å¹´{current_year}',
                            f'{current_year}-',
                            f'{current_year}/'
                        ]
                        
                        has_current_year = any(pattern in combined_text for pattern in current_year_patterns)
                        
                        if has_current_year:
                            # åŒ…å«å½“å‰å¹´ä»½ï¼Œä½†ä»ç„¶è¦æ±‚æ˜¯çŸ­æœŸå†…çš„å†…å®¹
                            if days_limit <= 30:
                                # å¯¹äº30å¤©ä»¥å†…çš„è¦æ±‚ï¼Œå³ä½¿æœ‰å½“å‰å¹´ä»½ä¹Ÿä¸å¤Ÿ
                                should_include = False
                                filter_reason = f"åŒ…å«{current_year}å¹´ä½†æ— å…·ä½“æ—¥æœŸï¼ˆä¸¥æ ¼æ¨¡å¼ï¼‰"
                            else:
                                # å¯¹äºè¾ƒé•¿æœŸçš„è¦æ±‚ï¼Œå¯ä»¥æ¥å—
                                should_include = True
                                filter_reason = f"åŒ…å«{current_year}å¹´æ ‡è¯†"
                        else:
                            # æ—¢æ²¡æœ‰å‘å¸ƒæ—¥æœŸï¼Œä¹Ÿæ²¡æœ‰å¹´ä»½ä¿¡æ¯
                            should_include = False
                            filter_reason = "æ— å‘å¸ƒæ—¥æœŸä¸”æ— å¹´ä»½ä¿¡æ¯"
            
            if should_include:
                filtered_items.append(item)
                if len(title) > 30:
                    print(f"    âœ… [Google] ä¿ç•™: {title[:30]}... ({filter_reason})")
                else:
                    print(f"    âœ… [Google] ä¿ç•™: {title} ({filter_reason})")
            else:
                if len(title) > 30:
                    print(f"    âŒ [Google] è¿‡æ»¤: {title[:30]}... ({filter_reason})")
                else:
                    print(f"    âŒ [Google] è¿‡æ»¤: {title} ({filter_reason})")
        
        original_count = len(items)
        filtered_count = len(filtered_items)
        
        if filtered_count < original_count:
            print(f"  â° [Google] ä¸¥æ ¼æ—¶é—´è¿‡æ»¤ç»“æœ: {original_count} â†’ {filtered_count} æ¡ï¼ˆæ’é™¤äº†{original_count - filtered_count}æ¡æ— å¯é æ—¶é—´ä¿¡æ¯çš„å†…å®¹ï¼‰")
        else:
            print(f"  â° [Google] ä¸¥æ ¼æ—¶é—´è¿‡æ»¤ç»“æœ: ä¿ç•™å…¨éƒ¨{filtered_count}æ¡å†…å®¹")
            
        return filtered_items
    
    def get_site_specific_search(self, topic: str, sites: List[str], days_back: int = 7) -> List[Dict]:
        """
        åœ¨ç‰¹å®šç½‘ç«™ä¸­æœç´¢å†…å®¹
        
        Args:
            topic (str): æœç´¢ä¸»é¢˜
            sites (List[str]): ç½‘ç«™åˆ—è¡¨
            days_back (int): æœç´¢å¤©æ•°èŒƒå›´
            
        Returns:
            List[Dict]: æœç´¢ç»“æœ
        """
        all_results = []
        
        for site in sites:
            try:
                results = self.search(
                    query=topic,
                    days_back=days_back,
                    max_results=5,
                    site_search=site
                )
                
                # ä¸ºç»“æœæ·»åŠ ç½‘ç«™ä¿¡æ¯
                for result in results:
                    result['target_site'] = site
                
                all_results.extend(results)
                
            except Exception as e:
                print(f"åœ¨{site}æœç´¢æ—¶å‡ºé”™: {str(e)}")
                continue
        
        return all_results 