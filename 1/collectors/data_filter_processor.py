#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ•°æ®ç­›é€‰å’Œè¯„åˆ†å¤„ç†å™¨
ç”¨äºå¯¹æ”¶é›†çš„èµ„æ–™è¿›è¡Œè´¨é‡è¯„ä¼°å’Œç­›é€‰
"""

import json
import re
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass, asdict
from urllib.parse import urlparse
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading


@dataclass
class DataSource:
    """æ•°æ®æºä¿¡æ¯"""
    content: str
    url: str
    title: str
    source_type: str  # 'academic', 'news', 'market', 'web'
    publish_date: Optional[str] = None
    author: Optional[str] = None
    domain: Optional[str] = None
    word_count: int = 0
    
    def __post_init__(self):
        if self.domain is None and self.url:
            self.domain = urlparse(self.url).netloc
        if self.word_count == 0:
            self.word_count = len(self.content.split())


@dataclass
class QualityScore:
    """è´¨é‡è¯„åˆ†"""
    relevance: float      # ç›¸å…³æ€§ 0-1
    practicality: float   # å®ç”¨æ€§ 0-1
    timeliness: float     # æ—¶æ•ˆæ€§ 0-1
    authority: float      # æƒå¨æ€§ 0-1
    completeness: float   # å®Œæ•´æ€§ 0-1
    accuracy: float       # å‡†ç¡®æ€§ 0-1
    total_score: float = 0.0    # æ€»åˆ† 0-1ï¼Œé»˜è®¤å€¼
    
    def __post_init__(self):
        # è®¡ç®—åŠ æƒæ€»åˆ†
        weights = {
            'relevance': 0.25,
            'practicality': 0.20,
            'timeliness': 0.15,
            'authority': 0.15,
            'completeness': 0.15,
            'accuracy': 0.10
        }
        
        self.total_score = (
            self.relevance * weights['relevance'] +
            self.practicality * weights['practicality'] +
            self.timeliness * weights['timeliness'] +
            self.authority * weights['authority'] +
            self.completeness * weights['completeness'] +
            self.accuracy * weights['accuracy']
        )


@dataclass
class FilteredData:
    """ç­›é€‰åçš„æ•°æ®"""
    source: DataSource
    quality_score: QualityScore
    selected_excerpts: List[str]  # é€‰ä¸­çš„å…³é”®æ‘˜å½•
    reasoning: str  # è¯„åˆ†ç†ç”±
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            'source': asdict(self.source),
            'quality_score': asdict(self.quality_score),
            'selected_excerpts': self.selected_excerpts,
            'reasoning': self.reasoning
        }


class DataFilterProcessor:
    """æ•°æ®ç­›é€‰å¤„ç†å™¨"""
    
    def __init__(self, llm_processor=None):
        self.llm_processor = llm_processor
        self.lock = threading.Lock()
        
        # æƒå¨åŸŸååˆ—è¡¨
        self.authoritative_domains = {
            'academic': ['arxiv.org', 'ieee.org', 'acm.org', 'springer.com', 'nature.com', 'sciencedirect.com'],
            'news': ['reuters.com', 'bloomberg.com', 'wsj.com', 'ft.com', 'economist.com'],
            'tech': ['techcrunch.com', 'wired.com', 'arstechnica.com', 'theverge.com'],
            'chinese': ['xinhua.net', 'people.com.cn', 'chinadaily.com.cn', 'caixin.com']
        }
        
        # å¹¶è¡Œåº¦é…ç½® - æ ¹æ®æ•°æ®æºé™åˆ¶è®¾ç½®
        self.parallel_config = {
            'brave': 2,     # Brave æœ‰é™åˆ¶ï¼Œå°‘ä¸€äº›
            'google': 6,    # Google å¯ä»¥å¤šä¸€äº›
            'tavily': 8,    # Tavily å¯ä»¥å¤šä¸€äº›
            'arxiv': 4,     # å­¦æœ¯æºé€‚ä¸­
            'news': 5,      # æ–°é—»æºé€‚ä¸­
            'web': 3,       # é€šç”¨ç½‘é¡µæºé€‚ä¸­
            'default': 3    # é»˜è®¤å€¼
        }
    
    def filter_and_score_data_parallel(self, data_sources: List[DataSource], topic: str, 
                                     section_title: str, min_score: float = 0.6) -> List[FilteredData]:
        """
        å¹¶è¡Œç­›é€‰å’Œè¯„åˆ†æ•°æ®æº
        
        Args:
            data_sources: åŸå§‹æ•°æ®æºåˆ—è¡¨
            topic: ä¸»é¢˜
            section_title: ç« èŠ‚æ ‡é¢˜
            min_score: æœ€ä½åˆ†æ•°é˜ˆå€¼
        
        Returns:
            ç­›é€‰åçš„æ•°æ®åˆ—è¡¨
        """
        print(f"ğŸš€ å¼€å§‹å¹¶è¡Œç­›é€‰æ•°æ®ï¼Œå…± {len(data_sources)} ä¸ªæ•°æ®æº")
        
        # æŒ‰æ•°æ®æºç±»å‹åˆ†ç»„
        grouped_sources = self._group_sources_by_type(data_sources)
        
        filtered_data = []
        
        # å¹¶è¡Œå¤„ç†æ¯ä¸ªç»„
        for source_type, sources in grouped_sources.items():
            if not sources:
                continue
                
            max_workers = self.parallel_config.get(source_type, self.parallel_config['default'])
            print(f"  ğŸ“Š å¤„ç† {source_type} ç±»æ•°æ®æº {len(sources)} ä¸ªï¼Œå¹¶è¡Œåº¦: {max_workers}")
            
            group_filtered = self._process_group_parallel(
                sources, topic, section_title, min_score, max_workers
            )
            filtered_data.extend(group_filtered)
        
        # æŒ‰åˆ†æ•°æ’åº
        filtered_data.sort(key=lambda x: x.quality_score.total_score, reverse=True)
        
        print(f"ğŸ¯ å¹¶è¡Œç­›é€‰å®Œæˆï¼Œ{len(filtered_data)} ä¸ªæ•°æ®æºé€šè¿‡ç­›é€‰")
        return filtered_data
    
    def _group_sources_by_type(self, data_sources: List[DataSource]) -> Dict[str, List[DataSource]]:
        """æŒ‰æ•°æ®æºç±»å‹åˆ†ç»„"""
        grouped = {}
        
        for source in data_sources:
            # æ ¹æ®åŸŸåæˆ–æ¥æºä¿¡æ¯ç¡®å®šåˆ†ç»„
            group_key = self._determine_group_key(source)
            if group_key not in grouped:
                grouped[group_key] = []
            grouped[group_key].append(source)
        
        return grouped
    
    def _determine_group_key(self, source: DataSource) -> str:
        """ç¡®å®šæ•°æ®æºçš„åˆ†ç»„é”®"""
        if not source.domain:
            return source.source_type or 'default'
        
        domain_lower = source.domain.lower()
        
        # ä¼˜å…ˆæ£€æŸ¥æ˜¯å¦æ¥è‡ªç‰¹å®šæœç´¢å¼•æ“ï¼ˆé€šè¿‡æˆ‘ä»¬æ·»åŠ çš„æ ‡è¯†ï¼‰
        if 'brave.search_engine' in domain_lower:
            return 'brave'
        elif 'google.search_engine' in domain_lower:
            return 'google'
        elif 'tavily.search_engine' in domain_lower:
            return 'tavily'
        
        # æ£€æŸ¥æ˜¯å¦æ¥è‡ªç‰¹å®šæœç´¢å¼•æ“ï¼ˆé€šè¿‡åŸŸååŒ¹é…ï¼‰
        elif 'brave' in domain_lower:
            return 'brave'
        elif 'google' in domain_lower:
            return 'google'
        elif 'tavily' in domain_lower:
            return 'tavily'
        elif 'arxiv' in domain_lower:
            return 'arxiv'
        elif any(news_domain in domain_lower for news_domain in ['reuters', 'bloomberg', 'wsj', 'ft', 'economist']):
            return 'news'
        else:
            return source.source_type or 'web'
    
    def _process_group_parallel(self, sources: List[DataSource], topic: str, 
                              section_title: str, min_score: float, max_workers: int) -> List[FilteredData]:
        """å¹¶è¡Œå¤„ç†å•ä¸ªåˆ†ç»„çš„æ•°æ®æº"""
        filtered_data = []
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_source = {
                executor.submit(self._process_single_source, source, topic, section_title, min_score): source
                for source in sources
            }
            
            # æ”¶é›†ç»“æœ
            for future in as_completed(future_to_source):
                source = future_to_source[future]
                try:
                    result = future.result()
                    if result:  # é€šè¿‡ç­›é€‰çš„æ•°æ®
                        filtered_data.append(result)
                        with self.lock:
                            print(f"    âœ… {source.title[:50]}... é€šè¿‡ç­›é€‰ï¼Œæ€»åˆ†: {result.quality_score.total_score:.2f}")
                    else:
                        with self.lock:
                            print(f"    âŒ {source.title[:50]}... æœªé€šè¿‡ç­›é€‰")
                except Exception as e:
                    with self.lock:
                        print(f"    âš ï¸ {source.title[:50]}... è¯„ä¼°å‡ºé”™: {str(e)}")
        
        return filtered_data
    
    def _process_single_source(self, source: DataSource, topic: str, 
                             section_title: str, min_score: float) -> Optional[FilteredData]:
        """å¤„ç†å•ä¸ªæ•°æ®æºï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰"""
        try:
            # è¯„ä¼°æ•°æ®è´¨é‡
            quality_score = self._evaluate_quality(source, topic, section_title)
            
            # å¦‚æœåˆ†æ•°ä¸è¾¾æ ‡ï¼Œç›´æ¥è¿”å›None
            if quality_score.total_score < min_score:
                return None
            
            # ç­›é€‰å…³é”®æ‘˜å½•
            excerpts = self._extract_key_excerpts(source, topic, section_title)
            
            # ç”Ÿæˆè¯„åˆ†ç†ç”±
            reasoning = self._generate_reasoning(source, quality_score)
            
            return FilteredData(
                source=source,
                quality_score=quality_score,
                selected_excerpts=excerpts,
                reasoning=reasoning
            )
            
        except Exception as e:
            # è®°å½•é”™è¯¯ä½†ä¸æŠ›å‡ºï¼Œè®©è°ƒç”¨è€…å¤„ç†
            return None
    
    def filter_and_score_data(self, data_sources: List[DataSource], topic: str, 
                            section_title: str, min_score: float = 0.6) -> List[FilteredData]:
        """
        ç­›é€‰å’Œè¯„åˆ†æ•°æ®æºï¼ˆä¿æŒåŸæœ‰æ¥å£ï¼Œä¼˜å…ˆä½¿ç”¨å¹¶è¡Œç‰ˆæœ¬ï¼‰
        
        Args:
            data_sources: åŸå§‹æ•°æ®æºåˆ—è¡¨
            topic: ä¸»é¢˜
            section_title: ç« èŠ‚æ ‡é¢˜
            min_score: æœ€ä½åˆ†æ•°é˜ˆå€¼
        
        Returns:
            ç­›é€‰åçš„æ•°æ®åˆ—è¡¨
        """
        # å¦‚æœæ•°æ®æºå°‘äº3ä¸ªï¼Œä½¿ç”¨ä¸²è¡Œå¤„ç†
        if len(data_sources) < 3:
            return self._filter_and_score_data_serial(data_sources, topic, section_title, min_score)
        
        # å¦åˆ™ä½¿ç”¨å¹¶è¡Œå¤„ç†
        return self.filter_and_score_data_parallel(data_sources, topic, section_title, min_score)
    
    def _filter_and_score_data_serial(self, data_sources: List[DataSource], topic: str, 
                                    section_title: str, min_score: float = 0.6) -> List[FilteredData]:
        """
        ä¸²è¡Œç­›é€‰å’Œè¯„åˆ†æ•°æ®æºï¼ˆåŸå§‹å®ç°ï¼‰
        """
        print(f"ğŸ” å¼€å§‹ä¸²è¡Œç­›é€‰æ•°æ®ï¼Œå…± {len(data_sources)} ä¸ªæ•°æ®æº")
        
        filtered_data = []
        
        for i, source in enumerate(data_sources):
            print(f"  æ­£åœ¨è¯„ä¼°æ•°æ®æº {i+1}/{len(data_sources)}: {source.title[:50]}...")
            
            try:
                # è¯„ä¼°æ•°æ®è´¨é‡
                quality_score = self._evaluate_quality(source, topic, section_title)
                
                # ç­›é€‰å…³é”®æ‘˜å½•
                excerpts = self._extract_key_excerpts(source, topic, section_title)
                
                # ç”Ÿæˆè¯„åˆ†ç†ç”±
                reasoning = self._generate_reasoning(source, quality_score)
                
                # å¦‚æœåˆ†æ•°è¾¾åˆ°é˜ˆå€¼ï¼ŒåŠ å…¥ç­›é€‰ç»“æœ
                if quality_score.total_score >= min_score:
                    filtered_data.append(FilteredData(
                        source=source,
                        quality_score=quality_score,
                        selected_excerpts=excerpts,
                        reasoning=reasoning
                    ))
                    print(f"    âœ… é€šè¿‡ç­›é€‰ï¼Œæ€»åˆ†: {quality_score.total_score:.2f}")
                else:
                    print(f"    âŒ æœªé€šè¿‡ç­›é€‰ï¼Œæ€»åˆ†: {quality_score.total_score:.2f}")
                    
            except Exception as e:
                print(f"    âš ï¸ è¯„ä¼°å‡ºé”™: {str(e)}")
                continue
        
        # æŒ‰åˆ†æ•°æ’åº
        filtered_data.sort(key=lambda x: x.quality_score.total_score, reverse=True)
        
        print(f"ğŸ¯ ä¸²è¡Œç­›é€‰å®Œæˆï¼Œ{len(filtered_data)} ä¸ªæ•°æ®æºé€šè¿‡ç­›é€‰")
        return filtered_data
    
    def _evaluate_quality(self, source: DataSource, topic: str, section_title: str) -> QualityScore:
        """è¯„ä¼°æ•°æ®è´¨é‡"""
        
        # 1. æƒå¨æ€§è¯„åˆ†
        authority_score = self._evaluate_authority(source)
        
        # 2. æ—¶æ•ˆæ€§è¯„åˆ†
        timeliness_score = self._evaluate_timeliness(source)
        
        # 3. å®Œæ•´æ€§è¯„åˆ†
        completeness_score = self._evaluate_completeness(source)
        
        # 4. ä½¿ç”¨LLMè¯„ä¼°ç›¸å…³æ€§ã€å®ç”¨æ€§ã€å‡†ç¡®æ€§
        if self.llm_processor:
            llm_scores = self._evaluate_with_llm(source, topic, section_title)
            relevance_score = llm_scores.get('relevance', 0.7)
            practicality_score = llm_scores.get('practicality', 0.7)
            accuracy_score = llm_scores.get('accuracy', 0.7)
        else:
            # å›é€€åˆ°è§„åˆ™è¯„åˆ†
            relevance_score = self._evaluate_relevance_fallback(source, topic, section_title)
            practicality_score = self._evaluate_practicality_fallback(source)
            accuracy_score = self._evaluate_accuracy_fallback(source)
        
        return QualityScore(
            relevance=relevance_score,
            practicality=practicality_score,
            timeliness=timeliness_score,
            authority=authority_score,
            completeness=completeness_score,
            accuracy=accuracy_score
        )
    
    def _evaluate_authority(self, source: DataSource) -> float:
        """è¯„ä¼°æƒå¨æ€§"""
        if not source.domain:
            return 0.3
        
        # æ£€æŸ¥æ˜¯å¦åœ¨æƒå¨åŸŸååˆ—è¡¨ä¸­
        for category, domains in self.authoritative_domains.items():
            if any(domain in source.domain for domain in domains):
                return 0.9
        
        # æ ¹æ®åŸŸåç‰¹å¾è¯„åˆ†
        domain_lower = source.domain.lower()
        
        # å­¦æœ¯æœºæ„
        if any(ext in domain_lower for ext in ['.edu', '.ac.', '.org']):
            return 0.8
        
        # æ”¿åºœæœºæ„
        if any(ext in domain_lower for ext in ['.gov', '.mil']):
            return 0.85
        
        # å•†ä¸šåŸŸå
        if domain_lower.endswith('.com'):
            return 0.5
        
        return 0.4
    
    def _evaluate_timeliness(self, source: DataSource) -> float:
        """è¯„ä¼°æ—¶æ•ˆæ€§"""
        if not source.publish_date:
            return 0.5  # æ— æ—¥æœŸä¿¡æ¯ï¼Œç»™ä¸­ç­‰åˆ†
        
        try:
            # å°è¯•è§£ææ—¥æœŸ
            pub_date = datetime.strptime(source.publish_date, '%Y-%m-%d')
            now = datetime.now()
            
            # è®¡ç®—å¤©æ•°å·®
            days_diff = (now - pub_date).days
            
            # æ ¹æ®å¤©æ•°å·®è¯„åˆ†
            if days_diff <= 30:
                return 1.0      # 30å¤©å†…ï¼šæœ€æ–°
            elif days_diff <= 90:
                return 0.9      # 3ä¸ªæœˆå†…ï¼šå¾ˆæ–°
            elif days_diff <= 180:
                return 0.8      # 6ä¸ªæœˆå†…ï¼šè¾ƒæ–°
            elif days_diff <= 365:
                return 0.6      # 1å¹´å†…ï¼šä¸€èˆ¬
            elif days_diff <= 730:
                return 0.4      # 2å¹´å†…ï¼šè¾ƒæ—§
            else:
                return 0.2      # 2å¹´ä»¥ä¸Šï¼šå¾ˆæ—§
                
        except:
            return 0.5
    
    def _evaluate_completeness(self, source: DataSource) -> float:
        """è¯„ä¼°å®Œæ•´æ€§"""
        content_length = len(source.content)
        
        # æ ¹æ®å†…å®¹é•¿åº¦è¯„åˆ†
        if content_length >= 2000:
            return 1.0      # é•¿æ–‡ç« 
        elif content_length >= 1000:
            return 0.8      # ä¸­ç­‰é•¿åº¦
        elif content_length >= 500:
            return 0.6      # çŸ­æ–‡ç« 
        elif content_length >= 200:
            return 0.4      # å¾ˆçŸ­
        else:
            return 0.2      # æçŸ­
    
    def _evaluate_with_llm(self, source: DataSource, topic: str, section_title: str) -> Dict[str, float]:
        """ä½¿ç”¨LLMè¯„ä¼°ç›¸å…³æ€§ã€å®ç”¨æ€§ã€å‡†ç¡®æ€§"""
        
        evaluation_prompt = f"""
è¯·ä½ ä½œä¸ºä¸€ä¸ªä¸“ä¸šçš„å†…å®¹è´¨é‡è¯„ä¼°ä¸“å®¶ï¼Œå¯¹ä»¥ä¸‹å†…å®¹è¿›è¡Œè¯„åˆ†ã€‚

**è¯„ä¼°ä¸»é¢˜**: {topic}
**ç« èŠ‚æ ‡é¢˜**: {section_title}
**å†…å®¹æ¥æº**: {source.title}
**å†…å®¹æ‘˜è¦**: {source.content[:800]}...

è¯·ä»ä»¥ä¸‹ä¸‰ä¸ªç»´åº¦å¯¹å†…å®¹è¿›è¡Œè¯„åˆ†ï¼ˆ0-1åˆ†ï¼Œä¿ç•™ä¸¤ä½å°æ•°ï¼‰ï¼š

1. **ç›¸å…³æ€§ï¼ˆRelevanceï¼‰**: å†…å®¹ä¸ä¸»é¢˜å’Œç« èŠ‚çš„åŒ¹é…åº¦
   - 1.0: é«˜åº¦ç›¸å…³ï¼Œç›´æ¥å¯¹åº”ä¸»é¢˜
   - 0.8: ç›¸å…³æ€§å¼ºï¼Œå¤§éƒ¨åˆ†å†…å®¹ç¬¦åˆ
   - 0.6: ä¸­ç­‰ç›¸å…³ï¼Œéƒ¨åˆ†å†…å®¹ç¬¦åˆ
   - 0.4: ç›¸å…³æ€§å¼±ï¼Œå°‘éƒ¨åˆ†å†…å®¹ç¬¦åˆ
   - 0.2: åŸºæœ¬ä¸ç›¸å…³

2. **å®ç”¨æ€§ï¼ˆPracticalityï¼‰**: å†…å®¹çš„å®é™…åº”ç”¨ä»·å€¼
   - 1.0: é«˜åº¦å®ç”¨ï¼Œæœ‰å…·ä½“æ–¹æ¡ˆæˆ–æ¡ˆä¾‹
   - 0.8: å®ç”¨æ€§å¼ºï¼Œæœ‰æ“ä½œæŒ‡å¯¼
   - 0.6: ä¸­ç­‰å®ç”¨ï¼Œæœ‰å‚è€ƒä»·å€¼
   - 0.4: å®ç”¨æ€§å¼±ï¼Œä¸»è¦æ˜¯ç†è®º
   - 0.2: åŸºæœ¬æ— å®ç”¨ä»·å€¼

3. **å‡†ç¡®æ€§ï¼ˆAccuracyï¼‰**: å†…å®¹çš„å‡†ç¡®æ€§å’Œå¯ä¿¡åº¦
   - 1.0: é«˜åº¦å‡†ç¡®ï¼Œæœ‰æ•°æ®æ”¯æ’‘
   - 0.8: å‡†ç¡®æ€§å¼ºï¼Œé€»è¾‘æ¸…æ™°
   - 0.6: ä¸­ç­‰å‡†ç¡®ï¼ŒåŸºæœ¬å¯ä¿¡
   - 0.4: å‡†ç¡®æ€§å¾…éªŒè¯
   - 0.2: å­˜åœ¨æ˜æ˜¾é”™è¯¯

è¯·ä»¥JSONæ ¼å¼è¿”å›è¯„åˆ†ç»“æœï¼š
```json
{{
    "relevance": 0.XX,
    "practicality": 0.XX,
    "accuracy": 0.XX,
    "reasoning": "ç®€è¦è¯´æ˜è¯„åˆ†ç†ç”±"
}}
```
"""
        
        try:
            response = self.llm_processor.call_llm_api(evaluation_prompt, 
                                                      "ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„å†…å®¹è´¨é‡è¯„ä¼°ä¸“å®¶ã€‚è¯·ä¸¥æ ¼æŒ‰ç…§è¦æ±‚è¿”å›JSONæ ¼å¼çš„è¯„åˆ†ç»“æœã€‚",
                                                      temperature=0.3)
            
            # å°è¯•è§£æJSONå“åº”
            json_match = re.search(r'```json\s*(\{.*?\})\s*```', response, re.DOTALL)
            if json_match:
                result = json.loads(json_match.group(1))
                return {
                    'relevance': float(result.get('relevance', 0.7)),
                    'practicality': float(result.get('practicality', 0.7)),
                    'accuracy': float(result.get('accuracy', 0.7)),
                    'reasoning': result.get('reasoning', '')
                }
            else:
                # å¦‚æœJSONè§£æå¤±è´¥ï¼Œå°è¯•ä»æ–‡æœ¬ä¸­æå–æ•°å€¼
                relevance = self._extract_score_from_text(response, 'relevance')
                practicality = self._extract_score_from_text(response, 'practicality')
                accuracy = self._extract_score_from_text(response, 'accuracy')
                
                return {
                    'relevance': relevance,
                    'practicality': practicality,
                    'accuracy': accuracy,
                    'reasoning': 'åŸºäºæ–‡æœ¬åˆ†æçš„è¯„åˆ†'
                }
                
        except Exception as e:
            print(f"LLMè¯„ä¼°å‡ºé”™: {str(e)}")
            return {
                'relevance': 0.7,
                'practicality': 0.7,
                'accuracy': 0.7,
                'reasoning': 'è¯„ä¼°è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯ï¼Œä½¿ç”¨é»˜è®¤åˆ†æ•°'
            }
    
    def _extract_score_from_text(self, text: str, score_type: str) -> float:
        """ä»æ–‡æœ¬ä¸­æå–è¯„åˆ†"""
        pattern = rf'{score_type}["\']?\s*:\s*([0-9.]+)'
        match = re.search(pattern, text, re.IGNORECASE)
        if match:
            try:
                return float(match.group(1))
            except:
                pass
        return 0.7
    
    def _evaluate_relevance_fallback(self, source: DataSource, topic: str, section_title: str) -> float:
        """å›é€€çš„ç›¸å…³æ€§è¯„ä¼°"""
        content_lower = source.content.lower()
        title_lower = source.title.lower()
        topic_lower = topic.lower()
        section_lower = section_title.lower()
        
        # å…³é”®è¯åŒ¹é…è¯„åˆ†
        topic_keywords = topic_lower.split()
        section_keywords = section_lower.split()
        
        matches = 0
        total_keywords = len(topic_keywords) + len(section_keywords)
        
        for keyword in topic_keywords + section_keywords:
            if keyword in content_lower or keyword in title_lower:
                matches += 1
        
        return min(matches / total_keywords * 2, 1.0)
    
    def _evaluate_practicality_fallback(self, source: DataSource) -> float:
        """å›é€€çš„å®ç”¨æ€§è¯„ä¼°"""
        practical_indicators = [
            'æ¡ˆä¾‹', 'å®ä¾‹', 'åº”ç”¨', 'æ–¹æ³•', 'æ­¥éª¤', 'å®ç°', 'æ“ä½œ', 'æŒ‡å—',
            'case', 'example', 'application', 'method', 'step', 'implementation'
        ]
        
        content_lower = source.content.lower()
        matches = sum(1 for indicator in practical_indicators if indicator in content_lower)
        
        return min(matches / len(practical_indicators) * 3, 1.0)
    
    def _evaluate_accuracy_fallback(self, source: DataSource) -> float:
        """å›é€€çš„å‡†ç¡®æ€§è¯„ä¼°"""
        # åŸºäºæ¥æºç±»å‹å’Œå†…å®¹ç‰¹å¾
        base_score = 0.7
        
        # å­¦æœ¯æ¥æºåŠ åˆ†
        if source.source_type == 'academic':
            base_score += 0.2
        
        # æœ‰æ•°æ®æ”¯æ’‘åŠ åˆ†
        if any(indicator in source.content.lower() for indicator in ['æ•°æ®', 'ç»Ÿè®¡', 'ç ”ç©¶', 'data', 'study', 'research']):
            base_score += 0.1
        
        return min(base_score, 1.0)
    
    def _extract_key_excerpts(self, source: DataSource, topic: str, section_title: str) -> List[str]:
        """æå–å…³é”®æ‘˜å½•"""
        
        if self.llm_processor:
            return self._extract_excerpts_with_llm(source, topic, section_title)
        else:
            return self._extract_excerpts_fallback(source, topic, section_title)
    
    def _extract_excerpts_with_llm(self, source: DataSource, topic: str, section_title: str) -> List[str]:
        """ä½¿ç”¨LLMæå–å…³é”®æ‘˜å½•"""
        
        excerpt_prompt = f"""
è¯·ä»ä»¥ä¸‹å†…å®¹ä¸­æå–3-5ä¸ªæœ€é‡è¦çš„å…³é”®æ‘˜å½•ï¼Œè¿™äº›æ‘˜å½•åº”è¯¥ï¼š
1. ä¸ä¸»é¢˜ "{topic}" å’Œç« èŠ‚ "{section_title}" é«˜åº¦ç›¸å…³
2. åŒ…å«å…·ä½“çš„æ•°æ®ã€æ¡ˆä¾‹æˆ–è§‚ç‚¹
3. æ¯ä¸ªæ‘˜å½•é•¿åº¦æ§åˆ¶åœ¨50-150å­—ä¹‹é—´
4. ä¿æŒåŸæ–‡çš„å‡†ç¡®æ€§

**å†…å®¹æ¥æº**: {source.title}
**å†…å®¹**: {source.content}

è¯·æŒ‰ä»¥ä¸‹æ ¼å¼è¿”å›æ‘˜å½•ï¼š
```
æ‘˜å½•1: [å…·ä½“å†…å®¹]
æ‘˜å½•2: [å…·ä½“å†…å®¹]
æ‘˜å½•3: [å…·ä½“å†…å®¹]
...
```
"""
        
        try:
            response = self.llm_processor.call_llm_api(excerpt_prompt, 
                                                      "ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„å†…å®¹åˆ†æä¸“å®¶ï¼Œæ“…é•¿æå–å…³é”®ä¿¡æ¯ã€‚è¯·ä¸¥æ ¼æŒ‰ç…§è¦æ±‚çš„æ ¼å¼è¿”å›æ‘˜å½•ã€‚",
                                                      temperature=0.3)
            
            # è§£ææ‘˜å½•
            excerpts = []
            for line in response.split('\n'):
                if line.strip().startswith('æ‘˜å½•'):
                    excerpt = line.split(':', 1)[1].strip()
                    if excerpt and len(excerpt) >= 10:
                        excerpts.append(excerpt)
            
            return excerpts[:5]  # æœ€å¤š5ä¸ªæ‘˜å½•
            
        except Exception as e:
            print(f"LLMæ‘˜å½•æå–å‡ºé”™: {str(e)}")
            return self._extract_excerpts_fallback(source, topic, section_title)
    
    def _extract_excerpts_fallback(self, source: DataSource, topic: str, section_title: str) -> List[str]:
        """å›é€€çš„æ‘˜å½•æå–"""
        sentences = source.content.split('ã€‚')
        topic_lower = topic.lower()
        section_lower = section_title.lower()
        
        scored_sentences = []
        for sentence in sentences:
            if len(sentence.strip()) < 20:
                continue
                
            sentence_lower = sentence.lower()
            score = 0
            
            # å…³é”®è¯åŒ¹é…
            for keyword in topic_lower.split() + section_lower.split():
                if keyword in sentence_lower:
                    score += 1
            
            # æ•°æ®æŒ‡æ ‡
            if any(indicator in sentence_lower for indicator in ['%', 'æ•°æ®', 'ç»Ÿè®¡', 'ç ”ç©¶', 'è°ƒæŸ¥']):
                score += 2
            
            if score > 0:
                scored_sentences.append((sentence.strip(), score))
        
        # æ’åºå¹¶è¿”å›å‰5ä¸ª
        scored_sentences.sort(key=lambda x: x[1], reverse=True)
        return [sentence for sentence, _ in scored_sentences[:5]]
    
    def _generate_reasoning(self, source: DataSource, quality_score: QualityScore) -> str:
        """ç”Ÿæˆè¯„åˆ†ç†ç”±"""
        reasons = []
        
        # æƒå¨æ€§
        if quality_score.authority >= 0.8:
            reasons.append(f"æ¥æºæƒå¨æ€§é«˜ï¼ˆ{quality_score.authority:.2f}ï¼‰")
        elif quality_score.authority <= 0.4:
            reasons.append(f"æ¥æºæƒå¨æ€§è¾ƒä½ï¼ˆ{quality_score.authority:.2f}ï¼‰")
        
        # æ—¶æ•ˆæ€§
        if quality_score.timeliness >= 0.8:
            reasons.append(f"å†…å®¹æ—¶æ•ˆæ€§å¼ºï¼ˆ{quality_score.timeliness:.2f}ï¼‰")
        elif quality_score.timeliness <= 0.4:
            reasons.append(f"å†…å®¹æ—¶æ•ˆæ€§è¾ƒå¼±ï¼ˆ{quality_score.timeliness:.2f}ï¼‰")
        
        # ç›¸å…³æ€§
        if quality_score.relevance >= 0.8:
            reasons.append(f"å†…å®¹é«˜åº¦ç›¸å…³ï¼ˆ{quality_score.relevance:.2f}ï¼‰")
        elif quality_score.relevance <= 0.4:
            reasons.append(f"å†…å®¹ç›¸å…³æ€§è¾ƒä½ï¼ˆ{quality_score.relevance:.2f}ï¼‰")
        
        # å®ç”¨æ€§
        if quality_score.practicality >= 0.8:
            reasons.append(f"å®ç”¨ä»·å€¼é«˜ï¼ˆ{quality_score.practicality:.2f}ï¼‰")
        elif quality_score.practicality <= 0.4:
            reasons.append(f"å®ç”¨ä»·å€¼è¾ƒä½ï¼ˆ{quality_score.practicality:.2f}ï¼‰")
        
        return "ï¼›".join(reasons) if reasons else "ç»¼åˆè¯„ä¼°ç»“æœ" 