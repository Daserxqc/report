import requests
import json
import time
from datetime import datetime, timedelta
from typing import List, Dict, Optional
import config
from collectors.llm_processor import LLMProcessor
from urllib3.util.retry import Retry
from requests.adapters import HTTPAdapter


class BraveSearchCollector:
    """
    Brave Web Search APIæ”¶é›†å™¨
    ä½¿ç”¨Braveç‹¬ç«‹æœç´¢å¼•æ“è·å–ç½‘ç»œæœç´¢ç»“æœ
    æ ¹æ®å®˜æ–¹æ–‡æ¡£ä¼˜åŒ–çš„ç‰ˆæœ¬
    """
    
    def __init__(self, api_key=None):
        """
        åˆå§‹åŒ–Braveæœç´¢æ”¶é›†å™¨
        
        Args:
            api_key (str): Brave Web Search APIå¯†é’¥
        """
        self.api_key = api_key if api_key else getattr(config, 'BRAVE_SEARCH_API_KEY', None)
        self.base_url = "https://api.search.brave.com/res/v1/web/search"
        self.local_search_url = "https://api.search.brave.com/res/v1/local/search"
        self.has_api_key = bool(self.api_key)
        
        # åˆå§‹åŒ–ä¼šè¯ï¼Œé…ç½®é‡è¯•å’Œè¿æ¥æ± 
        self.session = requests.Session()
        
        # é…ç½®é‡è¯•ç­–ç•¥
        retry_strategy = Retry(
            total=3,  # æ€»å…±é‡è¯•3æ¬¡
            status_forcelist=[429, 500, 502, 503, 504],  # å¯¹è¿™äº›çŠ¶æ€ç é‡è¯•
            backoff_factor=1,  # é‡è¯•é—´éš”ï¼š1s, 2s, 4s
            respect_retry_after_header=True
        )
        
        # é…ç½®HTTPé€‚é…å™¨
        adapter = HTTPAdapter(
            max_retries=retry_strategy,
            pool_connections=10,  # è¿æ¥æ± å¤§å°
            pool_maxsize=20,     # è¿æ¥æ± æœ€å¤§è¿æ¥æ•°
            pool_block=False     # éé˜»å¡è¿æ¥æ± 
        )
        
        self.session.mount("https://", adapter)
        self.session.mount("http://", adapter)
        
        # åˆå§‹åŒ–LLMå¤„ç†å™¨ç”¨äºå†…å®¹å¤„ç†
        try:
            self.llm_processor = LLMProcessor()
            self.has_llm = True
        except Exception as e:
            print(f"åˆå§‹åŒ–LLMå¤„ç†å™¨å¤±è´¥: {str(e)}")
            self.has_llm = False
        
        if not self.has_api_key:
            print("Brave Search APIå¯†é’¥æœªé…ç½®ï¼Œå°†æ— æ³•æ‰§è¡Œæœç´¢")
        else:
            pass  # print("âœ… Braveæœç´¢æ”¶é›†å™¨å·²åˆå§‹åŒ–")  # MCPéœ€è¦é™é»˜
    
    def _get_headers(self):
        """
        æ ¹æ®å®˜æ–¹æ–‡æ¡£æ„å»ºè¯·æ±‚å¤´
        
        Returns:
            dict: è¯·æ±‚å¤´å­—å…¸
        """
        return {
            'Accept': 'application/json',
            'Accept-Encoding': 'gzip',
            'X-Subscription-Token': self.api_key,
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        }
    
    def search(self, query: str, count: int = 10, offset: int = 0, 
               freshness: str = None, country: str = None, 
               language: str = None, safesearch: str = 'moderate',
               location_data: dict = None, user_agent: str = None) -> List[Dict]:
        """
        æ‰§è¡ŒBraveç½‘ç»œæœç´¢ - æ ¹æ®å®˜æ–¹æ–‡æ¡£ä¼˜åŒ–
        
        Args:
            query (str): æœç´¢æŸ¥è¯¢
            count (int): è¿”å›ç»“æœæ•°é‡ (1-20)
            offset (int): ç»“æœåç§»é‡
            freshness (str): æ—¶é—´è¿‡æ»¤ ('pd' - è¿‡å»ä¸€å¤©, 'pw' - è¿‡å»ä¸€å‘¨, 'pm' - è¿‡å»ä¸€ä¸ªæœˆ, 'py' - è¿‡å»ä¸€å¹´)
            country (str): å›½å®¶ä»£ç  (å¦‚ 'US', 'CN')
            language (str): è¯­è¨€ä»£ç  (å¦‚ 'en', 'zh')
            safesearch (str): å®‰å…¨æœç´¢çº§åˆ« ('off', 'moderate', 'strict')
            location_data (dict): åœ°ç†ä½ç½®æ•°æ®
            user_agent (str): è‡ªå®šä¹‰ç”¨æˆ·ä»£ç†
            
        Returns:
            List[Dict]: æœç´¢ç»“æœåˆ—è¡¨
        """
        if not self.has_api_key:
            print("âŒ Brave Search APIæœªé…ç½®ï¼Œæ— æ³•æ‰§è¡Œæœç´¢")
            return []
        
        # æ„å»ºè¯·æ±‚å‚æ•° - æŒ‰ç…§å®˜æ–¹æ–‡æ¡£
        params = {
            'q': query,
            'count': min(count, 20),  # APIé™åˆ¶æ¯æ¬¡æœ€å¤š20ä¸ªç»“æœ
            'safesearch': safesearch
        }
        
        # æ·»åŠ å¯é€‰å‚æ•°
        if offset > 0:
            params['offset'] = offset
        if freshness:
            params['freshness'] = freshness
        if country:
            params['country'] = country
        
        try:
            print(f"Braveæœç´¢: {query}")
            
            # ä½¿ç”¨é…ç½®å¥½çš„sessionè¿›è¡Œè¯·æ±‚
            response = self.session.get(
                self.base_url, 
                params=params, 
                headers=self._get_headers(),
                timeout=(10, 30)  # è¿æ¥è¶…æ—¶=10sï¼Œè¯»å–è¶…æ—¶=30s
            )
            
            response.raise_for_status()
            data = response.json()
            
            # å¤„ç†æœç´¢ç»“æœ
            results = []
            web_results = data.get('web', {}).get('results', [])
            
            for item in web_results:
                result = {
                    'title': item.get('title', ''),
                    'content': item.get('description', ''),
                    'url': item.get('url', ''),
                    'source': self._extract_domain(item.get('url', '')),
                    'published_date': item.get('age', ''),
                    'search_engine': 'Brave',
                    'query': query,
                    'language': item.get('language', ''),
                    'family_friendly': item.get('family_friendly', True)
                }
                
                # æ·»åŠ é¢å¤–ä¿¡æ¯
                if 'extra_snippets' in item:
                    result['extra_snippets'] = item['extra_snippets']
                
                results.append(result)
            
            # APIé€Ÿç‡é™åˆ¶
            time.sleep(0.2)  # å¢åŠ é—´éš”é¿å…è¿‡å¿«è¯·æ±‚
            
            print(f"âœ… Braveæœç´¢æˆåŠŸ: {query}, è·å¾—{len(results)}æ¡ç»“æœ")
            return results
            
        except requests.exceptions.ConnectTimeout:
            print(f"âŒ Braveæœç´¢è¿æ¥è¶…æ—¶: {query}")
            return []
        except requests.exceptions.ReadTimeout:
            print(f"âŒ Braveæœç´¢è¯»å–è¶…æ—¶: {query}")
            return []
        except requests.exceptions.ConnectionError as e:
            print(f"âŒ Braveæœç´¢è¿æ¥é”™è¯¯: {query} - {str(e)}")
            return []
        except requests.exceptions.HTTPError as e:
            if e.response.status_code == 422:
                print(f"âš ï¸ Braveæœç´¢å‚æ•°é”™è¯¯ï¼Œå°è¯•ç®€åŒ–æŸ¥è¯¢: {query}")
                return self._fallback_search(query, count)
            elif e.response.status_code == 429:
                print(f"âš ï¸ Braveæœç´¢APIé™æµï¼Œç­‰å¾…åé‡è¯•: {query}")
                time.sleep(2)
                return []
            else:
                print(f"âŒ Braveæœç´¢HTTPé”™è¯¯ {e.response.status_code}: {query}")
                return []
        except Exception as e:
            print(f"âŒ Braveæœç´¢æœªçŸ¥é”™è¯¯: {query} - {str(e)}")
            return []
    
    def _fallback_search(self, query: str, count: int) -> List[Dict]:
        """
        å‚æ•°é”™è¯¯æ—¶çš„é™çº§æœç´¢
        """
        try:
            # ä½¿ç”¨æœ€ç®€å‚æ•°
            simple_params = {
                'q': query,
                'count': min(count, 10)
            }
            
            response = self.session.get(
                self.base_url,
                params=simple_params,
                headers=self._get_headers(),
                timeout=(10, 30)
            )
            
            response.raise_for_status()
            data = response.json()
            
            results = []
            web_results = data.get('web', {}).get('results', [])
            
            for item in web_results:
                result = {
                    'title': item.get('title', ''),
                    'content': item.get('description', ''),
                    'url': item.get('url', ''),
                    'source': self._extract_domain(item.get('url', '')),
                    'published_date': item.get('age', ''),
                    'search_engine': 'Brave',
                    'query': query
                }
                results.append(result)
            
            print(f"âœ… Braveé™çº§æœç´¢æˆåŠŸ: {query}, è·å¾—{len(results)}æ¡ç»“æœ")
            return results
            
        except Exception as e:
            print(f"âŒ Braveé™çº§æœç´¢ä¹Ÿå¤±è´¥: {query} - {str(e)}")
            return []
    
    def search_news(self, topic: str, days_back: int = 7, max_results: int = 10) -> List[Dict]:
        """
        æœç´¢æ–°é—»å†…å®¹ - ä¼˜åŒ–ç‰ˆ
        """
        if not self.has_api_key:
            return []
        
        news_queries = [
            f"{topic} news latest {days_back} days",
            f"{topic} æœ€æ–°æ–°é—» {days_back}å¤©",
            f"{topic} breaking news recent",
            f"{topic} industry news 2025"
        ]
        
        all_results = []
        seen_urls = set()
        
        for query in news_queries:
            try:
                results = self.search(
                    query, 
                    count=max_results//len(news_queries) + 2,
                    freshness='pw' if days_back <= 7 else 'pm'
                )
                
                for result in results:
                    if result['url'] not in seen_urls:
                        seen_urls.add(result['url'])
                        all_results.append(result)
                
                if len(all_results) >= max_results:
                    break
                    
                time.sleep(0.3)  # å¢åŠ é—´éš”
                
            except Exception as e:
                print(f"æœç´¢æ–°é—»æ—¶å‡ºé”™: {query} - {str(e)}")
                continue
        
        return all_results[:max_results]
    
    def search_research_content(self, topic: str, days_back: int = 30, max_results: int = 10) -> List[Dict]:
        """
        æœç´¢ç ”ç©¶å†…å®¹ - ä¼˜åŒ–ç‰ˆ
        """
        if not self.has_api_key:
            return []
        
        research_queries = [
            f"{topic} research 2025 latest study",
            f"{topic} ç ”ç©¶æŠ¥å‘Š 2025å¹´",
            f"{topic} analysis report {days_back} days",
            f"{topic} white paper recent"
        ]
        
        all_results = []
        seen_urls = set()
        
        for query in research_queries:
            try:
                results = self.search(
                    query, 
                    count=max_results//len(research_queries) + 2,
                    freshness='pm' if days_back <= 30 else None
                )
                
                for result in results:
                    if result['url'] not in seen_urls:
                        seen_urls.add(result['url'])
                        all_results.append(result)
                
                if len(all_results) >= max_results:
                    break
                    
                time.sleep(0.3)
                
            except Exception as e:
                print(f"æœç´¢ç ”ç©¶å†…å®¹æ—¶å‡ºé”™: {query} - {str(e)}")
                continue
        
        return all_results[:max_results]
    
    def search_industry_insights(self, topic: str, days_back: int = 90, max_results: int = 10) -> List[Dict]:
        """
        æœç´¢è¡Œä¸šæ´å¯Ÿ - ä¼˜åŒ–ç‰ˆ
        """
        if not self.has_api_key:
            return []
        
        insight_queries = [
            f"{topic} industry trends 2025",
            f"{topic} market analysis recent",
            f"{topic} è¡Œä¸šè¶‹åŠ¿ 2025",
            f"{topic} business intelligence report"
        ]
        
        all_results = []
        seen_urls = set()
        
        for query in insight_queries:
            try:
                results = self.search(
                    query, 
                    count=max_results//len(insight_queries) + 2
                )
                
                for result in results:
                    if result['url'] not in seen_urls:
                        seen_urls.add(result['url'])
                        all_results.append(result)
                
                if len(all_results) >= max_results:
                    break
                    
                time.sleep(0.3)
                
            except Exception as e:
                print(f"æœç´¢è¡Œä¸šæ´å¯Ÿæ—¶å‡ºé”™: {query} - {str(e)}")
                continue
        
        return all_results[:max_results]
    
    def get_comprehensive_research(self, topic: str, days_back: int = 7) -> Dict:
        """
        ç»¼åˆç ”ç©¶æ–¹æ³• - ä¼˜åŒ–ç‰ˆ
        """
        print(f"å¼€å§‹Braveæœç´¢ç»¼åˆç ”ç©¶: {topic}")
        
        research_data = {
            "breaking_news": [],
            "innovation_news": [],
            "investment_news": [],
            "policy_news": [],
            "trend_news": [],
            "company_news": [],
            "total_count": 0
        }
        
        # å®šä¹‰ä¸åŒç±»å‹çš„æœç´¢æŸ¥è¯¢
        search_categories = {
            "breaking_news": [
                f"{topic} æ–°é—» æœ€æ–°",
                f"{topic} news latest",
                f"{topic} breaking news",
                f"{topic} æœ€æ–°åŠ¨æ€",
                f"{topic} research study"
            ],
            "innovation_news": [
                f"{topic} æŠ€æœ¯åˆ›æ–° 2025",
                f"{topic} innovation 2025",
                f"{topic} new technology",
                f"{topic} breakthrough research"
            ],
            "investment_news": [
                f"{topic} æŠ•èµ„ èèµ„ 2025",
                f"{topic} investment funding 2025",
                f"{topic} venture capital",
                f"{topic} IPO acquisition"
            ],
            "policy_news": [
                f"{topic} æ”¿ç­– æ³•è§„ 2025",
                f"{topic} policy regulation 2025",
                f"{topic} government policy",
                f"{topic} compliance standards"
            ],
            "trend_news": [
                f"{topic} è¶‹åŠ¿ å‘å±• 2025",
                f"{topic} trends 2025",
                f"{topic} market analysis",
                f"{topic} future outlook"
            ]
        }
        
        # ä¸ºæ¯ä¸ªç±»åˆ«æœç´¢
        for category, queries in search_categories.items():
            category_results = []
            seen_urls = set()
            
            for query in queries:
                try:
                    results = self.search(
                        query, 
                        count=3,  # æ¯ä¸ªæŸ¥è¯¢è·å–å°‘é‡ç»“æœé¿å…é‡å¤
                        freshness='pw' if days_back <= 7 else 'pm'
                    )
                    
                    for result in results:
                        if result['url'] not in seen_urls:
                            seen_urls.add(result['url'])
                            category_results.append(result)
                    
                    time.sleep(0.4)  # é€‚å½“å»¶è¿Ÿé¿å…é™æµ
                    
                except Exception as e:
                    print(f"{category}æœç´¢å‡ºé”™: {query} - {str(e)}")
                    continue
            
            research_data[category] = category_results[:10]  # æ¯ç±»æœ€å¤š10æ¡
            print(f"{category}: è·å¾—{len(research_data[category])}æ¡ç»“æœ")
        
        # è®¡ç®—æ€»æ•°
        research_data["total_count"] = sum(
            len(research_data[key]) for key in research_data.keys() 
            if key != "total_count"
        )
        
        print(f"Braveç»¼åˆæœç´¢å®Œæˆï¼Œæ€»è®¡{research_data['total_count']}æ¡ç»“æœ")
        return research_data
    
    def local_search(self, query: str, location: str = None, count: int = 10, 
                    offset: int = 0, country: str = None) -> List[Dict]:
        """
        æœ¬åœ°æœç´¢ - æš‚æ—¶ç¦ç”¨ï¼Œå› ä¸ºéœ€è¦ç‰¹æ®Šæƒé™
        """
        print("æœ¬åœ°æœç´¢éœ€è¦Proè®¡åˆ’ï¼Œæš‚æ—¶ç¦ç”¨")
        return []
    
    def _extract_domain(self, url: str) -> str:
        """
        ä»URLä¸­æå–åŸŸå
        
        Args:
            url (str): å®Œæ•´URL
            
        Returns:
            str: åŸŸå
        """
        try:
            from urllib.parse import urlparse
            parsed = urlparse(url)
            return parsed.netloc or url
        except:
            return url
    
    def _filter_by_date(self, items, days_limit):
        """
        æ ¹æ®æ—¶é—´é™åˆ¶è¿‡æ»¤å†…å®¹é¡¹ - åªä¾èµ–å¯é çš„æ—¶é—´ä¿¡æ¯
        
        Args:
            items (list): å†…å®¹é¡¹åˆ—è¡¨
            days_limit (int): å¤©æ•°é™åˆ¶
            
        Returns:
            list: è¿‡æ»¤åçš„å†…å®¹é¡¹åˆ—è¡¨
        """
        if not items or days_limit <= 0:
            return items
            
        filtered_items = []
        cutoff_date = datetime.now() - timedelta(days=days_limit)
        current_year = datetime.now().year
        
        print(f"  ğŸ” [Brave] å¼€å§‹ä¸¥æ ¼æ—¶é—´è¿‡æ»¤ï¼Œè¦æ±‚æœ€è¿‘{days_limit}å¤©å†…çš„å†…å®¹ï¼ˆæˆªæ­¢æ—¥æœŸï¼š{cutoff_date.strftime('%Y-%m-%d')}ï¼‰")
        print(f"  âš ï¸ [Brave] æ³¨æ„ï¼šåªæ¥å—æœ‰æ˜ç¡®å‘å¸ƒæ—¥æœŸçš„å†…å®¹ï¼Œå¿½ç•¥'æœ€æ–°'ã€'ä»Šæ—¥'ç­‰å…³é”®è¯")
        
        for item in items:
            should_include = False
            filter_reason = ""
            
            title = item.get('title', '')
            content = item.get('content', '')
            combined_text = f"{title} {content}".lower()
            
            # 1. é¦–å…ˆæ£€æŸ¥æ˜¯å¦åŒ…å«æ˜æ˜¾çš„æ—§å¹´ä»½æ ‡è¯†
            old_year_patterns = ['2024å¹´', '2023å¹´', '2022å¹´', '2021å¹´', '2020å¹´']
            has_old_year = any(pattern in combined_text for pattern in old_year_patterns)
            
            if has_old_year:
                filter_reason = "åŒ…å«æ—§å¹´ä»½æ ‡è¯†"
                should_include = False
            else:
                # 2. æ£€æŸ¥æ˜¯å¦æœ‰å¯é çš„å‘å¸ƒæ—¥æœŸä¿¡æ¯
                published_date = item.get("published_date")
                
                if published_date and published_date != "æœªçŸ¥æ—¥æœŸ":
                    try:
                        # å°è¯•è§£æå‘å¸ƒæ—¥æœŸ
                        pub_date = None
                        
                        if isinstance(published_date, str):
                            # å°è¯•å¤šç§æ—¥æœŸæ ¼å¼
                            date_formats = [
                                "%Y-%m-%d",
                                "%Y-%m-%d %H:%M:%S",
                                "%Y-%m-%dT%H:%M:%S",
                                "%Y-%m-%dT%H:%M:%S.%fZ",
                                "%Y-%m-%dT%H:%M:%SZ",
                                "%Y/%m/%d",
                                "%d/%m/%Y",
                                "%m/%d/%Y"
                            ]
                            
                            for fmt in date_formats:
                                try:
                                    pub_date = datetime.strptime(published_date, fmt)
                                    break
                                except ValueError:
                                    continue
                        
                        if pub_date:
                            # æœ‰æ˜ç¡®çš„å‘å¸ƒæ—¥æœŸï¼Œæ£€æŸ¥æ˜¯å¦åœ¨æ—¶é—´èŒƒå›´å†…
                            if pub_date >= cutoff_date:
                                should_include = True
                                filter_reason = f"å‘å¸ƒæ—¥æœŸç¬¦åˆè¦æ±‚ï¼ˆ{pub_date.strftime('%Y-%m-%d')}ï¼‰"
                            else:
                                should_include = False
                                filter_reason = f"å‘å¸ƒæ—¥æœŸè¿‡æ—©ï¼ˆ{pub_date.strftime('%Y-%m-%d')}ï¼‰"
                        else:
                            # æ— æ³•è§£æå‘å¸ƒæ—¥æœŸ
                            should_include = False
                            filter_reason = "æ— æ³•è§£æå‘å¸ƒæ—¥æœŸæ ¼å¼"
                            
                    except Exception as e:
                        should_include = False
                        filter_reason = f"å‘å¸ƒæ—¥æœŸè§£æå¤±è´¥: {str(e)}"
                        
                else:
                    # 3. æ²¡æœ‰å‘å¸ƒæ—¥æœŸçš„æƒ…å†µ - æ™ºèƒ½å¤„ç†æ¨¡ç³Šæ—¥æœŸ
                    
                    # æ£€æŸ¥æ˜¯å¦åŒ…å«æ¨¡ç³Šçš„æ—¥æœŸæ ¼å¼ï¼ˆå¦‚"3æœˆ27æ—¥"ã€"12æœˆ15æ—¥"ç­‰ï¼‰
                    import re
                    ambiguous_date_patterns = [
                        r'(\d{1,2})æœˆ(\d{1,2})æ—¥',  # 3æœˆ27æ—¥
                        r'(\d{1,2})/(\d{1,2})(?!\d)',  # 3/27 (ä½†ä¸æ˜¯ 3/27/2025)
                        r'(\d{1,2})-(\d{1,2})(?!\d)',  # 3-27 (ä½†ä¸æ˜¯ 3-27-2025)
                    ]
                    
                    ambiguous_date_match = None
                    for pattern in ambiguous_date_patterns:
                        match = re.search(pattern, combined_text)
                        if match:
                            ambiguous_date_match = match
                            break
                    
                    if ambiguous_date_match:
                        # æ‰¾åˆ°æ¨¡ç³Šæ—¥æœŸï¼Œå°è¯•æ™ºèƒ½åˆ¤æ–­
                        try:
                            month = int(ambiguous_date_match.group(1))
                            day = int(ambiguous_date_match.group(2))
                            
                            # å‡è®¾æ˜¯å½“å‰å¹´ä»½ï¼Œæ„é€ æ—¥æœŸ
                            current_date = datetime.now()
                            try:
                                assumed_date = datetime(current_date.year, month, day)
                                
                                # æ£€æŸ¥è¿™ä¸ªæ—¥æœŸæ˜¯å¦åœ¨åˆç†èŒƒå›´å†…
                                days_diff = (current_date - assumed_date).days
                                
                                if days_diff < 0:
                                    # æ—¥æœŸåœ¨æœªæ¥ï¼Œå¯èƒ½æ˜¯å»å¹´çš„æ—¥æœŸ
                                    assumed_date = datetime(current_date.year - 1, month, day)
                                    days_diff = (current_date - assumed_date).days
                                
                                if days_diff <= days_limit:
                                    # åœ¨æ—¶é—´èŒƒå›´å†…
                                    should_include = True
                                    filter_reason = f"æ¨¡ç³Šæ—¥æœŸæ¨æµ‹ä¸º{assumed_date.strftime('%Y-%m-%d')}ï¼Œåœ¨æ—¶é—´èŒƒå›´å†…"
                                else:
                                    # è¶…å‡ºæ—¶é—´èŒƒå›´
                                    should_include = False
                                    filter_reason = f"æ¨¡ç³Šæ—¥æœŸæ¨æµ‹ä¸º{assumed_date.strftime('%Y-%m-%d')}ï¼Œè¶…å‡º{days_limit}å¤©èŒƒå›´"
                                    
                            except ValueError:
                                # æ— æ•ˆæ—¥æœŸï¼ˆå¦‚2æœˆ30æ—¥ï¼‰
                                should_include = False
                                filter_reason = f"æ¨¡ç³Šæ—¥æœŸ{month}æœˆ{day}æ—¥æ— æ•ˆ"
                                
                        except (ValueError, IndexError):
                            # è§£æå¤±è´¥
                            should_include = False
                            filter_reason = "æ¨¡ç³Šæ—¥æœŸè§£æå¤±è´¥"
                    else:
                        # æ²¡æœ‰æ¨¡ç³Šæ—¥æœŸï¼Œæ£€æŸ¥æ˜¯å¦åŒ…å«å½“å‰å¹´ä»½çš„æ˜ç¡®æ ‡è¯†
                        current_year_patterns = [
                            f'{current_year}å¹´',
                            f'å¹´{current_year}',
                            f'{current_year}-',
                            f'{current_year}/'
                        ]
                        
                        has_current_year = any(pattern in combined_text for pattern in current_year_patterns)
                        
                        if has_current_year:
                            # åŒ…å«å½“å‰å¹´ä»½ï¼Œä½†ä»ç„¶è¦æ±‚æ˜¯çŸ­æœŸå†…çš„å†…å®¹
                            if days_limit <= 30:
                                # å¯¹äº30å¤©ä»¥å†…çš„è¦æ±‚ï¼Œå³ä½¿æœ‰å½“å‰å¹´ä»½ä¹Ÿä¸å¤Ÿ
                                should_include = False
                                filter_reason = f"åŒ…å«{current_year}å¹´ä½†æ— å…·ä½“æ—¥æœŸï¼ˆä¸¥æ ¼æ¨¡å¼ï¼‰"
                            else:
                                # å¯¹äºè¾ƒé•¿æœŸçš„è¦æ±‚ï¼Œå¯ä»¥æ¥å—
                                should_include = True
                                filter_reason = f"åŒ…å«{current_year}å¹´æ ‡è¯†"
                        else:
                            # æ—¢æ²¡æœ‰å‘å¸ƒæ—¥æœŸï¼Œä¹Ÿæ²¡æœ‰å¹´ä»½ä¿¡æ¯
                            should_include = False
                            filter_reason = "æ— å‘å¸ƒæ—¥æœŸä¸”æ— å¹´ä»½ä¿¡æ¯"
            
            if should_include:
                filtered_items.append(item)
                if len(title) > 30:
                    print(f"    âœ… [Brave] ä¿ç•™: {title[:30]}... ({filter_reason})")
                else:
                    print(f"    âœ… [Brave] ä¿ç•™: {title} ({filter_reason})")
            else:
                if len(title) > 30:
                    print(f"    âŒ [Brave] è¿‡æ»¤: {title[:30]}... ({filter_reason})")
                else:
                    print(f"    âŒ [Brave] è¿‡æ»¤: {title} ({filter_reason})")
        
        original_count = len(items)
        filtered_count = len(filtered_items)
        
        if filtered_count < original_count:
            print(f"  â° [Brave] ä¸¥æ ¼æ—¶é—´è¿‡æ»¤ç»“æœ: {original_count} â†’ {filtered_count} æ¡ï¼ˆæ’é™¤äº†{original_count - filtered_count}æ¡æ— å¯é æ—¶é—´ä¿¡æ¯çš„å†…å®¹ï¼‰")
        else:
            print(f"  â° [Brave] ä¸¥æ ¼æ—¶é—´è¿‡æ»¤ç»“æœ: ä¿ç•™å…¨éƒ¨{filtered_count}æ¡å†…å®¹")
            
        return filtered_items
    
    def get_site_specific_search(self, topic: str, sites: List[str], days_back: int = 7) -> List[Dict]:
        """
        ç«™ç‚¹ç‰¹å®šæœç´¢ - ä¼˜åŒ–ç‰ˆ
        """
        all_results = []
        
        for site in sites:
            try:
                site_query = f"site:{site} {topic} recent {days_back} days"
                results = self.search(
                    site_query, 
                    count=5,
                    freshness='pw' if days_back <= 7 else 'pm'
                )
                all_results.extend(results)
                time.sleep(0.5)  # å¢åŠ é—´éš”
            except Exception as e:
                print(f"ç«™ç‚¹æœç´¢å‡ºé”™ {site}: {str(e)}")
                continue
        
        return all_results
    
    def __del__(self):
        """
        æ¸…ç†èµ„æº
        """
        if hasattr(self, 'session'):
            self.session.close() 